---
layout: post
title: "Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs"
date: 2016-03-30 19:29:44
author: "Institute of Applied Physics of the Russian Academy of Sciences"
categories: ["Paper Reviews", "Vector-Search"]
tags: ["Hierarchical-Navigable-Small-World-Graphs", "Approximate-Nearest-Neighbor-Search", "Multi-Layer-Graph-Structure", "Navigable-Small-World-Networks", "Heuristic-Neighbor-Selection", "Logarithmic-Search-Complexity", "Proximity-Graph-Construction", "Scale-Separated-Link-Organization", "Incremental-Index-Building", "Relative-Neighborhood-Graph-Approximation"]
cover: /assets/images/vector-search.jpg
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

근사 최근접 이웃(Approximate Nearest Neighbor, ANN) 탐색은 정보 검색, 추천 시스템, 컴퓨터 비전 등 다양한 분야에서 핵심적인 역할을 수행하는 기본 문제입니다. 기존의 트리 기반 알고리즘(kd-트리, R-트리)은 저차원 데이터에서는 효과적이지만, 차원의 저주로 인해 고차원 데이터에서는 성능이 급격히 저하됩니다. 지역 민감 해싱(LSH)과 제품 양자화(PQ) 같은 방법들도 각각의 한계를 가지고 있으며, 특히 일반적인 메트릭 공간에서 적용하기 어렵다는 문제가 있습니다. 이전에 제안된 탐색 가능한 작은 세계(Navigable Small World, NSW) 알고리즘은 그래프 기반 접근법으로 일반 메트릭 공간에서 작동하는 장점이 있었지만, 탐색 복잡도가 멱법칙(power-law) 스케일링을 따르기 때문에 저차원 데이터에서 심각한 성능 저하를 겪었습니다. 이러한 배경에서 본 연구는 NSW의 멱법칙 복잡도 문제를 해결하면서도 다양한 데이터 특성에서 강건한 성능을 제공하는 새로운 알고리즘을 개발하고자 하는 동기에서 출발했습니다.

NSW의 성능 저하 원인을 분석하면, 탐욕적 라우팅 과정이 "줌아웃(zoom-out)"과 "줌인(zoom-in)"이라는 두 단계로 나뉘는데 두 단계 모두에서 평가해야 하는 노드의 수가 로그적으로 증가한다는 점에 있습니다. 이는 전체 복잡도가 $O(\log^2 N)$ 이상이 되도록 만들며, 특히 저차원 데이터에서 이 문제가 더욱 심각합니다. 연구자들은 이 문제를 해결하기 위해 확률적 스킵 리스트의 개념을 근접 그래프 영역으로 확장하는 아이디어에 주목했습니다. 스킵 리스트는 1차원 정렬된 데이터에서 확률적 계층화를 통해 $O(\log n)$의 탐색 복잡도를 달성하는 데이터 구조인데, 이 원리를 다차원 메트릭 공간으로 확장할 수 있다면 NSW의 문제를 근본적으로 해결할 수 있을 것으로 예상되었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

본 연구에서 제안하는 **계층적 탐색 가능한 작은 세계 그래프(Hierarchical Navigable Small World graphs, HNSW)**는 저장된 요소들의 중첩된 부분집합에 대한 근접 그래프들의 계층적 집합으로 구성된 다층 구조를 점진적으로 구축합니다. 핵심 혁신은 각 요소가 존재하는 최대 층을 지수적으로 감소하는 확률 분포 $P(l) \propto e^{-\lambda l}$를 따라 무작위로 선택한다는 점입니다. 이러한 확률적 층 배정 방식은 스킵 리스트의 개념을 메트릭 공간으로 확장한 것으로, 상위 층에는 가장 긴 링크를 가진 소수의 요소만 존재하여 데이터 공간을 빠르게 가로질러 탐색할 수 있게 합니다. 탐색은 최상위 층에서 시작하여 점차 하위 층으로 내려가면서 정밀도를 높여가는 방식으로 진행되며, 이는 지도를 볼 때 먼저 대륙 수준에서 시작하여 국가, 도시, 거리 순으로 확대해 나가는 것과 유사합니다.

HNSW의 로그 복잡도 달성 메커니즘은 층별 탐색에서 고정된 수의 연결만 평가한다는 점에 있습니다. 층 $l$에 존재하는 요소의 기대 개수는 $N \cdot e^{-\lambda l}$로 표현되며, 최상위 층에는 매우 적은 수의 요소만 존재하므로 진입점에서 시작하여 쿼리와 가까운 영역으로 빠르게 이동할 수 있습니다. 각 층에서의 탐색은 고정된 수의 연결을 평가하므로, 총 탐색 복잡도는 층의 개수에 비례하며, 층의 개수가 $O(\log N)$이므로 전체 탐색 복잡도도 $O(\log N)$이 됩니다. 이는 NSW의 멱법칙 복잡도와 대조적이며, 특히 저차원 데이터에서 획기적인 성능 개선을 가능하게 합니다. 또한 본 연구에서는 근접 그래프 이웃을 선택하는 고급 휴리스틱을 제시합니다. 단순히 거리가 가까운 이웃만 선택하는 것이 아니라, 상대 이웃 그래프(Relative Neighborhood Graph, RNG)의 개념을 활용하여 그래프의 연결성과 탐색 효율성을 고려한 이웃 선택이 이루어집니다. 이는 특히 고도로 클러스터링된 데이터에서 클러스터 간 연결을 유지하여 전역 연결성을 보장하고, 높은 재현율이 요구되는 경우 성능을 크게 향상시킵니다.

#### 제안된 방법은 어떻게 구현되었습니까?

HNSW의 구현은 C++로 작성되었으며 Non Metric Space Library(nmslib) 위에서 이루어졌습니다. 네트워크 구축 알고리즘은 저장된 요소들을 그래프 구조에 연속적으로 삽입하는 방식으로 동작하며, 각 요소가 삽입될 때마다 해당 요소가 속할 최대 층 레벨 $l$이 지수적으로 감소하는 확률 분포를 따라 무작위로 선택됩니다. 삽입 프로세스는 두 단계로 구성되는데, 첫 번째 단계에서는 최상위 층에서 시작하여 그래프를 탐욕적으로 순회하면서 삽입될 요소에 가장 가까운 이웃을 찾고, 두 번째 단계에서는 발견된 최근접 이웃들을 진입점으로 사용하여 다음 하위 층에서 탐색을 계속합니다. 층별 탐색 알고리즘은 특정 층 $l_c$에서 쿼리 요소에 가장 가까운 $ef$개의 이웃을 찾는 핵심 절차로, 동적 리스트를 유지하면서 진행됩니다. 탐색은 진입점들로 초기화된 후보 집합에서 쿼리에 가장 가까운 미평가 요소의 이웃들을 평가하고, 리스트의 모든 요소의 이웃이 평가될 때까지 계속됩니다.

이웃 선택 휴리스틱은 후보 요소들 간의 거리를 고려하여 다양한 연결을 생성합니다. 구체적으로, 후보들을 삽입될 요소에 대해 가장 가까운 것부터 시작하여 검사하고, 후보가 이미 연결된 후보들과 비교하여 기준 요소에 더 가까운 경우에만 연결을 생성합니다. 이는 수식으로 $e \in R \iff \forall r \in R, \; d(e, q) &lt; d(e, r)$로 표현되며, 후보의 수가 충분히 많을 때 정확한 상대 이웃 그래프를 부분그래프로 얻을 수 있습니다. 구축 파라미터의 최적 설정은 실험을 통해 검증되었습니다. $m_L = 1/\ln(M)$이 최적 성능을 제공하며, $M_{\max0} = 2 \cdot M$이 서로 다른 재현율에서 최적에 가까운 성능을 제공합니다. 메모리 소비는 요소당 평균 $(M_{\max0} + m_L \cdot M_{\max}) \times \text{bytes\_per\_link}$로 계산되며, 일반적인 최적에 가까운 $M$ 값(6에서 48 사이)에서 객체당 약 60-450바이트의 메모리를 사용합니다. 구축 복잡도는 $O(N \cdot \log(N))$으로 스케일링되어 대규모 데이터셋에 대해서도 효율적인 인덱스 구축이 가능합니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?

성능 평가 결과는 HNSW가 다양한 데이터셋과 차원에서 기존 알고리즘들을 크게 능가함을 보여줍니다. 기본 NSW와의 비교에서 HNSW는 저차원 데이터($d=4$)에서 거리 계산 횟수를 크게 감소시켰으며, 특히 높은 재현율에서 그 차이가 두드러집니다. 유클리드 공간에서의 비교에서 HNSW는 SIFT(1M, $d=128$), GloVe(1.2M, $d=100$), CoPhIR(2M, $d=272$) 같은 고차원 실제 데이터셋에서 FLANN, Annoy, VP-tree, FALCONN 등의 최신 알고리즘들을 수 배에서 수십 배 빠르게 능가했습니다. 일반 메트릭 공간에서의 테스트에서는 가장 인상적인 개선이 저차원 데이터인 Wiki-8 데이터셋에서 나타났으며, 거의 3차수에 달하는 성능 향상을 보였습니다. 200M SIFT 데이터셋에서 제품 양자화 기반 알고리즘(Faiss)과의 비교에서 HNSW는 훨씬 더 많은 RAM(64GB 대 23.5-30GB)을 필요로 함에도 불구하고, 훨씬 높은 정확도와 막대한 탐색 속도 우위를 달성했습니다. 고품질 인덱스의 경우 구축 시간이 5.6시간으로 Faiss의 11-12시간보다 빠르며, 표준 품질의 경우 42분으로 매우 빠른 구축이 가능합니다.

이 연구의 이론적 의의는 계층적 분해를 통해 로그 복잡도를 달성하면서도 다양한 데이터 분포와 차원에서 강건한 성능을 제공하는 방법을 제시했다는 점에 있습니다. HNSW는 스킵 리스트의 원리를 메트릭 공간으로 성공적으로 확장하여, 1차원 정렬된 데이터에서만 가능했던 로그 복잡도를 고차원 메트릭 공간에서도 달성할 수 있음을 보여주었습니다. 실용적 의의는 알고리즘의 강건성에 있습니다. HNSW는 일반화된 메트릭 공간에서 적용 가능하며, 테스트된 모든 데이터셋에서 최고의 성능을 발휘하여 특정 문제에 대한 최적 알고리즘을 복잡하게 선택할 필요를 없앱니다. 이는 벡터 데이터베이스, 시맨틱 검색, 추천 시스템 등 다양한 실제 응용에서 HNSW가 널리 채택된 이유입니다. 향후 개선 방향으로는 자동 파라미터 선택 메커니즘, 동적 업데이트 지원(요소 삭제 포함), 그리고 스킵 리스트 기반 분산 구현이 제시되었으며, 이들은 모두 알고리즘의 실용성을 더욱 높일 수 있는 중요한 개선 사항들입니다.
- - -
이 논문은 계층적 탐색 가능한 작은 세계 그래프(Hierarchical Navigable Small World graphs, HNSW)를 활용한 효율적이고 강건한 근사 최근접 이웃 탐색 방법을 제안합니다. 제안된 접근법은 제어 가능한 계층 구조를 가진 탐색 가능한 작은 세계 그래프를 기반으로 하며, 대부분의 근접 그래프 기법에서 일반적으로 사용되는 추가적인 탐색 구조 없이 완전히 그래프 기반으로 동작한다는 특징을 가지고 있습니다.

## 핵심 기술적 혁신

HNSW는 저장된 요소들의 중첩된 부분집합에 대한 근접 그래프들의 계층적 집합으로 구성된 다층 구조를 점진적으로 구축합니다. 각 요소가 존재하는 최대 층은 지수적으로 감소하는 확률 분포를 따라 무작위로 선택되는데, 이는 수학적으로 다음과 같이 표현됩니다. 요소가 층 $l$에 배치될 확률은 $P(l) \propto e^{-\lambda l}$의 형태를 따르며, 여기서 $\lambda$는 감쇠 상수입니다. 이러한 확률적 층 배정 방식은 [확률적 스킵 리스트](https://dl.acm.org/doi/10.1145/78973.78977)의 개념을 근접 그래프 영역으로 확장한 것으로 볼 수 있습니다.

스킵 리스트는 정렬된 데이터에 대한 효율적인 탐색을 위해 확률적 계층화를 사용하는 데이터 구조입니다. 각 요소가 상위 레벨에 나타날 확률이 지수적으로 감소하도록 설계되어, 평균적으로 $O(\log n)$의 탐색 복잡도를 달성합니다. HNSW는 이 원리를 1차원 정렬된 데이터가 아닌 다차원 메트릭 공간으로 확장하여, 연결 리스트 대신 근접 그래프를 사용합니다. 이를 통해 복잡한 균형 조정 알고리즘 없이도 효율적인 탐색이 가능하며, 분산 구현이 용이하다는 장점을 얻게 됩니다.

이러한 계층 구조는 이전에 연구된 [탐색 가능한 작은 세계(NSW) 구조](https://www.sciencedirect.com/science/article/pii/S0306437913001300)와 유사한 그래프를 생성하면서도, 링크들이 특성 거리 척도에 따라 분리되는 추가적인 특성을 가집니다. NSW는 작은 세계 네트워크의 특성을 활용하여 근사 최근접 이웃 탐색을 수행하는 기초적인 그래프 기반 방법입니다. 작은 세계 네트워크는 대부분의 노드가 이웃이 아니지만, 소수의 홉을 통해 대부분의 노드에 도달할 수 있는 특성을 가지며, 이는 평균 경로 길이가 $\log(N)$에 비례하여 증가한다는 것을 의미합니다.

그러나 NSW는 저차원 데이터나 클러스터링된 데이터에서 멱법칙(power-law) 스케일링으로 인한 성능 저하를 겪습니다. 이는 탐색 과정에서 평가해야 하는 노드의 수가 데이터셋 크기에 대해 차선형(sublinear)이지만 로그보다 빠르게 증가하기 때문입니다. HNSW는 이러한 문제를 계층적 구조를 통해 해결합니다.

## 로그 복잡도 달성 메커니즘

HNSW의 핵심 아이디어는 상위 층부터 탐색을 시작하고 척도 분리를 활용하는 것입니다. 상위 층에는 가장 긴 링크를 가진 소수의 요소만 존재하며, 이를 통해 데이터 공간을 빠르게 가로지를 수 있습니다. 탐색은 상위 층에서 시작하여 점차 하위 층으로 내려가면서 정밀도를 높여갑니다. 이는 마치 지도를 볼 때 먼저 대륙 수준에서 시작하여 국가, 도시, 거리 순으로 확대해 나가는 것과 유사합니다.

수학적으로, 층 $l$에 존재하는 요소의 기대 개수는 $N \cdot e^{-\lambda l}$로 표현되며, 여기서 $N$은 전체 데이터셋 크기입니다. 최상위 층에는 매우 적은 수의 요소만 존재하므로, 진입점에서 시작하여 쿼리와 가까운 영역으로 빠르게 이동할 수 있습니다. 각 층에서의 탐색은 고정된 수의 연결을 평가하므로, 총 탐색 복잡도는 층의 개수에 비례합니다. 층의 개수는 $O(\log N)$이므로, 전체 탐색 복잡도도 $O(\log N)$이 됩니다.

이는 NSW의 멱법칙 복잡도와 대조적입니다. NSW에서는 탐색 과정이 "줌아웃(zoom-out)"과 "줌인(zoom-in)" 두 단계로 나뉘는데, 줌아웃 단계에서 쿼리 주변의 넓은 영역을 탐색하고, 줌인 단계에서 최근접 이웃으로 수렴합니다. 문제는 두 단계 모두에서 평가해야 하는 노드의 수가 로그적으로 증가하여, 전체 복잡도가 $O(\log^2 N)$ 또는 그 이상이 된다는 것입니다. HNSW는 계층 구조를 통해 줌아웃 단계를 상위 층에서 효율적으로 처리하고, 각 층에서 고정된 작업량만 수행함으로써 이 문제를 해결합니다.

## 이웃 선택 휴리스틱의 중요성

HNSW의 또 다른 중요한 기여는 근접 그래프 이웃을 선택하는 고급 휴리스틱입니다. 단순히 거리가 가까운 이웃만 선택하는 것이 아니라, 그래프의 연결성과 탐색 효율성을 고려한 이웃 선택이 이루어집니다. 이는 특히 높은 재현율(recall)이 요구되거나 데이터가 고도로 클러스터링된 경우 성능을 크게 향상시킵니다.

재현율은 근사 최근접 이웃 탐색의 품질을 측정하는 지표로, 다음과 같이 정의됩니다.

$$\text{Recall} = \frac{\text{발견된 실제 최근접 이웃의 수}}{K}$$

여기서 $K$는 찾고자 하는 최근접 이웃의 개수입니다. 예를 들어, 10-최근접 이웃 탐색에서 실제 최근접 이웃 10개 중 9개를 찾았다면 재현율은 0.9 또는 90%입니다.

클러스터링된 데이터에서는 많은 요소들이 밀집된 영역에 모여 있고, 이러한 클러스터들 사이에는 상대적으로 큰 거리가 존재합니다. 단순한 거리 기반 이웃 선택은 클러스터 내부의 연결만 강화하여, 클러스터 간 탐색이 비효율적이 될 수 있습니다. HNSW의 휴리스틱은 클러스터를 연결하는 "다리" 역할을 하는 이웃도 선택하여, 전체 그래프의 탐색 가능성을 유지합니다.

## 기존 방법들과의 비교

논문은 HNSW를 여러 최신 기법들과 비교합니다. 트리 기반 알고리즘(예: kd-트리, R-트리)은 저차원 데이터에서는 효과적이지만, 차원의 저주로 인해 고차원 데이터에서는 성능이 급격히 저하됩니다. 차원이 증가할수록 데이터 공간의 부피가 지수적으로 증가하여, 트리의 분할이 비효율적이 되기 때문입니다.

[지역 민감 해싱(Locality-Sensitive Hashing, LSH)](https://arxiv.org/pdf/1509.02897v1)은 고차원 데이터에서도 작동하지만, 해시 함수와 테이블의 개수 등 파라미터 튜닝이 까다롭고, 메모리 사용량이 많을 수 있습니다. LSH는 유사한 항목들이 높은 확률로 같은 해시 버킷에 매핑되도록 설계된 해시 함수를 사용하여, 쿼리 시 해당 버킷만 검색함으로써 탐색 공간을 줄입니다.

[제품 양자화(Product Quantization, PQ)](https://ieeexplore.ieee.org/document/5432202) 기반 방법들은 벡터 압축을 통해 메모리 효율성을 높이지만, 벡터 공간에만 적용 가능하며 일반적인 메트릭 공간에서는 사용할 수 없습니다. PQ는 고차원 벡터를 여러 저차원 부분공간으로 분할하고 각 부분공간을 독립적으로 양자화하여, 원래 벡터를 짧은 코드로 표현합니다. 이를 통해 100-1000배의 압축률을 달성하면서도 탐색 정확도를 유지할 수 있습니다.

HNSW는 이러한 방법들과 달리 일반적인 메트릭 공간에서 작동하며, 추가적인 보조 구조 없이 순수하게 그래프만으로 구성됩니다. 또한 점진적 구축이 가능하여, 새로운 요소를 동적으로 추가할 수 있습니다. 성능 평가 결과, HNSW는 고차원 데이터에서 기존의 오픈소스 최신 기법들을 크게 능가하며, 저차원 데이터에서도 경쟁력 있는 성능을 보입니다.

## 연구의 의의와 영향

이 연구는 근사 최근접 이웃 탐색 분야에서 중요한 진전을 이루었습니다. 계층적 구조와 확률적 층 배정을 통해 로그 복잡도를 달성하면서도, 다양한 데이터 분포와 차원에서 강건한 성능을 보이는 방법을 제시했습니다. 스킵 리스트와의 유사성은 분산 구현의 가능성을 열어주며, 이는 대규모 시스템에서의 확장성을 의미합니다.

HNSW는 이후 벡터 데이터베이스와 시맨틱 검색 시스템에서 널리 채택되었으며, 실제 산업 환경에서 수십억 개의 벡터를 효율적으로 검색하는 데 사용되고 있습니다. 이는 이론적 혁신이 실용적 가치로 이어진 성공적인 사례라고 할 수 있습니다.
이 섹션에서는 제안된 HNSW 알고리즘의 이론적 기반이 되는 기존 연구들을 검토합니다. 크게 근접 그래프 기법과 탐색 가능한 작은 세계 모델이라는 두 가지 주요 연구 영역으로 나누어 살펴봅니다.

## 근접 그래프 기법

대부분의 그래프 기반 탐색 알고리즘은 k-최근접 이웃(k-NN) 그래프에서 탐욕적 라우팅(greedy routing) 형태로 동작합니다. 탐욕적 라우팅은 매우 직관적인 탐색 방식으로, 마치 산을 오를 때 항상 가장 가파른 경사를 따라 올라가는 것과 유사합니다. 구체적으로, 어떤 진입점에서 시작하여 현재 기준 노드의 이웃들과 쿼리 사이의 거리를 계산하고, 거리를 최소화하는 인접 노드를 다음 기준 노드로 선택하면서 그래프를 순회합니다. 이 과정에서 지금까지 발견한 최선의 이웃들을 지속적으로 추적하며, 특정 종료 조건(예: 거리 계산 횟수 제한)이 충족되면 탐색을 종료합니다.

k-NN 그래프의 링크들은 [들로네 그래프(Delaunay graph)](https://arxiv.org/pdf/0709.0303v3)의 단순한 근사로 작동합니다. 들로네 그래프는 기본적인 탐욕적 그래프 순회의 결과가 항상 최근접 이웃이 되도록 보장하는 이론적으로 이상적인 그래프 구조입니다. 그러나 들로네 그래프는 공간의 구조에 대한 사전 정보 없이는 효율적으로 구축할 수 없다는 근본적인 한계가 있습니다. 반면 최근접 이웃을 통한 근사는 저장된 요소들 간의 거리만을 사용하여 구축할 수 있어 실용적입니다. 이러한 근접 그래프 접근법은 kd-트리나 지역 민감 해싱(LSH)과 같은 다른 k-ANNS 기법들과 경쟁력 있는 성능을 보이는 것으로 입증되었습니다.

하지만 k-NN 그래프 접근법에는 두 가지 주요 단점이 존재합니다. 첫째, 라우팅 과정에서 단계 수가 데이터셋 크기에 대해 멱법칙 스케일링을 따른다는 점입니다. 이는 데이터가 증가할수록 탐색에 필요한 단계가 $O(N^\alpha)$ (여기서 $0 < \alpha < 1$)의 형태로 증가한다는 의미입니다. 예를 들어, 데이터가 10배 증가하면 탐색 단계가 3-4배 증가할 수 있습니다. 둘째, 전역 연결성의 손실 가능성으로 인해 클러스터링된 데이터에서 탐색 결과가 저하될 수 있습니다. 데이터가 여러 개의 밀집된 클러스터로 나뉘어 있을 때, 그래프가 클러스터 간을 연결하는 "다리"를 충분히 가지지 못하면 일부 클러스터에 도달하지 못하는 문제가 발생합니다.

이러한 문제들을 극복하기 위해 많은 하이브리드 접근법들이 제안되었습니다. 이들은 벡터 데이터에만 적용 가능한 보조 알고리즘(kd-트리나 제품 양자화)을 사용하여 거친 탐색(coarse search)을 수행함으로써 더 나은 진입점 후보를 찾습니다. 예를 들어, kd-트리를 사용하여 쿼리와 대략적으로 가까운 영역을 먼저 식별한 후, 해당 영역에서 정밀한 그래프 탐색을 수행하는 방식입니다.

[탐색 가능한 작은 세계(Navigable Small World, NSW) 알고리즘](https://www.sciencedirect.com/science/article/pii/S0306437913001300)은 이러한 한계를 극복하기 위한 중요한 진전을 이루었습니다. NSW는 탐색 가능한 그래프, 즉 탐욕적 순회 중 홉 수가 네트워크 크기에 대해 로그 또는 다중로그 스케일링을 따르는 그래프를 활용합니다. 이는 멱법칙 스케일링에 비해 훨씬 효율적인 성장 패턴입니다. 데이터가 10배 증가할 때 탐색 단계가 단지 2-3배 정도만 증가하는 것입니다.

NSW 그래프는 요소들을 무작위 순서로 연속적으로 삽입하면서 구축됩니다. 각 새로운 요소는 이전에 삽입된 요소들 중 $M$개의 최근접 이웃과 양방향으로 연결됩니다. 이 $M$개의 최근접 이웃은 구조 자체의 탐색 절차(여러 무작위 진입점에서 시작하는 탐욕적 탐색의 변형)를 사용하여 찾습니다. 초기에 삽입된 요소들의 최근접 이웃에 대한 링크는 나중에 네트워크 허브들 사이의 다리 역할을 하게 되어, 전체 그래프 연결성을 유지하고 탐욕적 라우팅 중 홉 수의 로그 스케일링을 가능하게 합니다.

이를 좀 더 직관적으로 이해하기 위해 도시 도로망에 비유할 수 있습니다. 초기에 건설된 주요 도로들(초기 삽입 요소들의 링크)은 도시의 여러 지역을 연결하는 고속도로 역할을 합니다. 나중에 추가되는 도로들(후기 삽입 요소들의 링크)은 주로 지역 내부의 연결을 담당합니다. 이러한 계층적 구조 덕분에 한 지역에서 다른 지역으로 이동할 때 먼저 고속도로를 타고 빠르게 이동한 후, 목적지 근처에서 지역 도로로 진입하는 효율적인 경로를 찾을 수 있습니다.

NSW 구조의 구축 단계는 전역 동기화 없이 효율적으로 병렬화될 수 있으며, 정확도에 측정 가능한 영향을 주지 않습니다. 이는 분산 탐색 시스템에 적합한 특성입니다. NSW 접근법은 일부 데이터셋에서 최첨단 성능을 제공했지만, 전체적인 다중로그 복잡도 스케일링으로 인해 저차원 데이터셋에서는 여전히 심각한 성능 저하를 겪었습니다. 저차원 데이터에서는 트리 기반 알고리즘에 비해 수 배에서 수천 배까지 느릴 수 있었습니다.

## 탐색 가능한 작은 세계 모델

탐욕적 그래프 라우팅의 로그 또는 다중로그 스케일링을 가진 네트워크는 [탐색 가능한 작은 세계 네트워크](https://arxiv.org/pdf/0901.4710v1)로 알려져 있습니다. 이러한 네트워크는 실제 네트워크 형성의 기저 메커니즘을 이해하고 확장 가능한 라우팅 및 분산 유사도 탐색 응용에 적용하기 위한 복잡 네트워크 이론의 중요한 주제입니다.

탐색 가능한 네트워크의 공간 모델을 고려한 최초의 연구는 [J. Kleinberg](https://arxiv.org/pdf/0709.0303v3)가 유명한 밀그램 실험(Milgram experiment)에 대한 사회 네트워크 모델로 수행했습니다. 밀그램 실험은 1960년대에 수행된 실험으로, 미국의 임의의 두 사람이 평균 6단계의 지인 관계로 연결되어 있다는 "6단계 분리" 현상을 발견했습니다. Kleinberg는 이 현상을 설명하기 위해 수학적 모델을 개발했습니다.

Kleinberg는 [Watts-Strogatz 네트워크](https://arxiv.org/pdf/0709.0303v3)의 변형을 연구했는데, $d$차원 벡터 공간의 정규 격자 그래프에 특정 장거리 링크 길이 분포 $r^{-\alpha}$를 따르는 장거리 링크를 추가하는 방식입니다. 여기서 $r$은 링크의 길이이고, $\alpha$는 분포의 지수입니다. 놀랍게도, $\alpha = d$일 때만 탐욕적 라우팅을 통해 목표에 도달하는 홉 수가 다중로그적으로 스케일링됩니다. 다른 모든 $\alpha$ 값에 대해서는 멱법칙 스케일링이 나타납니다.

이를 구체적으로 이해하기 위해 2차원 격자($d=2$)를 생각해봅시다. 각 노드가 격자 위의 점이고, 가까운 이웃들과 연결되어 있습니다. 여기에 장거리 링크를 추가하는데, 거리 $r$만큼 떨어진 노드와 연결될 확률이 $r^{-2}$에 비례한다면, 탐욕적 라우팅이 효율적으로 작동합니다. 하지만 $\alpha$가 2보다 크면 장거리 링크가 너무 적어서 먼 거리를 빠르게 이동할 수 없고, 2보다 작으면 장거리 링크가 너무 무작위적으로 분포되어 목표 방향으로의 진전이 느립니다.

이 아이디어는 탐색 효과를 기반으로 한 많은 K-NNS 및 K-ANNS 알고리즘의 개발에 영감을 주었습니다. 그러나 Kleinberg의 탐색 가능성 기준은 원칙적으로 더 일반적인 공간으로 확장될 수 있지만, 이러한 탐색 가능한 네트워크를 구축하려면 데이터 분포를 미리 알아야 합니다. 실제 응용에서는 데이터 분포를 사전에 알 수 없는 경우가 대부분이므로 이는 심각한 제약입니다. 또한 Kleinberg의 그래프에서 탐욕적 라우팅은 기껏해야 다중로그 복잡도를 겪습니다.

또 다른 잘 알려진 탐색 가능한 네트워크 클래스는 [척도 자유(scale-free) 모델](https://arxiv.org/pdf/0709.0303v3)입니다. 척도 자유 네트워크는 노드의 차수 분포가 멱법칙을 따르는 네트워크로, 소수의 노드가 매우 많은 연결을 가지고 대다수의 노드는 적은 연결을 가지는 특성을 보입니다. 이는 인터넷, 소셜 네트워크, 생물학적 네트워크 등 많은 실제 네트워크에서 관찰되는 특성입니다. 척도 자유 모델은 실제 네트워크의 여러 특징을 재현할 수 있으며 라우팅 응용을 위해 제안되었습니다.

그러나 이러한 모델로 생성된 네트워크는 탐욕적 탐색의 멱법칙 복잡도 스케일링이 더욱 악화되는 문제가 있습니다. 척도 자유 네트워크에서는 허브 노드들이 많은 연결을 가지지만, 이들이 항상 탐색에 유용한 방향으로 연결되어 있는 것은 아닙니다. 또한 Kleinberg의 모델과 마찬가지로, 척도 자유 모델도 데이터 분포에 대한 전역 지식을 필요로 하여 탐색 응용에 사용할 수 없습니다.

앞서 설명한 NSW 알고리즘은 탐색 가능한 네트워크의 더 단순하고 이전에 알려지지 않은 모델을 사용합니다. 이 모델은 분산된 그래프 구축을 허용하며 임의의 공간에서 데이터에 적합합니다. NSW 네트워크 형성 메커니즘이 대규모 생물학적 신경망의 탐색 가능성(그 존재 여부는 논쟁의 여지가 있음)을 담당할 수 있다고 제안되었습니다. 유사한 모델들이 작은 뇌 네트워크의 성장을 설명할 수 있었으며, 이 모델은 대규모 신경망에서 관찰되는 여러 고수준 특징들을 예측합니다.

그러나 NSW 모델도 라우팅 과정의 다중로그 탐색 복잡도로 인한 한계를 겪습니다. 이는 데이터셋 크기가 증가함에 따라 탐색 비용이 $O(\log^c N)$ (여기서 $c > 1$)의 형태로 증가한다는 의미입니다. 로그 복잡도 $O(\log N)$에 비해 여전히 비효율적이며, 특히 저차원 데이터나 대규모 데이터셋에서 성능 저하가 두드러집니다. 이러한 한계를 극복하는 것이 바로 이 논문에서 제안하는 HNSW의 핵심 동기가 됩니다.
## 동기

NSW 탐색 복잡도를 개선하는 방법은 라우팅 과정에 대한 상세한 분석을 통해 식별할 수 있습니다. 라우팅 과정은 "줌아웃(zoom-out)"과 "줌인(zoom-in)"이라는 두 단계로 구분됩니다. 탐욕적 알고리즘은 낮은 차수의 노드에서 줌아웃 단계를 시작하며, 노드의 링크 길이에 대한 특성 반경이 쿼리까지의 거리 척도에 도달할 때까지 차수를 동시에 증가시키면서 그래프를 순회합니다. 이 과정이 완료되기 전까지는 노드의 평균 차수가 상대적으로 작게 유지될 수 있으며, 이는 먼 거리의 잘못된 지역 최소값(false local minimum)에 갇힐 확률을 증가시킵니다.

### NSW의 멱법칙 복잡도 분석

NSW에서 설명된 문제는 최대 차수를 가진 노드(NSW 구조에 처음 삽입된 노드들이 좋은 후보입니다)에서 탐색을 시작함으로써 직접 줌인 단계로 진입하는 방식으로 회피할 수 있습니다. 실험 결과 허브를 시작점으로 설정하면 구조 내에서 성공적인 라우팅 확률이 크게 증가하고 저차원 데이터에서 훨씬 더 나은 성능을 제공하는 것으로 나타났습니다. 그러나 이 방식도 기껏해야 단일 탐욕적 탐색의 다중로그 복잡도 스케일링만을 가지며, 고차원 데이터에서는 계층적 NSW에 비해 성능이 떨어집니다.

NSW에서 단일 탐욕적 탐색의 다중로그 복잡도 스케일링이 나타나는 이유는 전체 거리 계산 횟수가 대략적으로 탐욕적 알고리즘의 평균 홉 수와 탐욕적 경로상의 노드들의 평균 차수의 곱에 비례하기 때문입니다. 평균 홉 수는 로그적으로 스케일링되며, 탐욕적 경로상의 노드들의 평균 차수 역시 다음과 같은 이유로 로그적으로 스케일링됩니다. 첫째, 탐욕적 탐색은 네트워크가 성장함에 따라 동일한 허브들을 통과하는 경향이 있습니다. 둘째, 허브 연결의 평균 개수는 네트워크 크기가 증가함에 따라 로그적으로 증가합니다. 따라서 전체적으로 $O(\log N) \times O(\log N) = O(\log^2 N)$의 다중로그 복잡도를 얻게 됩니다.

### 계층 분리를 통한 로그 복잡도 달성

계층적 NSW 알고리즘의 핵심 아이디어는 링크들을 길이 척도에 따라 서로 다른 층으로 분리한 다음 다층 그래프에서 탐색하는 것입니다. 이 경우 네트워크 크기와 무관하게 각 요소에 대해 필요한 고정된 비율의 연결만을 평가할 수 있으므로 로그 스케일링이 가능해집니다. 이러한 구조에서 탐색은 가장 긴 링크만 가진 상위 층에서 시작됩니다.

![계층적 NSW 개념 도해](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/0.png)

위 그림은 계층적 NSW의 아이디어를 보여줍니다. 탐색은 최상위 층의 요소(빨간색으로 표시)에서 시작됩니다. 빨간색 화살표는 진입점에서 쿼리(녹색으로 표시)까지 탐욕적 알고리즘의 방향을 나타냅니다. 알고리즘은 상위 층의 요소들을 통해 탐욕적으로 순회하며 지역 최소값에 도달할 때까지 진행됩니다. 그 후 탐색은 더 짧은 링크를 가진 하위 층으로 전환되고, 이전 층에서 지역 최소값이었던 요소에서 재시작하며 이 과정이 반복됩니다.

모든 층에서 요소당 최대 연결 수를 상수로 유지할 수 있으므로, 탐색 가능한 작은 세계 네트워크에서 라우팅의 로그 복잡도 스케일링이 가능해집니다.

### 층 할당 및 구조 형성

이러한 층별 구조를 형성하는 한 가지 방법은 층을 도입하여 서로 다른 길이 척도를 가진 링크를 명시적으로 설정하는 것입니다. 각 요소에 대해 정수 레벨 $l$을 선택하는데, 이는 해당 요소가 속하는 최대 층을 정의합니다. 각 층의 모든 요소에 대해 근접 그래프(즉, 들로네 그래프를 근사하는 "짧은" 링크만 포함하는 그래프)가 점진적으로 구축됩니다. $l$의 지수적으로 감소하는 확률(즉, 기하 분포를 따르는)을 설정하면 구조의 층 개수에 대한 로그 스케일링을 얻게 됩니다.

```python
import numpy as np

def assign_layer_level(m_L):
    """기하 분포를 사용한 층 레벨 할당

    Args:
        m_L: 정규화 인자 (일반적으로 1/ln(M))

    Returns:
        할당된 층 레벨 l
    """
    # 균등 분포에서 무작위 값 생성
    uniform_random = np.random.uniform(0, 1)

    # 지수적으로 감소하는 확률로 레벨 할당
    # P(l) ∝ exp(-l/m_L)
    level = int(-np.log(uniform_random) * m_L)

    return level

# 사용 예시: 1000개 요소의 레벨 분포 시각화
m_L = 1.0 / np.log(16)  # M=16일 때의 최적 m_L
levels = [assign_layer_level(m_L) for _ in range(1000)]
print(f"최상위 층: {max(levels)}, 평균 층: {np.mean(levels):.2f}")
```

탐색 절차는 최상위 층에서 시작하여 0층에서 끝나는 반복적인 탐욕적 탐색입니다. 모든 층의 연결을 병합하는 경우, 구조는 NSW 그래프와 유사해집니다(이 경우 $l$은 NSW의 노드 차수와 대응될 수 있습니다). NSW와 대조적으로, 계층적 NSW 구축 알고리즘은 삽입 전에 요소들을 섞을 필요가 없습니다. 확률성은 레벨 무작위화를 사용하여 달성되므로, 일시적으로 변화하는 데이터 분포의 경우에도 진정으로 점진적인 인덱싱이 가능합니다(다만 삽입 순서를 변경하면 부분적으로만 결정론적인 구축 절차로 인해 성능이 약간 변경됩니다).

### 이웃 선택 휴리스틱

요소 삽입 시 근접 그래프 연결을 선택하기 위해, 단순히 가장 가까운 이웃을 선택하는 대신 후보 요소들 간의 거리를 고려하여 다양한 연결을 생성하는 휴리스틱을 활용합니다. 이 휴리스틱은 후보들을 삽입될 요소에 대해 가장 가까운 것부터 시작하여 검사하고, 후보가 이미 연결된 후보들과 비교하여 기준(삽입되는) 요소에 더 가까운 경우에만 연결을 생성합니다.

```python
def select_neighbors_heuristic(base_element, candidates, M):
    """RNG 기반 이웃 선택 휴리스틱

    Args:
        base_element: 삽입되는 기준 요소
        candidates: 후보 이웃들의 리스트
        M: 선택할 최대 이웃 수

    Returns:
        선택된 이웃들의 리스트
    """
    # 거리 기준으로 후보들을 정렬
    sorted_candidates = sorted(
        candidates,
        key=lambda c: distance(base_element, c)
    )

    selected_neighbors = []

    for candidate in sorted_candidates:
        if len(selected_neighbors) >= M:
            break

        # 후보가 기준 요소에 이미 연결된 이웃들보다 가까운지 확인
        is_closer = True
        for neighbor in selected_neighbors:
            if distance(candidate, neighbor) < distance(base_element, candidate):
                is_closer = False
                break

        # 더 가까운 경우에만 연결
        if is_closer:
            selected_neighbors.append(candidate)

    return selected_neighbors
```

후보의 수가 충분히 많을 때, 이 휴리스틱은 노드 간의 거리만을 사용하여 추론 가능한 들로네 그래프의 최소 부분그래프인 정확한 상대 이웃 그래프(Relative Neighborhood Graph, RNG)를 부분그래프로 얻을 수 있습니다. 상대 이웃 그래프는 고도로 클러스터링된 데이터의 경우에도 전역 연결 요소를 쉽게 유지할 수 있게 합니다.

논문의 Figure 2는 두 개의 고립된 클러스터에 대한 그래프 이웃 선택 휴리스틱을 보여줍니다. 새로운 요소가 클러스터 1의 경계에 삽입됩니다. 요소의 가장 가까운 이웃들은 모두 클러스터 1에 속하므로, 클러스터 간 들로네 그래프의 에지를 놓치게 됩니다. 그러나 휴리스틱은 클러스터 2의 요소 e2를 선택하여, 삽입된 요소가 클러스터 1의 다른 요소들에 비해 e2에 가장 가까운 경우 전역 연결성을 유지합니다.

휴리스틱은 정확한 상대 이웃 그래프에 비해 추가 에지를 생성하므로, 탐색 성능에 중요한 연결 수를 제어할 수 있습니다. 1차원 데이터의 경우 휴리스틱은 요소들 간의 거리에 대한 정보만을 사용하여 정확한 들로네 부분그래프(이 경우 상대 이웃 그래프와 일치)를 얻을 수 있으므로, 계층적 NSW에서 1차원 확률적 스킵 리스트 알고리즘으로의 직접적인 전환이 가능합니다.

계층적 NSW 근접 그래프의 기본 변형은 근접 그래프 탐색을 위해 이전 연구에서 'sparse neighborhood graphs'로 사용되었습니다. 유사한 휴리스틱은 정확한 라우팅의 sparse neighborhood graph 속성에 기반한 약간 다른 해석으로 FANNG 알고리즘에서도 주목받았습니다.
이 섹션에서는 HNSW 알고리즘의 구체적인 구현 방법과 핵심 알고리즘들을 상세히 설명합니다. 앞서 동기 부분에서 계층적 구조의 필요성을 설명했다면, 이제는 이러한 아이디어를 실제로 어떻게 구현하는지 알고리즘 수준에서 다룹니다.

## 네트워크 구축 알고리즘의 핵심 원리

HNSW의 네트워크 구축 알고리즘은 저장된 요소들을 그래프 구조에 연속적으로 삽입하는 방식으로 동작합니다. 각 요소가 삽입될 때마다 해당 요소가 속할 최대 층 레벨 $l$이 지수적으로 감소하는 확률 분포를 따라 무작위로 선택됩니다. 이는 알고리즘 1의 4번째 라인에서 $m_L$ 파라미터에 의해 정규화된 분포로 구현됩니다.

삽입 프로세스는 두 단계로 구성됩니다. 첫 번째 단계에서는 최상위 층에서 시작하여 그래프를 탐욕적으로 순회하면서 삽입될 요소 $q$에 가장 가까운 $e_f$개의 이웃을 찾습니다. 이후 발견된 최근접 이웃들을 진입점으로 사용하여 다음 하위 층에서 탐색을 계속하며, 이 과정이 반복됩니다. 이는 마치 건물에서 엘리베이터를 타고 대략적인 층까지 빠르게 이동한 후, 계단을 이용해 정확한 목적지를 찾아가는 것과 유사합니다.

```python
import numpy as np
from collections import defaultdict
import heapq

class HNSWGraph:
    def __init__(self, M=16, M_max0=32, m_L=None, ef_construction=200):
        """HNSW 그래프 초기화

        Args:
            M: 각 요소의 최대 연결 수
            M_max0: 0층(ground layer)의 최대 연결 수
            m_L: 층 선택 정규화 인자 (기본값: 1/ln(M))
            ef_construction: 구축 시 동적 후보 리스트 크기
        """
        self.M = M
        self.M_max0 = M_max0
        self.m_L = m_L if m_L is not None else 1.0 / np.log(M)
        self.ef_construction = ef_construction

        # 각 층의 그래프를 저장하는 딕셔너리
        self.graphs = defaultdict(dict)
        # 각 요소의 최대 층 레벨
        self.element_levels = {}
        # 진입점 (최상위 층의 요소)
        self.entry_point = None
        # 저장된 요소들의 데이터
        self.data = {}

    def _select_level(self):
        """지수 분포를 사용하여 무작위 층 레벨 선택"""
        # 균등 분포에서 무작위 값 생성
        uniform = np.random.uniform(0, 1)
        # 지수 분포 변환: l = -ln(uniform) * m_L
        level = int(-np.log(uniform) * self.m_L)
        return level

    def insert(self, element_id, element_vector):
        """새로운 요소를 HNSW 그래프에 삽입

        Args:
            element_id: 삽입할 요소의 고유 식별자
            element_vector: 요소의 벡터 표현
        """
        # 요소 데이터 저장
        self.data[element_id] = element_vector

        # 1단계: 요소의 최대 층 레벨 선택
        l = self._select_level()
        self.element_levels[element_id] = l

        # 첫 번째 요소인 경우
        if self.entry_point is None:
            self.entry_point = element_id
            return

        # 2단계: 최상위 층부터 l층까지 탐색하여 진입점 찾기
        # 이 단계에서는 ef=1 (단순 탐욕적 탐색)을 사용
        current_nearest = [self.entry_point]
        current_level = self.element_levels[self.entry_point]

        # l층보다 높은 층들에서는 단순 탐욕적 탐색
        for lc in range(current_level, l, -1):
            current_nearest = self._search_layer(
                element_vector, current_nearest, ef=1, lc=lc
            )

        # 3단계: l층부터 0층까지 탐색 및 연결 생성
        for lc in range(l, -1, -1):
            # 확장된 ef_construction을 사용하여 후보 발견
            candidates = self._search_layer(
                element_vector, current_nearest,
                ef=self.ef_construction, lc=lc
            )

            # 이웃 선택 (휴리스틱 또는 단순 방법)
            M = self.M if lc > 0 else self.M_max0
            neighbors = self._select_neighbors_heuristic(
                element_id, candidates, M, lc
            )

            # 양방향 연결 생성
            self.graphs[lc][element_id] = neighbors
            for neighbor in neighbors:
                self.graphs[lc][neighbor].add(element_id)

                # 이웃의 연결 수가 최대치를 초과하면 가지치기
                M_max = self.M if lc > 0 else self.M_max0
                if len(self.graphs[lc][neighbor]) > M_max:
                    # 가장 먼 이웃 제거
                    self._prune_connections(neighbor, M_max, lc)

            current_nearest = candidates

        # 진입점 업데이트 (더 높은 층에 요소가 있으면)
        if l > self.element_levels[self.entry_point]:
            self.entry_point = element_id
```

## 층별 탐색 알고리즘의 상세 구현

알고리즘 2는 특정 층 $l_c$에서 쿼리 요소 $q$에 가장 가까운 $e_f$개의 이웃을 찾는 핵심 탐색 절차입니다. 이 알고리즘은 이전 연구의 NSW 알고리즘을 개선한 버전으로, 몇 가지 중요한 차이점이 있습니다.

탐색은 진입점들로 초기화된 동적 리스트 $W$를 유지하면서 진행됩니다. 각 단계에서 리스트에서 쿼리에 가장 가까운 미평가 요소의 이웃들을 평가하고, 리스트의 모든 요소의 이웃이 평가될 때까지 계속됩니다. NSW와의 주요 차이점은 첫째, 진입점이 고정된 파라미터라는 점과 둘째, 탐색 품질이 $K$ 대신 별도의 파라미터 $e_f$로 제어된다는 점입니다.

```python
def _search_layer(self, query_vector, enter_points, ef, lc):
    """특정 층에서 쿼리에 가장 가까운 ef개의 요소 탐색

    Args:
        query_vector: 쿼리 벡터
        enter_points: 진입점 요소들의 리스트
        ef: 반환할 최근접 이웃 개수
        lc: 탐색할 층 레벨

    Returns:
        쿼리에 가장 가까운 ef개의 요소 리스트
    """
    # v: 방문한 요소들의 집합
    visited = set(enter_points)

    # C: 후보 집합 (최소 힙 - 쿼리에 가까운 순)
    # 형식: (거리, 요소_id)
    candidates = []
    for ep in enter_points:
        dist = self._distance(query_vector, self.data[ep])
        heapq.heappush(candidates, (dist, ep))

    # W: 발견된 최근접 이웃의 동적 리스트 (최대 힙 - 쿼리에 먼 순)
    # 가장 먼 요소를 빠르게 제거하기 위해 최대 힙 사용
    nearest = []
    for ep in enter_points:
        dist = self._distance(query_vector, self.data[ep])
        heapq.heappush(nearest, (-dist, ep))  # 음수로 최대 힙 구현

    # 탐색 루프
    while candidates:
        # C에서 쿼리에 가장 가까운 요소 추출
        current_dist, current = heapq.heappop(candidates)

        # W에서 가장 먼 요소 확인
        furthest_dist = -nearest[0][0]

        # 종료 조건: 현재 요소가 W의 가장 먼 요소보다 멀면
        # W의 모든 요소가 평가되었음
        if current_dist > furthest_dist:
            break

        # 현재 요소의 이웃들 검사
        if current in self.graphs[lc]:
            for neighbor in self.graphs[lc][current]:
                if neighbor not in visited:
                    visited.add(neighbor)

                    # 이웃까지의 거리 계산
                    neighbor_dist = self._distance(
                        query_vector, self.data[neighbor]
                    )
                    furthest_dist = -nearest[0][0]

                    # 이웃이 W의 가장 먼 요소보다 가깝거나
                    # W의 크기가 ef보다 작으면 추가
                    if neighbor_dist < furthest_dist or len(nearest) < ef:
                        heapq.heappush(candidates, (neighbor_dist, neighbor))
                        heapq.heappush(nearest, (-neighbor_dist, neighbor))

                        # W의 크기가 ef를 초과하면 가장 먼 요소 제거
                        if len(nearest) > ef:
                            heapq.heappop(nearest)

    # 결과 반환 (거리와 함께)
    return [item[1] for item in nearest]

def _distance(self, vec1, vec2):
    """두 벡터 간의 유클리드 거리 계산"""
    return np.linalg.norm(vec1 - vec2)
```

이 알고리즘의 핵심적인 개선점은 종료 조건입니다. 거리 계산 횟수를 제한하는 대신, Hierarchical NSW는 리스트의 가장 먼 요소보다 멀리 있는 후보들을 평가 대상에서 제외함으로써 탐색 구조의 비대화를 방지합니다. 이는 성능상 두 개의 우선순위 큐로 리스트를 에뮬레이션하여 구현됩니다.

## 이웃 선택 방법의 전략적 중요성

삽입 프로세스의 두 번째 단계에서는 발견된 최근접 이웃들을 연결 후보로 사용합니다. 논문에서는 후보들로부터 $M$개의 이웃을 선택하는 두 가지 방법을 제시합니다.

알고리즘 3은 단순히 가장 가까운 요소들에 연결하는 방법입니다. 이는 구현이 간단하지만, 앞서 설명한 클러스터링된 데이터에서 전역 연결성을 잃을 수 있는 문제가 있습니다.

```python
def _select_neighbors_simple(self, base_element, candidates, M):
    """가장 가까운 M개의 이웃을 단순 선택

    Args:
        base_element: 기준 요소 ID
        candidates: 후보 이웃들의 리스트
        M: 선택할 이웃의 개수

    Returns:
        선택된 M개의 이웃 리스트
    """
    base_vector = self.data[base_element]

    # 거리 기준으로 정렬
    candidate_distances = [
        (self._distance(base_vector, self.data[c]), c)
        for c in candidates
    ]
    candidate_distances.sort()

    # 가장 가까운 M개 반환
    return [c for _, c in candidate_distances[:M]]
```

알고리즘 4는 후보 요소들 간의 거리를 고려하여 다양한 방향으로 연결을 생성하는 휴리스틱입니다. 이 방법은 앞서 설명한 상대 이웃 그래프(RNG)의 개념을 활용합니다.

```python
def _select_neighbors_heuristic(self, base_element, candidates, M, lc,
                                extend_candidates=False,
                                keep_pruned=True):
    """휴리스틱을 사용한 이웃 선택 (RNG 기반)

    Args:
        base_element: 기준 요소 ID
        candidates: 후보 이웃들의 리스트
        M: 선택할 이웃의 개수
        lc: 현재 층 레벨
        extend_candidates: 후보를 이웃의 이웃으로 확장할지 여부
        keep_pruned: 폐기된 연결을 일부 유지할지 여부

    Returns:
        선택된 이웃들의 집합
    """
    base_vector = self.data[base_element]

    # R: 선택된 이웃들
    selected = set()

    # W: 후보 작업 큐 (거리 기준 최소 힙)
    working_queue = []
    for c in candidates:
        dist = self._distance(base_vector, self.data[c])
        heapq.heappush(working_queue, (dist, c))

    # 후보 확장 옵션 (매우 클러스터링된 데이터에 유용)
    if extend_candidates:
        extended = set(candidates)
        for c in candidates:
            if c in self.graphs[lc]:
                for neighbor in self.graphs[lc][c]:
                    if neighbor not in extended:
                        extended.add(neighbor)
                        dist = self._distance(base_vector, self.data[neighbor])
                        heapq.heappush(working_queue, (dist, neighbor))

    # Wd: 폐기된 후보들의 큐
    discarded = []

    # 휴리스틱 선택 과정
    while working_queue and len(selected) < M:
        # 가장 가까운 후보 추출
        candidate_dist, candidate = heapq.heappop(working_queue)

        # 후보가 이미 선택된 이웃들보다 기준 요소에 더 가까운지 확인
        is_closer = True
        for neighbor in selected:
            # 후보와 이미 선택된 이웃 간의 거리
            dist_to_neighbor = self._distance(
                self.data[candidate], self.data[neighbor]
            )
            # 후보가 기준 요소에 더 가까운 경우에만 선택
            if dist_to_neighbor < candidate_dist:
                is_closer = False
                break

        if is_closer:
            selected.add(candidate)
        else:
            heapq.heappush(discarded, (candidate_dist, candidate))

    # 폐기된 연결 중 일부를 유지 (연결 수 고정을 위해)
    if keep_pruned:
        while discarded and len(selected) < M:
            _, candidate = heapq.heappop(discarded)
            selected.add(candidate)

    return selected
```

이 휴리스틱의 핵심은 11번째 라인의 조건입니다. 후보 요소 $e$가 기준 요소 $q$에 대해 이미 선택된 모든 이웃들보다 가까운 경우에만 선택됩니다. 이는 $e$와 이미 선택된 이웃 $r$ 사이의 거리 $d(e, r)$이 $q$와 $e$ 사이의 거리 $d(q, e)$보다 작으면, $e$를 선택하지 않는다는 의미입니다. 이를 수식으로 표현하면 다음과 같습니다.

$$e \in R \iff \forall r \in R, \; d(e, q) < d(e, r)$$

후보의 수가 충분히 많을 때, 이 조건은 정확히 상대 이웃 그래프(RNG)의 정의와 일치합니다. 앞서 설명한 클러스터링된 데이터의 예시에서, 이 휴리스틱은 클러스터 2의 요소를 선택함으로써 전역 연결성을 유지합니다.

![클러스터링된 데이터에서 휴리스틱의 효과](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/1.png)

위 그림은 두 개의 고립된 클러스터에 대한 휴리스틱의 작동 방식을 보여줍니다. 새로운 요소가 클러스터 1의 경계에 삽입될 때, 가장 가까운 이웃들은 모두 클러스터 1에 속하지만, 휴리스틱은 클러스터 2의 요소 e2를 선택하여 클러스터 간 연결을 생성합니다.

## K-최근접 이웃 탐색의 완성

알고리즘 5는 구축된 HNSW 그래프에서 K-최근접 이웃을 탐색하는 절차입니다. 이는 본질적으로 층 레벨 $l=0$인 요소에 대한 삽입 알고리즘과 동일합니다.

```python
def search(self, query_vector, K, ef=None):
    """K-최근접 이웃 탐색

    Args:
        query_vector: 쿼리 벡터
        K: 찾을 최근접 이웃의 개수
        ef: 동적 후보 리스트 크기 (기본값: max(ef_construction, K))

    Returns:
        K개의 최근접 이웃 ID 리스트
    """
    if ef is None:
        ef = max(self.ef_construction, K)

    # 진입점에서 시작
    current_nearest = [self.entry_point]
    current_level = self.element_levels[self.entry_point]

    # 1단계: 최상위 층부터 1층까지 탐욕적 탐색 (ef=1)
    for lc in range(current_level, 0, -1):
        current_nearest = self._search_layer(
            query_vector, current_nearest, ef=1, lc=lc
        )

    # 2단계: 0층에서 확장된 ef로 탐색
    current_nearest = self._search_layer(
        query_vector, current_nearest, ef=ef, lc=0
    )

    # 3단계: 가장 가까운 K개 반환
    distances = [
        (self._distance(query_vector, self.data[n]), n)
        for n in current_nearest
    ]
    distances.sort()

    return [n for _, n in distances[:K]]
```

탐색의 첫 번째 단계에서는 $e_f$ 파라미터를 1로 설정하여 추가 파라미터 도입을 피합니다. 탐색이 삽입될 요소의 층 레벨 $l$ 이하에 도달하면 두 번째 단계가 시작되며, 여기서 $e_f$가 $e_f^{\text{Construction}}$으로 증가되어 탐욕적 탐색 절차의 재현율을 제어합니다. 탐색 품질은 $e_f$ 파라미터로 제어되며, 이는 구축 알고리즘의 $e_f^{\text{Construction}}$에 해당합니다.

## 구축 파라미터의 영향과 최적 설정

알고리즘 구축 파라미터 $m_L$과 $M_{\max0}$는 구축된 그래프에서 작은 세계 탐색 가능성을 유지하는 역할을 합니다. $m_L$을 0으로 설정하면 단일 층 그래프가 되어 방향성 k-NN 그래프를 생성하며, 이는 멱법칙 탐색 복잡도를 가집니다. $m_L$을 0으로 설정하고 $M_{\max0}$를 무한대로 설정하면 다중로그 복잡도를 가진 NSW 그래프가 됩니다.

### $m_L$ 파라미터의 최적 선택

제어 가능한 계층의 최적 성능 이점을 달성하려면, 서로 다른 층의 이웃들 간 중복이 작아야 합니다. 중복을 줄이려면 $m_L$을 감소시켜야 하지만, 동시에 $m_L$ 감소는 각 층에서 탐욕적 탐색 중 평균 홉 수를 증가시켜 성능에 부정적 영향을 미칩니다. 따라서 최적 $m_L$ 값이 존재합니다.

최적 $m_L$의 간단한 선택은 $1/\ln(M)$이며, 이는 스킵 리스트 파라미터 $p=1/M$에 해당하여 층 간 평균적으로 단일 요소 중복을 가집니다. Intel Core i7 5930K CPU에서 1천만 개의 무작위 $d=4$ 벡터에 대한 시뮬레이션은 제안된 $m_L$ 선택이 합리적임을 보여줍니다.

![저차원 데이터에서 m_L의 영향](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/2.png)

위 그림은 1천만 개의 4차원 무작위 벡터에 대한 쿼리 시간 대 $m_L$ 파라미터의 플롯입니다. 화살표로 표시된 자동 선택 값 $1/\ln(M)$이 최적 성능 영역에 위치함을 확인할 수 있습니다. 플롯은 $m_L$을 0에서 증가시킬 때 저차원 데이터에서 막대한 속도 향상을 보여주며, 연결 선택에 휴리스틱을 사용한 효과도 함께 나타냅니다.

고차원 데이터에서 동일한 동작을 기대하기는 어렵습니다. k-NN 그래프가 이미 매우 짧은 탐욕적 알고리즘 경로를 가지기 때문입니다. 놀랍게도, $m_L$을 0에서 증가시키면 매우 고차원 데이터(10만 개의 밀집 무작위 $d=1024$ 벡터)에서도 측정 가능한 속도 향상이 나타나며, Hierarchical NSW 접근법에 대한 패널티가 발생하지 않습니다.

![고차원 데이터에서 m_L의 영향](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/3.png)

위 그림은 10만 개의 1024차원 무작위 벡터에 대한 결과입니다. 고차원에서도 계층적 구조가 성능 향상을 제공함을 볼 수 있습니다.

SIFT 벡터와 같은 실제 데이터의 경우, $m_L$을 증가시킴으로써 얻는 성능 향상이 더 높지만, 현재 설정에서는 휴리스틱에 의한 향상만큼 두드러지지는 않습니다.

![SIFT 데이터에서 m_L의 영향](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/4.png)

위 그림은 BIGANN 데이터셋의 학습 세트에서 5백만 개의 128차원 SIFT 벡터에 대한 1-NN 탐색 성능을 보여줍니다.

### $M_{\max0}$ 파라미터의 선택

$M_{\max0}$ (0층에서 요소가 가질 수 있는 최대 연결 수)의 선택도 탐색 성능에 강한 영향을 미치며, 특히 고품질(높은 재현율) 탐색의 경우 더욱 그렇습니다. 시뮬레이션은 $M_{\max0}$를 $M$으로 설정하면(이웃 선택 휴리스틱을 사용하지 않는 경우 각 층에서 k-NN 그래프에 해당) 높은 재현율에서 매우 강한 성능 저하가 발생함을 보여줍니다. 시뮬레이션은 또한 $2 \cdot M$이 $M_{\max0}$에 대한 좋은 선택임을 시사합니다.

![M_max0 파라미터의 영향](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/5.png)

위 그림은 Intel Core i5 2400 CPU에서 5백만 SIFT 학습 데이터셋에 대한 $M_{\max0}$ 파라미터에 따른 탐색 성능 결과입니다. 제안된 값 $2 \cdot M$이 서로 다른 재현율에서 최적에 가까운 성능을 제공함을 보여줍니다. 파라미터를 더 높게 설정하면 성능 저하와 과도한 메모리 사용이 발생합니다.

### 이웃 선택 방법의 비교

모든 고려된 경우에서, 근접 그래프 이웃 선택에 휴리스틱(알고리즘 4)을 사용하면 가장 가까운 이웃에 단순히 연결하는 것(알고리즘 3)과 비교하여 더 높거나 유사한 탐색 성능을 제공합니다.

![이웃 선택 방법의 효과](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/6.png)

위 그림은 클러스터링된 데이터(100개의 무작위로 고립된 클러스터)와 비클러스터링된 10차원 무작위 벡터 데이터에 대한 이웃 선택 방법의 효과를 보여줍니다. 휴리스틱은 저차원 데이터, 중차원 데이터의 높은 재현율, 그리고 고도로 클러스터링된 데이터의 경우에 가장 두드러진 효과를 보입니다.

가장 가까운 이웃을 연결로 사용할 때, Hierarchical NSW 알고리즘은 클러스터링된 데이터에 대해 높은 재현율을 달성하지 못합니다. 탐색이 클러스터 경계에서 멈추기 때문입니다. 반대로, 휴리스틱을 사용하고 후보 확장을 함께 사용하면, 클러스터링이 오히려 더 높은 성능으로 이어집니다.

균일하고 매우 고차원인 데이터의 경우 이웃 선택 방법 간 차이가 거의 없습니다. 이는 이 경우 거의 모든 최근접 이웃이 휴리스틱에 의해 선택되기 때문일 것입니다.

### $M$ 파라미터의 선택

사용자에게 남겨진 유일한 의미 있는 구축 파라미터는 $M$입니다. 합리적인 $M$ 범위는 5에서 48까지입니다. 시뮬레이션은 더 작은 $M$이 일반적으로 더 낮은 재현율 또는 더 낮은 차원 데이터에서 더 나은 결과를 생성하는 반면, 더 큰 $M$은 높은 재현율 또는 고차원 데이터에서 더 좋다는 것을 보여줍니다.

![M 파라미터의 영향](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/7.png)

위 그림은 5백만 SIFT 학습 데이터셋에 대한 서로 다른 $M$ 파라미터 값에 대한 재현율 오차 대 쿼리 시간을 보여줍니다. 파라미터는 또한 알고리즘의 메모리 소비를 정의하므로($M$에 비례) 신중하게 선택해야 합니다.

### $e_f^{\text{Construction}}$ 파라미터의 선택

$e_f^{\text{Construction}}$ 파라미터의 선택은 직관적입니다. 이전 연구에서 제안된 바와 같이, 구축 프로세스 중 K-ANNS 재현율이 1에 가깝게 생성될 만큼 충분히 커야 합니다(대부분의 사용 사례에서 0.95면 충분합니다). 이전 연구처럼, 이 파라미터는 샘플 데이터를 사용하여 자동으로 구성할 수 있습니다.

### 병렬 구축 및 구축 시간

구축 프로세스는 몇 개의 동기화 지점만으로 쉽고 효율적으로 병렬화될 수 있으며, 인덱스 품질에 측정 가능한 영향을 미치지 않습니다.

![병렬 구축 성능](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/8.png)

위 그림은 두 개의 CPU에서 서로 다른 스레드 수에 대한 10백만 SIFT 데이터셋의 Hierarchical NSW 구축 시간을 보여줍니다. 병렬 확장성이 우수함을 확인할 수 있습니다.

구축 속도와 인덱스 품질 간의 트레이드오프는 $e_f^{\text{Construction}}$ 파라미터를 통해 제어됩니다.

![구축 시간과 탐색 시간의 트레이드오프](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/9.png)

위 그림은 10백만 SIFT 데이터셋에 대한 Hierarchical NSW의 쿼리 시간 대 구축 시간 트레이드오프를 보여줍니다. $e_f^{\text{Construction}}=100$에서 합리적인 품질의 인덱스가 4X 2.4 GHz 10코어 Xeon E5-4650 v2 CPU 서버에서 단 3분 만에 구축될 수 있음을 보여줍니다. $e_f^{\text{Construction}}$을 더 증가시키면 약간의 추가 성능을 얻지만 훨씬 더 긴 구축 시간이 필요합니다.

### 데이터셋 크기에 따른 $e_f$ 파라미터 요구사항

![데이터셋 크기에 따른 ef 파라미터](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/10.png)

위 그림은 4차원 무작위 벡터 데이터에 대해 고정된 정확도를 얻기 위해 필요한 $e_f$ 파라미터 대 데이터셋 크기를 보여줍니다. $e_f$ 파라미터가 데이터셋 크기에 대해 로그적으로 증가함을 확인할 수 있으며, 이는 알고리즘의 로그 복잡도 스케일링을 실증적으로 뒷받침합니다.

이러한 파라미터 분석을 통해, HNSW는 다양한 데이터 특성과 성능 요구사항에 맞게 조정 가능한 강건한 알고리즘임을 알 수 있습니다. 특히 $m_L = 1/\ln(M)$과 $M_{\max0} = 2 \cdot M$이라는 간단한 규칙으로 대부분의 경우 우수한 성능을 달성할 수 있습니다.
이 섹션에서는 HNSW 알고리즘의 이론적 복잡도를 엄밀하게 분석합니다. 탐색 복잡도, 구축 복잡도, 메모리 비용이라는 세 가지 측면에서 알고리즘의 효율성을 수학적으로 증명하고 실증적으로 검증합니다.

## 탐색 복잡도의 이론적 증명

탐색 복잡도 분석은 정확한 들로네 그래프를 구축한다는 가정 하에서 엄밀하게 수행될 수 있습니다. 들로네 그래프는 탐욕적 그래프 순회의 결과가 항상 최근접 이웃이 되도록 보장하는 이론적으로 이상적인 구조입니다. 이러한 가정 하에서, 어떤 층에서 가장 가까운 요소를 찾았고 다음 하위 층으로 내려갔다고 가정합니다.

### 층별 탐색 단계의 상수 경계

핵심적인 수학적 통찰은 층에서 가장 가까운 요소를 찾기 전까지의 평균 단계 수가 상수로 제한된다는 것입니다. 이를 이해하기 위해서는 층의 구조가 데이터 요소들의 공간적 위치와 상관관계가 없다는 점을 인식해야 합니다. 층 배정은 순전히 확률적으로 이루어지므로, 그래프를 순회할 때 다음 노드가 상위 층에 속할 고정된 확률 $p = \exp(-m_L)$이 존재합니다.

이를 직관적으로 설명하면, 층을 순회하면서 각 단계마다 "상위 층으로 올라가는 출구"를 만날 확률이 일정하다는 의미입니다. 마치 미로를 탐색하면서 각 방마다 일정한 확률로 출구가 있는 것과 유사합니다. 중요한 점은 층에서의 탐색이 항상 상위 층에 속하는 요소에 도달하기 전에 종료된다는 것입니다. 만약 그렇지 않다면, 상위 층에서의 탐색이 다른 요소에서 멈췄을 것이기 때문입니다.

이러한 논리를 수학적으로 표현하면, $s$번째 단계에서 목표에 도달하지 못할 확률은 $\exp(-s \cdot m_L)$로 제한됩니다. 따라서 층에서의 기대 단계 수는 기하급수의 합으로 표현됩니다.

$$S = \sum_{s=1}^{\infty} s \cdot p \cdot (1-p)^{s-1} = \frac{1}{1-\exp(-m_L)}$$

이 값은 데이터셋 크기 $N$과 무관한 상수입니다. 예를 들어, $m_L = 1/\ln(16) \approx 0.36$일 때, $S \approx 3.3$으로 평균적으로 약 3-4단계만에 층에서 가장 가까운 요소를 찾게 됩니다.

### 들로네 그래프의 차수 제약과 전체 복잡도

들로네 그래프에서 노드의 평균 차수가 대규모 데이터셋의 극한에서 상수 $C$로 제한된다고 가정합니다. 이는 무작위 유클리드 데이터의 경우 성립하는 것으로 알려져 있으며, [들로네 그래프의 차수 분석 연구](https://arxiv.org/pdf/0709.0303v3)에서 이론적으로 증명되었습니다. 이 연구는 무작위로 분포된 점들에 대해 들로네 삼각분할을 수행할 때, 각 점이 연결되는 평균 이웃 수가 차원에 따라 결정되는 상수로 수렴함을 보였습니다.

이 가정이 성립하면, 층에서의 전체 평균 거리 계산 횟수는 $C \cdot S$로 제한되며, 이는 데이터셋 크기와 무관한 상수입니다. 구축 과정에서 최대 층 인덱스의 기댓값은 $O(\log(N))$으로 스케일링되므로, 전체 복잡도 스케일링은 다음과 같이 유도됩니다.

$$\text{총 거리 계산 횟수} = O(\log(N)) \times (C \cdot S) = O(\log(N))$$

이는 저차원 데이터셋에 대한 시뮬레이션 결과와 일치합니다.

### 근사 들로네 그래프와 백트래킹의 영향

실제 Hierarchical NSW에서는 고정된 이웃 수를 가진 근사적 에지 선택 휴리스틱을 사용하기 때문에 정확한 들로네 그래프 가정이 위배됩니다. 이로 인해 지역 최소값(local minimum)에 갇힐 가능성이 생기며, 이를 피하기 위해 탐욕적 탐색 알고리즘은 0층에서 백트래킹 절차를 사용합니다.

백트래킹은 탐색이 막다른 길에 도달했을 때 이전 단계로 돌아가 다른 경로를 시도하는 과정입니다. 이는 마치 미로에서 막힌 길을 만났을 때 되돌아가서 다른 갈림길을 선택하는 것과 유사합니다. 시뮬레이션 결과는 적어도 저차원 데이터($d=4$)의 경우, 고정된 재현율을 얻기 위해 필요한 $ef$ 파라미터(백트래킹 중 최소 홉 수를 통해 복잡도를 결정)가 데이터셋 크기 증가에 따라 포화된다는 것을 보여줍니다.

![저차원 데이터에서 ef 파라미터의 포화](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/10.png)

위 그림은 4차원 무작위 벡터 데이터에 대해 고정된 정확도를 얻기 위해 필요한 $ef$ 파라미터가 데이터셋 크기에 대해 로그적으로 증가함을 보여줍니다. 이는 백트래킹 복잡도가 최종 복잡도에 대한 가산 항이므로, 실증 데이터에서 알 수 있듯이 들로네 그래프 근사의 부정확성이 스케일링을 변경하지 않는다는 것을 의미합니다.

### 고차원 데이터에서의 이론적 한계

들로네 그래프 근사의 복원력에 대한 이러한 실증적 조사는 들로네 그래프 에지의 평균 개수가 데이터셋과 무관하다는 것을 전제로 합니다. 이는 Hierarchical NSW에서 고정된 연결 수로 에지가 얼마나 잘 근사되는지를 입증하기 위해 필요합니다. 그러나 들로네 그래프의 평균 차수는 차원성에 따라 지수적으로 스케일링됩니다.

구체적으로, $d$차원 공간에서 들로네 그래프의 평균 차수는 대략 $O(2^d)$로 증가합니다. 이는 고차원 데이터(예: $d=128$)의 경우 앞서 언급한 조건을 만족하려면 극도로 큰 데이터셋이 필요하다는 것을 의미하며, 이러한 실증적 조사를 실행 불가능하게 만듭니다. 예를 들어, 128차원 공간에서는 들로네 그래프의 평균 차수가 $2^{128}$에 가까운 값이 될 수 있으며, 이를 고정된 $M$ 값(예: 16)으로 근사하는 것의 타당성을 실증적으로 검증하려면 천문학적 크기의 데이터셋이 필요합니다.

따라서 들로네 그래프 근사의 복원력이 고차원 공간으로 일반화되는지 확인하기 위해서는 추가적인 분석적 증거가 필요합니다. 이는 현재 이론적으로 완전히 해결되지 않은 문제로 남아 있으며, 실제 응용에서는 실증적 성능 평가에 의존하게 됩니다.

## 구축 복잡도의 분석

구축은 모든 요소를 반복적으로 삽입하여 수행되며, 요소의 삽입은 단순히 서로 다른 층에서의 K-ANN 탐색 시퀀스와 그에 따른 휴리스틱 사용(고정된 $ef^{\text{Construction}}$에서 고정된 복잡도를 가짐)으로 이루어집니다.

요소가 추가될 평균 층 수는 $m_L$에 의존하는 상수입니다. 이는 다음과 같이 계산됩니다.

$$E[l+1] = E[-\ln(\text{unif}(0,1)) \cdot m_L] + 1 = m_L + 1 \quad \text{...(1)}$$

이 수식을 단계별로 이해해봅시다. 먼저 $\text{unif}(0,1)$은 0과 1 사이의 균등 분포에서 추출한 무작위 값입니다. 이 값에 자연로그를 취하고 음수를 곱하면 지수 분포를 얻게 됩니다. 이는 확률론에서 잘 알려진 변환으로, 균등 분포를 지수 분포로 변환하는 표준적인 방법입니다.

지수 분포의 기댓값은 그 파라미터의 역수이므로, $-\ln(\text{unif}(0,1))$의 기댓값은 1입니다. 여기에 $m_L$을 곱하면 기댓값이 $m_L$이 되고, 1을 더하면 최종적으로 $m_L + 1$이 됩니다. 이는 각 요소가 평균적으로 $m_L + 1$개의 층에 존재한다는 의미입니다.

예를 들어, $M=16$일 때 최적 $m_L = 1/\ln(16) \approx 0.36$이므로, 평균적으로 각 요소는 약 1.36개의 층에 존재합니다. 이는 대부분의 요소가 0층에만 존재하고, 일부만 상위 층에 존재한다는 것을 의미합니다.

따라서 삽입 복잡도 스케일링은 탐색과 동일하며, 이는 적어도 상대적으로 저차원인 데이터셋의 경우 구축 시간이 $O(N \cdot \log(N))$으로 스케일링됨을 의미합니다. 전체 데이터셋을 구축하려면 $N$개의 요소를 삽입해야 하고, 각 삽입은 평균적으로 $m_L + 1$개의 층에서 탐색을 수행하며, 각 층에서의 탐색은 $O(\log(N))$ 복잡도를 가지므로:

$$\text{구축 복잡도} = N \times (m_L + 1) \times O(\log(N)) = O(N \cdot \log(N))$$

이는 정렬 알고리즘의 최적 복잡도와 동일하며, 대규모 데이터셋에 대해서도 효율적인 인덱스 구축이 가능함을 의미합니다.

## 메모리 비용의 정량적 분석

Hierarchical NSW의 메모리 소비는 주로 그래프 연결의 저장에 의해 정의됩니다. 요소당 연결 수는 0층의 경우 $M_{\max0}$이고 다른 모든 층의 경우 $M_{\max}$입니다. 따라서 요소당 평균 메모리 소비는 다음과 같이 계산됩니다.

$$\text{메모리/요소} = (M_{\max0} + m_L \cdot M_{\max}) \times \text{bytes\_per\_link}$$

이 수식을 구체적으로 분석해봅시다. 각 요소는 0층에 반드시 존재하므로 $M_{\max0}$개의 연결을 가집니다. 추가로, 평균적으로 $m_L$개의 상위 층에 존재하며, 각 상위 층에서 $M_{\max}$개의 연결을 가지므로 총 $m_L \cdot M_{\max}$개의 추가 연결이 필요합니다.

최대 총 요소 수를 약 40억 개로 제한하면, 연결을 저장하기 위해 4바이트 부호 없는 정수를 사용할 수 있습니다. 이는 각 연결이 다른 요소의 인덱스를 가리키는데, 40억($2^{32}$)까지의 인덱스를 표현하려면 4바이트면 충분하기 때문입니다.

테스트 결과 일반적인 최적에 가까운 $M$ 값은 보통 6에서 48 사이에 있습니다. 이는 인덱스의 일반적인 메모리 요구사항(데이터 크기 제외)이 객체당 약 60-450바이트임을 의미하며, 이는 시뮬레이션과 잘 일치합니다.

구체적인 예를 들어봅시다. $M=16$, $M_{\max0}=32$, $m_L=0.36$인 경우:

$$\text{메모리/요소} = (32 + 0.36 \times 16) \times 4 = (32 + 5.76) \times 4 \approx 151 \text{ 바이트}$$

이는 각 요소가 약 150바이트의 메모리를 사용한다는 의미입니다. 1백만 개의 요소를 저장하면 약 150MB의 메모리가 필요하며, 이는 현대 시스템에서 충분히 관리 가능한 수준입니다.

더 작은 $M$ 값(예: $M=6$)을 사용하면 메모리 사용량이 약 60바이트/객체로 감소하지만, 탐색 성능이 다소 저하될 수 있습니다. 반대로 더 큰 $M$ 값(예: $M=48$)을 사용하면 메모리 사용량이 약 450바이트/객체로 증가하지만, 특히 고차원 데이터나 높은 재현율이 필요한 경우 탐색 성능이 향상됩니다.

이러한 메모리 효율성은 HNSW가 실용적인 응용에서 사용 가능한 중요한 이유 중 하나입니다. 예를 들어, 제품 양자화 기반 방법들은 벡터 자체를 압축하여 메모리를 절약하지만, HNSW는 그래프 구조만으로도 효율적인 메모리 사용을 달성하면서 일반적인 메트릭 공간에서 작동할 수 있다는 장점을 가집니다.
이 섹션에서는 C++로 구현된 HNSW 알고리즘의 실제 성능을 다양한 측면에서 평가합니다. 구현은 Non Metric Space Library(nmslib) 위에서 이루어졌으며, 이미 기능적인 NSW 구현("sw-graph"라는 이름으로)을 포함하고 있었습니다. 성능 최적화를 위해 라이브러리의 몇 가지 제약을 극복하고자 커스텀 거리 함수와 C 스타일 메모리 관리를 사용했는데, 이는 불필요한 암묵적 주소 지정을 피하고 그래프 순회 중 효율적인 하드웨어 및 소프트웨어 프리페칭을 가능하게 합니다.

## 성능 비교의 복잡성과 접근 방법

K-ANNS 알고리즘의 성능을 비교하는 것은 새로운 알고리즘과 구현이 지속적으로 등장하면서 최신 기술이 끊임없이 변화하기 때문에 간단하지 않은 작업입니다. 이 연구에서는 오픈소스 구현이 있는 유클리드 공간에서의 최고 알고리즘들과의 비교에 집중했습니다. 논문에서 제시된 HNSW 알고리즘의 구현도 오픈소스 nmslib 라이브러리의 일부로 배포되며, 점진적 인덱스 구축을 지원하는 메모리 효율적인 헤더 전용 C++ 외부 버전도 함께 제공됩니다.

비교 섹션은 네 부분으로 구성됩니다. 첫째, 기본 NSW와의 비교를 통해 계층적 구조의 개선 효과를 검증합니다. 둘째, 유클리드 공간에서 최신 알고리즘들과의 비교를 수행합니다. 셋째, NSW가 실패했던 일반 메트릭 공간에서의 테스트를 재실행합니다. 넷째, 대규모 200M SIFT 데이터셋에서 최신 PQ 알고리즘들과 비교합니다.

## 기본 NSW와의 성능 비교

기본 NSW 알고리즘 구현으로는 nmslib 1.1의 "sw-graph"를 사용했는데, 이는 이전 연구에서 테스트된 구현에 비해 약간 업데이트된 버전입니다. 이를 통해 속도와 알고리즘 복잡도(거리 계산 횟수로 측정)의 개선을 입증합니다.

![HNSW와 NSW의 성능 비교](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/12.png)

위 그림 (a)는 Core i5 2400 CPU에서 $d=4$ 무작위 하이퍼큐브 데이터에 대한 10-NN 탐색에서 HNSW와 기본 NSW 알고리즘을 비교한 결과입니다. HNSW는 데이터셋 탐색 중 훨씬 적은 거리 계산을 사용하며, 특히 높은 재현율에서 그 차이가 두드러집니다. 이는 계층적 구조가 탐색 효율성을 크게 향상시킨다는 것을 보여줍니다.

그림 (b)는 $d=8$ 무작위 하이퍼큐브 데이터셋에서 10-NN 탐색의 스케일링을 0.95 재현율로 고정하여 비교한 결과입니다. HNSW가 이 설정에서 로그보다 나쁘지 않은 복잡도 스케일링을 가지며 모든 데이터셋 크기에서 NSW를 능가함을 명확히 보여줍니다. 데이터셋 크기가 증가함에 따라 HNSW의 거리 계산 횟수는 로그적으로 증가하는 반면, NSW는 더 빠르게 증가합니다.

그림 (c)는 절대 시간 측면에서의 성능 우위를 보여주는데, 개선된 알고리즘 구현으로 인해 거리 계산 횟수 감소보다 더 큰 성능 향상을 달성합니다. 이는 이론적 복잡도 개선이 실제 실행 시간 개선으로 효과적으로 전환됨을 의미합니다.

이러한 결과는 앞서 이론적으로 분석한 로그 복잡도가 실제로 달성되고 있음을 실증적으로 확인시켜 줍니다. 특히 저차원 데이터에서 계층적 구조의 이점이 명확하게 나타나며, 이는 NSW의 멱법칙 복잡도 문제를 성공적으로 해결했음을 보여줍니다.

## 유클리드 공간에서의 최신 알고리즘과의 비교

유클리드 공간에서의 비교는 ann-benchmark 테스트베드를 사용하여 FLANN 1.8.4, Annoy, VP-tree, FALCONN 1.2와 같은 최신 알고리즘들을 대상으로 수행되었습니다. 테스트 시스템은 4X Xeon E5-4650 v2 CPU를 사용했습니다.

![유클리드 공간에서의 성능 비교](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/13.png)

위 그림은 여러 데이터셋에서의 성능 비교 결과를 보여줍니다. SIFT(1M, $d=128$), GloVe(1.2M, $d=100$), CoPhIR(2M, $d=272$), Random(30M, $d=4$), DEEP(1M, $d=96$), MNIST(60k, $d=784$) 데이터셋에서 테스트가 수행되었습니다.

결과는 HNSW가 고차원 데이터에서 경쟁 알고리즘들을 큰 차이로 능가함을 보여줍니다. 특히 SIFT, GloVe, CoPhIR와 같은 실제 고차원 데이터셋에서 HNSW는 동일한 재현율에서 다른 알고리즘들보다 수 배에서 수십 배 빠른 쿼리 시간을 달성합니다. 이는 계층적 구조와 이웃 선택 휴리스틱이 고차원 공간에서 특히 효과적임을 시사합니다.

저차원 데이터인 Random($d=4$) 데이터셋에서는 HNSW가 높은 재현율에서 Annoy보다 약간 빠른 성능을 보입니다. 이는 저차원에서도 HNSW가 경쟁력 있는 성능을 유지함을 의미하며, 앞서 기본 NSW와의 비교에서 본 개선이 다른 최신 알고리즘들과 비교해도 유효함을 확인시켜 줍니다.

MNIST 데이터셋에서도 HNSW는 우수한 성능을 보이며, 특히 높은 재현율 영역에서 다른 알고리즘들과의 격차가 더욱 벌어집니다. 이는 HNSW가 다양한 차원과 데이터 특성에서 강건한 성능을 제공함을 보여줍니다.

## 일반 메트릭 공간에서의 성능 검증

일반 메트릭 공간에서의 테스트는 이전 연구에서 NSW가 성능 저하를 겪었던 데이터셋들에 대해 재실행되었습니다. VP-tree, 순열 기법(permutation techniques), NNDescent와 비교가 이루어졌으며, Wiki-sparse(4M), Wiki-8(2M, $d=8$), Wiki-128(2M, $d=128$), ImageNet(1M, $d=272$), DNA(1M) 데이터셋이 사용되었습니다.

![일반 메트릭 공간에서의 성능 비교](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/14.png)

위 그림은 일반 메트릭 공간에서의 성능 비교 결과를 보여줍니다. HNSW는 NSW의 성능을 크게 개선하며 테스트된 모든 데이터셋에서 선두를 차지합니다. 가장 인상적인 개선은 가장 낮은 차원인 Wiki-8 데이터셋에서 나타나는데, 거의 3차수(orders of magnitude)에 달하는 성능 향상을 보입니다.

이는 앞서 이론적으로 설명한 계층적 구조의 이점이 저차원 데이터에서 특히 크게 나타난다는 것을 실증적으로 확인시켜 줍니다. NSW가 저차원 데이터에서 멱법칙 복잡도로 인해 심각한 성능 저하를 겪었던 반면, HNSW는 로그 복잡도를 달성하여 이 문제를 해결했습니다.

Wiki-128과 ImageNet과 같은 중고차원 데이터셋에서도 HNSW는 일관되게 우수한 성능을 보이며, 이는 알고리즘이 다양한 차원 범위에서 강건함을 의미합니다. DNA 데이터셋과 같은 비유클리드 메트릭 공간에서도 HNSW는 효과적으로 작동하여, 일반 메트릭 공간에 대한 적용 가능성을 입증합니다.

## 제품 양자화 기반 알고리즘과의 비교

대규모 데이터셋에서의 성능을 검증하기 위해 Facebook Faiss 라이브러리와 200M SIFT 부분집합에서 비교가 수행되었습니다. 테스트 시스템은 128GB RAM을 가진 4X Xeon E5-4650 v2를 사용했습니다.

![제품 양자화 알고리즘과의 비교](/assets/2026-01-29-efficient-and-robust-approximate-nearest-neighbor-search-using-hierarchical-navigable-small-world-graphs/15.png)

| 알고리즘         | 메모리 사용량 | 구축 시간 | 탐색 속도 | 정확도    |
| ---------------- | ------------- | --------- | --------- | --------- |
| Faiss (고품질)   | 23.5-30GB     | 11-12시간 | 기준      | 기준      |
| Faiss (표준품질) | 23.5-30GB     | 11-12시간 | 기준      | 낮음      |
| HNSW (고품질)    | 64GB          | 5.6시간   | 매우 빠름 | 매우 높음 |
| HNSW (표준품질)  | 64GB          | 42분      | 빠름      | 높음      |

위 그림과 표는 제품 양자화 기반 알고리즘과의 비교 결과를 보여줍니다. HNSW는 훨씬 더 많은 RAM(64GB 대 23.5-30GB)을 필요로 함에도 불구하고, 훨씬 높은 정확도와 막대한 탐색 속도 우위를 달성합니다.

인덱스 구축 시간도 HNSW가 더 빠른데, 고품질 인덱스의 경우 5.6시간 대 11-12시간, 표준 품질의 경우 42분 대 11-12시간입니다. 이는 HNSW의 구축 알고리즘이 효율적이며 병렬화가 잘 되어 있음을 보여줍니다.

메모리 사용량 측면에서 HNSW는 더 많은 RAM을 필요로 하지만, 이는 그래프 구조를 명시적으로 저장하기 때문입니다. 제품 양자화 방법은 벡터를 압축하여 메모리를 절약하지만, 이로 인해 정확도가 저하되고 탐색 속도도 느려집니다. HNSW는 메모리를 더 사용하는 대신 훨씬 높은 정확도와 속도를 제공하는 트레이드오프를 선택한 것입니다.

이러한 결과는 HNSW가 메모리가 충분한 환경에서 대규모 데이터셋에 대해 최고 수준의 성능을 제공할 수 있음을 보여줍니다. 특히 높은 정확도가 요구되는 응용에서 HNSW는 제품 양자화 기반 방법들보다 훨씬 우수한 선택이 될 수 있습니다.

## 성능 평가의 종합적 의미

전체적인 성능 평가 결과는 HNSW가 다양한 데이터 특성과 차원에서 강건하고 우수한 성능을 제공함을 보여줍니다. 기본 NSW와의 비교는 계층적 구조의 이론적 이점이 실제로 달성되고 있음을 확인시켜 주며, 유클리드 공간과 일반 메트릭 공간에서의 비교는 HNSW가 최신 알고리즘들을 능가하는 실용적 가치를 입증합니다.

특히 주목할 만한 점은 HNSW가 저차원에서 고차원까지, 유클리드 공간에서 일반 메트릭 공간까지, 소규모에서 대규모 데이터셋까지 일관되게 우수한 성능을 보인다는 것입니다. 이러한 강건성은 HNSW를 다양한 실제 응용에 적용할 수 있게 만드는 중요한 특성입니다.

구현 측면에서도 HNSW는 효율적인 병렬 구축과 점진적 인덱스 업데이트를 지원하여 실용성을 높입니다. 오픈소스로 배포되어 연구 커뮤니티와 산업계에서 쉽게 접근하고 활용할 수 있다는 점도 중요한 기여입니다.
이 섹션에서는 HNSW 알고리즘의 실제 성능을 다양한 데이터셋과 비교 대상을 통해 종합적으로 평가합니다. 평가는 유클리드 공간에서의 벡터 데이터셋 비교와 일반 메트릭 공간에서의 비교, 그리고 대규모 데이터셋에서 제품 양자화 기반 알고리즘과의 비교로 구성됩니다.

## 유클리드 공간에서의 성능 비교

유클리드 공간에서의 비교는 ann-benchmark 테스트베드를 활용하여 수행되었습니다. 이 테스트베드는 파이썬 바인딩을 사용하여 알고리즘들을 순차적으로 실행하며, 무작위로 추출된 1천 개의 쿼리에 대해 K-ANN 탐색을 수행하여 재현율과 평균 탐색 시간을 측정합니다. 테스트 시스템은 128GB RAM을 탑재한 4X Xeon E5-4650 v2 Debian OS 시스템이며, 모든 테스트는 단일 스레드 환경에서 수행되었습니다.

비교 대상 알고리즘으로는 nmslib 1.1의 기본 NSW 알고리즘("sw-graph"), FLANN 1.8.4, Annoy, VP-tree, FALCONN 1.2가 선정되었습니다. FLANN은 OpenCV에 내장된 여러 알고리즘을 포함하는 라이브러리로, 자동 튜닝 절차를 통해 최적 파라미터를 추론했습니다. Annoy는 무작위 투영 트리 포레스트 기반의 알고리즘이며, VP-tree는 메트릭 가지치기를 사용하는 일반 메트릭 공간 알고리즘입니다. FALCONN은 코사인 유사도 데이터를 위한 효율적인 LSH 알고리즘입니다.

테스트에 사용된 데이터셋의 특성은 다음과 같습니다. SIFT 데이터셋은 1백만 개의 128차원 이미지 특징 벡터로 구성되며 L2 거리를 사용합니다. GloVe 데이터셋은 트윗에서 학습된 1.2백만 개의 100차원 단어 임베딩으로 코사인 유사도를 사용합니다. CoPhIR 데이터셋은 2백만 개의 272차원 MPEG-7 특징 벡터입니다. Random 데이터셋은 하이퍼큐브 내 30백만 개의 4차원 무작위 벡터입니다. DEEP 데이터셋은 10억 개 규모 딥 이미지 특징 데이터셋의 1백만 개 부분집합으로 96차원입니다. MNIST 데이터셋은 60,000개의 784차원 손글씨 숫자 이미지입니다.

성능 비교 결과는 SIFT, GloVe, DEEP, CoPhIR 데이터셋에서 Hierarchical NSW가 경쟁 알고리즘들을 큰 차이로 능가함을 보여줍니다. 특히 고차원 실제 데이터에서 HNSW의 우위가 두드러지며, 동일한 재현율에서 다른 알고리즘들보다 훨씬 빠른 쿼리 시간을 달성합니다. 저차원 데이터인 4차원 무작위 벡터 데이터셋에서는 HNSW가 높은 재현율에서 Annoy보다 약간 빠른 성능을 보이며, 다른 알고리즘들을 강력하게 능가합니다.

## 일반 메트릭 공간에서의 성능 검증

일반 메트릭 공간에서의 테스트는 이전 연구에서 기본 NSW 알고리즘이 성능 저하를 겪었던 데이터셋들에 대해 재실행되었습니다. 이 테스트는 nmslib의 내장 테스트 시스템을 사용했으며, VP-tree, 순열 기법(NAPP와 브루트포스 필터링), 기본 NSW 알고리즘, NNDescent로 생성된 근접 그래프와 비교가 이루어졌습니다. 각 데이터셋에 대해 NSW 또는 NNDescent 중 더 나은 성능을 보인 구조의 결과가 포함되었습니다.

테스트 데이터셋은 다음과 같습니다. Wiki-sparse는 GENSIM을 통해 생성된 4백만 개의 TF-IDF 벡터로 희소 코사인 거리를 사용합니다. Wiki-8은 Wiki-sparse의 TF-IDF 벡터로부터 생성된 2백만 개의 8차원 토픽 히스토그램으로 Jensen-Shannon 발산을 사용합니다. Wiki-128은 동일한 방식으로 생성된 2백만 개의 128차원 토픽 히스토그램입니다. ImageNet은 LSVRC-2014에서 SQFD 거리로 추출된 1백만 개의 272차원 시그니처입니다. DNA는 Human Genome에서 샘플링된 1백만 개의 DNA 서열로 Levenshtein 거리를 사용합니다.

성능 비교 결과는 Hierarchical NSW가 NSW의 성능을 크게 개선하며 테스트된 모든 데이터셋에서 선두를 차지함을 보여줍니다. 가장 인상적인 개선은 가장 낮은 차원인 Wiki-8 데이터셋에서 나타나는데, 거의 3차수에 달하는 성능 향상을 보입니다. 이는 계층적 구조가 저차원 데이터에서 특히 효과적임을 실증적으로 확인시켜 줍니다. Wiki-128과 ImageNet과 같은 중고차원 데이터셋에서도 HNSW는 일관되게 우수한 성능을 보이며, DNA 데이터셋과 같은 비유클리드 메트릭 공간에서도 효과적으로 작동합니다.

## 제품 양자화 기반 알고리즘과의 비교

제품 양자화 K-ANNS 알고리즘들은 저장된 데이터를 효율적으로 압축하여 적은 RAM 사용량으로 현대 CPU에서 밀리초 단위의 탐색 시간을 달성할 수 있어 10억 규모 데이터셋에서 최신 기술로 간주됩니다. Hierarchical NSW의 성능을 PQ 알고리즘들과 비교하기 위해 Facebook Faiss 라이브러리를 기준으로 사용했습니다. Faiss는 최신 PQ 알고리즘 구현을 포함하는 라이브러리로, OpenBLAS 백엔드로 컴파일되었습니다.

테스트는 128GB RAM을 탑재한 4X Xeon E5-4650 v2 서버에서 1B SIFT 데이터셋의 200M 부분집합에 대해 수행되었습니다. ann-benchmark 테스트베드는 32비트 부동소수점 형식에 의존하여 데이터 저장만으로 100GB 이상이 필요하므로 이 실험에는 적합하지 않았습니다. Faiss PQ 알고리즘의 결과를 얻기 위해 Faiss 위키의 파라미터를 사용한 내장 스크립트를 활용했습니다. Hierarchical NSW의 경우 메모리 사용량이 적고 단순한 비벡터화 정수 거리 함수를 사용하며 점진적 인덱스 구축을 지원하는 특별한 빌드를 nmslib 외부에서 사용했습니다.

구축 파라미터와 성능 특성은 다음과 같습니다. Hierarchical NSW의 첫 번째 설정은 $M=16$, $e_f^{\text{Construction}}=500$으로 5.6시간의 구축 시간과 64GB의 피크 메모리를 사용합니다. 두 번째 설정은 $M=16$, $e_f^{\text{Construction}}=40$으로 42분의 구축 시간과 64GB의 피크 메모리를 사용합니다. Faiss의 첫 번째 설정은 OPQ64, IMI2x14, PQ64로 12시간의 구축 시간과 30GB의 피크 메모리를 사용합니다. 두 번째 설정은 OPQ32, IMI2x14, PQ32로 11시간의 구축 시간과 23.5GB의 피크 메모리를 사용합니다.

성능 비교 결과는 Hierarchical NSW가 훨씬 더 많은 RAM을 필요로 함에도 불구하고 훨씬 높은 정확도와 막대한 탐색 속도 우위를 달성함을 보여줍니다. 인덱스 구축 시간도 HNSW가 더 빠른데, 고품질 인덱스의 경우 5.6시간 대 12시간, 표준 품질의 경우 42분 대 11시간입니다. 그림의 인셋은 Hierarchical NSW의 쿼리 시간 대 데이터셋 크기의 스케일링을 보여주는데, 데이터셋의 상대적으로 높은 차원성으로 인해 순수한 로그 스케일링에서 약간 벗어남을 확인할 수 있습니다.

이러한 결과는 HNSW가 메모리가 충분한 환경에서 대규모 데이터셋에 대해 최고 수준의 성능을 제공할 수 있음을 보여줍니다. 제품 양자화 방법은 벡터를 압축하여 메모리를 절약하지만 정확도가 저하되고 탐색 속도도 느려지는 반면, HNSW는 메모리를 더 사용하는 대신 훨씬 높은 정확도와 속도를 제공하는 트레이드오프를 선택합니다.
이 섹션에서는 HNSW 알고리즘의 전반적인 의의와 향후 발전 방향을 논의합니다. 앞서 상세히 분석한 계층적 구조와 이웃 선택 휴리스틱을 통해 HNSW는 기본 NSW 구조의 여러 중요한 문제점들을 극복하고 K-ANN 탐색 분야의 최신 기술을 크게 발전시켰습니다.

## HNSW의 핵심 성과와 강건성

HNSW는 다양한 데이터셋에서 탁월한 성능을 보이며 명확한 선두주자로 자리매김했습니다. 특히 고차원 데이터의 경우 오픈소스 경쟁 알고리즘들을 큰 차이로 능가합니다. 이전 NSW 알고리즘이 수 차수(orders of magnitude) 차이로 뒤처졌던 데이터셋에서도 HNSW는 1위를 차지할 수 있었습니다. 이는 계층적 분해와 스마트 이웃 선택이라는 두 가지 핵심 혁신이 실제로 효과적임을 입증합니다.

HNSW는 연속적인 점진적 인덱싱을 지원하며, k-NN 그래프와 상대 이웃 그래프의 근사를 얻는 효율적인 방법으로도 사용될 수 있습니다. 이러한 그래프들은 인덱스 구축의 부산물로 자연스럽게 생성됩니다. 앞서 설명한 이웃 선택 휴리스틱은 본질적으로 상대 이웃 그래프의 근사를 생성하며, 이는 다양한 그래프 기반 응용에 활용될 수 있습니다.

알고리즘의 강건성은 실용적 응용에서 매우 매력적인 특징입니다. HNSW는 일반화된 메트릭 공간에서 적용 가능하며, 테스트된 모든 데이터셋에서 최고의 성능을 발휘하여 특정 문제에 대한 최적 알고리즘을 복잡하게 선택할 필요를 없앱니다. 이는 실무자들에게 매우 중요한 장점입니다. 많은 경우 데이터의 특성을 사전에 정확히 파악하기 어렵고, 여러 알고리즘을 시도해보는 것은 시간과 자원이 많이 소요되기 때문입니다.

알고리즘의 강건성이 중요한 이유는 데이터가 척도에 따라 서로 다른 유효 차원성을 가진 복잡한 구조를 가질 수 있기 때문입니다. 예를 들어, 데이터셋이 고차원 큐브를 무작위로 채우는 곡선 위의 점들로 구성될 수 있습니다. 이 경우 대규모 척도에서는 고차원이지만 소규모 척도에서는 저차원입니다. 이러한 데이터셋에서 효율적인 탐색을 수행하려면 근사 최근접 이웃 알고리즘이 고차원과 저차원 모두에서 잘 작동해야 합니다. HNSW의 계층적 구조는 바로 이러한 다중 척도 특성을 자연스럽게 처리할 수 있습니다. 상위 층은 대규모 구조를 포착하고, 하위 층은 세밀한 구조를 다루기 때문입니다.

## 향후 개선 방향

HNSW 접근법의 효율성과 적용 가능성을 더욱 높일 수 있는 여러 방법들이 있습니다. 인덱스 구축에 강하게 영향을 미치는 의미 있는 파라미터가 하나 남아 있는데, 바로 층당 추가되는 연결 수 $M$입니다. 잠재적으로 이 파라미터는 다양한 휴리스틱을 사용하여 직접 추론될 수 있습니다. 앞서 분석에서 $M$의 최적 값이 데이터 특성과 요구되는 재현율에 따라 달라진다는 것을 확인했습니다. 자동 파라미터 선택 메커니즘은 사용자가 데이터셋의 샘플을 제공하면 최적의 $M$ 값을 추정하는 방식으로 구현될 수 있습니다.

전체 1B SIFT와 1B DEEP 데이터셋에서 HNSW를 비교하고 요소 업데이트 및 제거에 대한 지원을 추가하는 것도 흥미로운 연구 방향입니다. 현재 구현은 주로 삽입 연산에 초점을 맞추고 있지만, 실제 응용에서는 데이터가 동적으로 변화하는 경우가 많습니다. 요소 삭제는 그래프의 연결성을 유지하면서 노드를 제거해야 하므로 기술적으로 더 복잡합니다. 삭제된 노드의 이웃들을 적절히 재연결하여 탐색 가능성을 보존하는 전략이 필요합니다.

## 분산 구현의 과제와 해결 방안

제안된 접근법의 명백한 단점 중 하나는 기본 NSW에 비해 분산 탐색 가능성을 잃는다는 것입니다. HNSW 구조에서 탐색은 항상 최상위 층에서 시작하므로, 상위 층 요소들의 집중화로 인해 이전 연구에서 설명된 것과 동일한 기법을 사용하여 구조를 분산시킬 수 없습니다. 기본 NSW는 여러 무작위 진입점에서 탐색을 시작할 수 있어 데이터를 여러 노드에 분산시키고 각 노드가 독립적으로 탐색을 수행할 수 있었습니다. 그러나 HNSW는 단일 진입점(최상위 층의 요소)에서 시작해야 하므로 이러한 단순한 분산 전략이 작동하지 않습니다.

클러스터 노드 간 데이터 분할과 같은 간단한 해결 방법을 사용하여 구조를 분산시킬 수 있지만, 이 경우 시스템의 총 병렬 처리량이 컴퓨터 노드 수에 따라 잘 확장되지 않습니다. 각 쿼리가 여전히 단일 진입점을 통과해야 하므로 병목 현상이 발생하기 때문입니다.

그러나 이 특정 구조를 분산시킬 수 있는 다른 가능한 방법들이 알려져 있습니다. HNSW는 이념적으로 잘 알려진 1차원 정확 탐색 확률적 스킵 리스트 구조와 매우 유사하므로, 구조를 분산시키기 위해 동일한 기법을 사용할 수 있습니다. [스킵 리스트](https://dl.acm.org/doi/10.1145/78973.78977)는 확률적 계층화를 통해 효율적인 탐색을 제공하는 데이터 구조로, 분산 환경에서의 구현 방법이 잘 연구되어 있습니다.

특히 [레인보우 스킵 그래프](https://dl.acm.org/citation.cfm?id=1109557.1109591)와 같은 분산 스킵 리스트 변형은 내결함성과 로그 복잡도를 유지하면서 분산 구현을 가능하게 합니다. 이러한 기법을 HNSW에 적용하면 로그 확장성과 노드에 대한 이상적으로 균일한 부하로 인해 기본 NSW에 비해 더 나은 분산 성능으로 이어질 수 있습니다. 스킵 리스트 기반 분산 전략은 각 노드가 여러 층의 부분집합을 담당하도록 하여, 쿼리가 여러 노드를 거치면서 점진적으로 목표에 접근하는 방식으로 작동합니다. 이는 단일 진입점 문제를 우회하면서도 계층적 구조의 이점을 유지할 수 있습니다.

## 연구의 종합적 의의

HNSW는 근사 최근접 이웃 탐색 분야에서 이론적 혁신과 실용적 성능을 모두 달성한 중요한 연구입니다. 계층적 분해를 통해 로그 복잡도를 달성하면서도 다양한 데이터 분포와 차원에서 강건한 성능을 보이는 방법을 제시했습니다. 이는 단순히 성능 개선에 그치지 않고, 그래프 기반 탐색 알고리즘의 설계 원칙에 대한 깊은 통찰을 제공합니다.

알고리즘의 강건성은 실무 응용에서 특히 가치가 있습니다. 데이터의 특성을 사전에 정확히 알기 어려운 상황에서도 HNSW는 일관되게 우수한 성능을 제공하므로, 복잡한 알고리즘 선택 과정 없이 신뢰할 수 있는 솔루션을 제공합니다. 이는 벡터 데이터베이스, 시맨틱 검색, 추천 시스템 등 다양한 실제 응용에서 HNSW가 널리 채택된 이유입니다.

향후 연구 방향으로는 자동 파라미터 선택, 동적 업데이트 지원, 분산 구현 등이 제시되었으며, 이들은 모두 알고리즘의 실용성을 더욱 높일 수 있는 중요한 개선 사항들입니다. 특히 스킵 리스트 기반 분산 전략은 대규모 시스템에서의 확장성을 크게 향상시킬 수 있는 유망한 방향입니다.

이 연구는 이론적 엄밀성과 실용적 효율성을 모두 갖춘 알고리즘 설계의 모범 사례를 보여줍니다. 작은 세계 네트워크 이론과 확률적 데이터 구조의 원리를 창의적으로 결합하여, 고차원 데이터 탐색이라는 어려운 문제에 대한 효과적인 해결책을 제시했습니다.
- - -
### References
* [Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs](https://arxiv.org/pdf/1603.09320v4)
* [hnswlib](https://github.com/nmslib/hnswlib)
