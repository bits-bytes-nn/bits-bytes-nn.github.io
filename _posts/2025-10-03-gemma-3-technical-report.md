---
layout: post
title: "Gemma 3 Technical Report"
date: 2025-03-25 15:52:34
author: "Google DeepMind"
categories: "Multimodal Learning"
tags: ["Alternating-Local-Global-Attention", "Pan-&-Scan-Image-Processing", "Long-Context-Adaptation", "Multimodal-Knowledge-Distillation", "Vision-Encoder-Token-Condensation", "RoPE-Positional-Embedding-Extension", "Grouped-Query-Attention", "Quantization-Aware-Training", "Efficient-Long-Context-Attention-Mechanism", "Multimodal-Safety-Evaluation-Framework"]
cover: /assets/images/default.jpg
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

대규모 언어 모델의 발전은 인공지능 분야에서 가장 중요한 연구 방향 중 하나로 자리 잡았습니다. 기존 언어 모델들은 텍스트 처리에 집중하면서 여러 가지 한계점을 노출했습니다. 특히 긴 컨텍스트 처리, 다국어 지원, 그리고 멀티모달 능력의 부족은 실용적인 AI 솔루션 개발을 제한하는 주요 장애물이었습니다. Google DeepMind 연구팀은 이러한 한계를 극복하고, 더욱 유연하고 강력한 언어 모델을 개발하고자 Gemma 3 프로젝트를 시작했습니다.

연구팀은 기존 모델들이 직면한 주요 기술적 도전 과제들을 해결하고자 했습니다. 메모리 효율성, 긴 컨텍스트 처리, 멀티모달 능력, 그리고 다국어 지원은 모두 현대 AI 시스템에서 핵심적인 요구사항입니다. 특히 소비자급 하드웨어에서도 실행 가능한 고성능 모델을 개발하는 것이 이 연구의 주요 동기였습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

Gemma 3의 핵심 혁신은 로컬-글로벌 어텐션 레이어의 혁신적인 교대 배치 방식입니다. 연구팀은 5개의 로컬 어텐션 레이어와 1개의 글로벌 어텐션 레이어를 번갈아 배치하는 새로운 아키텍처를 설계했습니다. 이 접근법은 메모리 효율성을 크게 개선하면서도 모델의 성능을 유지할 수 있게 해줍니다. 특히 128K 토큰까지의 긴 컨텍스트 처리를 가능하게 하는 혁신적인 방법론을 제시했습니다.

또 다른 주요 혁신은 멀티모달 능력의 통합입니다. SigLIP 비전 인코더를 활용하여 이미지를 256개의 토큰으로 압축하고, Pan & Scan 기법을 도입하여 다양한 해상도와 종횡비의 이미지를 효과적으로 처리할 수 있게 했습니다. 이는 텍스트와 이미지를 동시에 이해하고 처리할 수 있는 능력을 제공하며, 기존 모델들의 한계를 뛰어넘는 중요한 기술적 진보를 의미합니다.

#### 제안된 방법은 어떻게 구현되었습니까?

Gemma 3의 구현은 매우 정교한 훈련 전략을 포함합니다. 1B부터 27B까지 다양한 크기의 모델들을 개발했으며, 각 모델은 지식 증류 기법을 활용하여 훈련되었습니다. 특히 지시 조정(Instruction-Tuning) 과정에서는 다양한 보상 함수를 사용하여 모델의 성능을 최적화했습니다. BOND, WARM, WARP와 같은 강화학습 기법을 통해 수학, 코딩, 다국어 능력을 향상시켰습니다.

비전 인코더의 구현에서는 896×896 픽셀 해상도의 이미지를 처리할 수 있도록 설계했으며, Pan & Scan 알고리즘을 통해 다양한 종횡비의 이미지를 효과적으로 처리할 수 있게 했습니다. 특히 4×4 평균 풀링을 사용하여 출력을 256 토큰으로 압축하는 방식으로 계산 효율성을 확보했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?

Gemma 3는 대규모 언어 모델 분야에서 중요한 이정표를 제시했습니다. LMSYS Chatbot Arena에서 1338점의 Elo 점수를 달성하며, 27B 매개변수라는 상대적으로 작은 크기로도 최상위 성능을 입증했습니다. 특히 멀티모달 능력과 긴 컨텍스트 처리 능력을 추가하면서도 기존의 언어 이해 성능을 유지하거나 오히려 향상시켰다는 점에서 큰 의의가 있습니다.

이 연구는 단순한 기술적 혁신을 넘어 AI의 실용성과 접근성을 높이는 중요한 진전을 보여줍니다. 소비자급 하드웨어에서 실행 가능한 고성능 모델을 개발함으로써, AI 기술의 민주화에 기여했습니다. 또한 책임감 있는 AI 개발을 위한 포괄적인 안전성 평가 프레임워크를 제시하여, 기술 혁신과 사회적 책임 사이의 균형을 추구했다는 점에서 높이 평가될 수 있습니다.
- - -
Gemma 3는 Google DeepMind에서 개발한 최신 오픈 소스 언어 모델 패밀리로, 1억 개부터 270억 개 매개변수까지 다양한 크기로 제공되는 경량화된 멀티모달 모델입니다. 이 모델은 기존 Gemma 패밀리에 비전 이해 능력, 확장된 언어 지원, 그리고 최소 128K 토큰의 긴 컨텍스트 처리 능력을 새롭게 추가한 혁신적인 발전을 보여줍니다.

## 주요 기술적 혁신

Gemma 3의 가장 중요한 기술적 진보는 멀티모달 능력의 도입입니다. 이 모델은 맞춤형 SigLIP 비전 인코더와 통합되어 이미지를 소프트 토큰 시퀀스로 처리할 수 있습니다. 이러한 접근 방식은 [Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485v2)에서 제안된 멀티모달 학습 패러다임을 따르며, 언어 모델이 텍스트와 시각적 정보를 동시에 이해하고 처리할 수 있게 합니다.

비전 임베딩은 추론 비용을 줄이기 위해 고정된 크기인 256개의 벡터로 압축됩니다. 이는 계산 효율성과 성능 사이의 균형을 맞추는 중요한 설계 결정입니다. 인코더는 고정된 해상도에서 작동하며, [LLaVA](https://arxiv.org/pdf/2304.08485v2)에서 영감을 받은 Pan and Scan (P&S) 방법을 사용하여 유연한 해상도를 지원합니다.

## 아키텍처 개선사항

Gemma 3의 두 번째 주요 개선사항은 128K 토큰까지의 컨텍스트 크기 확장입니다. 이는 기존 모델들의 컨텍스트 제한을 크게 뛰어넘는 성과입니다. 그러나 긴 컨텍스트 처리에서 발생하는 KV 캐시의 메모리 폭발 문제를 해결하기 위해 혁신적인 아키텍처 설계를 도입했습니다.

핵심 아이디어는 로컬 어텐션 레이어와 글로벌 어텐션 레이어를 교대로 배치하는 것입니다. 구체적으로, 각 글로벌 레이어 사이에 여러 개의 로컬 레이어를 삽입하고, 로컬 레이어의 스팬을 1024 토큰으로 제한합니다. 이러한 설계에서는 글로벌 레이어만이 긴 컨텍스트에 어텐션을 수행하며, 글로벌 레이어 1개당 로컬 레이어 5개의 비율을 유지합니다.

이 접근법은 [Longformer](https://arxiv.org/pdf/2004.05150)에서 제안된 슬라이딩 윈도우 어텐션과 유사한 개념을 활용하면서도, 레이어 수준에서의 분리를 통해 더욱 효율적인 메모리 사용을 달성합니다. 로컬 어텐션은 인접한 토큰들 간의 세밀한 상호작용을 포착하고, 글로벌 어텐션은 전체 시퀀스에 걸친 장거리 의존성을 모델링합니다.

## 사전 훈련 및 지식 증류

Gemma 3의 사전 훈련 최적화 방법은 Gemma 2와 유사하지만, 아키텍처 설계의 변화에 맞춰 수정되었습니다. 모든 Gemma 3 모델은 [지식 증류](https://arxiv.org/pdf/1503.02531) 기법을 사용하여 훈련됩니다. 이는 큰 교사 모델의 지식을 작은 학생 모델로 전달하는 효과적인 방법으로, Geoffrey Hinton 등이 제안한 기법입니다.

지식 증류에서 학생 모델은 교사 모델이 생성하는 "소프트 타겟"을 학습합니다. 이는 단순히 정답 레이블을 학습하는 것보다 더 풍부한 정보를 제공합니다. 수학적으로, 증류 손실은 다음과 같이 표현됩니다.

\\[
L_{distill} = -\sum_i p_T(x_i) \log p_S(x_i)
\\]

여기서 \\(p_T(x_i)\\)는 교사 모델의 확률 분포이고, \\(p_S(x_i)\\)는 학생 모델의 확률 분포입니다.

모델은 Gemini 2.0과 동일한 토크나이저를 사용하며, 멀티링구얼 능력을 향상시키기 위해 데이터 혼합을 재검토했습니다. 이미지 이해 능력을 도입하면서도 기존의 텍스트 처리 성능을 유지하거나 향상시키는 것이 중요한 과제였습니다.

## 사후 훈련의 혁신

Gemma 3의 사후 훈련 접근법은 특히 주목할 만합니다. 수학, 추론, 채팅 능력 향상에 중점을 두면서, 동시에 긴 컨텍스트와 이미지 입력이라는 새로운 능력을 통합해야 했습니다. 이를 위해 새로운 사후 훈련 방법을 개발했으며, 이는 수학, 코딩, 채팅, 지시 따르기, 다국어 능력 등 모든 영역에서 성능 향상을 가져왔습니다.

결과적으로 Gemma3-4B-IT 모델은 Gemma2-27B-IT와 경쟁할 수 있는 성능을 보여주며, Gemma3-27B-IT는 다양한 벤치마크에서 Gemini-1.5-Pro와 비교할 만한 성능을 달성했습니다. 이는 모델 크기 대비 성능의 현저한 개선을 의미합니다.

## 실제 적용 예시

![레스토랑 영수증 분석 예시](/assets/2025-10-03-gemma-3-technical-report/0.png)

위 그림은 Gemma 3 27B IT 모델의 시각적 상호작용 예시를 보여줍니다. 이 예시에서 모델은 레스토랑 영수증 이미지를 분석하고 그 내용을 이해하여 적절한 응답을 생성할 수 있음을 보여줍니다. 이는 단순한 이미지 분류를 넘어서 복잡한 문서 이해와 정보 추출이 가능함을 시사합니다.

## 기술적 의의와 영향

Gemma 3는 오픈 소스 언어 모델 생태계에서 중요한 이정표를 제시합니다. 특히 소비자급 하드웨어에서 실행 가능하도록 설계된 점이 주목할 만합니다. 스마트폰, 노트북, 고급 GPU 등에서 실행할 수 있도록 최적화되어 있어, 연구자와 개발자들이 더 쉽게 접근할 수 있습니다.

멀티모달 능력의 추가는 단순히 기능을 확장하는 것을 넘어서, 언어 모델의 활용 범위를 크게 넓힙니다. 텍스트와 이미지를 동시에 처리할 수 있는 능력은 교육, 의료, 창작 등 다양한 분야에서의 응용 가능성을 열어줍니다.

긴 컨텍스트 처리 능력은 문서 분석, 코드 이해, 복잡한 대화 등에서 특히 유용합니다. 128K 토큰은 일반적인 소설 한 권 정도의 길이에 해당하므로, 긴 문서를 한 번에 처리하고 이해할 수 있는 능력을 제공합니다.

Gemma 3의 개발은 [Transformer 아키텍처](https://arxiv.org/pdf/1706.03762v7)의 지속적인 발전과 최적화를 보여주는 사례입니다. 어텐션 메커니즘의 효율적인 구현, 메모리 사용량 최적화, 그리고 멀티모달 통합은 모두 현대 AI 연구의 핵심 과제들을 다루고 있습니다.
Gemma 3의 모델 아키텍처는 기존 Gemma 시리즈의 기본 구조를 유지하면서도 혁신적인 개선사항들을 도입한 정교한 설계입니다. 이 모델은 [Transformer 아키텍처](https://arxiv.org/pdf/1706.03762v7)를 기반으로 하는 디코더 전용 구조를 채택하고 있으며, 특히 긴 컨텍스트 처리와 멀티모달 능력을 위한 핵심적인 아키텍처 혁신을 포함하고 있습니다.

## 핵심 아키텍처 혁신

### Grouped-Query Attention과 QK-norm

Gemma 3의 가장 중요한 아키텍처 개선 중 하나는 [Grouped-Query Attention (GQA)](https://arxiv.org/pdf/2305.13245)의 도입입니다. 이는 기존의 Multi-Head Attention을 효율적으로 개선한 방식으로, 메모리 사용량을 크게 줄이면서도 성능을 유지할 수 있게 합니다.

GQA의 핵심 아이디어는 쿼리 헤드들을 그룹으로 나누어 각 그룹이 하나의 키-값 헤드를 공유하도록 하는 것입니다. 수학적으로 표현하면, 전체 \\(H\\)개의 헤드가 있을 때 \\(G\\)개의 그룹으로 나누어 각 그룹이 하나의 키-값 헤드를 공유합니다. 이를 통해 키-값 캐시 크기가 \\(H/G\\) 배만큼 감소하게 됩니다.

```python
def get_config_for_4b(dtype: str) -> GemmaConfig:
  return GemmaConfig(
      dtype=dtype,
      architecture=Architecture.GEMMA_3,
      num_hidden_layers=34,
      num_attention_heads=8,
      num_key_value_heads=4,  # GQA: 8개 쿼리 헤드가 4개 키-값 헤드 공유
      hidden_size=2560,
      intermediate_size=10240,
      use_pre_ffw_norm=True,
      use_post_ffw_norm=True,
      head_dim=256,
      use_qk_norm=True,  # QK 정규화 활성화
      # ...
  )
```

또한 Gemma 3는 기존의 소프트 캐핑(soft-capping) 대신 QK-norm을 도입했습니다. 이는 [Chameleon](https://arxiv.org/pdf/2405.09818)과 [Small-scale proxies for large-scale Transformer training instabilities](https://arxiv.org/pdf/2309.14322)에서 영감을 받은 기법으로, 어텐션 계산의 안정성을 크게 향상시킵니다. QK-norm은 쿼리와 키 벡터에 [RMSNorm](https://arxiv.org/pdf/1910.07467v1)을 적용하여 어텐션 점수의 분산을 제어합니다.

### 로컬-글로벌 어텐션 레이어 교대 배치

Gemma 3의 가장 혁신적인 아키텍처 특징은 5:1 비율의 로컬-글로벌 어텐션 레이어 교대 배치입니다. 이는 긴 컨텍스트 처리에서 발생하는 메모리 폭발 문제를 해결하기 위한 정교한 설계입니다.

구체적으로, 모델의 첫 번째 레이어부터 시작하여 5개의 로컬 슬라이딩 윈도우 어텐션 레이어 다음에 1개의 글로벌 어텐션 레이어가 배치됩니다. 이러한 패턴이 전체 모델에 걸쳐 반복됩니다.

```python
attn_types=(
    AttentionType.LOCAL_SLIDING,  # 1번째 레이어
    AttentionType.LOCAL_SLIDING,  # 2번째 레이어
    AttentionType.LOCAL_SLIDING,  # 3번째 레이어
    AttentionType.LOCAL_SLIDING,  # 4번째 레이어
    AttentionType.LOCAL_SLIDING,  # 5번째 레이어
    AttentionType.GLOBAL,         # 6번째 레이어
),
```

로컬 어텐션 레이어는 [Longformer](https://arxiv.org/pdf/2004.05150)에서 제안된 슬라이딩 윈도우 어텐션을 사용합니다. 각 토큰은 고정된 크기의 윈도우 내에서만 다른 토큰들과 어텐션을 수행하며, 이 윈도우 크기는 1024 토큰으로 설정되어 있습니다. 이를 통해 지역적인 의존성을 효과적으로 포착하면서도 계산 복잡도를 \\(O(n \times w)\\)로 유지할 수 있습니다. 여기서 \\(n\\)은 시퀀스 길이, \\(w\\)는 윈도우 크기입니다.

글로벌 어텐션 레이어는 전체 시퀀스에 대해 어텐션을 수행하여 장거리 의존성을 모델링합니다. 이러한 설계를 통해 모델은 지역적 패턴과 전역적 패턴을 모두 효과적으로 학습할 수 있습니다.

### 긴 컨텍스트 지원을 위한 RoPE 최적화

Gemma 3는 128K 토큰까지의 긴 컨텍스트를 지원하기 위해 RoPE(Rotary Position Embedding) 기법을 정교하게 조정했습니다. 특히 로컬 어텐션 레이어와 글로벌 어텐션 레이어에 서로 다른 RoPE 기본 주파수를 적용합니다.

로컬 어텐션 레이어는 기존의 10,000 주파수를 유지하는 반면, 글로벌 어텐션 레이어는 1,000,000으로 크게 증가시켰습니다. 이는 [Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/pdf/2306.15595)에서 제안된 위치 보간 기법과 유사한 접근법입니다.

```python
rope_wave_length={
    AttentionType.LOCAL_SLIDING: 10_000,    # 로컬 레이어
    AttentionType.GLOBAL: 1_000_000,        # 글로벌 레이어
},
rope_scaling_factor=8,  # 글로벌 레이어용 스케일링
```

이러한 차별화된 RoPE 설정을 통해 로컬 레이어는 세밀한 위치 정보를 유지하면서, 글로벌 레이어는 긴 거리의 위치 관계를 효과적으로 처리할 수 있습니다.

## 모델 크기별 매개변수 구성

Gemma 3는 다양한 사용 환경에 맞춰 1B, 4B, 12B, 27B의 네 가지 크기로 제공됩니다. 각 모델의 매개변수 구성은 다음과 같습니다.

| 모델 | 비전 인코더 | 임베딩 매개변수 | 비임베딩 매개변수 |
|------|-------------|----------------|------------------|
| 1B   | 302M        | 698M           | -                |
| 4B   | 417M        | 675M           | 3,209M           |
| 12B  | 417M        | 1,012M         | 10,759M          |
| 27B  | 417M        | 1,416M         | 25,600M          |

모든 모델은 256K 항목의 어휘를 사용하며, 4B 이상의 모델들은 동일한 비전 인코더를 공유합니다. 1B 모델만 32K 토큰의 컨텍스트 길이를 가지며, 나머지 모델들은 모두 128K 토큰을 지원합니다.

## 비전 모달리티 통합

### SigLIP 비전 인코더

Gemma 3의 멀티모달 능력은 400M 매개변수의 [SigLIP 인코더](https://arxiv.org/pdf/2303.15343v4)를 통해 구현됩니다. 이는 [Vision Transformer](https://arxiv.org/pdf/2010.11929v2) 아키텍처를 기반으로 하며, [CLIP](https://arxiv.org/pdf/2103.00020v1) 손실의 변형으로 훈련된 모델입니다.

```python
@dataclasses.dataclass
class SiglipVisionModelConfig:
  num_hidden_layers: int = 27        # 27개 트랜스포머 인코더 블록
  embedding_dim: int = 1152          # 임베딩 차원
  image_size: int = 896              # 입력 이미지 크기 (896x896)
  conv2d_patch_size = 14             # 패치 크기
  num_attention_heads: int = 16      # 어텐션 헤드 수
  head_dim: int = 72                 # 헤드 차원
  intermediate_size: int = 4304      # MLP 중간 크기
  encoding_sequence_length: int = 256 # 인코딩 시퀀스 길이
```

비전 인코더는 896×896 픽셀의 정사각형 이미지를 입력으로 받아 256개의 고정 크기 벡터로 압축합니다. 이러한 압축은 추론 비용을 줄이면서도 시각적 정보의 핵심을 보존하는 효율적인 설계입니다.

### Pan & Scan 적응형 윈도잉

비정사각형 종횡비와 고해상도 이미지 처리를 위해 Gemma 3는 Pan & Scan (P&S) 적응형 윈도잉 알고리즘을 도입했습니다. 이는 추론 시에만 적용되는 최적화 기법으로, 필요에 따라 활성화할 수 있습니다.

```python
def gemma3_input_preprocessor(
    raw_user_prompt: Sequence[Union[Image.Image, str]],
) -> Sequence[Union[torch.Tensor, str]]:
  preprocessed_input: list[Union[torch.Tensor, str]] = []
  for element in raw_user_prompt:
    if isinstance(element, Image.Image):
      cropped_images = pan_and_scan.pan_and_scan(element)
      preprocessed_images_cropped = siglip_vision_preprocessor.preprocess_images_for_siglip_vision(cropped_images)
      preprocessed_images_uncropped = siglip_vision_preprocessor.preprocess_images_for_siglip_vision([element])
      
      if len(preprocessed_images_cropped) == 1:
        preprocessed_input.append(preprocessed_images_uncropped[0])
      elif len(preprocessed_images_cropped) > 1:
        preprocessed_input.append(CROPPED_IMAGE_PREFIX)
        preprocessed_input.append(preprocessed_images_uncropped[0])
        preprocessed_input.append(CROPPED_IMAGE_FILLER)
        preprocessed_input.extend(preprocessed_images_cropped)
```

P&S 알고리즘은 이미지를 겹치지 않는 동일한 크기의 자르기로 분할하여 전체 이미지를 커버하고, 각 자르기를 896×896 픽셀로 크기 조정하여 인코더에 전달합니다. 이를 통해 텍스트가 읽을 수 없게 되거나 작은 객체가 사라지는 문제를 해결할 수 있습니다.

## 훈련 인프라 최적화

Gemma 3의 훈련은 모델 크기에 따라 최적화된 분산 컴퓨팅 환경에서 수행됩니다.

| 모델 | 타입     | 칩 수 | 데이터 | 시퀀스 | 복제본 |
|------|----------|-------|--------|--------|--------|
| 1B   | TPUv5e   | 512   | 16     | 16     | 2      |
| 4B   | TPUv5e   | 2048  | 16     | 16     | 8      |
| 12B  | TPUv4    | 6144  | 16     | 16     | 24     |
| 27B  | TPUv5p   | 6144  | 24     | 8      | 32     |

이러한 분산 훈련 설정은 데이터, 시퀀스, 복제본 차원에서의 샤딩을 통해 대규모 모델의 효율적인 훈련을 가능하게 합니다. 특히 27B 모델의 경우 6144개의 TPUv5p 칩을 사용하여 24개의 데이터 샤드와 32개의 복제본으로 구성된 고도로 병렬화된 환경에서 훈련됩니다.

이러한 아키텍처 혁신들을 통해 Gemma 3는 기존 모델들의 한계를 뛰어넘는 성능과 효율성을 달성하면서도, 다양한 하드웨어 환경에서 실행 가능한 실용적인 모델로 설계되었습니다.
Gemma 3의 사전 훈련 과정은 Gemma 2에서 사용된 지식 증류 기법을 기반으로 하면서도, 멀티모달 능력과 확장된 언어 지원을 위한 혁신적인 개선사항들을 포함하고 있습니다. 이 과정에서 가장 주목할 만한 점은 텍스트와 이미지를 동시에 처리할 수 있는 능력을 확보하면서도 기존의 텍스트 처리 성능을 유지하거나 향상시켰다는 것입니다.

## 훈련 데이터 확장과 최적화

Gemma 3의 훈련 데이터는 이전 버전보다 상당히 확장되었습니다. 27B 모델의 경우 14조 토큰, 12B 모델은 12조 토큰, 4B 모델은 4조 토큰, 그리고 1B 모델은 2조 토큰으로 훈련되었습니다. 이러한 토큰 수의 증가는 단순히 텍스트 데이터만의 확장이 아니라, 이미지와 텍스트가 혼합된 멀티모달 데이터의 포함을 반영합니다.

특히 다국어 데이터의 비중이 크게 증가했습니다. 이는 단순히 단일 언어 데이터만을 추가한 것이 아니라, 병렬 데이터(parallel data)도 함께 포함시켜 언어 간 이해 능력을 향상시켰습니다. [Chung et al. (2023)](https://arxiv.org/pdf/2402.09668)에서 제안된 전략에 영감을 받아 언어 표현의 불균형 문제를 해결하는 방법을 적용했습니다. 이는 특정 언어에 편중되지 않고 다양한 언어에서 균등한 성능을 발휘할 수 있도록 하는 중요한 개선사항입니다.

## 토크나이저 개선과 어휘 확장

Gemma 3는 Gemini 2.0과 동일한 토크나이저를 사용하지만, 이는 기존 Gemma 시리즈와는 다른 중요한 변화입니다. 새로운 토크나이저는 [SentencePiece](https://arxiv.org/pdf/1808.06226) 기반으로 구축되었으며, 숫자 분할(split digits), 공백 보존(preserved whitespace), 그리고 바이트 수준 인코딩(byte-level encodings)을 지원합니다.

결과적으로 어휘 크기가 262,000개 항목으로 확장되었습니다. 이는 기존 Gemma 모델들의 256,000개 어휘보다 증가한 것으로, 특히 비영어권 언어에 대한 균형 잡힌 표현을 제공합니다. 이러한 개선은 다국어 성능 향상에 직접적으로 기여하며, 토큰 효율성을 높여 동일한 의미를 표현하는 데 필요한 토큰 수를 줄입니다.

## 데이터 필터링과 품질 관리

Gemma 3의 사전 훈련에서는 포괄적인 데이터 필터링 기법이 적용되었습니다. 이는 원하지 않거나 안전하지 않은 발화의 위험을 줄이고, 개인 정보 및 기타 민감한 데이터를 제거하는 것을 목표로 합니다.

평가 데이터셋의 오염 제거(decontamination)도 중요한 과정입니다. 이는 사전 훈련 데이터 혼합에서 평가 세트를 제거하여 모델이 평가 시에 이미 본 데이터로 인한 부정확한 성능 측정을 방지합니다. 또한 민감한 출력의 확산을 최소화하여 암송(recitation) 위험을 줄입니다.

[Sachdeva et al. (2024)](https://arxiv.org/pdf/2402.09668)에서 영감을 받은 품질 재가중치(quality reweighing) 단계도 적용되었습니다. 이는 저품질 데이터의 발생을 줄이는 혁신적인 접근법으로, ASK-LLM과 같은 품질 기반 샘플링 기법을 활용합니다. 이 방법은 지시 조정된 LLM의 추론 능력을 활용하여 훈련 예시의 품질과 정보성을 직접 평가합니다.

구체적으로, ASK-LLM은 각 훈련 예시에 대해 지시 조정된 LLM에 프롬프트를 제공하고 "yes" 토큰의 확률을 품질 점수로 사용합니다. 이를 통해 샘플러가 예시의 품질에 대해 미묘하고 맥락을 고려한 결정을 내릴 수 있습니다. 연구 결과에 따르면 ASK-LLM은 원본 데이터의 최대 90%를 거부하면서도 전체 데이터셋으로 훈련하는 것보다 우수한 성능을 보이며, 수렴 속도도 최대 70% 빨라집니다.

## 지식 증류 메커니즘

Gemma 3의 핵심 훈련 기법 중 하나는 지식 증류입니다. 이는 [Hinton et al. (2015)](https://arxiv.org/pdf/1503.02531)에서 제안된 기법을 발전시킨 것으로, 큰 교사 모델의 지식을 작은 학생 모델로 전달하는 효과적인 방법입니다.

Gemma 3에서는 토큰당 256개의 로짓을 교사 확률로 가중치를 적용하여 샘플링합니다. 학생 모델은 이러한 샘플 내에서 교사의 분포를 교차 엔트로피 손실을 통해 학습합니다. 수학적으로 이는 다음과 같이 표현됩니다.

\\[
L_{distill} = -\sum_{i \in S} p_T(x_i) \log p_S(x_i)
\\]

여기서 \\(S\\)는 샘플링된 256개 로짓의 집합이고, \\(p_T(x_i)\\)는 교사 모델의 확률 분포, \\(p_S(x_i)\\)는 학생 모델의 확률 분포입니다.

교사의 목표 분포는 샘플링되지 않은 로짓에 대해 0 확률로 설정되고 재정규화됩니다. 이러한 접근법은 전체 어휘에 대한 소프트 타겟을 계산하는 것보다 계산 효율적이면서도, 교사 모델의 핵심 지식을 효과적으로 전달할 수 있습니다.

## 양자화 인식 훈련

Gemma 3는 원본 체크포인트와 함께 다양한 표준 형식의 양자화된 버전도 제공합니다. 이러한 양자화 모델들은 [Quantization Aware Training (QAT)](https://arxiv.org/pdf/1712.05877v1) 기법을 사용하여 각 모델을 일반적으로 5,000 스텝 동안 소량의 추가 미세 조정을 통해 얻어집니다.

QAT는 양자화로 인한 정확도 손실을 최소화하는 공동 설계된 양자화 훈련 절차입니다. 이 과정에서는 양자화되지 않은 체크포인트의 확률을 목표로 사용하고, 사전 훈련 및 사후 훈련 분포와 일치하도록 데이터를 조정합니다.

가장 인기 있는 오픈 소스 양자화 추론 엔진(예: llama.cpp)을 기반으로, 세 가지 가중치 표현에 중점을 둡니다. 채널별 int4, 블록별 int4, 그리고 전환된 fp8입니다.

| 모델 | Raw (GB) bf16 | Quantized (GB) Int4 | Int4 blocks=32 | SFP8 |
|------|---------------|---------------------|----------------|------|
| 1B   | 2.0           | 0.5                 | 0.7            | 1.0  |
| 4B   | 8.0           | 2.6                 | 2.9            | 4.4  |
| 12B  | 24.0          | 6.6                 | 7.1            | 12.4 |
| 27B  | 54.0          | 14.1                | 15.3           | 27.4 |

KV 캐시를 포함한 경우(32,768 컨텍스트 크기에서 8비트로 양자화):

| 모델 | Raw+KV (GB) | Int4+KV | Int4 blocks+KV | SFP8+KV |
|------|-------------|---------|----------------|---------|
| 1B   | 2.9         | 1.4     | 1.6            | 1.9     |
| 4B   | 12.7        | 7.3     | 7.6            | 9.1     |
| 12B  | 38.9        | 21.5    | 22.0           | 27.3    |
| 27B  | 72.7        | 32.8    | 34.0           | 46.1    |

이러한 양자화는 메모리 사용량을 크게 줄여줍니다. 예를 들어, 27B 모델의 경우 원본 54GB에서 int4 양자화를 통해 14.1GB로 약 74% 감소시킬 수 있습니다. 이는 소비자급 하드웨어에서도 대규모 모델을 실행할 수 있게 하는 중요한 개선사항입니다.

## 컴퓨팅 인프라와 분산 훈련

Gemma 3의 훈련은 TPUv4, TPUv5e, TPUv5p를 활용한 대규모 분산 환경에서 수행되었습니다. 각 모델 구성은 훈련 스텝 시간을 최소화하도록 최적화되었습니다.

비전 인코더의 경우, 각 이미지에 대한 임베딩을 사전 계산하고 임베딩으로 직접 훈련하여 언어 모델 훈련에 추가 비용을 발생시키지 않습니다. 이는 멀티모달 훈련의 효율성을 크게 향상시키는 중요한 최적화입니다.

옵티마이저 상태는 [ZeRO-3](https://arxiv.org/pdf/2104.07857)의 구현을 사용하여 샤딩됩니다. 이는 메모리 사용량을 최적화하고 대규모 모델의 훈련을 가능하게 하는 핵심 기술입니다. 멀티 포드 훈련의 경우, [Pathways](https://arxiv.org/pdf/2203.12533v1) 접근법을 사용하여 데이터 센터 네트워크를 통한 데이터 복제 감소를 수행합니다.

Pathways의 비동기 분산 데이터플로우 설계는 수천 개의 가속기에서 이종 병렬 계산의 효율적인 갱 스케줄링을 가능하게 합니다. 이 시스템은 데이터 평면의 종속성에도 불구하고 제어 평면이 병렬로 실행될 수 있도록 하는 비동기 데이터플로우 설계를 사용합니다.

[JAX](https://arxiv.org/pdf/2203.12533v1)와 Pathways의 '단일 컨트롤러' 프로그래밍 패러다임을 사용하며, [GSPMD 파티셔너](https://arxiv.org/pdf/2105.04663v2)와 MegaScale XLA 컴파일러를 함께 활용합니다. GSPMD는 사용자가 단일 장치용 프로그램을 작성하고 간단한 주석을 제공하여 장치 간에 텐서를 분산시킬 수 있는 자동 컴파일러 기반 병렬화 시스템입니다.

## 대화형 모델 포맷팅

Gemma 3 IT(Instruction-Tuned) 모델들은 특별한 대화 포맷을 사용합니다.

| 컨텍스트 | 포맷팅 |
|----------|--------|
| 사용자 턴 | `user` |
| 모델 턴 | `model` |
| 턴 종료 | `End of turn` |

대화 예시:
```
사용자: Who are you?
모델: My name is Gemma!
사용자: What is 2+2?
모델: 2+2=4.
```

모델 입력 형식:
```
[BOS]user Who are you? model My name is Gemma! user What is 2+2? model
```

모델 출력:
```
2+2=4.
```

토큰화 후에 [BOS] 토큰을 명시적으로 추가하거나 토크나이저에서 `add_bos=True` 옵션을 사용해야 합니다. 중요한 점은 "[BOS]" 텍스트 자체를 토큰화해서는 안 된다는 것입니다.

이러한 포맷팅은 모델이 대화의 맥락을 정확히 이해하고 적절한 응답을 생성할 수 있도록 하는 중요한 구조적 요소입니다. 특히 멀티턴 대화에서 각 참여자의 발화를 명확히 구분하여 모델의 이해도를 높입니다.
Gemma 3의 지시 조정(Instruction-Tuning) 과정은 사전 훈련된 모델을 실제 사용자의 요구사항에 맞춰 최적화하는 핵심적인 단계입니다. 이 과정에서는 이전 버전보다 크게 개선된 사후 훈련 접근법을 사용하여 모델의 유용성, 수학적 추론, 코딩, 다국어 능력을 향상시키면서 동시에 안전성을 확보합니다.

## 개선된 사후 훈련 접근법

Gemma 3의 사후 훈련은 대규모 지시 조정 교사 모델로부터의 향상된 지식 증류 기법을 핵심으로 합니다. 이는 [Geoffrey Hinton et al.](https://arxiv.org/pdf/1503.02531)에서 제안된 기본 증류 개념을 발전시킨 것으로, 큰 교사 모델의 지식을 작은 학생 모델로 효과적으로 전달하는 방법입니다.

기존의 지식 증류에서는 교사 모델의 소프트 타겟을 학습하는 것이 핵심이었습니다. 수학적으로 이는 다음과 같이 표현됩니다.

\\[
L_{distill} = -\sum_i p_T(x_i) \log p_S(x_i)
\\]

여기서 \\(p_T(x_i)\\)는 교사 모델의 확률 분포이고, \\(p_S(x_i)\\)는 학생 모델의 확률 분포입니다. Gemma 3에서는 이러한 기본 원리를 바탕으로 하되, 지시 조정에 특화된 개선사항들을 도입했습니다.

## 강화학습 기반 미세 조정

지식 증류와 함께 Gemma 3는 BOND, WARM, WARP 기법의 개선된 버전을 기반으로 한 강화학습 미세 조정 단계를 포함합니다. 이러한 접근법은 다양한 보상 함수를 활용하여 모델의 성능을 다각도로 향상시킵니다.

WARM(Weight Averaged Reward Models) 기법은 인간 피드백 데이터로 훈련된 여러 보상 모델의 가중 평균을 사용합니다. 이는 단일 보상 모델의 편향을 줄이고 더 안정적인 학습을 가능하게 합니다. 수학적으로 가중 평균 보상은 다음과 같이 계산됩니다.

\\[
R_{weighted}(s, a) = \sum_{i=1}^{N} w_i R_i(s, a)
\\]

여기서 \\(w_i\\)는 각 보상 모델의 가중치이고, \\(R_i(s, a)\\)는 \\(i\\)번째 보상 모델의 출력입니다.

## 다양한 보상 함수 활용

Gemma 3의 강화학습 과정에서는 여러 종류의 보상 함수를 동시에 활용합니다. 이는 모델이 단일 측면에서만 최적화되는 것을 방지하고 균형 잡힌 성능 향상을 달성하기 위함입니다.

**인간 피드백 기반 보상**: 인간 평가자들이 제공한 피드백 데이터를 바탕으로 훈련된 보상 모델을 사용합니다. 이는 모델의 유용성과 지시 따르기 능력을 향상시키는 데 중점을 둡니다.

**코드 실행 피드백**: [RLEF 방법론](https://arxiv.org/pdf/2410.02089)에서 영감을 받은 코드 실행 피드백을 활용합니다. 이 접근법에서는 모델이 생성한 코드를 실제로 실행하여 그 결과를 바탕으로 보상을 계산합니다. 실행 성공 여부, 테스트 케이스 통과율, 코드의 효율성 등이 보상 계산에 반영됩니다.

**수학 문제 해결을 위한 정답 기반 보상**: 수학적 추론 능력 향상을 위해 정답이 명확한 수학 문제에 대해서는 정답 여부를 직접적인 보상으로 사용합니다. 이는 모델이 논리적 추론과 계산 능력을 체계적으로 학습할 수 있게 합니다.

## 데이터 필터링과 품질 관리

사후 훈련 과정에서 사용되는 데이터의 품질은 최종 모델 성능에 결정적인 영향을 미칩니다. Gemma 3에서는 포괄적인 데이터 필터링 전략을 적용하여 훈련 데이터의 품질을 최적화합니다.

**개인정보 및 안전성 필터링**: 개인정보가 포함된 예시, 안전하지 않거나 독성이 있는 모델 출력, 잘못된 자기 식별 데이터, 중복된 예시들을 체계적으로 제거합니다. 이는 모델이 안전하고 신뢰할 수 있는 출력을 생성하도록 보장하는 중요한 단계입니다.

**맥락 기반 귀속 및 거부 능력 향상**: 환각(hallucination)을 최소화하기 위해 더 나은 맥락 내 귀속(in-context attribution), 불확실성 표현(hedging), 그리고 적절한 거부(refusal) 능력을 장려하는 데이터 부분집합을 포함시킵니다. 이러한 데이터는 사실성 지표에서의 성능을 향상시키면서도 다른 지표에서의 성능 저하를 방지합니다.

## 토큰 포맷팅과 제어 토큰

Gemma 3의 지시 조정 모델들은 특별한 토큰 포맷팅 체계를 사용합니다. 모든 모델이 동일한 토크나이저를 공유하지만, 지시 조정 포맷팅을 위한 전용 제어 토큰들이 있습니다.

**BOS 토큰 처리**: 사전 훈련(PT) 모델과 지시 조정(IT) 모델 모두에서 텍스트는 [BOS] 토큰으로 시작합니다. 이 토큰은 텍스트 "[BOS]"가 실제 [BOS] 토큰에 매핑되지 않기 때문에 명시적으로 추가해야 합니다. 예를 들어, Flax에서는 토큰화 시 `add_bos=True` 옵션을 사용하여 이 토큰을 자동으로 추가할 수 있습니다.

**종료 토큰의 차이점**: 사전 훈련 모델과 지시 조정 모델 간의 주요 차이점 중 하나는 생성 종료 시 사용하는 토큰입니다. 사전 훈련 모델은 생성 끝에 특정 토큰을 출력하는 반면, 지시 조정 모델은 다른 종료 토큰을 사용합니다. 따라서 어느 모델 유형을 미세 조정하든 해당하는 종료 토큰을 추가해야 합니다.

다음은 지시 조정 모델의 포맷팅 예시입니다.

| 컨텍스트 | 포맷팅 |
|----------|--------|
| 사용자 턴 | `user` |
| 모델 턴 | `model` |
| 턴 종료 | `End of turn` |

실제 대화 예시:
```
사용자: 당신은 누구인가요?
모델: 제 이름은 Gemma입니다!
사용자: 2+2는 무엇인가요?
모델: 2+2=4입니다.
```

이는 다음과 같은 모델 입력 형식으로 변환됩니다.
```
[BOS]user 당신은 누구인가요? model 제 이름은 Gemma입니다! user 2+2는 무엇인가요? model
```

이러한 구조화된 포맷팅은 모델이 대화의 맥락을 정확히 이해하고 각 참여자의 역할을 명확히 구분할 수 있게 합니다. 특히 멀티턴 대화에서 이전 대화 내용을 참조하거나 일관성 있는 응답을 생성하는 데 중요한 역할을 합니다.

## 성능 향상 결과

이러한 개선된 사후 훈련 접근법을 통해 Gemma 3는 모든 주요 능력 영역에서 상당한 성능 향상을 달성했습니다. 수학적 추론, 코딩, 채팅, 지시 따르기, 다국어 능력 등에서 이전 버전 대비 현저한 개선을 보였으며, 동시에 모델의 안전성도 강화되었습니다.

특히 주목할 만한 점은 Gemma3-4B-IT 모델이 훨씬 큰 Gemma2-27B-IT 모델과 경쟁할 수 있는 성능을 보여준다는 것입니다. 이는 개선된 사후 훈련 방법론의 효과를 명확히 보여주는 결과로, 모델 크기를 늘리는 것 외에도 훈련 방법의 개선을 통해 상당한 성능 향상을 달성할 수 있음을 시사합니다.
Gemma 3의 최종 모델 평가는 다양한 자동화된 벤치마크와 인간 평가를 통해 수행되었으며, 특히 LMSYS Chatbot Arena에서의 성과와 표준 벤치마크에서의 결과를 중심으로 모델의 실제 성능을 종합적으로 검증했습니다.

## LMSYS Chatbot Arena 성능 평가

Gemma 3 27B IT 모델은 [LMSYS Chatbot Arena](https://arxiv.org/pdf/2403.04132v1)에서 인상적인 성과를 보여주었습니다. 이 플랫폼은 대규모 언어 모델들을 인간 평가자들의 선호도를 통해 평가하는 혁신적인 시스템으로, 실제 사용자들이 두 모델의 응답을 비교하여 더 나은 것을 선택하는 방식으로 운영됩니다.

Chatbot Arena의 핵심 기술적 특징은 Bradley-Terry 모델을 사용한 효율적인 순위 산정 시스템입니다. 모델 \\(m\\)이 모델 \\(m'\\)을 이기는 확률은 다음과 같이 계산됩니다.

\\[
\theta((m', m)) = \frac{e^{\xi_m}}{e^{\xi_m} + e^{\xi_{m'}}}
\\]

여기서 \\(\xi_1, \ldots, \xi_M\\)은 각 모델의 Bradley-Terry 계수로, 최대우도추정법을 통해 산출됩니다. 이러한 수학적 프레임워크를 통해 Gemma 3 27B IT는 1338점의 Elo 점수를 달성했습니다.

| 순위 | 모델 | Elo | 95% CI | 오픈 | 타입 | 매개변수 |
|------|------|-----|--------|------|------|----------|
| 1 | Grok-3-Preview-02-24 | 1412 | +8/-10 | - | - | - |
| 1 | GPT-4.5-Preview | 1411 | +11/-11 | - | - | - |
| 3 | Gemini-2.0-Flash-Thinking-Exp-01-21 | 1384 | +6/-5 | - | - | - |
| 3 | Gemini-2.0-Pro-Exp-02-05 | 1380 | +5/-6 | - | - | - |
| 3 | ChatGPT-4o-latest (2025-01-29) | 1377 | +5/-4 | - | - | - |
| 6 | DeepSeek-R1 | 1363 | +8/-6 | yes | MoE | 671B/37B |
| 6 | Gemini-2.0-Flash-001 | 1357 | +6/-5 | - | - | - |
| 8 | o1-2024-12-17 | 1352 | +4/-6 | - | - | - |
| 9 | **Gemma-3-27B-IT** | **1338** | **+8/-9** | **yes** | **Dense** | **27B** |
| 9 | Qwen2.5-Max | 1336 | +7/-5 | - | - | - |
| 9 | o1-preview | 1335 | +4/-3 | - | - | - |
| 9 | o3-mini-high | 1329 | +8/-6 | - | - | - |
| 13 | DeepSeek-V3 | 1318 | +8/-6 | yes | MoE | 671B/37B |
| ... | ... | ... | ... | ... | ... | ... |
| 28 | Meta-Llama-3.1-405B-Instruct-bf16 | 1269 | +4/-3 | yes | Dense | 405B |
| ... | ... | ... | ... | ... | ... | ... |
| 38 | Llama-3.3-70B-Instruct | 1257 | +5/-3 | yes | Dense | 70B |
| 39 | Qwen2.5-72B-Instruct | 1257 | +3/-3 | yes | Dense | 72B |
| ... | ... | ... | ... | ... | ... | ... |
| 59 | Gemma-2-27B-it | 1220 | +3/-2 | yes | Dense | 27B |

이 결과는 여러 측면에서 주목할 만합니다. 첫째, Gemma 3 27B IT는 전체 9위에 랭크되어 상위 10개 모델에 포함되었습니다. 둘째, 훨씬 큰 오픈 소스 모델들을 능가하는 성능을 보였습니다. DeepSeek-V3(1318점, 671B 매개변수), Meta-Llama-3.1-405B(1269점, 405B 매개변수), Qwen2.5-72B(1257점, 72B 매개변수) 등 모두 Gemma 3보다 훨씬 많은 매개변수를 가지고 있음에도 불구하고 낮은 점수를 기록했습니다.

특히 이전 버전인 Gemma-2-27B-it(1220점)와 비교했을 때 118점의 상당한 개선을 보여주었습니다. 이는 새로운 아키텍처 설계와 개선된 사후 훈련 방법론의 효과를 명확히 보여주는 결과입니다.

중요한 점은 이러한 Elo 점수가 시각적 능력을 고려하지 않는다는 것입니다. Gemma 3는 멀티모달 능력을 갖추고 있지만, 평가된 다른 모델들은 대부분 텍스트 전용 모델이므로 시각적 이해 능력이 점수에 반영되지 않았습니다. 만약 시각적 능력까지 고려한다면 Gemma 3의 실제 유용성은 더욱 높을 것으로 예상됩니다.

## 표준 벤치마크 성능

표준 벤치마크 평가에서는 Gemma 3 모델들이 이전 버전 대비 일관된 성능 향상을 보여주었습니다. 특히 [MMLU(Massive Multitask Language Understanding)](https://arxiv.org/pdf/2009.03300v3) 벤치마크를 포함한 다양한 정적 벤치마크에서 경쟁력 있는 결과를 달성했습니다.

MMLU 벤치마크는 57개의 다양한 주제를 다루는 종합적인 평가 도구로, STEM 분야부터 인문학과 사회과학까지 광범위한 지식을 평가합니다. 이 벤치마크는 제로샷과 퓨샷 설정에서 평가되도록 설계되어 있어, 모델이 대규모 데이터셋에 미세 조정되지 않고도 얼마나 잘 수행할 수 있는지를 측정합니다.

연구진은 외부 모델들과의 직접적인 비교를 피했는데, 이는 각기 다른 평가 설정에서 보고된 결과들이 공정한 비교를 보장하지 못하기 때문입니다. 대신 동일한 평가 환경에서 Gemma 2와 Gemini 1.5와의 비교를 통해 성능 개선을 입증했습니다.

평가 결과는 수학, 코딩, 다국어 능력에서 특히 두드러진 개선을 보여주었습니다. 이는 개선된 사후 훈련 접근법의 직접적인 결과로, 다양한 보상 함수를 활용한 강화학습 미세 조정과 향상된 지식 증류 기법이 효과적으로 작용했음을 시사합니다.

연구진은 더 공정한 모델 간 비교를 위해 제3자 정적 리더보드를 참조할 것을 권장했습니다. 이는 각 모델이 서로 다른 평가 프로토콜과 설정을 사용할 수 있기 때문에, 표준화된 평가 환경에서의 비교가 더 신뢰할 만한 결과를 제공할 수 있기 때문입니다.

이러한 평가 결과들은 Gemma 3가 단순히 모델 크기를 늘리는 것이 아니라, 아키텍처 혁신과 훈련 방법론의 개선을 통해 실질적인 성능 향상을 달성했음을 보여줍니다. 특히 27B 매개변수라는 상대적으로 작은 크기로도 훨씬 큰 모델들과 경쟁할 수 있는 성능을 보인 것은, 효율적인 모델 설계의 중요성을 강조하는 결과입니다.
Gemma 3의 절제 연구(Ablation Studies)는 모델의 아키텍처 변경사항과 새로운 비전 능력이 성능에 미치는 영향을 체계적으로 분석한 중요한 실험입니다. 이 연구를 통해 각 구성 요소의 기여도를 정량적으로 평가하고, 설계 결정의 타당성을 검증할 수 있습니다.

## 사전 훈련 능력 탐지

사전 훈련 과정에서 모델이 일반적인 능력을 제대로 습득하고 있는지 확인하기 위해 여러 표준 벤치마크를 프로브로 사용했습니다. 이는 모델이 특정 작업에만 특화되지 않고 범용적인 지능을 발달시키고 있는지 모니터링하는 중요한 과정입니다.

![Gemma 2와 Gemma 3 모델의 일반 능력 비교](/assets/2025-10-03-gemma-3-technical-report/2.png)

위 레이더 차트는 Gemma 2와 Gemma 3 사전 훈련 모델들의 성능을 과학(Science), 코드(Code), 사실성(Factuality), 다국어(Multilingual), 추론(Reasoning), 비전(Vision) 등 6개 핵심 영역에서 비교한 결과를 보여줍니다. 전반적으로 Gemma 3는 비전 능력이 추가되었음에도 불구하고 대부분의 카테고리에서 성능 향상을 보였습니다.

특히 주목할 만한 점은 다국어 능력의 현저한 개선입니다. 이는 Gemma 3에서 다국어 데이터의 비중을 크게 늘리고 병렬 데이터를 포함시킨 결과입니다. [Chung et al. (2023)](https://arxiv.org/pdf/2402.09668)에서 제안된 언어 표현 균형화 전략을 적용하여 특정 언어에 편중되지 않는 균등한 성능을 달성했습니다.

그러나 이러한 프로브 결과 해석에는 주의가 필요합니다. [Mirzadeh et al. (2024)](https://arxiv.org/pdf/2309.14322)에서 지적한 바와 같이, 오염 제거 기법을 사용했음에도 불구하고 항상 오염의 위험이 존재하므로 더 확정적인 결론을 내리기는 어렵습니다.

## 로컬-글로벌 어텐션 레이어 분석

Gemma 3의 핵심 혁신 중 하나인 로컬-글로벌 어텐션 레이어 교대 배치의 효과를 다각도로 분석했습니다. 이 분석은 메모리 효율성과 성능 사이의 균형을 이해하는 데 중요한 통찰을 제공합니다.

### 로컬-글로벌 비율의 영향

![로컬-글로벌 비율이 퍼플렉시티에 미치는 영향](/assets/2025-10-03-gemma-3-technical-report/3.png)

위 그래프는 로컬 어텐션과 글로벌 어텐션 레이어의 비율을 1:1부터 7:1까지 변경했을 때 검증 세트에서의 퍼플렉시티 변화를 보여줍니다. 놀랍게도 7:1의 극단적인 비율에서도 성능 저하가 미미했습니다. 이는 대부분의 언어 모델링 작업에서 지역적 의존성이 전역적 의존성보다 더 중요하다는 것을 시사합니다.

Gemma 2에서는 1:1 비율을 사용했지만, Gemma 3에서는 5:1 비율을 채택했습니다. 이러한 결정은 성능을 거의 손상시키지 않으면서도 메모리 효율성을 크게 개선할 수 있음을 보여줍니다.

### 슬라이딩 윈도우 크기의 최적화

![슬라이딩 윈도우 크기가 퍼플렉시티에 미치는 영향](/assets/2025-10-03-gemma-3-technical-report/4.png)

슬라이딩 윈도우 크기 실험에서는 512토큰부터 4096토큰까지 다양한 윈도우 크기를 테스트했습니다. 결과적으로 윈도우 크기를 상당히 줄여도 퍼플렉시티에 미치는 영향이 미미했습니다. 이는 [Longformer](https://arxiv.org/pdf/2004.05150)에서 제안된 슬라이딩 윈도우 어텐션의 효과성을 재확인하는 결과입니다.

Longformer에서는 각 토큰이 고정된 크기의 윈도우 내에서만 어텐션을 수행하여 계산 복잡도를 \\(O(n \times w)\\)로 유지합니다. 여기서 \\(n\\)은 시퀀스 길이, \\(w\\)는 윈도우 크기입니다. Gemma 3에서는 1024토큰의 윈도우 크기를 사용하여 성능과 효율성의 최적 균형점을 찾았습니다.

### KV 캐시 메모리 최적화

![모델과 KV 캐시 메모리 사용량 비교](/assets/2025-10-03-gemma-3-technical-report/5.png)

32K 토큰 컨텍스트에서의 메모리 사용량 분석 결과는 매우 인상적입니다. 기존의 "글로벌 전용" 구성에서는 KV 캐시가 모델 메모리의 60% 오버헤드를 발생시켰지만, 1:3 비율과 1024 슬라이딩 윈도우를 사용하면 이 오버헤드가 15% 미만으로 감소했습니다.

![컨텍스트 길이에 따른 KV 캐시 메모리 사용량](/assets/2025-10-03-gemma-3-technical-report/6.png)

컨텍스트 길이에 따른 메모리 사용량 비교에서는 Gemma 3의 아키텍처(L:G=5:1, sw=1024)가 글로벌 전용 모델 대비 현저한 메모리 절약 효과를 보여줍니다. 128K 토큰에서 글로벌 전용 모델이 6GB의 KV 캐시 메모리를 사용하는 반면, Gemma 3 아키텍처는 훨씬 적은 메모리만 사용합니다.

이러한 메모리 효율성은 [ZeRO-3](https://arxiv.org/pdf/2104.07857) 최적화 기법과 결합되어 대규모 모델의 실용적 배포를 가능하게 합니다. ZeRO-3는 옵티마이저 상태를 샤딩하여 메모리 사용량을 최적화하는 핵심 기술입니다.

## 긴 컨텍스트 지원 구현

Gemma 3는 처음부터 128K 시퀀스로 훈련하는 대신, 32K 시퀀스로 사전 훈련한 후 4B, 12B, 27B 모델을 훈련 마지막 단계에서 128K 토큰으로 확장하는 전략을 사용했습니다. 이 과정에서 [Chen et al. (2023)](https://arxiv.org/pdf/2306.15595)에서 제안된 RoPE 재스케일링 기법을 적용했습니다.

RoPE 재스케일링에서는 스케일링 팩터 8을 사용했으며, 이는 위치 보간(Position Interpolation) 방법의 핵심입니다. 기존 연구에서 제안된 바와 같이, 직접적인 외삽(extrapolation) 대신 선형 보간을 사용하여 위치 인덱스를 원래 범위 \\([0, L]\\)에서 확장된 범위 \\([0, L']\\)로 매핑합니다.

\\[
\mathbf{f}'(\mathbf{x}, m) = \mathbf{f}(\mathbf{x}, mL/L')
\\]

또한 Gemma 2와 비교하여 글로벌 셀프 어텐션 레이어의 RoPE 기본 주파수를 10K에서 1M으로 증가시켰지만, 로컬 셀프 어텐션 레이어는 10K를 유지했습니다. 이러한 차별화된 접근법은 로컬 레이어에서는 세밀한 위치 정보를 보존하면서, 글로벌 레이어에서는 긴 거리의 위치 관계를 효과적으로 처리할 수 있게 합니다.

![긴 컨텍스트 성능 평가](/assets/2025-10-03-gemma-3-technical-report/7.png)

RoPE 재스케일링 전후의 성능 비교 결과, 모델들이 128K까지는 잘 일반화되지만 그 이상으로 확장할 때는 급격한 성능 저하를 보입니다. 이는 현재 기술의 한계를 보여주는 동시에, 128K 컨텍스트 길이가 실용적인 상한선임을 시사합니다.

## 교사 모델 크기의 영향

지식 증류에서 일반적으로 알려진 통념과는 다른 흥미로운 발견이 있었습니다. 기존 연구들에서는 작은 학생 모델을 훈련할 때 작은 교사 모델을 사용하는 것이 더 효과적이라고 여겨졌습니다.

![작은 교사 vs 큰 교사 모델 비교](/assets/2025-10-03-gemma-3-technical-report/8.png)

그러나 Gemma 3의 실험 결과는 이와 다른 패턴을 보여줍니다. 짧은 훈련 기간에서는 작은 교사가 더 나은 성능을 보이지만, 훈련이 길어질수록 큰 교사 모델의 장점이 나타납니다. 이는 기존 연구들이 주로 짧은 훈련 환경에서 수행되었기 때문에 작은 교사의 정규화 효과가 큰 교사의 이점을 상쇄했을 가능성을 시사합니다.

이러한 발견은 [Hinton et al. (2015)](https://arxiv.org/pdf/1503.02531)의 원래 지식 증류 개념을 재검토하게 합니다. 충분한 훈련 시간이 주어진다면, 큰 교사 모델의 풍부한 지식이 학생 모델의 성능 향상에 더 큰 기여를 할 수 있음을 보여줍니다.

## 비전 인코더 최적화

Gemma 3의 멀티모달 능력을 위해 [SigLIP](https://arxiv.org/pdf/2303.15343v4) 기반 비전 인코더를 사용했습니다. 비전 인코더는 고정되어 있고 언어 모델만 훈련됩니다. 각 이미지는 해당 비전 인코더에서 256개의 이미지 토큰으로 표현됩니다.

### 이미지 해상도의 영향

| 해상도 | DocVQA | InfoVQA | TextVQA |
|--------|--------|---------|---------|
| 256    | 31.9   | 23.1    | 44.1    |
| 448    | 45.4   | 31.6    | 53.5    |
| 896    | 59.8   | 33.7    | 58.0    |

이미지 인코더 입력 해상도 실험 결과, 높은 해상도 인코더가 낮은 해상도보다 일관되게 우수한 성능을 보였습니다. 896 해상도 인코더는 4x4 평균 풀링을 사용하여 출력을 256 토큰으로 줄입니다. 이는 계산 비용을 제어하면서도 고해상도 정보를 효과적으로 활용할 수 있는 균형점을 제공합니다.

### Pan & Scan의 효과

| 모델 | DocVQA | InfoVQA | TextVQA |
|------|--------|---------|---------|
| 4B   | 72.8   | 44.1    | 58.9    |
| 4B w/ P&S | 81.0 | 57.0 | 60.8 |
| 개선도 | (+8.2) | (+12.9) | (+1.9) |
| 27B  | 85.6   | 59.4    | 68.6    |
| 27B w/ P&S | 90.4 | 76.4 | 70.2 |
| 개선도 | (+4.8) | (+17.0) | (+1.6) |

Pan & Scan (P&S) 기법의 효과는 매우 인상적입니다. 특히 InfoVQA에서 27B 모델의 경우 17.0%의 성능 향상을 보였습니다. P&S는 이미지를 원래 종횡비에 가깝게 캡처하고 원래 이미지 해상도에 가깝게 처리할 수 있게 합니다.

이 기법은 다양한 종횡비의 이미지나 이미지 내 텍스트 읽기와 관련된 작업에서 특히 효과적입니다. 기존의 정사각형 크롭 방식에서는 중요한 정보가 손실되거나 텍스트가 읽을 수 없게 되는 문제가 있었지만, P&S를 통해 이러한 한계를 극복할 수 있습니다.

P&S 알고리즘은 이미지를 겹치지 않는 동일한 크기의 자르기로 분할하여 전체 이미지를 커버하고, 각 자르기를 896×896 픽셀로 크기 조정하여 인코더에 전달합니다. 이는 [LLaVA](https://arxiv.org/pdf/2304.08485v2)에서 영감을 받은 방법으로, 시각적 언어 모델에서 특히 중요한 개선사항입니다.

이러한 절제 연구 결과들은 Gemma 3의 각 설계 결정이 과학적 근거에 기반하고 있음을 보여줍니다. 특히 로컬-글로벌 어텐션 비율, 슬라이딩 윈도우 크기, 그리고 비전 처리 방식 등의 최적화가 모델의 전반적인 성능 향상에 기여했음을 확인할 수 있습니다.
Gemma 3의 기억화와 개인정보 보호 분석은 대규모 언어 모델의 훈련 데이터 기억화 위험을 정량적으로 평가하는 중요한 연구 영역입니다. 이 분석에서는 모델이 훈련 과정에서 학습한 텍스트를 얼마나 정확하게 재생산할 수 있는지를 측정하여 개인정보 유출 위험을 평가합니다.

## 기억화 측정 방법론

Gemma 3의 기억화 평가는 [Gemma Team (2024b)](https://arxiv.org/pdf/2408.00118)에서 제안된 방법론을 따릅니다. 이 접근법은 [Carlini et al. (2021, 2022)](https://arxiv.org/pdf/2202.07646)과 [Nasr et al. (2023)](https://arxiv.org/pdf/2311.17035)에서 개발된 발견 가능한 추출(discoverable extraction) 기법을 기반으로 합니다.

구체적인 측정 과정은 다음과 같습니다. 먼저 훈련 데이터에서 다양한 코퍼스에 걸쳐 균등하게 분포된 대규모 부분집합을 샘플링합니다. 그 다음 길이 50토큰의 접두사(prefix)와 길이 50토큰의 접미사(suffix)를 사용하여 발견 가능한 추출을 테스트합니다.

기억화는 두 가지 유형으로 분류됩니다. "정확한 기억화(exactly memorized)"는 생성된 연속 텍스트의 모든 토큰이 원본 접미사와 일치하는 경우를 의미합니다. "근사적 기억화(approximately memorized)"는 생성된 텍스트가 원본과 최대 10%의 편집 거리(edit distance) 내에서 일치하는 경우를 나타냅니다.

이러한 방법론은 [Ippolito et al. (2022)](https://arxiv.org/pdf/2210.17546)에서 제기된 중요한 통찰을 반영합니다. 단순히 정확한 기억화만을 방지하는 것으로는 충분하지 않으며, 근사적 기억화 역시 개인정보 보호와 저작권 관련 우려를 야기할 수 있다는 것입니다.

## 기억화 비율 분석 결과

![Gemma 3 모델들의 총 기억화 비율](/assets/2025-10-03-gemma-3-technical-report/8.png)

위 그래프는 Gemma와 Gemini 모델들의 정확한 기억화와 근사적 기억화 비율을 비교한 결과를 보여줍니다. 모델들은 역시간순으로 배열되어 있으며, 가장 최신인 Gemma 3 모델들이 왼쪽에 위치합니다.

결과는 매우 인상적입니다. Gemma 3 모델들은 이전 모델들에 비해 현저히 낮은 기억화 비율을 보여줍니다. 특히 로그 스케일 y축을 고려할 때, 이 개선은 상당한 수준입니다. 4B, 12B, 27B 모델 간의 기억화 비율 차이는 미미한 반면, 1B 모델은 더 큰 모델들보다 약간 낮은 기억화 비율을 보입니다.

흥미롭게도 근사적 기억화의 비중이 크게 증가했습니다. 정확한 기억화 대비 근사적 기억화의 상대적 증가는 평균적으로 약 24배에 달합니다. 이는 모델이 훈련 데이터를 완전히 동일하게 재생산하기보다는 유사한 형태로 변형하여 생성하는 경향이 강해졌음을 의미합니다.

이러한 결과는 [Carlini et al.](https://arxiv.org/pdf/2202.07646)의 연구에서 관찰된 패턴과 일치합니다. 해당 연구에서는 더 큰 모델일수록 훈련 데이터셋의 더 많은 부분을 기억화하는 경향을 보인다고 보고했습니다. 그러나 Gemma 3의 경우 모델 크기에 따른 기억화 비율의 차이가 상대적으로 작다는 점이 주목할 만합니다.

## 개인정보 탐지 및 분석

기억화된 텍스트에 개인정보가 포함될 위험을 평가하기 위해 Google Cloud Sensitive Data Protection (SDP) 서비스를 활용했습니다. SDP는 개인정보가 포함될 가능성이 있는 텍스트를 식별하기 위한 광범위한 탐지 규칙을 사용합니다.

SDP는 높은 재현율(high recall)을 목표로 설계되었으며, 정보가 나타나는 맥락을 고려하지 않습니다. 이로 인해 많은 거짓 양성(false positive) 결과가 발생할 수 있어, 실제로는 기억화된 출력에 포함된 잠재적 개인정보의 양을 과대평가할 가능성이 높습니다.

SDP는 또한 낮음(low), 중간(medium), 높음(high)의 광범위한 심각도 수준을 제공합니다. 연구에서는 SDP가 어떤 심각도 수준에서든 개인정보로 분류한 텍스트를 개인정보로 간주했습니다.

분석 결과, 모든 Gemma 3 모델에서 기억화로 특성화된 출력에서 개인정보가 관찰되지 않았습니다. 이는 기억화로 분류된 출력에서 개인 데이터의 비율이 탐지 임계값 이하의 낮은 수준임을 나타냅니다.

## 기억화 감소의 기술적 의미

Gemma 3에서 관찰된 기억화 비율의 현저한 감소는 여러 기술적 개선사항의 결과로 해석됩니다. 첫째, 향상된 데이터 필터링 기법이 적용되었습니다. 원하지 않거나 안전하지 않은 발화의 위험을 줄이고, 개인정보 및 기타 민감한 데이터를 제거하는 포괄적인 필터링이 수행되었습니다.

둘째, 평가 데이터셋의 오염 제거(decontamination) 과정이 강화되었습니다. 이는 사전 훈련 데이터 혼합에서 평가 세트를 제거하여 모델이 평가 시에 이미 본 데이터로 인한 부정확한 성능 측정을 방지하는 동시에, 암송(recitation) 위험을 줄이는 효과를 가져왔습니다.

셋째, [Sachdeva et al. (2024)](https://arxiv.org/pdf/2402.09668)에서 영감을 받은 품질 재가중치(quality reweighing) 단계가 적용되었습니다. ASK-LLM과 같은 품질 기반 샘플링 기법을 활용하여 저품질 데이터의 발생을 줄이는 혁신적인 접근법이 사용되었습니다.

## 개인정보 보호 관점에서의 평가

기억화 분석 결과는 개인정보 보호 관점에서 여러 중요한 시사점을 제공합니다. 먼저 Gemma 3 모델들이 이전 버전 대비 현저히 낮은 기억화 비율을 보인다는 점은 개인정보 유출 위험이 상당히 감소했음을 의미합니다.

그러나 [Nasr et al. (2023)](https://arxiv.org/pdf/2311.17035)의 연구에서 지적한 바와 같이, "발견 가능한 기억화"와 "추출 가능한 기억화" 사이에는 중요한 차이가 있습니다. 현재 측정된 것은 발견 가능한 기억화이며, 실제 추출 가능한 기억화는 이보다 훨씬 높을 수 있습니다.

또한 근사적 기억화의 비중이 크게 증가한 점도 주목해야 합니다. 비록 정확한 복사본은 아니지만, 근사적으로 기억화된 텍스트 역시 개인정보나 저작권 보호 대상 콘텐츠를 포함할 수 있습니다.

SDP를 통한 개인정보 탐지에서 아무런 개인정보가 발견되지 않았다는 결과는 긍정적이지만, SDP의 높은 거짓 양성률과 맥락을 고려하지 않는 특성을 감안할 때 신중하게 해석해야 합니다.

## 기억화 연구의 한계와 향후 과제

현재의 기억화 측정 방법론에는 몇 가지 한계가 있습니다. 첫째, 50토큰의 고정된 접두사와 접미사 길이를 사용하는 것이 모든 유형의 기억화를 포착하기에 충분한지에 대한 의문이 있습니다. 더 짧거나 더 긴 시퀀스에서 발생하는 기억화는 놓칠 수 있습니다.

둘째, 10%의 편집 거리 임계값이 근사적 기억화를 정의하는 데 적절한지에 대한 검토가 필요합니다. 이 임계값은 다소 임의적이며, 실제 개인정보 보호 위험과의 상관관계에 대한 더 깊은 연구가 필요합니다.

셋째, 현재의 방법론은 주로 텍스트 기반 기억화에 초점을 맞추고 있습니다. Gemma 3와 같은 멀티모달 모델에서는 이미지와 텍스트 간의 연관성을 통한 기억화 위험도 고려해야 합니다.

향후 연구에서는 이러한 한계를 극복하기 위한 더 정교한 기억화 측정 방법론의 개발이 필요합니다. 또한 기억화 감소 기법의 효과성을 더욱 체계적으로 평가하고, 모델의 유용성과 개인정보 보호 사이의 균형을 최적화하는 방법에 대한 연구가 중요할 것입니다.
Gemma 3의 책임감 있는 AI 개발과 안전성 확보는 단순히 기술적 성능 향상을 넘어서 사회적 영향과 윤리적 고려사항을 포괄하는 종합적인 접근법을 필요로 합니다. Google DeepMind는 Gemma 3 개발 과정에서 강화된 내부 안전 프로세스를 통합하여 사용자와 사회 전반의 위험을 최소화하고자 했습니다.

## 거버넌스와 위험 평가 체계

Gemma 3의 위험 평가 접근법은 기존 Gemma 1에서 수립된 프레임워크를 기반으로 하되, 새롭게 도입된 멀티모달 능력을 고려하여 확장되었습니다. 이 접근법의 핵심은 AI 개방성의 이익과 악의적 사용으로 인한 위험 사이의 균형을 찾는 것입니다.

[Weidinger et al. (2021)](https://arxiv.org/pdf/2112.04359v1)에서 제시된 포괄적인 위험 분류 체계는 Gemma 3의 안전성 평가에 중요한 이론적 기반을 제공합니다. 해당 연구에서는 대규모 언어 모델의 위험을 6개 주요 영역으로 분류했습니다. 차별과 독성, 정보 위험, 잘못된 정보 확산, 악의적 사용, 인간-컴퓨터 상호작용 위험, 그리고 자동화와 환경적 위험입니다. 이러한 분류는 Gemma 3의 멀티모달 능력이 가져올 수 있는 새로운 위험 유형을 이해하는 데 특히 중요합니다.

멀티모달 모델의 경우 텍스트와 이미지를 동시에 처리할 수 있는 능력으로 인해 기존 텍스트 전용 모델보다 복잡한 위험 시나리오가 발생할 수 있습니다. 예를 들어, 이미지 내 텍스트 정보를 추출하여 개인정보를 유출하거나, 시각적 콘텐츠와 텍스트를 조합하여 더욱 설득력 있는 허위 정보를 생성할 가능성이 있습니다.

Gemma 3 출시 이후 개발된 ShieldGemma 2는 이러한 위험 관리 접근법의 구체적인 성과물입니다. 이는 40억 매개변수의 이미지 안전 분류기로, Gemma 3를 기반으로 구축되어 위험한 콘텐츠, 성적으로 노골적인 내용, 폭력적 콘텐츠 등의 카테고리에서 안전 레이블을 제공하는 즉시 사용 가능한 솔루션을 제공합니다.

## 안전 정책과 훈련 시점 완화 기법

Gemma 3의 안전성 확보를 위한 핵심 전략 중 하나는 Google의 안전 정책과 일치하도록 미세 조정된 모델을 정렬하는 것입니다. 이는 Gemini 모델에서 사용된 접근법과 동일한 원칙을 따르며, 다음과 같은 유해 콘텐츠 생성을 방지하도록 설계되었습니다.

**아동 성적 학대 및 착취 방지**: 모델이 아동을 대상으로 한 성적 콘텐츠나 착취적 내용을 생성하지 않도록 하는 것은 최우선 안전 정책입니다. 이는 단순히 명시적인 콘텐츠뿐만 아니라 암시적이거나 간접적인 형태의 유해 콘텐츠도 포함합니다.

**개인정보 보호**: 사회보장번호와 같이 개인에게 해를 끼칠 수 있는 개인식별정보의 노출을 방지합니다. 이는 특히 멀티모달 모델에서 중요한데, 이미지 내에 포함된 개인정보를 텍스트로 추출할 수 있는 능력 때문입니다.

**혐오 발언 및 괴롭힘 방지**: 특정 집단이나 개인을 대상으로 한 차별적이거나 괴롭힘에 해당하는 콘텐츠 생성을 방지합니다. 이는 [Weidinger et al.](https://arxiv.org/pdf/2112.04359v1)에서 지적한 차별과 독성 위험 범주와 직접적으로 연관됩니다.

**위험하거나 악의적인 콘텐츠**: 자해를 조장하거나 유해한 활동에 대한 지침을 제공하는 콘텐츠를 방지합니다. 이는 모델이 실제 물리적 피해로 이어질 수 있는 정보를 제공하지 않도록 하는 중요한 안전장치입니다.

이러한 정책들을 구현하기 위해 Gemma 3는 포괄적인 사전 훈련 데이터 필터링을 수행했습니다. 이는 사전 훈련된 체크포인트와 미세 조정된 체크포인트 모두에서 유해 콘텐츠 생성 가능성을 줄이기 위한 예방적 조치입니다. 미세 조정된 모델의 경우 지도 학습 미세 조정(SFT)과 인간 피드백을 통한 강화학습(RLHF)을 모두 활용하여 바람직하지 않은 행동으로부터 모델을 유도합니다.

## 보증 평가와 위험 측정

Gemma 3의 안전성 평가는 모델이 야기할 수 있는 잠재적 위험을 이해하기 위한 기준선 보증 평가를 포함합니다. 오픈 모델의 특성상 가중치 공개의 비가역적 특성을 고려할 때, 엄격한 위험 평가가 필수적입니다.

**기준선 평가**: 기준선 보증은 대규모 합성 적대적 사용자 쿼리를 사용하여 안전 정책 위반률을 측정하고, 인간 평가자가 답변을 정책 위반 여부로 분류하는 방식으로 수행됩니다. Gemma 3는 이러한 안전 정책에서 전반적으로 현저히 낮은 위반률을 보였습니다.

**CBRN 지식 평가**: STEM 관련 작업에서의 향상된 성능을 고려하여, 생물학적, 방사선학적, 핵 위험과 관련된 지식을 내부 데이터셋의 폐쇄형 지식 기반 객관식 문제를 사용하여 평가했습니다. 화학 지식 평가의 경우 Macknight et al.이 개발한 화학 위험에 대한 폐쇄형 지식 기반 접근법을 사용했습니다. 평가 결과 Gemma 3 모델들의 이러한 영역에서의 지식 수준은 낮은 것으로 나타났습니다.

이러한 평가 접근법은 [Phuong et al. (2024)](https://arxiv.org/pdf/2403.13793v2)와 [Shevlane et al. (2023)](https://arxiv.org/pdf/2305.15324v2)에서 제안된 극단적 위험 평가 프레임워크와 일치합니다. 해당 연구들에서는 대규모 AI 모델의 위험한 능력과 정렬 상태를 평가하기 위한 체계적인 방법론을 제시했습니다. 특히 위험한 능력 평가는 사이버 공격, 기만, 설득/조작, 무기 획득, 자기 증식 등의 능력을 측정하는 것을 포함하며, 정렬 평가는 장기적 실제 목표 추구, 권력 추구, 종료 저항, 다른 AI 시스템과의 공모 등의 행동을 찾는 것을 목표로 합니다.

Gemma 3의 경우 더 능력 있는 모델을 철저히 평가하는 것이 덜 능력 있는 모델에 대한 충분한 보증을 제공한다는 휴리스틱을 따라 간소화된 평가 세트를 우선시했습니다. 이는 개발 속도와 표적화된 안전 테스트 사이의 균형을 맞추면서도 Frontier Safety Framework에서 제시된 약속을 유지하는 효율적인 접근법입니다.

## 책임감 있는 오픈 모델에 대한 접근법

안전하고 보안이 확보된 책임감 있는 애플리케이션을 설계하려면 시스템 수준의 접근법이 필요하며, 각 특정 사용 사례와 환경과 관련된 위험을 완화하기 위해 노력해야 합니다. Google DeepMind는 모델의 잠재적 위험에 비례하는 평가와 안전 완화 조치를 지속적으로 채택할 것이며, 예측 가능한 위험보다 이익이 현저히 클 때만 이러한 모델을 커뮤니티와 공유할 것입니다.

이러한 접근법은 AI 개방성의 이익과 위험 사이의 신중한 균형을 반영합니다. 오픈 소스 모델은 연구 투명성, 재현 가능성, 그리고 광범위한 혁신을 촉진할 수 있는 잠재력을 가지고 있지만, 동시에 악의적 사용자에 의한 오남용 위험도 존재합니다.

Gemma 3의 경우 현재 사용 가능한 더 큰 강력한 오픈 모델들의 수를 고려할 때, 이번 출시가 전체 위험 환경에 미치는 영향은 미미할 것으로 평가됩니다. 이는 모델의 상대적 능력과 기존 모델 생태계 내에서의 위치를 고려한 현실적인 위험 평가를 반영합니다.

또한 Gemma에 대한 악의적 사용 보고는 아직 접수되지 않았지만, 그러한 보고가 있을 경우 조사할 것을 약속하며, 학술 및 개발자 커뮤니티와 협력하고 자체 모니터링을 수행하여 그러한 사례를 표시할 것입니다. 이는 지속적인 모니터링과 커뮤니티 기반 안전성 확보의 중요성을 강조합니다.

결론적으로 Gemma 3의 안전성과 책임감 있는 개발 접근법은 기술적 혁신과 사회적 책임 사이의 균형을 추구하는 포괄적인 프레임워크를 제시합니다. 이는 단순히 모델의 기술적 성능만을 고려하는 것이 아니라, 실제 배포 환경에서의 사회적 영향과 윤리적 고려사항을 종합적으로 평가하는 성숙한 AI 개발 접근법을 보여줍니다.
Gemma 3 연구진은 이 논문의 결론에서 텍스트, 이미지, 코드를 지원하는 최신 오픈 언어 모델 패밀리인 Gemma 3의 핵심 성과와 기술적 기여를 종합적으로 정리하고 있습니다. 이 모델은 이미지 이해 능력, 긴 컨텍스트 처리, 향상된 다국어 지원, 그리고 STEM 관련 능력 개선에 중점을 두고 개발되었습니다.

## 주요 기술적 성과

Gemma 3의 가장 중요한 성과 중 하나는 표준 하드웨어와의 호환성을 유지하면서도 성능을 크게 향상시킨 것입니다. 연구진은 모델 크기와 아키텍처를 표준 하드웨어에서 실행 가능하도록 설계했으며, 대부분의 아키텍처 개선사항들이 하드웨어 제약 조건 내에서 성능을 최적화하도록 맞춤화되었습니다.

특히 주목할 만한 점은 Gemma3-4B-IT 모델이 훨씬 큰 Gemma2-27B-IT 모델과 경쟁할 수 있는 성능을 달성했다는 것입니다. 이는 단순히 모델 크기를 늘리는 것보다 아키텍처 혁신과 훈련 방법론의 개선이 더 효과적일 수 있음을 보여주는 중요한 결과입니다.

## 아키텍처 혁신의 실용적 가치

앞서 설명한 로컬-글로벌 어텐션 레이어의 5:1 교대 배치와 128K 토큰 컨텍스트 지원은 실제 배포 환경에서 매우 실용적인 가치를 제공합니다. 이러한 설계는 메모리 효율성과 성능 사이의 최적 균형점을 찾아 소비자급 하드웨어에서도 긴 문서 처리가 가능하게 합니다.

멀티모달 능력의 추가 역시 단순한 기능 확장을 넘어서 실제 응용 분야를 크게 넓혔습니다. 텍스트와 이미지를 동시에 처리할 수 있는 능력은 교육, 의료, 창작, 문서 분석 등 다양한 분야에서 즉시 활용 가능한 실용적 가치를 제공합니다.

## 오픈 소스 생태계에 대한 기여

Gemma 3는 오픈 소스 언어 모델 생태계에서 중요한 이정표를 제시합니다. 연구진이 강조하는 바와 같이, 이 모델은 연구 투명성과 재현 가능성을 촉진하면서도 광범위한 혁신을 가능하게 하는 플랫폼 역할을 합니다.

특히 [LMSYS Chatbot Arena](https://arxiv.org/pdf/2403.04132v1)에서 달성한 1338점의 Elo 점수는 27B 매개변수라는 상대적으로 작은 크기로도 최상위 성능을 달성할 수 있음을 입증했습니다. 이는 훨씬 큰 모델들과 경쟁하면서도 접근성과 실용성을 동시에 확보한 균형 잡힌 접근법의 성공을 보여줍니다.

## 책임감 있는 AI 개발의 모범 사례

연구진이 구현한 포괄적인 안전성 평가와 위험 관리 체계는 대규모 언어 모델의 책임감 있는 개발과 배포를 위한 모범 사례를 제시합니다. 기억화 비율의 현저한 감소, 개인정보 보호 강화, 그리고 다층적 안전 정책의 구현은 오픈 소스 모델이 사회적 책임을 다하면서도 혁신을 촉진할 수 있음을 보여줍니다.

## 미래 연구 방향에 대한 시사점

Gemma 3의 성과는 향후 언어 모델 연구에 여러 중요한 시사점을 제공합니다. 첫째, 효율적인 아키텍처 설계가 단순한 모델 크기 확장보다 더 효과적일 수 있다는 점입니다. 둘째, 멀티모달 능력의 통합이 모델의 실용성을 크게 향상시킬 수 있다는 점입니다. 셋째, 적절한 사후 훈련 방법론이 모델 성능에 결정적인 영향을 미친다는 점입니다.

## 기술적 한계와 향후 과제

연구진은 현재 기술의 한계도 솔직하게 인정하고 있습니다. 128K 토큰을 넘어서는 컨텍스트 확장에서는 급격한 성능 저하가 관찰되었으며, 이는 향후 연구에서 해결해야 할 중요한 과제입니다. 또한 멀티모달 처리에서 발생할 수 있는 새로운 위험 시나리오에 대한 지속적인 연구와 대응이 필요합니다.

## 결론적 평가

Gemma 3는 대규모 언어 모델 분야에서 기술적 혁신, 실용적 가치, 그리고 사회적 책임을 균형 있게 달성한 성공적인 사례입니다. 특히 표준 하드웨어에서의 실행 가능성을 유지하면서도 최상위 성능을 달성한 것은 AI 기술의 민주화와 접근성 향상에 중요한 기여를 했습니다.

이 연구는 단순히 새로운 모델을 제시하는 것을 넘어서, 효율적인 아키텍처 설계, 혁신적인 훈련 방법론, 그리고 책임감 있는 개발 프로세스의 통합을 통해 차세대 언어 모델 개발의 새로운 패러다임을 제시했습니다. Gemma 3의 성과는 향후 오픈 소스 AI 생태계의 발전과 실용적 AI 응용 분야의 확장에 지속적인 영향을 미칠 것으로 예상됩니다.
Gemma 3 기술 보고서의 부록에서는 사전 훈련된 모델들의 상세한 성능 지표와 지시 조정된 모델들의 추가적인 평가 결과를 포괄적으로 제시하고 있습니다. 이 부록은 본문에서 다루지 못한 세부적인 벤치마크 결과들과 평가 프로토콜의 구체적인 내용을 담고 있어, Gemma 3 모델들의 실제 성능을 정량적으로 이해하는 데 중요한 자료를 제공합니다.

## 사전 훈련 모델의 사실성과 상식 추론 성능

사전 훈련 단계에서의 성능 평가는 모델이 기본적인 언어 이해 능력과 상식적 추론 능력을 얼마나 잘 습득했는지를 보여주는 중요한 지표입니다. 다음 표는 Gemma 2와 Gemma 3 모델들의 다양한 벤치마크에서의 성능을 비교한 결과입니다.

| 벤치마크 | Gemma 2 2B | Gemma 2 9B | Gemma 2 27B | Gemma 3 1B | Gemma 3 4B | Gemma 3 12B | Gemma 3 27B |
|----------|------------|------------|-------------|------------|------------|-------------|-------------|
| HellaSwag | 72.9 | 81.9 | 86.4 | 62.3 | 77.2 | 84.2 | 85.6 |
| BoolQ | 75.6 | 77.5 | 76.2 | 63.2 | 72.3 | 78.8 | 82.4 |
| PIQA | 78.1 | 81.9 | 83.5 | 73.8 | 79.6 | 81.8 | 83.3 |
| SIQA | 51.8 | 53.3 | 53.8 | 48.9 | 51.9 | 53.4 | 54.9 |
| TQA | 60.2 | 76.5 | 83.8 | 39.8 | 65.8 | 78.2 | 85.5 |
| NQ | 17.2 | 29.2 | 34.7 | 9.48 | 20.0 | 31.4 | 36.1 |
| ARC-C | 55.8 | 69.1 | 71.4 | 38.4 | 56.2 | 68.9 | 70.6 |
| ARC-E | 80.6 | 88.3 | 88.6 | 73.0 | 82.4 | 88.3 | 89.0 |
| WinoG | 65.4 | 73.9 | 79.4 | 58.2 | 64.7 | 74.3 | 78.8 |
| BBH | 42.4 | 69.4 | 74.8 | 28.4 | 50.9 | 72.6 | 77.7 |
| Drop | 53.2 | 71.5 | 75.2 | 42.4 | 60.1 | 72.2 | 77.2 |

이 결과들은 Gemma 3 모델들이 전반적으로 이전 버전과 유사하거나 약간 향상된 성능을 보여주고 있음을 나타냅니다. 특히 주목할 만한 점은 이러한 능력들이 이번 버전의 주요 개선 목표가 아니었음에도 불구하고 성능이 유지되거나 향상되었다는 것입니다. 이는 멀티모달 능력과 긴 컨텍스트 처리 능력을 추가하면서도 기존의 언어 이해 능력을 손상시키지 않았음을 의미합니다.

[HellaSwag](https://arxiv.org/pdf/1905.07830)는 상식적 추론을 평가하는 벤치마크로, 문장의 끝을 자연스럽게 완성하는 능력을 측정합니다. [BoolQ](https://arxiv.org/pdf/1905.10044)는 예/아니오 질문에 대한 답변 능력을 평가하며, [PIQA](https://arxiv.org/pdf/1911.11641)는 물리적 상식 추론을 측정합니다.

## STEM 및 코딩 능력 평가

STEM 분야와 코딩 능력에서는 Gemma 3 모델들이 일관된 개선을 보여주었습니다.

| 벤치마크 | Gemma 2 2B | Gemma 2 9B | Gemma 2 27B | Gemma 3 4B | Gemma 3 12B | Gemma 3 27B |
|----------|------------|------------|-------------|------------|-------------|-------------|
| MMLU | 52.2 | 71.2 | 75.2 | 59.6 | 74.5 | 78.6 |
| MMLU-Pro | 22.2 | 43.7 | 49.4 | 29.2 | 45.3 | 52.2 |
| AGIEval | 31.6 | 53.1 | 55.1 | 42.1 | 57.4 | 66.2 |
| MATH | 16.4 | 36.4 | 42.1 | 24.2 | 43.3 | 50.0 |
| GSM8K | 25.0 | 70.2 | 74.6 | 38.4 | 71.0 | 82.6 |
| GPQA Diamond | 12.5 | 24.8 | 26.3 | 15.0 | 25.4 | 24.3 |
| MBPP | 31.0 | 51.2 | 60.8 | 46.0 | 60.4 | 65.6 |
| HumanEval | 19.5 | 40.2 | 51.2 | 36.0 | 45.7 | 48.8 |

이 결과들은 Gemma 3 모델들이 수학적 추론과 코딩 능력에서 상당한 개선을 보였음을 보여줍니다. 특히 [MATH](https://arxiv.org/pdf/2103.03874) 벤치마크에서 27B 모델이 50.0점을 달성한 것은 복잡한 수학 문제 해결 능력이 크게 향상되었음을 의미합니다. [GSM8K](https://arxiv.org/pdf/2110.14168)에서도 82.6점으로 높은 성능을 보여 초등학교 수준의 수학 문제 해결에서 우수한 능력을 입증했습니다.

코딩 능력 평가에서는 4B와 12B 모델에서 유의미한 개선이 관찰되었지만, 27B 모델에서는 상대적으로 작은 향상을 보였습니다. 이는 모델 크기에 따른 성능 향상이 항상 선형적이지 않음을 시사합니다.

## 멀티모달 성능 평가

멀티모달 능력은 Gemma 3의 핵심 혁신 중 하나로, 다양한 시각적 질문 답변 벤치마크에서 평가되었습니다.

| 벤치마크 | Gemma 3 4B | Gemma 3 12B | Gemma 3 27B |
|----------|------------|-------------|-------------|
| COCO Caption | 102 | 111 | 116 |
| DocVQA | 72.8 | 82.3 | 85.6 |
| InfoVQA | 44.1 | 54.8 | 59.4 |
| MMMU | 39.2 | 50.3 | 56.1 |
| TextVQA | 58.9 | 66.5 | 68.6 |
| RealWorldQA | 45.5 | 52.2 | 53.9 |
| ReMI | 27.3 | 38.5 | 44.8 |
| AI2D | 63.2 | 75.2 | 79.0 |
| ChartQA | 63.6 | 74.7 | 76.3 |
| VQAv2 | 63.9 | 71.2 | 72.9 |

이 결과들은 모델 크기가 증가함에 따라 멀티모달 성능이 일관되게 향상됨을 보여줍니다. 특히 [DocVQA](https://arxiv.org/pdf/2007.00398)에서 27B 모델이 85.6점을 달성한 것은 문서 이해 능력이 매우 우수함을 나타냅니다. [AI2D](https://arxiv.org/pdf/1603.07396)에서의 79.0점은 과학 다이어그램 이해 능력이 뛰어남을 보여줍니다.

## PaliGemma 2와의 비교

Gemma 3 모델들을 기존의 PaliGemma 2와 비교한 결과는 다음과 같습니다.

| 벤치마크 | PaliGemma 2 2B | PaliGemma 2 9B | PaliGemma 2 27B | Gemma 3 4B | Gemma 3 12B | Gemma 3 27B |
|----------|----------------|----------------|-----------------|------------|-------------|-------------|
| DocVQA | 81.6 | 86.3 | 85.1 | 86.1 | 89.0 | 89.5 |
| InfoVQA | 41.4 | 53.1 | 50.2 | 55.6 | 61.6 | 64.6 |
| TextVQA | 76.3 | 76.3 | 75.1 | 79.1 | 81.6 | 83.2 |
| ChartQA | 70.7 | 79.1 | 71.3 | 79.8 | 83.5 | 83.4 |
| AI2D | 76.0 | 84.4 | 84.6 | 80.9 | 85.6 | 86.5 |

이 비교 결과는 Gemma 3가 문서 이해 관련 벤치마크에서 더 큰 PaliGemma 2 변형보다도 우수한 성능을 보임을 나타냅니다. 특히 주목할 점은 비전 인코더의 평균 풀링으로 인해 Gemma 3 4B와 12B 모델들이 동일한 896×896 해상도에서 PaliGemma 2 9B와 27B 모델들보다 약 10배 저렴한 전이 비용을 가진다는 것입니다.

## 다국어 성능 평가

Gemma 3의 다국어 능력은 여러 언어에 걸친 다양한 작업에서 평가되었습니다.

| 벤치마크 | Gemma 2 2B | Gemma 2 9B | Gemma 2 27B | Gemma 3 1B | Gemma 3 4B | Gemma 3 12B | Gemma 3 27B |
|----------|------------|------------|-------------|------------|------------|-------------|-------------|
| MGSM | 18.7 | 57.3 | 68.0 | 2.04 | 34.7 | 64.3 | 74.3 |
| GMMLU | 43.3 | 64.0 | 69.4 | 24.9 | 57.0 | 69.4 | 75.7 |
| WMT24++ | 38.8 | 50.3 | 53.0 | 36.7 | 48.4 | 53.9 | 55.7 |
| Flores | 30.2 | 41.3 | 44.3 | 29.5 | 39.2 | 46.0 | 48.8 |
| XQuAD | 53.7 | 72.2 | 73.9 | 43.9 | 68.0 | 74.5 | 76.8 |

[MGSM](https://arxiv.org/pdf/2210.03057)은 다국어 수학 문제 해결 능력을 평가하며, Gemma 3 27B 모델이 74.3점으로 우수한 성능을 보였습니다. [Global-MMLU-Lite](https://arxiv.org/pdf/2312.07302)에서도 75.7점을 달성하여 다양한 언어에서의 지식 이해 능력이 크게 향상되었음을 보여줍니다.

## 긴 컨텍스트 성능 평가

Gemma 3의 핵심 기능 중 하나인 긴 컨텍스트 처리 능력은 RULER와 MRCR 벤치마크를 통해 평가되었습니다.

| 모델 | 컨텍스트 | RULER 32K | RULER 128K | MRCR 32K | MRCR 128K |
|------|----------|-----------|------------|----------|-----------|
| Gemma 3 PT 4B | - | 67.1 | 51.7 | 44.7 | 40.6 |
| Gemma 3 PT 12B | - | 90.6 | 80.7 | 59.8 | 56.9 |
| Gemma 3 PT 27B | - | 85.9 | 72.9 | 63.2 | 60.0 |
| Gemma 3 IT 4B | - | 61.4 | 46.8 | 49.8 | 44.6 |
| Gemma 3 IT 12B | - | 80.3 | 57.1 | 53.7 | 49.8 |
| Gemma 3 IT 27B | - | 91.1 | 66.0 | 63.2 | 59.3 |

이 결과들은 Gemma 3 모델들이 32K 토큰에서는 매우 우수한 성능을 보이지만, 128K 토큰으로 확장될 때는 성능 저하가 발생함을 보여줍니다. 그럼에도 불구하고 12B와 27B 모델들은 128K 컨텍스트에서도 실용적인 수준의 성능을 유지하고 있습니다.

## 지시 조정 모델의 추가 평가

지시 조정된 모델들의 성능은 더욱 다양한 벤치마크에서 평가되었으며, 특히 수학, 코딩, 추론 능력에서 현저한 개선을 보였습니다. 이러한 결과들은 개선된 사후 훈련 접근법의 효과를 명확히 보여주며, Gemma 3가 실제 사용자 요구사항에 얼마나 잘 대응할 수 있는지를 입증합니다.

부록에서 제시된 이러한 상세한 평가 결과들은 Gemma 3 모델들이 다양한 영역에서 균형 잡힌 성능 향상을 달성했음을 종합적으로 보여줍니다. 특히 멀티모달 능력의 추가와 긴 컨텍스트 처리 능력의 도입이 기존 성능을 저해하지 않으면서도 새로운 가능성을 열어주었다는 점이 주목할 만합니다.
- - -
### References
* [Gemma 3 Technical Report](http://arxiv.org/pdf/2503.19786v1)