---
layout: post
title: "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"
date: 2024-01-11 17:31:42
author: "DeepSeek-AI"
categories: "Language-Models"
tags: ["Fine-Grained-Expert-Segmentation", "Shared-Expert-Isolation", "Ultimate-Expert-Specialization", "Mixture-of-Experts-Architecture", "Parameter-Efficient-Language-Model-Scaling"]
use_math: true
cover: /assets/images/language-models.webp
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
대규모 언어 모델의 발전에서 계산 비용과 효율성은 중요한 도전 과제로 대두되어 왔습니다. 전문가 혼합(Mixture-of-Experts, MoE) 아키텍처는 이러한 문제를 해결할 수 있는 유망한 접근 방식으로 주목받았으나, 기존 MoE 모델들은 지식의 혼재성과 중복성이라는 한계에 직면해 있었습니다. 특히 GShard와 같은 기존 아키텍처에서는 제한된 수의 전문가를 활용하는 방식으로 인해 각 전문가가 특정 입력 패턴에 대해 충분히 특화되지 못하는 문제가 있었습니다. 이러한 한계를 극복하고 전문가 특화를 극대화하는 새로운 접근 방식의 필요성이 이 연구의 주요 동기가 되었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
DeepSeekMoE는 두 가지 핵심 전략을 통해 전문가 특화 문제를 해결합니다. 첫째, '세밀한 전문가 분할' 전략을 도입하여 전문가를 더 작은 단위로 분할하고 활성화되는 전문가의 수를 증가시켰습니다. 이를 통해 각 전문가가 더욱 특화된 지식을 학습할 수 있게 되었습니다. 둘째, '공유 전문가 분리' 전략을 통해 일부 전문가를 항상 활성화되는 공유 전문가로 지정하여 공통 지식을 효율적으로 처리하고, 라우팅된 전문가들 간의 지식 중복을 줄였습니다. 이러한 혁신적인 접근은 전문가 네트워크의 특화도를 극대화하면서도 계산 효율성을 유지하는 것을 가능하게 했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
DeepSeekMoE의 구현은 트랜스포머 아키텍처를 기반으로 하되, 피드포워드 네트워크를 MoE 계층으로 대체하는 방식을 채택했습니다. 16B 모델의 경우, 2T 토큰으로 학습을 진행했으며, 전문가 수준과 디바이스 수준의 균형을 위한 특별한 손실 함수를 도입했습니다. 구체적으로, 전문가 수준 균형 인자를 0.01로, 디바이스 수준 균형 인자를 0.05로 설정하여 라우팅 붕괴를 방지하고 계산 부하의 균형을 맞추었습니다. 또한, AdamW 옵티마이저와 웜업-스텝-감소 학습률 전략을 사용하여 효과적인 학습을 가능하게 했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
DeepSeekMoE의 연구 결과는 대규모 언어 모델 분야에 중요한 의미를 가집니다. 16B 모델은 약 40%의 계산량만으로 DeepSeek 7B와 같은 밀집 모델과 대등한 성능을 달성했으며, 더 나아가 145B 규모로의 확장 가능성도 입증했습니다. 특히 주목할 만한 점은 이 모델이 유사한 수의 활성화된 매개변수를 가진 다른 모델들보다 일관되게 우수한 성능을 보여주었다는 것입니다. 이는 MoE 아키텍처가 계산 효율성과 모델 성능 사이의 최적 균형을 달성할 수 있다는 것을 입증하며, 향후 대규모 언어 모델 개발의 새로운 방향을 제시합니다.
- - -
## DeepSeekMoE: 대규모 언어 모델에서의 궁극적 전문가 특화를 향하여

DeepSeek-AI의 연구진이 제시한 DeepSeekMoE는 대규모 언어 모델에서 전문가 혼합(Mixture-of-Experts, MoE) 아키텍처의 성능을 극대화하기 위한 혁신적인 접근 방식을 소개합니다. 이 연구는 Jacobs와 연구진이 처음 제안한 적응형 전문가 혼합 모델의 기본 원리를 현대적인 트랜스포머 기반 언어 모델에 확장 적용하여, 각 전문가 네트워크의 특화도를 최대한으로 끌어올리는 것을 목표로 합니다.

본 연구는 DeepSeek-AI의 연구진들이 주도하였으며, 북경대학교, 칭화대학교, 난징대학교의 연구진들이 협력하여 수행되었습니다. 특히 이 연구는 대규모 언어 모델의 효율성과 성능을 동시에 향상시키기 위한 전문가 특화 방법론을 제시하는 데 중점을 두고 있습니다.

Switch Transformer와 같은 기존의 MoE 모델들이 보여준 한계를 극복하고자, DeepSeekMoE는 각 전문가 네트워크가 특정 입력 패턴에 대해 더욱 전문화된 처리를 수행할 수 있도록 하는 새로운 아키텍처와 학습 방법을 제안합니다. 이는 Jordan과 Jacobs가 제시한 계층적 전문가 혼합 모델의 개념을 현대적으로 재해석하고 확장한 것으로, 대규모 언어 모델의 맥락에서 전문가 네트워크들의 효과적인 특화를 실현하고자 합니다.

### 서론

대규모 언어 모델의 시대에서 전문가 혼합(Mixture-of-Experts, MoE) 아키텍처는 모델 매개변수를 확장할 때 계산 비용을 효율적으로 관리할 수 있는 유망한 접근 방식으로 주목받고 있습니다. 그러나 GShard와 같은 기존의 MoE 아키텍처는 \\(N\\)개의 전문가 중 상위 \\(K\\)개를 활성화하는 방식을 사용하면서 전문가 특화, 즉 각 전문가가 중복되지 않고 집중된 지식을 획득하는 데 있어 한계에 직면해 있습니다.

이러한 문제를 해결하기 위해 본 연구에서는 궁극적인 전문가 특화를 목표로 하는 DeepSeekMoE 아키텍처를 제안합니다. 이 아키텍처는 두 가지 핵심 전략을 도입합니다. 첫째, 전문가를 \\(mN\\)개로 세분화하고 이 중 \\(mK\\)개를 활성화함으로써 활성화된 전문가들의 더욱 유연한 조합을 가능하게 합니다. 둘째, \\(K_s\\)개의 전문가를 공유 전문가로 분리하여 공통 지식을 포착하고 라우팅된 전문가들 간의 중복성을 줄이고자 합니다.

최근 연구와 실험들은 충분한 학습 데이터가 주어졌을 때 매개변수와 계산 예산을 늘려 언어 모델을 확장하면 현저히 강력한 모델을 얻을 수 있다는 것을 경험적으로 입증했습니다. 그러나 모델을 극도로 큰 규모로 확장하는 시도는 매우 높은 계산 비용을 수반한다는 점을 인식해야 합니다. 이러한 막대한 비용을 고려할 때, MoE 아키텍처는 계산 비용을 적정 수준으로 유지하면서 매개변수 확장을 가능하게 하는 인기 있는 해결책으로 부상했습니다.

트랜스포머에 MoE 아키텍처를 적용한 최근의 시도들은 언어 모델을 상당한 규모로 확장하는 데 성공을 거두었으며, 주목할 만한 성능을 보여주었습니다. 이러한 성과는 MoE 언어 모델의 상당한 잠재력과 가능성을 강조합니다.
MoE 아키텍처의 잠재력에도 불구하고, 기존 MoE 아키텍처는 지식의 혼재성과 지식의 중복성이라는 문제에 직면해 있어 전문가 특화를 제한하고 있습니다. 기존의 MoE 아키텍처는 트랜스포머의 피드포워드 네트워크(Feed-Forward Networks, FFNs)를 MoE 계층으로 대체합니다. 각 MoE 계층은 여러 전문가로 구성되며, 각 전문가는 구조적으로 표준 FFN과 동일하고, 각 토큰은 하나 또는 두 개의 전문가에 할당됩니다.

이러한 아키텍처는 두 가지 잠재적 문제를 드러냅니다. 첫째, 지식의 혼재성입니다. 기존 MoE 방식은 제한된 수의 전문가(예: 8개 또는 16개)를 사용하므로, 특정 전문가에 할당된 토큰들이 다양한 지식을 포함할 가능성이 높습니다. 결과적으로, 해당 전문가는 매개변수에 매우 다른 유형의 지식을 동시에 활용하기 어려운 방식으로 축적하게 됩니다.

둘째, 지식의 중복성입니다. 서로 다른 전문가에 할당된 토큰들이 공통 지식을 필요로 할 수 있습니다. 이로 인해 여러 전문가가 각자의 매개변수에서 공유 지식을 획득하게 되어 전문가 매개변수의 중복을 초래합니다. 이러한 문제들은 기존 MoE 방식에서 전문가 특화를 저해하여, MoE 모델이 이론적 상한 성능에 도달하지 못하게 합니다.

이러한 문제들에 대응하여, 본 연구는 궁극적인 전문가 특화를 위해 설계된 혁신적인 MoE 아키텍처인 DeepSeekMoE를 소개합니다. 우리의 아키텍처는 두 가지 주요 전략을 포함합니다. 첫째, 세밀한 전문가 분할입니다. 매개변수 수를 일정하게 유지하면서, FFN 중간 은닉 차원을 분할하여 전문가를 더 세밀한 단위로 분할합니다. 이에 상응하여, 계산 비용을 일정하게 유지하면서 더 많은 세밀한 전문가를 활성화하여 활성화된 전문가의 더욱 유연하고 적응적인 조합을 가능하게 합니다.

세밀한 전문가 분할을 통해 다양한 지식을 더 세밀하게 분해하고 각기 다른 전문가들이 더 정확하게 학습할 수 있게 되며, 각 전문가는 더 높은 수준의 특화도를 유지할 수 있습니다. 또한, 활성화된 전문가를 조합하는 유연성이 증가함으로써 더욱 정확하고 목표지향적인 지식 획득에 기여합니다.

둘째, 공유 전문가 분리입니다. 특정 전문가들을 항상 활성화되는 공유 전문가로 분리하여 다양한 맥락에서 공통 지식을 포착하고 통합하는 것을 목표로 합니다. 공통 지식을 이러한 공유 전문가들에 압축함으로써, 다른 라우팅된 전문가들 간의 중복성이 감소됩니다. 이는 매개변수 효율성을 향상시키고 각 라우팅된 전문가가 고유한 측면에 집중하여 특화될 수 있도록 보장합니다.

DeepSeekMoE 아키텍처의 혁신적인 설계는 각 전문가가 고도로 특화된 매개변수 효율적인 MoE 언어 모델을 학습할 수 있는 기회를 제공합니다. 2B 매개변수 규모의 모델로 시작하여 우리는 DeepSeekMoE 아키텍처의 장점을 검증했습니다. 다양한 작업을 포괄하는 12개의 제로샷 또는 퓨샷 벤치마크에서 평가를 수행한 결과, DeepSeekMoE 2B는 GShard 2B를 상당한 차이로 능가하고, 1.5배 많은 전문가 매개변수와 계산량을 가진 더 큰 모델인 GShard 2.9B와 대등한 성능을 달성했습니다.

주목할 만한 점은 DeepSeekMoE 2B가 동일한 수의 총 매개변수를 가진 밀집 모델의 성능에 거의 근접했다는 것입니다. 이는 MoE 모델의 엄격한 상한선을 설정합니다. 더 깊은 통찰을 얻기 위해 우리는 DeepSeekMoE의 전문가 특화에 대한 상세한 절제 연구와 분석을 수행했습니다. 이러한 연구들은 세밀한 전문가 분할과 공유 전문가 분리의 효과를 검증하고, DeepSeekMoE가 높은 수준의 전문가 특화를 달성할 수 있다는 주장을 뒷받침하는 경험적 증거를 제공합니다.

우리의 아키텍처를 활용하여 모델 매개변수를 16B로 확장하고 2T 토큰의 대규모 말뭉치에서 DeepSeekMoE 16B를 학습했습니다. 평가 결과는 약 40%의 계산량만으로 DeepSeekMoE 16B가 동일한 2T 말뭉치에서 학습된 밀집 모델인 DeepSeek 7B와 대등한 성능을 달성했음을 보여줍니다. 또한 DeepSeekMoE를 오픈소스 모델들과 비교한 결과, DeepSeekMoE 16B는 유사한 수의 활성화된 매개변수를 가진 모델들보다 일관되게 큰 차이로 우수한 성능을 보이며, 약 2.5배의 활성화된 매개변수를 가진 LLaMA2 7B와 대등한 성능을 달성했습니다.

추가로 우리는 정렬을 위한 지도 학습 미세조정(SFT)을 수행하여 모델을 채팅 모델로 변환했습니다. 평가 결과는 DeepSeekMoE Chat 16B가 채팅 환경에서도 DeepSeek Chat 7B와 LLaMA2 SFT 7B와 대등한 성능을 달성했음을 보여줍니다. 이러한 결과에 고무되어 우리는 DeepSeekMoE를 145B로 확장하는 예비 시도를 추가로 수행했습니다. 실험 결과는 GShard 아키텍처에 대한 상당한 장점을 일관되게 검증했습니다. 또한 DeepSeek 67B와 대등한 성능을 보이면서도 단 28.5%(심지어 18.2%)의 계산량만을 사용했습니다.

### 트랜스포머 언어 모델의 MoE 아키텍처

트랜스포머 언어 모델의 기본 구조는 \\(L\\)개의 트랜스포머 블록을 쌓아 올린 형태로 구성됩니다. 각 블록은 다음과 같은 수식으로 표현할 수 있습니다.

\\[
\mathbf{u}\_{1:T}^{l} = \operatorname{Self-Att}(\mathbf{h}\_{1:T}^{l-1}) + \mathbf{h}_{1:T}^{l-1}
\\]

\\[
\mathbf{h}\_{t}^{l} = \operatorname{FFN}(\mathbf{u}\_{t}^{l}) + \mathbf{u}_{t}^{l}
\\]

여기서 \\(T\\)는 시퀀스 길이를 나타내며, \\(\operatorname{Self-Att}(\cdot)\\)은 셀프 어텐션 모듈을, \\(\operatorname{FFN}(\cdot)\\)은 피드포워드 네트워크를 의미합니다. \\(\mathbf{u}\_{1:T}^{l} \in \mathbb{R}^{T\times d}\\)는 \\(l\\)번째 어텐션 모듈 이후의 모든 토큰의 은닉 상태를 나타내고, \\(\mathbf{h}_{t}^{l} \in \mathbb{R}^{d}\\)는 \\(l\\)번째 트랜스포머 블록에서 \\(t\\)번째 토큰의 출력 은닉 상태를 나타냅니다.

전문가 혼합(MoE) 언어 모델을 구성하기 위해서는 트랜스포머의 FFN을 특정 간격으로 MoE 계층으로 대체합니다. MoE 계층은 여러 전문가로 구성되며, 각 전문가는 구조적으로 표준 FFN과 동일합니다. 각 토큰은 하나 또는 두 개의 전문가에 할당됩니다. \\(l\\)번째 FFN이 MoE 계층으로 대체될 때, 출력 은닉 상태 \\(\mathbf{h}_{t}^{l}\\)는 다음과 같이 계산됩니다.

\\[
\mathbf{h}\_{t}^{l} = \sum_{i=1}^{N}(g_{i,t}\operatorname{FFN}\_{i}(\mathbf{u}\_{t}^{l})) + \mathbf{u}_{t}^{l}
\\]

\\[
g_{i,t} = \begin{cases}
s_{i,t}, & s_{i,t} \in \operatorname{Topk}(\{s_{j,t}|1 \leq j \leq N\}, K) \\
0, & \text{otherwise}
\end{cases}
\\]

\\[
s_{i,t} = \operatorname{Softmax}\_{i}({\mathbf{u}\_{t}^{l}}^{T}\mathbf{e}_{i}^{l})
\\]

여기서 \\(N\\)은 전체 전문가 수를, \\(\operatorname{FFN}\_{i}(\cdot)\\)는 \\(i\\)번째 전문가 FFN을, \\(g_{i,t}\\)는 \\(i\\)번째 전문가에 대한 게이트 값을, \\(s_{i,t}\\)는 토큰-전문가 간 친화도를, \\(\operatorname{Topk}(\cdot, K)\\)는 \\(t\\)번째 토큰과 모든 \\(N\\)개 전문가 간에 계산된 친화도 점수 중 상위 \\(K\\)개의 점수를 포함하는 집합을 나타내며, \\(\mathbf{e}_{i}^{l}\\)은 \\(l\\)번째 계층의 \\(i\\)번째 전문가의 중심점을 나타냅니다.

![DeepSeekMoE 아키텍처](https://ar5iv.org//html/2401.06066/assets/x2.png)

위 그림은 DeepSeekMoE 아키텍처의 진화 과정을 보여줍니다. (a)는 기존의 top-2 라우팅 전략을 보여주고, (b)는 세밀한 전문가 분할 접근 방식을 도입하며, (c)는 공유 전문가 분리 전략이 통합된 완전한 DeepSeekMoE 아키텍처를 보여줍니다. 이러한 구성 요소들은 MoE 모델의 효율성과 성능을 향상시키면서도 전문가 매개변수 수와 계산 비용을 일정하게 유지하는 것을 목표로 합니다.

### DeepSeekMoE의 전문가 특화 전략

DeepSeekMoE는 위에서 설명한 일반적인 MoE 아키텍처를 기반으로, 전문가 특화를 극대화하기 위한 두 가지 핵심 전략을 도입합니다. 첫 번째는 세밀한 전문가 분할(Fine-Grained Expert Segmentation)이고, 두 번째는 공유 전문가 분리(Shared Expert Isolation)입니다.

#### 세밀한 전문가 분할

전문가의 수가 제한된 상황에서는 특정 전문가에 할당된 토큰들이 다양한 유형의 지식을 포함할 가능성이 높습니다. 이로 인해 해당 전문가는 매개변수에 매우 다른 유형의 지식을 동시에 학습하게 되며, 이러한 지식들을 효과적으로 활용하기 어렵게 됩니다. 그러나 각 토큰이 더 많은 전문가에 라우팅될 수 있다면, 다양한 지식이 서로 다른 전문가들에 분해되어 학습될 수 있는 잠재력을 갖게 됩니다. 이러한 맥락에서 각 전문가는 여전히 높은 수준의 특화도를 유지하면서 전문가들 간에 더욱 집중된 지식 분포를 달성할 수 있습니다.

이러한 목표를 달성하기 위해, DeepSeekMoE는 전문가 매개변수의 수와 계산 비용을 일정하게 유지하면서 전문가들을 더 세밀한 단위로 분할합니다. 구체적으로, 위 그림 (a)에 나타난 일반적인 MoE 아키텍처를 기반으로, 각 전문가 FFN을 \\(m\\)개의 더 작은 전문가로 분할합니다. 이는 FFN의 중간 은닉 차원을 원래 크기의 \\(\frac{1}{m}\\)배로 줄임으로써 달성됩니다. 각 전문가가 더 작아짐에 따라, 동일한 계산 비용을 유지하기 위해 활성화되는 전문가의 수도 \\(m\\)배로 증가시킵니다.

세밀한 전문가 분할을 적용한 MoE 계층의 출력은 다음과 같이 표현됩니다.

\\[
\mathbf{h}\_{t}^{l} = \sum_{i=1}^{mN}(g_{i,t}\operatorname{FFN}\_{i}(\mathbf{u}\_{t}^{l})) + \mathbf{u}_{t}^{l}
\\]

\\[
g_{i,t} = \begin{cases}
s_{i,t}, & s_{i,t} \in \operatorname{Topk}(\{s_{j,t}|1 \leq j \leq mN\}, mK) \\
0, & \text{otherwise}
\end{cases}
\\]

\\[
s_{i,t} = \operatorname{Softmax}\_{i}({\mathbf{u}\_{t}^{l}}^{T}\mathbf{e}_{i}^{l})
\\]

여기서 전체 전문가 매개변수의 수는 표준 FFN 매개변수 수의 \\(N\\)배이며, \\(mN\\)은 세밀하게 분할된 전문가의 총 수를 나타냅니다. 세밀한 전문가 분할 전략에 따라 0이 아닌 게이트의 수도 \\(mK\\)로 증가합니다.

조합론적 관점에서 볼 때, 세밀한 전문가 분할 전략은 활성화된 전문가들의 조합 유연성을 크게 향상시킵니다. 예를 들어, \\(N=16\\)인 경우를 고려해보면, 일반적인 top-2 라우팅 전략은 \\(\binom{16}{2}=120\\)개의 가능한 조합을 생성할 수 있습니다. 반면, 각 전문가를 4개의 더 작은 전문가로 분할하면, 세밀한 라우팅 전략은 \\(\binom{64}{8}=4,426,165,368\\)개의 잠재적 조합을 생성할 수 있습니다. 이러한 조합 유연성의 증가는 더욱 정확하고 목표지향적인 지식 획득의 가능성을 높입니다.

### 라우팅 붕괴 방지를 위한 균형 손실 함수

DeepSeekMoE는 라우팅 붕괴 현상을 방지하기 위해 전문가 수준의 균형 손실 함수를 도입합니다. 이 손실 함수는 다음과 같이 계산됩니다.

\\[
\mathcal{L}\_{\mathrm{ExpBal}} = \alpha_{1}\sum_{i=1}^{N^{\prime}}{f_{i}P_{i}}
\\]

여기서 \\(f_i\\)는 각 전문가의 활용 빈도를 나타내며 다음과 같이 정의됩니다.

\\[
f_{i} = \frac{N^{\prime}}{K^{\prime}T}\sum_{t=1}^{T}{\mathbb{1}(\text{Token $t$ selects Expert $i$})}
\\]

\\(P_i\\)는 각 전문가에 대한 라우팅 확률을 나타내며 다음과 같이 계산됩니다.

\\[
P_{i} = \frac{1}{T}\sum_{t=1}^{T}{s_{i,t}}
\\]

이때 \\(\alpha_1\\)은 전문가 수준 균형 인자라고 하는 하이퍼파라미터이며, \\(N^{\prime}\\)은 \\((mN-K_s)\\)와 같고 \\(K^{\prime}\\)은 \\((mK-K_s)\\)와 같습니다. \\(\mathbb{1}(\cdot)\\)은 지시 함수를 나타냅니다.

### 디바이스 수준 균형 손실 함수

전문가 수준의 균형 손실 함수에 더하여, DeepSeekMoE는 디바이스 수준의 균형 손실 함수를 도입합니다. 계산 병목 현상을 완화하는 것이 목표일 때는 전문가 수준에서 엄격한 균형 제약을 강제하는 것이 불필요할 수 있습니다. 과도한 부하 균형 제약은 모델 성능을 저하시킬 수 있기 때문입니다. 대신, 우리의 주요 목표는 디바이스 간의 균형 잡힌 계산을 보장하는 것입니다.

모든 라우팅된 전문가들을 \\(D\\)개의 그룹 \\(\{\mathcal{E}_1,\mathcal{E}_2,...,\mathcal{E}_D\}\\)로 분할하고, 각 그룹을 하나의 디바이스에 배치한다고 할 때, 디바이스 수준의 균형 손실은 다음과 같이 계산됩니다.

\\[
\mathcal{L}\_{\mathrm{DevBal}} = \alpha_{2}\sum_{i=1}^{D}{f_{i}^{\prime}P_{i}^{\prime}}
\\]

여기서 \\(f_i^{\prime}\\)와 \\(P_i^{\prime}\\)는 각각 다음과 같이 정의됩니다.

\\[
f_{i}^{\prime} = \frac{1}{|\mathcal{E}_i|}\sum_{j\in\mathcal{E}_i}{f_{j}}
\\]

\\[
P_{i}^{\prime} = \sum_{j\in\mathcal{E}_i}{P_{j}}
\\]

\\(\alpha_2\\)는 디바이스 수준 균형 인자라고 하는 하이퍼파라미터입니다. 실제 구현에서는 라우팅 붕괴의 위험을 완화하기 위해 작은 전문가 수준 균형 인자를 설정하고, 동시에 디바이스 간의 균형 잡힌 계산을 촉진하기 위해 더 큰 디바이스 수준 균형 인자를 설정합니다.

### 실험 설정 및 평가

DeepSeekMoE의 실험은 HAI-LLM 프레임워크를 기반으로 수행되었습니다. 이 프레임워크는 텐서 병렬화, ZeRO 데이터 병렬화, PipeDream 파이프라인 병렬화, 그리고 전문가 병렬화와 같은 다양한 병렬화 전략을 통합한 효율적이고 경량화된 학습 프레임워크입니다. 성능 최적화를 위해 게이팅 알고리즘과 서로 다른 전문가들의 선형 계층 간 연산을 융합하는 CUDA와 Triton 기반의 GPU 커널을 개발했습니다.

모든 실험은 NVIDIA A100 또는 H800 GPU가 장착된 클러스터에서 진행되었습니다. A100 클러스터의 각 노드는 NVLink 브릿지로 페어링된 8개의 GPU를 포함하고 있으며, H800 클러스터 역시 노드당 8개의 GPU가 NVLink와 NVSwitch를 통해 연결되어 있습니다. 두 클러스터 모두 노드 간 통신을 위해 InfiniBand 인터커넥트를 사용합니다.

#### 하이퍼파라미터 설정

검증 실험에서는 트랜스포머 계층을 9개로 설정하고 은닉 차원을 1280으로 지정했습니다. 멀티헤드 어텐션 메커니즘은 총 10개의 어텐션 헤드를 사용하며, 각 헤드의 차원은 128입니다. 모든 학습 가능한 파라미터는 0.006의 표준편차로 무작위 초기화되었습니다. 모든 FFN을 MoE 계층으로 대체했으며, 전체 전문가 파라미터가 표준 FFN의 16배가 되도록 설정했습니다. 또한 공유 전문가 파라미터와 활성화된 라우팅된 전문가 파라미터를 포함한 활성화된 전문가 파라미터가 표준 FFN의 2배가 되도록 유지했습니다. 이 구성에서 각 MoE 모델은 약 2B의 총 파라미터를 가지며, 활성화된 파라미터는 약 0.3B입니다.

학습을 위해 AdamW 옵티마이저를 사용했으며, \\(\beta_1=0.9\\), \\(\beta_2=0.95\\), \\(\text{weight\_decay}=0.1\\)로 설정했습니다. 학습률은 웜업-스텝-감소 전략을 통해 조절되었습니다. 처음 2K 스텝 동안 학습률이 0에서 최대값까지 선형적으로 증가하고, 이후 전체 학습 스텝의 80%에서 0.316을 곱하고, 90%에서 다시 0.316을 곱합니다. 검증 실험의 최대 학습률은 \\(1.08 \times 10^{-3}\\)으로 설정되었으며, 그래디언트 클리핑 노름은 1.0으로 지정되었습니다.

배치 크기는 2K로 설정되었고, 최대 시퀀스 길이 2K와 함께 각 학습 배치는 4M 토큰을 포함합니다. 이에 따라 100B 학습 토큰을 달성하기 위해 총 학습 스텝 수는 25,000으로 설정되었습니다. 풍부한 학습 데이터로 인해 학습 중 드롭아웃은 사용하지 않았습니다. 상대적으로 작은 모델 크기를 고려하여, 불균형한 계산을 피하기 위해 전문가 파라미터를 포함한 모든 파라미터를 단일 GPU 디바이스에 배치했습니다. 이에 따라 학습 중 토큰 드롭은 수행하지 않았으며 디바이스 수준의 균형 손실도 사용하지 않았습니다. 라우팅 붕괴를 방지하기 위해 전문가 수준 균형 인자를 0.01로 설정했습니다.

![DeepSeekMoE 아키텍처의 진화](https://ar5iv.org//html/2401.06066/assets/x3.png)

위 그림은 DeepSeekMoE 모델에 대한 절제 연구 결과를 보여줍니다. 다양한 벤치마크 태스크에서의 정규화된 성능을 나타내며, 세밀한 전문가 분할과 공유 전문가 분리가 전체 모델 성능에 기여하는 정도를 평가합니다. 주요 기술적 구성 요소는 공유 전문가와 라우팅된 전문가의 서로 다른 구성이며, 중요한 발견은 세밀한 전문가 분할과 공유 전문가 분리가 모두 평가된 태스크 전반에 걸쳐 더 강력한 전체 성능에 기여한다는 것을 보여줍니다.
### 실험 설정 및 평가

#### 평가 벤치마크

DeepSeekMoE의 성능을 평가하기 위해 다양한 유형의 태스크를 포괄하는 광범위한 벤치마크에서 평가를 수행했습니다. 언어 모델링 평가를 위해 Pile 테스트 셋을 사용했으며, 평가 지표로는 교차 엔트로피 손실을 채택했습니다. 

언어 이해 및 추론 능력 평가를 위해서는 HellaSwag, PIQA, ARC-challenge, ARC-easy를 사용했습니다. 이러한 태스크들에서는 정확도를 평가 지표로 활용했습니다. 독해 능력 평가를 위해서는 RACE-high와 RACE-middle을 사용했으며, 마찬가지로 정확도를 평가 지표로 사용했습니다.

코드 생성 능력 평가를 위해서는 HumanEval과 MBPP를 사용했습니다. 이 태스크들에서는 Pass@1을 평가 지표로 사용했는데, 이는 단 한 번의 생성 시도에서의 통과율을 의미합니다. 폐쇄형 질의응답 능력 평가를 위해서는 TriviaQA와 NaturalQuestions를 사용했으며, 정확히 일치하는 비율(Exactly Matching rate)을 평가 지표로 활용했습니다.

![DeepSeekMoE와 GShard 모델의 비교](https://ar5iv.org//html/2401.06066/assets/x4.png)

위 그래프는 비활성화된 상위 라우팅 전문가의 비율에 따른 Pile 손실을 보여줍니다. DeepSeekMoE가 GShard×1.5 모델에 비해 비활성화된 상위 라우팅 전문가의 비율에 더 민감한 반응을 보이는 것이 주목할 만한 점입니다. 이는 DeepSeekMoE의 라우팅된 전문가들 간의 중복성이 더 낮다는 것을 시사합니다. 이러한 특성은 DeepSeekMoE가 전문가 실패에 더 취약할 수 있지만, 모든 전문가가 정상적으로 작동할 때 더 높은 성능을 달성할 수 있음을 의미합니다.

![DeepSeekMoE의 활성화된 라우팅 전문가 수에 따른 성능](https://ar5iv.org//html/2401.06066/assets/x5.png)

위 그래프는 DeepSeekMoE에서 활성화된 라우팅 전문가의 수에 따른 Pile 손실을 보여줍니다. 단 4개의 라우팅 전문가만 활성화된 상태에서도 DeepSeekMoE는 전체 top-2 활성화 GShard 모델과 비슷한 Pile 손실을 달성했습니다. 이는 DeepSeekMoE가 더 효율적이고 확장 가능한 전문가 기반 아키텍처로 유사한 성능을 달성할 수 있음을 보여줍니다.

![GShard와 DeepSeekMoE의 성능 비교](https://ar5iv.org//html/2401.06066/assets/x6.png)

위 그래프는 GShard와 DeepSeekMoE의 성능을 다양한 자연어 처리 태스크에서 비교합니다. DeepSeekMoE는 활성화된 전문가 파라미터가 절반임에도 불구하고, 동일한 총 전문가 파라미터를 가진 GShard보다 우수한 성능을 보여줍니다. 이는 DeepSeekMoE의 더 효율적인 전문가 활용이 여러 평가 지표에서 우수한 성능으로 이어진다는 것을 입증하며, 파라미터 효율적이고 고성능의 MoE 모델을 구축하는 데 있어 DeepSeekMoE 접근 방식의 중요성을 보여줍니다.

### DeepSeekMoE의 미세조정 효과 분석

기존 연구에서는 MoE 모델이 미세조정을 통해 큰 성능 향상을 얻지 못한다는 결과가 보고되었습니다. 그러나 최근 Shen과 연구진의 연구는 MoE 모델이 지시어 튜닝을 통해 실제로 이점을 얻을 수 있다는 것을 보여주었습니다. 이러한 맥락에서 DeepSeekMoE 16B 모델의 미세조정 효과를 평가하기 위해 지도 학습 기반 미세조정을 수행하여 채팅 모델을 구축했습니다.

### 실험 설정

미세조정을 위한 학습 데이터는 140만 개의 학습 예제로 구성된 자체 큐레이션 데이터셋을 사용했습니다. 이 데이터셋은 수학, 코딩, 글쓰기, 질의응답, 추론, 요약 등 다양한 카테고리를 포괄하며, 대부분의 데이터는 영어와 중국어로 구성되어 있어 이중 언어 시나리오에서의 활용이 가능합니다.

하이퍼파라미터 설정에서는 배치 크기를 1024 예제로 설정하고 AdamW 옵티마이저를 사용하여 8 에포크 동안 학습을 진행했습니다. 최대 시퀀스 길이는 4K로 설정했으며, 학습 예제들은 시퀀스 길이 제한에 도달할 때까지 최대한 조밀하게 패킹되었습니다. 미세조정 과정에서는 드롭아웃을 사용하지 않았으며, 학습률 스케줄링 전략 없이 \\(10^{-5}\\)의 고정 학습률을 사용했습니다.

![DeepSeekMoE와 다른 모델들의 성능 비교](https://ar5iv.org//html/2401.06066/assets/x5.png)

### 평가 결과

평가는 이전 섹션에서 사용된 벤치마크와 유사한 기준을 사용했으나, 몇 가지 조정이 이루어졌습니다. 순수 언어 모델링에 거의 사용되지 않는 Pile 벤치마크는 제외되었고, 결과의 불안정성으로 인해 신뢰할 만한 결론을 도출하기 어려운 CHID도 제외되었습니다. 대신 채팅 모델의 추론 능력을 더욱 포괄적으로 평가하기 위해 BBH가 추가되었습니다.

실험 결과는 DeepSeekMoE Chat 16B가 LLaMA2 SFT 7B와 DeepSeek Chat 7B에 견줄 만한 성능을 달성했음을 보여줍니다. 특히 주목할 만한 점은 DeepSeekMoE Chat 16B가 약 40%의 계산량만으로 이러한 성능을 달성했다는 것입니다. 이는 MoE 아키텍처의 효율성을 입증하는 중요한 결과입니다.
### 평가 벤치마크 분석

DeepSeekMoE Chat 16B의 성능을 LLaMA2 SFT 7B와 DeepSeek Chat 7B와 비교한 결과, 언어 이해와 추론 능력에서 주목할 만한 성과를 보였습니다. HellaSwag에서 72.2%의 정확도를 달성했으며, PIQA에서는 79.7%의 정확도를 기록했습니다. 이는 LLaMA2 SFT 7B의 67.9%와 76.9%보다 각각 높은 수치입니다.

![DeepSeekMoE와 다른 모델들의 성능 비교](https://ar5iv.org//html/2401.06066/assets/x6.png)

코드 생성 태스크에서 DeepSeekMoE Chat 16B는 특히 뛰어난 성능을 보여주었습니다. HumanEval에서 45.7%의 Pass@1 성능을 달성했으며, MBPP에서는 46.2%를 기록했습니다. 이는 LLaMA2 SFT 7B의 35.4%와 27.8%를 크게 상회하는 결과입니다. 이러한 성능 향상은 모델의 전문가 네트워크가 코드 생성과 관련된 특화된 지식을 효과적으로 학습했음을 시사합니다.

지식 집약적 태스크에서도 DeepSeekMoE Chat 16B는 강점을 보였습니다. TriviaQA에서 63.3%의 정확도를 달성했으며, NaturalQuestions에서는 35.1%를 기록했습니다. 이는 LLaMA2 SFT 7B의 60.1%와 35.2%에 비해 대등하거나 더 나은 성능입니다.

다중 선택형 질의응답 벤치마크에서는 DeepSeekMoE Chat 16B가 DeepSeek Chat 7B에 비해 다소 낮은 성능을 보였습니다. MMLU에서 47.2%, CEval에서 40.0%, CMMLU에서 49.3%를 기록했는데, 이는 DeepSeek Chat 7B의 49.7%, 44.7%, 51.2%보다 낮은 수치입니다. 그러나 이러한 성능 차이는 기본 모델 평가 때보다 감소했으며, 미세조정을 통해 성능 격차가 좁혀졌음을 보여줍니다.

중국어 벤치마크에서 DeepSeekMoE Chat 16B는 이중 언어 사전 학습의 이점을 보여주었습니다. CLUEWSC에서 68.2%의 정확도를 달성했으며, 이는 LLaMA2 SFT 7B의 48.4%를 크게 상회하는 결과입니다. 이는 영어와 중국어 데이터로 구성된 사전 학습 말뭉치의 효과를 입증합니다.

### 효율성 분석

DeepSeekMoE Chat 16B는 총 16.4B의 매개변수를 가지고 있지만, 실제로 활성화되는 매개변수는 2.8B에 불과합니다. 4K 토큰 처리에 필요한 FLOPs는 74.4T로, LLaMA2 SFT 7B의 187.9T와 DeepSeek Chat 7B의 183.5T에 비해 크게 낮습니다. 이는 MoE 아키텍처가 계산 효율성과 모델 성능 사이의 균형을 효과적으로 달성했음을 보여줍니다.

### DeepSeekMoE 145B 모델의 확장 및 평가

DeepSeekMoE 16B의 우수한 성능에 고무되어 연구진은 DeepSeekMoE를 145B 규모로 확장하는 시도를 진행했습니다. 이 예비 연구에서 DeepSeekMoE 145B는 245B 토큰으로 학습되었으며, GShard 아키텍처와 비교하여 일관된 우위를 보여주었고 DeepSeek 67B(Dense) 모델의 성능에 근접하거나 이를 능가할 가능성을 보여주었습니다.

#### 실험 설정

DeepSeekMoE 145B는 DeepSeekMoE 16B와 동일한 학습 데이터와 토크나이저를 사용했으며, 245B 토큰으로 초기 학습을 진행했습니다. 모델 구조는 62개의 트랜스포머 계층과 4096 차원의 은닉 상태를 가지도록 설계되었습니다. 32개의 어텐션 헤드를 사용했으며, 각 헤드는 128 차원을 가집니다. 모든 학습 가능한 파라미터는 0.006의 표준편차로 무작위 초기화되었습니다.

DeepSeekMoE 16B와 마찬가지로, 첫 번째 계층을 제외한 모든 FFN을 MoE 계층으로 대체했습니다. 각 MoE 계층은 4개의 공유 전문가와 128개의 라우팅된 전문가로 구성되며, 각 전문가는 표준 FFN 크기의 0.125배입니다. 각 토큰은 4개의 공유 전문가와 128개의 라우팅된 전문가 중 12개로 라우팅됩니다. 이러한 구성에서 DeepSeekMoE 145B는 약 144.6B의 총 파라미터를 가지며, 활성화되는 파라미터는 약 22.2B입니다.

#### 학습 설정

학습에는 AdamW 옵티마이저를 사용했으며, \\(\beta_1=0.9\\), \\(\beta_2=0.95\\), \\(\text{weight\_decay}=0.1\\)로 설정했습니다. DeepSeekMoE 145B의 예비 연구에서는 웜업-상수 학습률 스케줄러를 사용했습니다. 처음 2K 스텝 동안 학습률이 0에서 최대값까지 선형적으로 증가하고, 이후 학습 과정에서는 일정하게 유지됩니다. 최대 학습률은 \\(3.0 \times 10^{-4}\\)로 설정되었으며, 그래디언트 클리핑 노름은 1.0으로 지정되었습니다.

배치 크기는 4.5K로 설정되었고, 최대 시퀀스 길이 4K와 함께 각 학습 배치는 18M 토큰을 포함합니다. DeepSeekMoE 145B는 13,000 스텝 동안 학습되어 총 245B 토큰의 학습을 달성했습니다. 학습 중에는 드롭아웃을 사용하지 않았습니다.

파이프라인 병렬화를 활용하여 모델의 서로 다른 계층을 다른 디바이스에 배치했으며, 각 계층에서 모든 라우팅된 전문가는 4개의 디바이스에 균일하게 배치됩니다(전문가 병렬화와 데이터 병렬화 결합). DeepSeekMoE 145B에서는 전문가 병렬화를 사용하므로 디바이스 수준의 부하 균형을 고려해야 합니다. 이에 대응하여 디바이스 간 계산의 균형을 촉진하기 위해 디바이스 수준 균형 인자를 0.05로 설정했습니다. 또한 라우팅 붕괴를 방지하기 위해 0.003의 작은 전문가 수준 균형 인자를 설정했습니다.

### 전문가 혼합 모델의 발전 과정

전문가 혼합(Mixture of Experts, MoE) 기법은 Jacobs와 연구진이 1991년에 처음 제안했으며, 서로 다른 샘플을 독립적인 전문가 모듈로 처리하는 방식을 도입했습니다. 이후 Jordan과 Jacobs는 1994년에 이 개념을 확장하여 계층적 전문가 혼합 모델을 발전시켰습니다. 2017년에는 Shazeer와 연구진이 언어 모델 학습에 MoE를 도입하여 LSTM 기반의 대규모 MoE 모델을 구축했습니다.

트랜스포머가 자연어 처리 분야에서 주류 아키텍처로 자리잡으면서, 많은 연구자들이 트랜스포머의 피드포워드 네트워크(FFN)를 MoE 계층으로 확장하여 MoE 언어 모델을 구축하기 시작했습니다. GShard와 Switch Transformer는 이 분야의 선구자적 연구로, 학습 가능한 top-2 또는 top-1 라우팅 전략을 도입하여 MoE 언어 모델을 극도로 큰 규모로 확장했습니다.

Hash Layer와 StableMoE는 더욱 안정적인 라우팅과 학습을 위해 고정된 라우팅 전략을 사용했습니다. Zhou와 연구진은 각 토큰이 서로 다른 수의 전문가에 할당될 수 있는 전문가 선택 라우팅 전략을 제안했습니다. Zoph와 연구진은 MoE 모델의 학습 불안정성과 미세조정의 어려움에 초점을 맞추어, 이러한 문제를 해결하기 위한 ST-MoE를 제안했습니다.

MoE 아키텍처와 학습 전략에 대한 연구 외에도, 최근에는 기존 MoE 아키텍처를 기반으로 한 다수의 대규모 언어 모델이나 멀티모달 모델이 등장했습니다. Lin과 연구진, Du와 연구진, Ren과 연구진, Xue와 연구진의 연구가 대표적입니다. 그러나 대부분의 기존 MoE 모델들은 전통적인 top-1 또는 top-2 라우팅 전략을 기반으로 하고 있어, 전문가 특화를 개선할 여지가 많이 남아있습니다. 이러한 맥락에서 DeepSeekMoE 아키텍처는 전문가 특화를 최대한으로 끌어올리는 것을 목표로 합니다.

## DeepSeekMoE: 대규모 언어 모델의 궁극적 전문가 특화를 향하여

본 연구에서는 대규모 언어 모델을 위한 DeepSeekMoE 아키텍처를 소개합니다. 이 아키텍처는 세밀한 전문가 분할과 공유 전문가 분리를 통해 기존 MoE 아키텍처들과 비교하여 현저히 높은 전문가 특화도와 성능을 달성했습니다.

2B 매개변수 규모의 초기 검증에서 DeepSeekMoE는 MoE 모델의 성능 상한에 근접하는 우수한 결과를 보여주었습니다. 또한 GShard와 비교하여 더 높은 전문가 특화도를 달성했다는 실험적 증거를 제시했습니다.

16B 매개변수 규모로 확장한 모델은 2T 토큰으로 학습을 진행했으며, 약 40%의 계산량만으로 DeepSeek 7B와 LLaMA2 7B에 견줄만한 뛰어난 성능을 보여주었습니다. 이후 지도 학습 기반 미세조정을 통해 DeepSeekMoE 16B를 기반으로 한 채팅 모델을 구축했으며, 이를 통해 모델의 적응성과 범용성을 입증했습니다.

![DeepSeekMoE 16B와 오픈소스 모델 비교](https://ar5iv.org//html/2401.06066/assets/x1.png)

위 그래프는 Open LLM 리더보드에서 DeepSeekMoE 16B와 다양한 오픈소스 언어 모델들의 성능을 비교한 결과입니다. 빨간 점선은 DeepSeekMoE 16B를 제외한 모든 모델의 데이터 포인트를 기반으로 한 선형 추세선을 나타냅니다. DeepSeekMoE 16B는 유사한 수의 활성화된 매개변수를 가진 모델들보다 일관되게 우수한 성능을 보여주며, 약 2.5배의 활성화된 매개변수를 가진 LLaMA2 7B와 비슷한 성능을 달성했습니다.

추가로 DeepSeekMoE를 145B 매개변수 규모로 확장하는 예비 연구를 진행했습니다. 이 실험에서도 GShard 아키텍처에 대한 우위를 유지했으며, DeepSeek 67B와 비교하여 단 28.5%(혹은 18.2%)의 계산량으로 대등한 성능을 보여주었습니다.

연구 목적으로 DeepSeekMoE 16B의 모델 체크포인트를 공개했으며, 이는 40GB 메모리를 가진 단일 GPU에서도 구동이 가능합니다. 본 연구가 학계와 산업계 모두에 가치 있는 통찰을 제공하고, 대규모 언어 모델의 발전을 가속화하는 데 기여하기를 기대합니다.

- - -
### References
* [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/pdf/2401.06066v1)