---
layout: post
title: "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
date: 2025-01-22 15:19:35
author: "DeepSeek AI Research"
categories: ["Paper Reviews", "Reinforcement Learning"]
tags: ["Large-Scale-Reinforcement-Learning-on-Base-Model", "Group-Relative-Policy-Optimization", "Reasoning-Oriented-Reinforcement-Learning", "Reinforcement-Learning-with-Cold-Start", "Rejection-Sampling-and-Supervised-Fine-Tuning", "Distillation-of-Reasoning-Capability", "Multi-Stage-Reinforcement-Learning-with-Self-Critique", "Verifiable-Rewards-Reinforcement-Learning", "Iterative-Reinforcement-Learning", "Unified-Paradigm-for-Reinforcement-Learning"]
cover: /assets/images/default.jpg
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

대규모 언어 모델(LLM)의 추론 능력 향상은 인공지능 연구에서 가장 중요한 도전 과제 중 하나로 자리 잡고 있습니다. 기존의 언어 모델들은 대부분 대량의 지도 학습 데이터에 의존하여 성능을 개선해왔지만, 이는 막대한 비용과 시간이 소요되며 모델의 진정한 추론 능력 개발을 제한했습니다. 특히 수학, 코딩, 과학적 추론과 같은 복잡한 문제 해결 영역에서 모델들은 여전히 근본적인 한계를 보여왔습니다.

DeepSeek-AI 연구팀은 이러한 한계를 극복하기 위해 순수한 강화학습(RL)을 통해 언어 모델의 추론 능력을 자율적으로 향상시키는 혁신적인 접근법을 탐구하게 되었습니다. 그들의 핵심 질문은 간단하면서도 도전적이었습니다. 지도 학습 데이터 없이 모델이 스스로 복잡한 추론 전략을 개발할 수 있을까? 이는 인공지능의 자기 진화 가능성을 탐구하는 근본적인 연구 질문이었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

연구팀은 **DeepSeek-R1-Zero**와 **DeepSeek-R1**이라는 두 가지 혁신적인 모델을 개발했습니다. DeepSeek-R1-Zero는 지도 학습 미세 조정 없이 순수한 강화학습만으로 훈련된 최초의 모델로, 모델이 자율적으로 추론 능력을 개발할 수 있음을 입증했습니다. 이 모델은 **Group Relative Policy Optimization(GRPO)**이라는 새로운 강화학습 알고리즘을 사용하여, 모델이 스스로 문제 해결 전략을 학습하도록 유도했습니다.

DeepSeek-R1은 DeepSeek-R1-Zero의 한계를 극복하기 위해 개발되었습니다. 이 모델은 소량의 콜드 스타트 데이터와 다단계 훈련 파이프라인을 도입하여 추론 성능을 더욱 향상시켰습니다. 특히 언어 일관성 보상과 거부 샘플링 기법을 통해 모델의 추론 과정을 더욱 정제하고 가독성을 개선했습니다. 또한 증류 기법을 활용하여 DeepSeek-R1의 추론 능력을 작은 모델들로 전달하는 혁신적인 접근법을 제시했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?

DeepSeek-R1-Zero의 구현은 Group Relative Policy Optimization(GRPO) 알고리즘을 중심으로 이루어졌습니다. 이 알고리즘은 각 질문에 대해 여러 개의 답변을 생성하고, 이들 간의 상대적 성능을 비교하여 학습 신호를 생성합니다. 규칙 기반 보상 시스템을 사용하여 모델의 추론 과정을 평가하고, `<think>` 태그 안에서 추론 과정을, `<answer>` 태그 안에서 최종 답변을 생성하도록 유도했습니다.

DeepSeek-R1의 훈련은 네 단계로 구성됩니다. 첫째, 콜드 스타트 단계에서 수천 개의 긴 추론 예시를 수집하여 기본 모델을 미세 조정합니다. 둘째, 추론 지향적 강화학습을 통해 모델의 추론 능력을 향상시킵니다. 셋째, 거부 샘플링을 통해 새로운 훈련 데이터를 생성하고 모델을 지도 미세 조정합니다. 마지막으로, 모든 시나리오에 대한 추가 강화학습을 수행하여 모델의 유용성과 무해성을 개선합니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?

연구 결과는 매우 인상적입니다. DeepSeek-R1-Zero는 AIME 2024 벤치마크에서 pass@1 점수를 15.6%에서 71.0%로 크게 향상시켰으며, 다수결 투표를 통해 86.7%까지 개선했습니다. DeepSeek-R1은 OpenAI-o1-1217과 비교할 만한 성능을 달성하여, 강화학습을 통한 추론 능력 향상의 가능성을 입증했습니다.

특히 주목할 만한 성과는 증류를 통한 소형 모델의 추론 능력 향상입니다. DeepSeek-R1-Distill-Qwen-1.5B와 같은 작은 모델들이 AIME 2024에서 28.9%, MATH-500에서 83.9%의 성능을 달성하여 상용 대형 모델들을 능가했습니다. 이는 효과적인 지식 증류가 계산 자원이 제한된 환경에서도 고성능 추론 모델을 개발할 수 있음을 보여주는 중요한 발견입니다.

이 연구는 언어 모델의 추론 능력 개발에 있어 강화학습의 잠재력을 보여주었으며, 모델이 자율적으로 복잡한 문제 해결 전략을 학습할 수 있음을 입증했습니다. 향후 연구에서는 다국어 지원, 프롬프트 견고성 개선, 그리고 다양한 도메인에서의 일관된 성능 향상이 중요한 과제가 될 것입니다.

</answer></think>
- - -
# DeepSeek-R1: 강화학습을 통한 대규모 언어 모델의 추론 능력 향상

## 초록

DeepSeek-AI에서 개발한 첫 번째 추론 모델인 DeepSeek-R1-Zero와 DeepSeek-R1을 소개합니다. DeepSeek-R1-Zero는 지도 학습 미세 조정(SFT) 없이 순수한 대규모 강화학습(RL)만으로 훈련된 모델로, 놀라운 추론 능력을 보여줍니다. 강화학습 과정을 통해 DeepSeek-R1-Zero는 자연스럽게 강력하고 흥미로운 추론 행동들을 발현시켰습니다. 하지만 가독성 저하와 언어 혼용 문제와 같은 한계점들이 발견되었습니다.

이러한 문제들을 해결하고 추론 성능을 더욱 향상시키기 위해 DeepSeek-R1이 개발되었습니다. DeepSeek-R1은 다단계 훈련 파이프라인과 콜드 스타트 데이터를 강화학습 이전에 도입하는 방식을 채택했습니다. 그 결과 DeepSeek-R1은 추론 과제에서 OpenAI-o1-1217과 비교할 만한 성능을 달성했습니다.

연구 커뮤니티를 지원하기 위해 DeepSeek-R1-Zero, DeepSeek-R1, 그리고 DeepSeek-R1에서 증류된 6개의 밀집 모델들(1.5B, 7B, 8B, 14B, 32B, 70B)을 Qwen과 Llama 기반으로 오픈소스로 공개합니다.

![벤치마크 성능 비교](https://arxiv.org/html/2501.12948v1/x1.png)

위 그림은 DeepSeek-R1과 다른 모델들의 벤치마크 성능을 비교한 결과입니다. AIME 2024, Codeforces, GPQA Diamond, MATH-500, MMLU 등 다양한 학술 및 프로그래밍 관련 벤치마크에서 각 모델의 정확도나 성능 지표(Pass@1, Percentile, Resolved)를 보여줍니다. 이는 인공지능 분야의 발전을 평가하는 데 중요한 의미를 가지며, 서로 다른 딥러닝 모델들의 능력과 진보를 비교 분석할 수 있게 해줍니다.

## 서론

최근 몇 년간 대규모 언어 모델(LLM)은 급속한 반복과 진화를 거듭하며 인공일반지능(AGI)을 향한 격차를 점진적으로 줄여나가고 있습니다. 최근에는 후처리(post-training)가 전체 훈련 파이프라인의 중요한 구성 요소로 부상했습니다. 후처리는 사전 훈련 대비 상대적으로 적은 계산 자원으로도 추론 과제의 정확도를 향상시키고, 사회적 가치와 일치시키며, 사용자 선호도에 적응시킬 수 있음이 입증되었습니다.

추론 능력의 맥락에서 OpenAI의 o1 시리즈 모델은 체인 오브 소트 추론 과정의 길이를 늘려 추론 시간 스케일링을 도입한 최초의 모델입니다. 이러한 접근법은 수학, 코딩, 과학적 추론 등 다양한 추론 과제에서 상당한 개선을 달성했습니다. 하지만 효과적인 테스트 시간 스케일링의 도전은 여전히 연구 커뮤니티에게 열린 문제로 남아있습니다.

이전 연구들은 프로세스 기반 보상 모델, 강화학습, 몬테카를로 트리 탐색과 빔 탐색 같은 탐색 알고리즘 등 다양한 접근법을 탐구해왔습니다. 하지만 이러한 방법들 중 어느 것도 OpenAI의 o1 시리즈 모델과 비교할 만한 일반적인 추론 성능을 달성하지 못했습니다.

### 순수 강화학습을 통한 추론 능력 개발

본 논문에서는 순수 강화학습을 사용하여 언어 모델의 추론 능력을 향상시키는 첫 번째 단계를 제시합니다. 목표는 지도 데이터 없이 LLM이 추론 능력을 개발할 수 있는 잠재력을 탐구하고, 순수한 강화학습 과정을 통한 자기 진화에 초점을 맞추는 것입니다.

구체적으로 DeepSeek-V3-Base를 기본 모델로 사용하고 GRPO(Group Relative Policy Optimization)를 강화학습 프레임워크로 채택하여 추론에서의 모델 성능을 향상시켰습니다. 훈련 과정에서 DeepSeek-R1-Zero는 자연스럽게 수많은 강력하고 흥미로운 추론 행동들을 발현시켰습니다.

수천 번의 강화학습 단계를 거친 후, DeepSeek-R1-Zero는 추론 벤치마크에서 뛰어난 성능을 보였습니다. 예를 들어, AIME 2024에서 pass@1 점수가 15.6%에서 71.0%로 향상되었고, 다수결 투표를 통해 86.7%까지 개선되어 OpenAI-o1-0912의 성능과 일치했습니다.

### DeepSeek-R1의 개발과 개선

하지만 DeepSeek-R1-Zero는 가독성 저하와 언어 혼용 같은 문제점들에 직면했습니다. 이러한 문제들을 해결하고 추론 성능을 더욱 향상시키기 위해 DeepSeek-R1이 개발되었습니다. DeepSeek-R1은 소량의 콜드 스타트 데이터와 다단계 훈련 파이프라인을 통합했습니다.

구체적으로, 먼저 수천 개의 콜드 스타트 데이터를 수집하여 DeepSeek-V3-Base 모델을 미세 조정했습니다. 이후 DeepSeek-R1-Zero와 유사한 추론 지향적 강화학습을 수행했습니다. 강화학습 과정이 수렴에 가까워지면, 강화학습 체크포인트에서 거부 샘플링을 통해 새로운 SFT 데이터를 생성하고, 이를 글쓰기, 사실적 질의응답, 자기 인식 등의 도메인에서 DeepSeek-V3의 지도 데이터와 결합하여 DeepSeek-V3-Base 모델을 재훈련했습니다.

새로운 데이터로 미세 조정한 후, 체크포인트는 모든 시나리오의 프롬프트를 고려한 추가적인 강화학습 과정을 거쳤습니다. 이러한 단계들을 통해 OpenAI-o1-1217과 동등한 성능을 달성하는 DeepSeek-R1 체크포인트를 얻었습니다.

### 증류를 통한 소형 모델 강화

DeepSeek-R1에서 더 작은 밀집 모델로의 증류도 탐구했습니다. Qwen2.5-32B를 기본 모델로 사용하여 DeepSeek-R1에서 직접 증류하는 것이 해당 모델에 강화학습을 적용하는 것보다 우수한 성능을 보였습니다. 이는 더 큰 기본 모델에서 발견된 추론 패턴이 추론 능력 향상에 중요함을 보여줍니다.

증류된 Qwen과 Llama 시리즈를 오픈소스로 공개합니다. 특히 증류된 14B 모델은 최첨단 오픈소스 QwQ-32B-Preview를 큰 차이로 능가하며, 증류된 32B와 70B 모델은 밀집 모델 중 추론 벤치마크에서 새로운 기록을 세웠습니다.

### 주요 기여사항

**후처리: 기본 모델에서의 대규모 강화학습**

지도 학습 미세 조정을 사전 단계로 거치지 않고 기본 모델에 직접 강화학습을 적용했습니다. 이 접근법을 통해 모델이 복잡한 문제 해결을 위한 체인 오브 소트를 탐구할 수 있게 되어 DeepSeek-R1-Zero가 개발되었습니다. DeepSeek-R1-Zero는 자기 검증, 반성, 긴 체인 오브 소트 생성 등의 능력을 보여주며 연구 커뮤니티에 중요한 이정표가 되었습니다.

특히 이는 SFT 없이 순수한 강화학습만으로 LLM의 추론 능력을 유도할 수 있음을 검증한 최초의 공개 연구입니다. 이러한 돌파구는 이 분야의 미래 발전을 위한 길을 열었습니다.

또한 DeepSeek-R1 개발을 위한 파이프라인을 소개했습니다. 이 파이프라인은 개선된 추론 패턴 발견과 인간 선호도 정렬을 목표로 하는 두 개의 강화학습 단계와, 모델의 추론 및 비추론 능력의 씨앗 역할을 하는 두 개의 SFT 단계를 통합합니다. 이 파이프라인이 더 나은 모델 생성을 통해 업계에 도움이 될 것으로 믿습니다.

**증류: 작은 모델도 강력할 수 있다**

더 큰 모델의 추론 패턴을 더 작은 모델로 증류할 수 있으며, 이는 작은 모델에서 강화학습을 통해 발견된 추론 패턴보다 더 나은 성능을 보임을 입증했습니다. 오픈소스로 공개되는 DeepSeek-R1과 API는 연구 커뮤니티가 미래에 더 나은 소형 모델을 증류하는 데 도움이 될 것입니다.

DeepSeek-R1이 생성한 추론 데이터를 사용하여 연구 커뮤니티에서 널리 사용되는 여러 밀집 모델을 미세 조정했습니다. 평가 결과 증류된 소형 밀집 모델들이 벤치마크에서 뛰어난 성능을 보였습니다. DeepSeek-R1-Distill-Qwen-7B는 AIME 2024에서 55.5%를 달성하여 QwQ-32B-Preview를 능가했습니다. 또한 DeepSeek-R1-Distill-Qwen-32B는 AIME 2024에서 72.6%, MATH-500에서 94.3%, LiveCodeBench에서 57.2%를 기록했습니다.

이러한 결과는 이전 오픈소스 모델들을 크게 능가하며 o1-mini와 비교할 만한 수준입니다. Qwen2.5와 Llama3 시리즈 기반의 1.5B, 7B, 8B, 14B, 32B, 70B 증류 체크포인트를 커뮤니티에 오픈소스로 공개합니다.

### 평가 결과 요약

**추론 과제**: DeepSeek-R1은 AIME 2024에서 79.8% Pass@1 점수를 달성하여 OpenAI-o1-1217을 약간 능가했습니다. MATH-500에서는 97.3%라는 인상적인 점수를 달성하여 OpenAI-o1-1217과 동등한 성능을 보이며 다른 모델들을 크게 앞섰습니다.

코딩 관련 과제에서 DeepSeek-R1은 전문가 수준을 보여주며, Codeforces에서 2,029 Elo 등급을 달성하여 대회 참가자의 96.3%를 능가했습니다. 엔지니어링 관련 과제에서는 DeepSeek-V3보다 약간 나은 성능을 보여 실제 과제에서 개발자들에게 도움이 될 수 있습니다.

**지식**: MMLU, MMLU-Pro, GPQA Diamond 등의 벤치마크에서 DeepSeek-R1은 뛰어난 결과를 달성했습니다. MMLU에서 90.8%, MMLU-Pro에서 84.0%, GPQA Diamond에서 71.5%를 기록하여 DeepSeek-V3를 크게 능가했습니다. 이러한 벤치마크에서 OpenAI-o1-1217보다는 약간 낮은 성능을 보이지만, 다른 폐쇄형 모델들을 능가하여 교육 과제에서의 경쟁력을 보여줍니다.

사실적 벤치마크인 SimpleQA에서 DeepSeek-R1은 DeepSeek-V3를 능가하여 사실 기반 질의 처리 능력을 보여줍니다. OpenAI-o1이 4o를 능가하는 유사한 경향이 관찰됩니다.

**기타**: DeepSeek-R1은 창의적 글쓰기, 일반 질의응답, 편집, 요약 등 광범위한 과제에서도 뛰어난 성능을 보입니다. AlpacaEval 2.0에서 87.6%의 길이 제어 승률과 ArenaHard에서 92.3%의 승률을 달성하여 시험 지향적이지 않은 질의를 지능적으로 처리하는 강력한 능력을 보여줍니다. 또한 DeepSeek-R1은 긴 맥락 이해가 필요한 과제에서 뛰어난 성능을 보이며, 긴 맥락 벤치마크에서 DeepSeek-V3를 크게 능가합니다.
## 접근 방법

### 개요

기존 연구들은 모델 성능 향상을 위해 대량의 지도 학습 데이터에 크게 의존해왔습니다. 하지만 이 연구에서는 지도 미세 조정(SFT)을 콜드 스타트로 사용하지 않고도 대규모 강화학습(RL)을 통해 추론 능력을 크게 향상시킬 수 있음을 보여줍니다. 더 나아가 소량의 콜드 스타트 데이터를 포함하면 성능을 더욱 향상시킬 수 있습니다.

이 연구에서는 세 가지 주요 접근법을 제시합니다. 첫째, SFT 데이터 없이 기본 모델에 직접 RL을 적용하는 DeepSeek-R1-Zero, 둘째, 수천 개의 긴 체인 오브 소트(CoT) 예시로 미세 조정된 체크포인트에서 시작하여 RL을 적용하는 DeepSeek-R1, 셋째, DeepSeek-R1에서 작은 밀집 모델로 추론 능력을 증류하는 방법입니다.

### DeepSeek-R1-Zero: 기본 모델에서의 강화학습

강화학습은 추론 과제에서 상당한 효과를 보여왔지만, 기존 연구들은 수집하기 어려운 지도 데이터에 크게 의존했습니다. 이 섹션에서는 지도 데이터 없이 LLM이 추론 능력을 개발할 수 있는 잠재력을 탐구하며, 순수한 강화학습 과정을 통한 자기 진화에 초점을 맞춥니다.

#### 강화학습 알고리즘: Group Relative Policy Optimization

RL 훈련 비용을 절약하기 위해 Group Relative Policy Optimization(GRPO)을 채택했습니다. GRPO는 일반적으로 정책 모델과 같은 크기인 비평 모델을 사용하지 않고, 대신 그룹 점수에서 기준선을 추정합니다.

GRPO의 핵심 아이디어는 매우 직관적입니다. 전통적인 강화학습에서는 각 행동의 가치를 평가하기 위해 별도의 가치 함수(비평 모델)를 훈련해야 했습니다. 하지만 GRPO는 같은 질문에 대해 여러 개의 답변을 생성하고, 이들 간의 상대적 성능을 비교하여 어떤 답변이 더 좋은지 판단합니다. 마치 학생들이 같은 문제를 풀고 서로의 답을 비교하여 누가 더 잘했는지 평가하는 것과 같습니다.

구체적으로, 각 질문 $q$에 대해 GRPO는 이전 정책 $\pi_{\theta_{old}}$에서 출력 그룹 $\{o_1, o_2, \cdots, o_G\}$를 샘플링하고, 다음 목적 함수를 최대화하여 정책 모델 $\pi_\theta$를 최적화합니다.

$$\mathcal{J}_{GRPO}(\theta) = \mathbb{E}_{[q\sim P(Q),\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{old}}(O|q)]} \frac{1}{G}\sum_{i=1}^{G}\left(\min\left(\frac{\pi_{\theta}(o_{i}|q)}{\pi_{\theta_{old}}(o_{i}|q)}A_{i},\text{clip}\left(\frac{\pi_{\theta}(o_{i}|q)}{\pi_{\theta_{old}}(o_{i}|q)},1-\varepsilon,1+\varepsilon\right)A_{i}\right)-\beta\mathbb{D}_{KL}\left(\pi_{\theta}||\pi_{ref}\right)\right)$$

여기서 KL 발산은 다음과 같이 정의됩니다.

$$\mathbb{D}_{KL}\left(\pi_{\theta}||\pi_{ref}\right)=\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}-\log\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}-1$$

이 수식에서 $\varepsilon$과 $\beta$는 하이퍼파라미터이며, $A_i$는 그룹 내 보상들 $\{r_1, r_2, \ldots, r_G\}$을 사용하여 계산된 이점(advantage)입니다.

$$A_{i}=\frac{r_{i}-\mathrm{mean}(\{r_{1},r_{2},\cdots,r_{G}\})}{\mathrm{std}(\{r_{1},r_{2},\cdots,r_{G}\})}$$

이 이점 계산 방식은 매우 영리합니다. 각 그룹 내에서 평균보다 좋은 성능을 보인 답변은 양의 이점을, 평균보다 나쁜 성능을 보인 답변은 음의 이점을 받게 됩니다. 표준편차로 정규화함으로써 그룹 간 점수 분포의 차이를 보정할 수 있습니다.

#### 보상 모델링

보상은 훈련 신호의 원천으로, RL의 최적화 방향을 결정합니다. DeepSeek-R1-Zero 훈련을 위해 주로 두 가지 유형의 보상으로 구성된 규칙 기반 보상 시스템을 채택했습니다.

**정확도 보상**은 응답이 올바른지 평가합니다. 예를 들어, 결정적 결과를 가진 수학 문제의 경우, 모델은 지정된 형식(예: 박스 안)으로 최종 답을 제공해야 하며, 이를 통해 정확성을 규칙 기반으로 신뢰성 있게 검증할 수 있습니다. 마찬가지로 LeetCode 문제의 경우, 컴파일러를 사용하여 미리 정의된 테스트 케이스를 기반으로 피드백을 생성할 수 있습니다.

**형식 보상**은 정확도 보상 모델 외에도 모델이 사고 과정을 '<think>'와 '</think>' 태그 사이에 넣도록 강제하는 형식 보상 모델을 사용합니다.

![DeepSeek-R1-Zero 템플릿](https://arxiv.org/html/2501.12948v1/S2.T1)

위 표는 DeepSeek-R1-Zero의 훈련 템플릿을 보여줍니다. 사용자와 어시스턴트 간의 대화 형식으로, 어시스턴트는 먼저 `<think>` 태그 안에서 추론 과정을 생각하고, `<answer>` 태그 안에서 답변을 제공합니다.

신경망 기반 결과 또는 과정 보상 모델은 DeepSeek-R1-Zero 개발에 적용하지 않았습니다. 대규모 강화학습 과정에서 신경망 보상 모델이 보상 해킹(reward hacking)에 취약할 수 있고, 보상 모델 재훈련에 추가적인 훈련 자원이 필요하며 전체 훈련 파이프라인을 복잡하게 만들기 때문입니다.

#### 훈련 템플릿

DeepSeek-R1-Zero 훈련을 위해 기본 모델이 지정된 지시사항을 준수하도록 안내하는 간단한 템플릿을 설계했습니다. 이 템플릿은 DeepSeek-R1-Zero가 먼저 추론 과정을 생성한 다음 최종 답변을 제공하도록 요구합니다.

의도적으로 제약을 이러한 구조적 형식으로 제한하고, 반성적 추론을 의무화하거나 특정 문제 해결 전략을 촉진하는 것과 같은 내용별 편향을 피했습니다. 이는 RL 과정에서 모델의 자연스러운 진행을 정확히 관찰할 수 있도록 하기 위함입니다.

#### DeepSeek-R1-Zero의 성능, 자기 진화 과정 및 아하 모멘트

**성능**

![AIME 정확도 변화](https://arxiv.org/html/2501.12948v1/extracted/6147501/figures/plot_aime_with_maj.png)

위 그림은 RL 훈련 과정 전반에 걸친 AIME 2024 벤치마크에서 DeepSeek-R1-Zero의 성능 궤적을 보여줍니다. 안정적인 평가를 위해 각 질문에 대해 16개의 응답을 샘플링하고 전체 평균 정확도를 계산했습니다.

그림에서 보듯이 DeepSeek-R1-Zero는 RL 훈련이 진행됨에 따라 꾸준하고 일관된 성능 향상을 보여줍니다. 특히 AIME 2024에서 평균 pass@1 점수가 초기 15.6%에서 인상적인 71.0%로 크게 증가하여 OpenAI-o1-0912와 비교할 만한 성능 수준에 도달했습니다. 이러한 상당한 개선은 시간이 지남에 따라 모델 성능을 최적화하는 RL 알고리즘의 효과를 강조합니다.

| Model | AIME 2024 | MATH-500 | GPQA | LiveCode | CodeForces | Diamond |
|-------|-----------|----------|------|----------|------------|---------|
| | pass@1 | cons@64 | pass@1 | pass@1 | pass@1 | rating |
| OpenAI-o1-mini | 63.6 | 80.0 | 90.0 | 60.0 | 53.8 | 1820 |
| OpenAI-o1-0912 | 74.4 | 83.3 | 94.8 | 77.3 | 63.4 | 1843 |
| DeepSeek-R1-Zero | 71.0 | 86.7 | 95.9 | 73.3 | 50.0 | 1444 |

위 표는 DeepSeek-R1-Zero와 OpenAI o1 모델들의 추론 관련 벤치마크 비교 결과를 제시합니다. 결과는 RL이 DeepSeek-R1-Zero가 지도 미세 조정 데이터 없이도 강력한 추론 능력을 달성할 수 있게 함을 보여줍니다. 이는 모델이 RL만으로도 효과적으로 학습하고 일반화할 수 있는 능력을 강조하는 주목할 만한 성과입니다.

또한 다수결 투표를 적용하면 DeepSeek-R1-Zero의 성능을 더욱 향상시킬 수 있습니다. 예를 들어, AIME 벤치마크에서 다수결 투표를 사용하면 DeepSeek-R1-Zero의 성능이 71.0%에서 86.7%로 상승하여 OpenAI-o1-0912의 성능을 능가합니다.

**자기 진화 과정**

![평균 응답 길이 변화](https://arxiv.org/html/2501.12948v1/extracted/6147501/figures/plot_length.png)

위 그림은 RL 과정에서 훈련 세트에 대한 DeepSeek-R1-Zero의 평균 응답 길이를 보여줍니다. DeepSeek-R1-Zero는 자연스럽게 더 많은 사고 시간을 통해 추론 과제를 해결하는 방법을 학습합니다.

DeepSeek-R1-Zero의 자기 진화 과정은 RL이 모델의 추론 능력을 자율적으로 향상시킬 수 있는 방법을 보여주는 매혹적인 시연입니다. 기본 모델에서 직접 RL을 시작함으로써 지도 미세 조정 단계의 영향 없이 모델의 진행을 면밀히 모니터링할 수 있습니다.

그림에서 보듯이 DeepSeek-R1-Zero의 사고 시간은 훈련 과정 전반에 걸쳐 일관된 개선을 보여줍니다. 이러한 개선은 외부 조정의 결과가 아니라 모델 내부의 본질적 발전입니다. DeepSeek-R1-Zero는 확장된 테스트 시간 계산을 활용하여 점점 더 복잡한 추론 과제를 해결하는 능력을 자연스럽게 습득합니다. 이 계산은 수백에서 수천 개의 추론 토큰을 생성하는 범위에 이르며, 모델이 사고 과정을 더 깊이 탐구하고 정제할 수 있게 합니다.

이 자기 진화의 가장 놀라운 측면 중 하나는 테스트 시간 계산이 증가함에 따라 정교한 행동들이 나타나는 것입니다. 모델이 이전 단계를 재검토하고 재평가하는 반성과 같은 행동, 그리고 문제 해결에 대한 대안적 접근법의 탐구가 자발적으로 나타납니다. 이러한 행동들은 명시적으로 프로그래밍되지 않았지만 모델과 강화학습 환경의 상호작용 결과로 나타납니다.

**아하 모멘트**

훈련 과정에서 관찰된 특히 흥미로운 현상은 "아하 모멘트"의 발생입니다. 이 모멘트는 중간 버전의 모델에서 발생하며, DeepSeek-R1-Zero가 초기 접근법을 재평가하여 문제에 더 많은 사고 시간을 할당하는 방법을 학습합니다.

다음은 실제 아하 모멘트의 예시입니다.

**질문**: $a > 1$일 때, $\sqrt{a-\sqrt{a+x}}=x$의 실근의 합은 얼마인가?

**응답**: 
```
<think>
방정식 $\sqrt{a-\sqrt{a+x}}=x$를 풀기 위해 양변을 제곱해보겠습니다...
$(\sqrt{a-\sqrt{a+x}})^{2}=x^{2}\implies a-\sqrt{a+x}=x^{2}$

내부 제곱근 항을 분리하기 위해 정리하면:
$(a-x^{2})^{2}=a+x\implies a^{2}-2ax^{2}+(x^{2})^{2}=a+x\implies x^{4}-2ax^{2}-x+(a^{2}-a)=0$

...잠깐, 잠깐. 잠깐. 여기서 아하 모멘트를 표시할 수 있습니다.

이 단계별로 다시 평가해서 올바른 합이 무엇인지 확인해보겠습니다...

방정식부터 다시 시작하겠습니다.
$\sqrt{a-\sqrt{a+x}}=x$

먼저 양변을 제곱합니다.
$a-\sqrt{a+x}=x^{2}\implies\sqrt{a+x}=a-x^{2}$

다시 양변을 제곱하면:
...
</think>
```

이 예시에서 모델은 계산 중간에 "잠깐, 잠깐. 잠깐. 여기서 아하 모멘트를 표시할 수 있습니다"라고 말하며 자신의 접근법을 재고하기 시작합니다. 이는 모델이 의인화된 톤을 사용하여 다시 생각하는 방법을 학습했음을 보여줍니다.

이 행동은 모델의 성장하는 추론 능력에 대한 증거일 뿐만 아니라 강화학습이 예상치 못한 정교한 결과로 이어질 수 있는 방법을 보여주는 매혹적인 예시입니다. 이 순간은 모델뿐만 아니라 그 행동을 관찰하는 연구자들에게도 "아하 모멘트"입니다. 이는 모델에게 문제 해결 방법을 명시적으로 가르치는 대신, 단순히 올바른 인센티브를 제공하면 모델이 자율적으로 고급 문제 해결 전략을 개발한다는 강화학습의 힘과 아름다움을 강조합니다.

**DeepSeek-R1-Zero의 한계**

DeepSeek-R1-Zero가 강력한 추론 능력을 보이고 예상치 못한 강력한 추론 행동들을 자율적으로 개발하지만, 몇 가지 문제에 직면합니다. 예를 들어, DeepSeek-R1-Zero는 가독성 저하와 언어 혼용과 같은 문제로 어려움을 겪습니다. 추론 과정을 더 읽기 쉽게 만들고 오픈 커뮤니티와 공유하기 위해 인간 친화적인 콜드 스타트 데이터와 함께 RL을 활용하는 방법인 DeepSeek-R1을 탐구합니다.
이 연구에서는 세 가지 주요 접근법을 제시합니다. 첫째, SFT 데이터 없이 기본 모델에 직접 RL을 적용하는 DeepSeek-R1-Zero, 둘째, 수천 개의 긴 체인 오브 소트(CoT) 예시로 미세 조정된 체크포인트에서 시작하여 RL을 적용하는 DeepSeek-R1, 셋째, DeepSeek-R1에서 작은 밀집 모델로 추론 능력을 증류하는 방법입니다.

### DeepSeek-R1-Zero: 기본 모델에서의 강화학습

강화학습은 이전 연구들(Wang et al., 2023; Shao et al., 2024)에서 보여준 바와 같이 추론 과제에서 상당한 효과를 입증했습니다. 하지만 이러한 연구들은 수집하기 시간 집약적인 지도 데이터에 크게 의존했습니다. 이 섹션에서는 지도 데이터 없이 LLM이 추론 능력을 개발할 수 있는 잠재력을 탐구하며, 순수한 강화학습 과정을 통한 자기 진화에 초점을 맞춥니다.

#### 강화학습 알고리즘: Group Relative Policy Optimization

RL 훈련 비용을 절약하기 위해 Group Relative Policy Optimization(GRPO)을 채택했습니다. GRPO는 일반적으로 정책 모델과 같은 크기인 비평 모델을 사용하지 않고, 대신 그룹 점수에서 기준선을 추정합니다.

GRPO의 핵심 아이디어는 매우 직관적입니다. 전통적인 강화학습에서는 각 행동의 가치를 평가하기 위해 별도의 가치 함수(비평 모델)를 훈련해야 했습니다. 하지만 GRPO는 같은 질문에 대해 여러 개의 답변을 생성하고, 이들 간의 상대적 성능을 비교하여 어떤 답변이 더 좋은지 판단합니다. 마치 학생들이 같은 문제를 풀고 서로의 답을 비교하여 누가 더 잘했는지 평가하는 것과 같습니다.

구체적으로, 각 질문 $q$에 대해 GRPO는 이전 정책 $\pi_{\theta_{old}}$에서 출력 그룹 $\{o_1, o_2, \cdots, o_G\}$를 샘플링하고, 다음 목적 함수를 최대화하여 정책 모델 $\pi_\theta$를 최적화합니다.

$$\mathcal{J}_{GRPO}(\theta) = \mathbb{E}_{[q\sim P(Q),\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{old}}(O|q)]} \frac{1}{G}\sum_{i=1}^{G}\left(\min\left(\frac{\pi_{\theta}(o_{i}|q)}{\pi_{\theta_{old}}(o_{i}|q)}A_{i},\text{clip}\left(\frac{\pi_{\theta}(o_{i}|q)}{\pi_{\theta_{old}}(o_{i}|q)},1-\varepsilon,1+\varepsilon\right)A_{i}\right)-\beta\mathbb{D}_{KL}\left(\pi_{\theta}||\pi_{ref}\right)\right)$$

이 수식을 단계별로 이해해보겠습니다. 첫 번째 항은 정책 비율 $\frac{\pi_{\theta}(o_{i}|q)}{\pi_{\theta_{old}}(o_{i}|q)}$에 이점 $A_i$를 곱한 값입니다. 이는 현재 정책이 이전 정책보다 좋은 출력에 더 높은 확률을 할당하도록 유도합니다. 클리핑 메커니즘은 정책 업데이트가 너무 급격하게 변하는 것을 방지하여 훈련 안정성을 보장합니다.

KL 발산 항은 다음과 같이 정의됩니다.

$$\mathbb{D}_{KL}\left(\pi_{\theta}||\pi_{ref}\right)=\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}-\log\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}-1$$

이 정규화 항은 현재 정책이 참조 정책 $\pi_{ref}$에서 너무 멀어지지 않도록 제약을 가합니다. 이는 훈련 과정에서 모델이 원래의 언어 생성 능력을 잃지 않도록 보호하는 역할을 합니다.

이점(advantage) $A_i$는 그룹 내 보상들 $\{r_1, r_2, \ldots, r_G\}$을 사용하여 계산됩니다.

$$A_{i}=\frac{r_{i}-\mathrm{mean}(\{r_{1},r_{2},\cdots,r_{G}\})}{\mathrm{std}(\{r_{1},r_{2},\cdots,r_{G}\})}$$

이 이점 계산 방식은 매우 영리합니다. 각 그룹 내에서 평균보다 좋은 성능을 보인 답변은 양의 이점을, 평균보다 나쁜 성능을 보인 답변은 음의 이점을 받게 됩니다. 표준편차로 정규화함으로써 그룹 간 점수 분포의 차이를 보정할 수 있습니다. 이는 절대적인 점수보다는 상대적인 성능에 기반하여 학습이 이루어지도록 하여, 다양한 난이도의 문제들에 대해 일관된 학습 신호를 제공합니다.

| Template Component | Content |
|-------------------|---------|
| User | prompt |
| Assistant | <think> reasoning process here </think><answer> answer here </answer> |

위 표는 DeepSeek-R1-Zero의 훈련 템플릿을 보여줍니다. 사용자와 어시스턴트 간의 대화 형식으로, 어시스턴트는 먼저 `<think>` 태그 안에서 추론 과정을 생각하고, `<answer>` 태그 안에서 답변을 제공합니다. 훈련 중에는 prompt가 구체적인 추론 질문으로 대체됩니다.

#### 보상 모델링

보상은 훈련 신호의 원천으로, RL의 최적화 방향을 결정합니다. DeepSeek-R1-Zero 훈련을 위해 주로 두 가지 유형의 보상으로 구성된 규칙 기반 보상 시스템을 채택했습니다.

**정확도 보상**은 응답이 올바른지 평가합니다. 예를 들어, 결정적 결과를 가진 수학 문제의 경우, 모델은 지정된 형식(예: 박스 안)으로 최종 답을 제공해야 하며, 이를 통해 정확성을 규칙 기반으로 신뢰성 있게 검증할 수 있습니다. 마찬가지로 LeetCode 문제의 경우, 컴파일러를 사용하여 미리 정의된 테스트 케이스를 기반으로 피드백을 생성할 수 있습니다.

**형식 보상**은 정확도 보상 모델 외에도 모델이 사고 과정을 '<think>'와 '</think>' 태그 사이에 넣도록 강제하는 형식 보상 모델을 사용합니다. 이는 모델이 단순히 답만 제시하는 것이 아니라 추론 과정을 명시적으로 보여주도록 유도합니다.

신경망 기반 결과 또는 과정 보상 모델은 DeepSeek-R1-Zero 개발에 적용하지 않았습니다. 대규모 강화학습 과정에서 신경망 보상 모델이 보상 해킹(reward hacking)에 취약할 수 있고, 보상 모델 재훈련에 추가적인 훈련 자원이 필요하며 전체 훈련 파이프라인을 복잡하게 만들기 때문입니다. 보상 해킹은 모델이 실제 성능 향상 없이 보상 모델을 속이는 방법을 학습하는 현상으로, 이는 진정한 추론 능력 향상을 저해할 수 있습니다.

#### 훈련 템플릿

DeepSeek-R1-Zero 훈련을 위해 기본 모델이 지정된 지시사항을 준수하도록 안내하는 간단한 템플릿을 설계했습니다. 이 템플릿은 DeepSeek-R1-Zero가 먼저 추론 과정을 생성한 다음 최종 답변을 제공하도록 요구합니다.

의도적으로 제약을 이러한 구조적 형식으로 제한하고, 반성적 추론을 의무화하거나 특정 문제 해결 전략을 촉진하는 것과 같은 내용별 편향을 피했습니다. 이는 RL 과정에서 모델의 자연스러운 진행을 정확히 관찰할 수 있도록 하기 위함입니다. 만약 특정한 추론 패턴을 강제했다면, 모델이 자발적으로 개발하는 추론 전략을 관찰하기 어려웠을 것입니다.

#### DeepSeek-R1-Zero의 성능, 자기 진화 과정 및 아하 모멘트

**성능**

![AIME 정확도 변화](https://arxiv.org/html/2501.12948v1/extracted/6147501/figures/plot_aime_with_maj.png)

위 그림은 RL 훈련 과정 전반에 걸친 AIME 2024 벤치마크에서 DeepSeek-R1-Zero의 성능 궤적을 보여줍니다. 안정적인 평가를 위해 각 질문에 대해 16개의 응답을 샘플링하고 전체 평균 정확도를 계산했습니다.

그림에서 보듯이 DeepSeek-R1-Zero는 RL 훈련이 진행됨에 따라 꾸준하고 일관된 성능 향상을 보여줍니다. 특히 AIME 2024에서 평균 pass@1 점수가 초기 15.6%에서 인상적인 71.0%로 크게 증가하여 OpenAI-o1-0912와 비교할 만한 성능 수준에 도달했습니다. 이러한 상당한 개선은 시간이 지남에 따라 모델 성능을 최적화하는 RL 알고리즘의 효과를 강조합니다.

| Model | AIME 2024 | MATH-500 | GPQA | LiveCode | CodeForces | Diamond |
|-------|-----------|----------|------|----------|------------|---------|
| | pass@1 | cons@64 | pass@1 | pass@1 | pass@1 | rating |
| OpenAI-o1-mini | 63.6 | 80.0 | 90.0 | 60.0 | 53.8 | 1820 |
| OpenAI-o1-0912 | 74.4 | 83.3 | 94.8 | 77.3 | 63.4 | 1843 |
| DeepSeek-R1-Zero | 71.0 | 86.7 | 95.9 | 73.3 | 50.0 | 1444 |

위 표는 DeepSeek-R1-Zero와 OpenAI o1 모델들의 추론 관련 벤치마크 비교 결과를 제시합니다. 결과는 RL이 DeepSeek-R1-Zero가 지도 미세 조정 데이터 없이도 강력한 추론 능력을 달성할 수 있게 함을 보여줍니다. 이는 모델이 RL만으로도 효과적으로 학습하고 일반화할 수 있는 능력을 강조하는 주목할 만한 성과입니다.

또한 다수결 투표를 적용하면 DeepSeek-R1-Zero의 성능을 더욱 향상시킬 수 있습니다. 예를 들어, AIME 벤치마크에서 다수결 투표를 사용하면 DeepSeek-R1-Zero의 성능이 71.0%에서 86.7%로 상승하여 OpenAI-o1-0912의 성능을 능가합니다. 다수결 투표가 효과적인 이유는 모델이 같은 문제에 대해 여러 가지 추론 경로를 탐색할 수 있고, 이 중에서 가장 일관성 있는 답을 선택할 수 있기 때문입니다.

**자기 진화 과정**

![평균 응답 길이 변화](https://arxiv.org/html/2501.12948v1/extracted/6147501/figures/plot_length.png)

위 그림은 RL 과정에서 훈련 세트에 대한 DeepSeek-R1-Zero의 평균 응답 길이를 보여줍니다. DeepSeek-R1-Zero는 자연스럽게 더 많은 사고 시간을 통해 추론 과제를 해결하는 방법을 학습합니다.

DeepSeek-R1-Zero의 자기 진화 과정은 RL이 모델의 추론 능력을 자율적으로 향상시킬 수 있는 방법을 보여주는 매혹적인 시연입니다. 기본 모델에서 직접 RL을 시작함으로써 지도 미세 조정 단계의 영향 없이 모델의 진행을 면밀히 모니터링할 수 있습니다. 이는 순수한 강화학습이 어떻게 복잡한 추론 능력을 유도할 수 있는지에 대한 귀중한 통찰을 제공합니다.

그림에서 보듯이 DeepSeek-R1-Zero의 사고 시간은 훈련 과정 전반에 걸쳐 일관된 개선을 보여줍니다. 이러한 개선은 외부 조정의 결과가 아니라 모델 내부의 본질적 발전입니다. DeepSeek-R1-Zero는 확장된 테스트 시간 계산을 활용하여 점점 더 복잡한 추론 과제를 해결하는 능력을 자연스럽게 습득합니다. 이 계산은 수백에서 수천 개의 추론 토큰을 생성하는 범위에 이르며, 모델이 사고 과정을 더 깊이 탐구하고 정제할 수 있게 합니다.

이 자기 진화의 가장 놀라운 측면 중 하나는 테스트 시간 계산이 증가함에 따라 정교한 행동들이 나타나는 것입니다. 모델이 이전 단계를 재검토하고 재평가하는 반성과 같은 행동, 그리고 문제 해결에 대한 대안적 접근법의 탐구가 자발적으로 나타납니다. 이러한 행동들은 명시적으로 프로그래밍되지 않았지만 모델과 강화학습 환경의 상호작용 결과로 나타납니다. 이는 적절한 인센티브 구조가 주어졌을 때 AI 시스템이 얼마나 정교한 전략을 자율적으로 개발할 수 있는지를 보여주는 놀라운 사례입니다.

**아하 모멘트**

훈련 과정에서 관찰된 특히 흥미로운 현상은 "아하 모멘트"의 발생입니다. 이 모멘트는 중간 버전의 모델에서 발생하며, DeepSeek-R1-Zero가 초기 접근법을 재평가하여 문제에 더 많은 사고 시간을 할당하는 방법을 학습합니다.

실제 아하 모멘트의 예시를 살펴보겠습니다.

**질문**: $a > 1$일 때, $\sqrt{a-\sqrt{a+x}}=x$의 실근의 합은 얼마인가?

**응답**: 
```
<think>
방정식 $\sqrt{a-\sqrt{a+x}}=x$를 풀기 위해 양변을 제곱해보겠습니다...
$(\sqrt{a-\sqrt{a+x}})^{2}=x^{2}\implies a-\sqrt{a+x}=x^{2}$

내부 제곱근 항을 분리하기 위해 정리하면:
$(a-x^{2})^{2}=a+x\implies a^{2}-2ax^{2}+(x^{2})^{2}=a+x\implies x^{4}-2ax^{2}-x+(a^{2}-a)=0$

...잠깐, 잠깐. 잠깐. 여기서 아하 모멘트를 표시할 수 있습니다.

이 단계별로 다시 평가해서 올바른 합이 무엇인지 확인해보겠습니다...

방정식부터 다시 시작하겠습니다.
$\sqrt{a-\sqrt{a+x}}=x$

먼저 양변을 제곱합니다.
$a-\sqrt{a+x}=x^{2}\implies\sqrt{a+x}=a-x^{2}$

다시 양변을 제곱하면:
...
</think>
```

이 예시에서 모델은 계산 중간에 "잠깐, 잠깐. 잠깐. 여기서 아하 모멘트를 표시할 수 있습니다"라고 말하며 자신의 접근법을 재고하기 시작합니다. 이는 모델이 의인화된 톤을 사용하여 다시 생각하는 방법을 학습했음을 보여줍니다.

이 행동은 모델의 성장하는 추론 능력에 대한 증거일 뿐만 아니라 강화학습이 예상치 못한 정교한 결과로 이어질 수 있는 방법을 보여주는 매혹적인 예시입니다. 이 순간은 모델뿐만 아니라 그 행동을 관찰하는 연구자들에게도 "아하 모멘트"입니다. 이는 모델에게 문제 해결 방법을 명시적으로 가르치는 대신, 단순히 올바른 인센티브를 제공하면 모델이 자율적으로 고급 문제 해결 전략을 개발한다는 강화학습의 힘과 아름다움을 강조합니다.

이러한 자발적인 정교한 행동의 발현은 강화학습이 단순히 성능을 향상시키는 것을 넘어서 모델의 내재적 추론 능력을 근본적으로 변화시킬 수 있음을 시사합니다. 모델이 스스로 문제를 재검토하고 대안적 접근법을 탐색하는 능력을 개발한 것은 미래의 더 자율적이고 적응적인 모델을 위한 길을 열어줍니다.

**DeepSeek-R1-Zero의 한계**

DeepSeek-R1-Zero가 강력한 추론 능력을 보이고 예상치 못한 강력한 추론 행동들을 자율적으로 개발하지만, 몇 가지 문제에 직면합니다. 예를 들어, DeepSeek-R1-Zero는 가독성 저하와 언어 혼용과 같은 문제로 어려움을 겪습니다. 추론 과정을 더 읽기 쉽게 만들고 오픈 커뮤니티와 공유하기 위해 인간 친화적인 콜드 스타트 데이터와 함께 RL을 활용하는 방법인 DeepSeek-R1을 탐구합니다.

이러한 한계점들은 순수한 강화학습 접근법의 트레이드오프를 보여줍니다. 모델이 자율적으로 강력한 추론 능력을 개발할 수 있지만, 인간이 읽기 쉬운 형태로 그 과정을 표현하는 데는 어려움이 있습니다. 이는 다음 단계인 DeepSeek-R1 개발의 동기가 되었습니다.
## DeepSeek-R1: 콜드 스타트를 활용한 강화학습

DeepSeek-R1-Zero의 유망한 결과에서 영감을 받아 두 가지 자연스러운 질문이 제기됩니다. 첫째, 소량의 고품질 데이터를 콜드 스타트로 포함하여 추론 성능을 더욱 향상시키거나 수렴을 가속화할 수 있는가? 둘째, 명확하고 일관된 체인 오브 소트를 생성할 뿐만 아니라 강력한 일반 능력을 보여주는 사용자 친화적인 모델을 어떻게 훈련할 수 있는가? 이러한 질문들을 해결하기 위해 DeepSeek-R1을 훈련하는 파이프라인을 설계했습니다. 이 파이프라인은 네 단계로 구성되어 있습니다.

### 콜드 스타트

DeepSeek-R1-Zero와 달리, DeepSeek-R1에서는 기본 모델에서 시작하는 RL 훈련의 초기 불안정한 콜드 스타트 단계를 방지하기 위해 소량의 긴 CoT 데이터를 구성하고 수집하여 모델을 초기 RL 액터로 미세 조정합니다. 이러한 데이터를 수집하기 위해 여러 접근법을 탐구했습니다. 긴 CoT를 예시로 사용하는 퓨 샷 프롬프팅, 반성과 검증을 포함한 상세한 답변을 생성하도록 모델에 직접 프롬프팅하기, 읽기 쉬운 형식으로 DeepSeek-R1-Zero 출력을 수집하기, 그리고 인간 주석자의 후처리를 통해 결과를 정제하는 방법들을 사용했습니다.

이 연구에서는 수천 개의 콜드 스타트 데이터를 수집하여 DeepSeek-V3-Base를 RL의 시작점으로 미세 조정했습니다. DeepSeek-R1-Zero와 비교했을 때 콜드 스타트 데이터의 장점은 다음과 같습니다.

**가독성**: DeepSeek-R1-Zero의 주요 한계 중 하나는 내용이 종종 읽기에 적합하지 않다는 것입니다. 응답이 여러 언어를 혼용하거나 사용자를 위해 답변을 강조하는 마크다운 형식이 부족할 수 있습니다. 반면 DeepSeek-R1의 콜드 스타트 데이터를 생성할 때는 각 응답 끝에 요약을 포함하고 독자 친화적이지 않은 응답을 필터링하는 읽기 쉬운 패턴을 설계했습니다. 여기서 출력 형식을 `|special_token|<reasoning_process>|special_token|<summary>`로 정의했는데, 추론 과정은 질의에 대한 CoT이고 요약은 추론 결과를 요약하는 데 사용됩니다.

이러한 형식 설계는 매우 중요한 의미를 가집니다. 특수 토큰으로 구분된 추론 과정과 요약 부분은 모델이 내부적인 사고 과정과 사용자에게 제시할 최종 답변을 명확히 구분할 수 있게 해줍니다. 이는 마치 학생이 문제를 풀 때 초안지에서 계산하고 정리된 답안지에 최종 답을 적는 것과 유사합니다. 추론 과정에서는 모델이 자유롭게 탐색하고 시행착오를 거칠 수 있지만, 요약 부분에서는 명확하고 간결한 답변을 제공합니다.

**잠재력**: 인간의 사전 지식으로 콜드 스타트 데이터의 패턴을 신중하게 설계함으로써 DeepSeek-R1-Zero 대비 더 나은 성능을 관찰했습니다. 반복적 훈련이 추론 모델에게 더 나은 방법이라고 믿습니다.

이러한 접근법은 [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/pdf/2203.11171)에서 제안된 자기 일관성 개념과 연결됩니다. 해당 연구에서는 복잡한 추론 과제가 일반적으로 올바른 답으로 이어지는 여러 추론 경로를 허용한다는 직관을 활용했습니다. 마찬가지로 콜드 스타트 데이터에서도 다양한 추론 경로를 통해 일관된 답변에 도달하는 패턴을 학습할 수 있습니다.

### 추론 지향적 강화학습

DeepSeek-V3-Base를 콜드 스타트 데이터로 미세 조정한 후, DeepSeek-R1-Zero에서 사용한 것과 동일한 대규모 강화학습 훈련 과정을 적용합니다. 이 단계는 모델의 추론 능력 향상에 초점을 맞추며, 특히 코딩, 수학, 과학, 논리 추론과 같은 추론 집약적 과제에서 명확한 해결책이 있는 잘 정의된 문제들을 다룹니다.

훈련 과정에서 CoT가 종종 언어 혼용을 보이는 것을 관찰했습니다. 특히 RL 프롬프트가 여러 언어를 포함할 때 이러한 현상이 나타납니다. 언어 혼용 문제를 완화하기 위해 RL 훈련 중에 언어 일관성 보상을 도입했습니다. 이는 CoT에서 목표 언어 단어의 비율로 계산됩니다.

언어 일관성 보상의 수학적 정의는 다음과 같습니다.

$$R_{\text{lang}} = \frac{\text{목표 언어 토큰 수}}{\text{전체 토큰 수}}$$

이 보상은 모델이 일관된 언어로 추론 과정을 표현하도록 유도합니다. 예를 들어, 한국어 질문에 대해서는 한국어로, 영어 질문에 대해서는 영어로 일관되게 답변하도록 학습시킵니다. 이는 사용자 경험을 크게 개선하는 중요한 요소입니다.

절제 실험에서 이러한 정렬이 모델 성능에 약간의 저하를 가져오지만, 이 보상은 인간 선호도와 일치하여 더 읽기 쉽게 만듭니다. 최종적으로 추론 과제의 정확도와 언어 일관성 보상을 직접 합산하여 최종 보상을 형성합니다.

$$R_{\text{final}} = R_{\text{accuracy}} + \lambda \cdot R_{\text{lang}}$$

여기서 $\lambda$는 언어 일관성의 중요도를 조절하는 하이퍼파라미터입니다. 이후 미세 조정된 모델에 RL 훈련을 적용하여 추론 과제에서 수렴에 도달할 때까지 진행합니다.

이러한 접근법은 [Training Language Models to Self-Correct via Reinforcement Learning](https://arxiv.org/pdf/2409.12917)에서 제안된 자기 교정 학습과 유사한 철학을 공유합니다. 두 연구 모두 강화학습을 통해 모델이 자신의 출력을 개선하고 인간 선호도에 맞추는 방법을 탐구합니다.

### 거부 샘플링과 지도 미세 조정

추론 지향적 RL이 수렴하면, 결과 체크포인트를 활용하여 후속 라운드를 위한 SFT(지도 미세 조정) 데이터를 수집합니다. 주로 추론에 초점을 맞춘 초기 콜드 스타트 데이터와 달리, 이 단계에서는 다른 도메인의 데이터를 통합하여 글쓰기, 역할 연기, 기타 범용 과제에서 모델의 능력을 향상시킵니다.

구체적으로 다음과 같이 데이터를 생성하고 모델을 미세 조정합니다.

**추론 데이터**: 추론 프롬프트를 큐레이션하고 위의 RL 훈련에서 나온 체크포인트에서 거부 샘플링을 수행하여 추론 궤적을 생성합니다. 이전 단계에서는 규칙 기반 보상으로 평가할 수 있는 데이터만 포함했습니다. 하지만 이 단계에서는 추가 데이터를 통합하여 데이터셋을 확장하며, 일부는 정답과 모델 예측을 DeepSeek-V3에 입력하여 판단하는 생성적 보상 모델을 사용합니다.

거부 샘플링의 과정은 다음과 같습니다.

1. 각 추론 프롬프트에 대해 여러 개의 응답을 샘플링합니다
2. 규칙 기반 보상 또는 생성적 보상 모델을 사용하여 각 응답을 평가합니다
3. 올바른 응답만을 선택하여 훈련 데이터로 사용합니다

또한 모델 출력이 때때로 혼란스럽고 읽기 어렵기 때문에, 언어가 혼용된 체인 오브 소트, 긴 문단, 코드 블록을 필터링했습니다. 각 프롬프트에 대해 여러 응답을 샘플링하고 올바른 것만 유지합니다. 총 약 60만 개의 추론 관련 훈련 샘플을 수집했습니다.

**비추론 데이터**: 글쓰기, 사실적 QA, 자기 인식, 번역과 같은 비추론 데이터의 경우 DeepSeek-V3 파이프라인을 채택하고 DeepSeek-V3의 SFT 데이터셋 일부를 재사용합니다. 특정 비추론 과제의 경우 프롬프팅을 통해 질문에 답하기 전에 잠재적인 체인 오브 소트를 생성하도록 DeepSeek-V3를 호출합니다. 하지만 "안녕하세요"와 같은 간단한 질의의 경우 응답에 CoT를 제공하지 않습니다.

이러한 접근법은 모델이 상황에 따라 적절한 수준의 추론을 제공하도록 학습시킵니다. 복잡한 문제에는 상세한 추론 과정을, 간단한 인사에는 직접적인 응답을 제공하는 것입니다. 최종적으로 추론과 관련이 없는 약 20만 개의 훈련 샘플을 수집했습니다.

위에서 큐레이션한 약 80만 개 샘플의 데이터셋을 사용하여 DeepSeek-V3-Base를 2 에포크 동안 미세 조정합니다.

### 모든 시나리오를 위한 강화학습

모델을 인간 선호도와 더욱 일치시키기 위해 모델의 유용성과 무해성을 개선하는 동시에 추론 능력을 정제하는 것을 목표로 하는 2차 강화학습 단계를 구현합니다. 구체적으로 보상 신호와 다양한 프롬프트 분포의 조합을 사용하여 모델을 훈련합니다.

추론 데이터의 경우 DeepSeek-R1-Zero에서 설명한 방법론을 준수하며, 수학, 코드, 논리 추론 도메인에서 학습 과정을 안내하기 위해 규칙 기반 보상을 활용합니다. 일반 데이터의 경우 복잡하고 미묘한 시나리오에서 인간 선호도를 포착하기 위해 보상 모델에 의존합니다.

DeepSeek-V3 파이프라인을 기반으로 구축하고 유사한 선호도 쌍과 훈련 프롬프트 분포를 채택합니다. 유용성의 경우 최종 요약에만 독점적으로 초점을 맞춰 평가가 기본 추론 과정에 대한 간섭을 최소화하면서 사용자에 대한 응답의 유용성과 관련성을 강조하도록 보장합니다.

무해성의 경우 추론 과정과 요약을 모두 포함한 모델의 전체 응답을 평가하여 생성 과정에서 발생할 수 있는 잠재적 위험, 편향 또는 유해한 콘텐츠를 식별하고 완화합니다.

이러한 이중 평가 시스템은 매우 중요합니다. 유용성 평가는 사용자가 실제로 받게 되는 최종 답변의 품질에 집중하여 실용적 가치를 보장합니다. 반면 무해성 평가는 추론 과정을 포함한 전체 응답을 검토하여 모델이 내부적으로 부적절한 내용을 생성하지 않도록 보장합니다.

궁극적으로 보상 신호와 다양한 데이터 분포의 통합을 통해 유용성과 무해성을 우선시하면서 추론에서 뛰어난 모델을 훈련할 수 있습니다. 이는 [Solving math word problems with process- and outcome-based feedback](https://arxiv.org/pdf/2211.14275)에서 제안된 과정 기반 피드백과 결과 기반 피드백의 조합과 유사한 접근법입니다.

이러한 다단계 파이프라인을 통해 DeepSeek-R1은 강력한 추론 능력과 인간 친화적인 출력 형식, 그리고 일반적인 언어 모델 능력을 모두 갖춘 모델로 발전했습니다. 각 단계는 이전 단계의 한계를 보완하고 개선하는 방향으로 설계되어, 최종적으로 OpenAI-o1-1217과 비교할 만한 성능을 달성할 수 있었습니다.
## 증류: 작은 모델에 추론 능력 부여

DeepSeek-R1과 같은 추론 능력을 더 효율적인 작은 모델들에게 부여하기 위해, 앞서 2.3.3절에서 자세히 설명한 DeepSeek-R1로 큐레이션된 80만 개 샘플을 사용하여 Qwen과 Llama 같은 오픈소스 모델들을 직접 미세 조정했습니다. 연구 결과 이러한 직접적인 증류 방법이 작은 모델들의 추론 능력을 상당히 향상시킨다는 것을 확인했습니다.

이 연구에서 사용한 기본 모델들은 Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, 그리고 Llama-3.3-70B-Instruct입니다. Llama-3.3을 선택한 이유는 Llama-3.1보다 추론 능력이 약간 더 우수하기 때문입니다.

증류된 모델들의 경우 지도 미세 조정(SFT)만을 적용하고 강화학습(RL) 단계는 포함하지 않았습니다. RL을 통합하면 모델 성능을 상당히 향상시킬 수 있음에도 불구하고 이러한 선택을 한 이유는, 이 연구의 주요 목표가 증류 기법의 효과를 입증하는 것이며, RL 단계의 탐구는 더 넓은 연구 커뮤니티에 맡기기 위함입니다.

### 증류 방법론의 핵심 원리

증류 과정은 본질적으로 큰 모델(교사 모델)이 학습한 복잡한 추론 패턴을 작은 모델(학생 모델)이 모방할 수 있도록 하는 지식 전달 과정입니다. 이는 마치 숙련된 전문가가 초보자에게 문제 해결 방법을 단계별로 가르치는 것과 유사합니다. DeepSeek-R1이 생성한 80만 개의 고품질 추론 궤적은 작은 모델들이 학습할 수 있는 풍부한 교육 자료 역할을 합니다.

이러한 접근법의 장점은 여러 가지입니다. 첫째, 작은 모델들이 처음부터 추론 능력을 개발하는 것보다 훨씬 효율적입니다. 둘째, DeepSeek-R1이 이미 검증된 추론 패턴을 제공하므로 학습 과정에서 발생할 수 있는 오류나 비효율성을 줄일 수 있습니다. 셋째, 다양한 크기의 모델에 일관된 추론 능력을 부여할 수 있어 실제 응용에서 계산 자원에 따라 적절한 모델을 선택할 수 있습니다.

### 증류 데이터의 품질과 다양성

앞서 설명한 80만 개의 샘플은 단순히 양적으로 많은 것이 아니라 질적으로도 우수합니다. 이 데이터는 수학, 코딩, 과학, 논리 추론 등 다양한 도메인을 포괄하며, 각 샘플은 DeepSeek-R1의 강화학습 과정을 통해 검증된 고품질 추론 궤적입니다. 특히 거부 샘플링을 통해 올바른 답변만을 선별했기 때문에, 작은 모델들이 잘못된 추론 패턴을 학습할 위험을 최소화했습니다.

데이터의 구성을 살펴보면, 약 60만 개는 추론 관련 샘플이고 약 20만 개는 비추론 샘플입니다. 이러한 균형잡힌 구성은 작은 모델들이 추론 능력뿐만 아니라 일반적인 언어 모델 능력도 유지할 수 있도록 보장합니다. 추론 샘플의 경우 복잡한 수학 문제부터 프로그래밍 과제까지 다양한 난이도와 유형을 포함하여, 모델이 다양한 상황에서 적절한 추론 전략을 학습할 수 있도록 했습니다.

### 모델별 증류 결과와 특성

각 기본 모델의 특성에 따라 증류 결과도 다르게 나타납니다. Qwen2.5-Math 시리즈의 경우 이미 수학적 추론에 특화되어 있어 증류 과정에서 더욱 정교한 수학적 추론 능력을 보여줍니다. 반면 Llama 시리즈는 더 일반적인 언어 모델이므로 증류를 통해 추론 능력을 새롭게 획득하는 과정을 보여줍니다.

특히 주목할 점은 모델 크기에 따른 증류 효과의 차이입니다. 1.5B 모델의 경우 기본적인 추론 패턴을 학습하는 데 집중하는 반면, 70B 모델은 더 복잡하고 미묘한 추론 전략까지 습득할 수 있습니다. 이는 모델의 용량이 클수록 더 정교한 지식을 저장하고 활용할 수 있음을 보여줍니다.

### 증류 과정의 기술적 세부사항

증류 과정에서는 표준적인 지도 미세 조정 방법을 사용합니다. 각 모델은 80만 개의 샘플에 대해 교사 모델의 출력을 모방하도록 훈련됩니다. 손실 함수는 일반적인 언어 모델링 손실을 사용하며, 이는 다음과 같이 정의됩니다.

$$\mathcal{L} = -\sum_{i=1}^{N} \log P(y_i | x_i, y_{<i})$$

여기서 $x_i$는 입력 프롬프트, $y_i$는 목표 토큰, $N$은 시퀀스 길이입니다. 이 손실 함수는 모델이 주어진 맥락에서 다음 토큰을 정확히 예측하도록 학습시킵니다.

훈련 과정에서는 적절한 학습률 스케줄링과 정규화 기법을 사용하여 과적합을 방지하고 안정적인 학습을 보장합니다. 특히 작은 모델의 경우 용량 제한으로 인해 모든 지식을 완벽히 흡수하기 어려울 수 있으므로, 가장 중요한 추론 패턴에 우선순위를 두어 학습합니다.

### 증류의 효율성과 실용성

증류 방법의 가장 큰 장점 중 하나는 효율성입니다. 작은 모델에서 처음부터 강화학습을 수행하는 것은 계산 비용이 매우 높고 시간이 오래 걸리지만, 증류는 상대적으로 적은 자원으로 빠르게 추론 능력을 전달할 수 있습니다. 이는 실제 산업 환경에서 매우 중요한 고려사항입니다.

또한 증류된 모델들은 추론 과정에서 DeepSeek-R1과 유사한 구조화된 사고 과정을 보여줍니다. 이들은 `<think>` 태그 안에서 단계별 추론을 수행하고 `<answer>` 태그에서 최종 답변을 제공하는 패턴을 학습했습니다. 이러한 구조화된 접근법은 모델의 추론 과정을 투명하게 만들어 사용자가 모델의 판단 근거를 이해할 수 있게 해줍니다.

### 증류 모델의 한계와 향후 개선 방향

증류 방법이 효과적이지만 몇 가지 한계점도 존재합니다. 첫째, 작은 모델의 용량 제한으로 인해 교사 모델의 모든 능력을 완벽히 재현하기는 어렵습니다. 둘째, 증류 과정에서 일부 미묘한 추론 전략이나 창의적 사고 능력이 손실될 수 있습니다.

이러한 한계를 극복하기 위한 향후 연구 방향으로는 강화학습을 증류 과정에 통합하는 방법, 더 정교한 지식 증류 기법 개발, 그리고 모델별 특성을 고려한 맞춤형 증류 전략 등이 있습니다. 특히 강화학습을 추가하면 증류된 모델의 성능을 더욱 향상시킬 수 있을 것으로 기대됩니다.

연구진이 강화학습 단계를 포함하지 않은 것은 증류 기법 자체의 효과를 명확히 입증하기 위함이며, 이는 연구 커뮤니티가 이 기반 위에서 더 발전된 방법을 개발할 수 있는 토대를 제공합니다. 실제로 이후 실험 결과에서 보듯이, 단순한 증류만으로도 작은 모델들이 상당한 추론 능력 향상을 보여주어 이 접근법의 유효성을 입증했습니다.
## 실험 벤치마크

DeepSeek-R1의 성능을 종합적으로 평가하기 위해 다양한 벤치마크에서 모델을 평가했습니다. 평가에 사용된 벤치마크는 MMLU, MMLU-Redux, MMLU-Pro, C-Eval, CMMLU, IFEval, FRAMES, GPQA Diamond, SimpleQA, C-SimpleQA, SWE-Bench Verified, Aider, LiveCodeBench, Codeforces, 중국 전국 고등학교 수학 올림피아드(CNMO 2024), 그리고 미국 수학 초청 시험 2024(AIME 2024)입니다.

이러한 벤치마크들은 각각 다른 측면의 능력을 평가합니다. [MMLU](https://arxiv.org/pdf/2009.03300)는 57개 다양한 주제에 걸친 대규모 다과제 언어 이해 능력을 측정하는 포괄적인 벤치마크입니다. 이 벤치마크는 STEM 분야부터 인문학과 사회과학까지 다양한 수준의 난이도를 포함하여 모델의 학술적이고 전문적인 이해도를 평가합니다. [MMLU-Redux](https://arxiv.org/pdf/2406.04127)는 원본 MMLU의 오류를 수정한 5,700개의 수동 재주석 질문으로 구성된 개선된 버전입니다. [MMLU-Pro](https://arxiv.org/pdf/2406.01574)는 선택지를 4개에서 10개로 늘리고 더 도전적인 추론 중심 질문을 포함하여 벤치마크의 난이도와 견고성을 크게 향상시켰습니다.

중국어 평가를 위해서는 [C-Eval](https://arxiv.org/pdf/2305.08322)과 [CMMLU](https://arxiv.org/pdf/2306.09212)를 사용했습니다. C-Eval은 52개 분야에 걸친 13,948개의 객관식 질문으로 구성되어 중국어 맥락에서 파운데이션 모델의 고급 지식과 추론 능력을 평가합니다. CMMLU는 67개 주제에 걸친 11,528개의 질문으로 중국어 언어 이해 능력을 종합적으로 측정합니다.

표준 벤치마크 외에도 LLM을 판정자로 사용하는 개방형 생성 과제에서도 모델을 평가했습니다. 구체적으로 AlpacaEval 2.0과 Arena-Hard의 원래 구성을 따라 GPT-4-Turbo-1106을 쌍별 비교의 판정자로 활용했습니다. 여기서는 길이 편향을 피하기 위해 최종 요약만을 평가에 제공했습니다.

증류된 모델들의 경우 AIME 2024, MATH-500, GPQA Diamond, Codeforces, LiveCodeBench에서 대표적인 결과를 보고합니다.

### 평가 프롬프트

DeepSeek-V3의 설정을 따라 MMLU, DROP, GPQA Diamond, SimpleQA와 같은 표준 벤치마크는 simple-evals 프레임워크의 프롬프트를 사용하여 평가했습니다. MMLU-Redux의 경우 제로샷 설정에서 Zero-Eval 프롬프트 형식을 채택했습니다. MMLU-Pro, C-Eval, CLUE-WSC의 경우 원래 프롬프트가 퓨샷이므로 DeepSeek-R1의 퓨샷 CoT가 성능을 저해할 수 있어 프롬프트를 제로샷 설정으로 약간 수정했습니다. 다른 데이터셋들은 제작자가 제공한 기본 프롬프트와 함께 원래의 평가 프로토콜을 따랐습니다.

코드와 수학 벤치마크의 경우, HumanEval-Mul 데이터셋은 8개의 주요 프로그래밍 언어(Python, Java, C++, C#, JavaScript, TypeScript, PHP, Bash)를 다룹니다. LiveCodeBench에서의 모델 성능은 2024년 8월부터 2025년 1월까지 수집된 데이터를 사용하여 CoT 형식으로 평가했습니다. Codeforces 데이터셋은 전문가가 제작한 테스트 케이스와 함께 10개의 Div.2 대회 문제를 사용하여 평가하며, 이후 예상 등급과 경쟁자 백분율을 계산합니다. SWE-Bench 검증 결과는 agentless 프레임워크를 통해 얻었습니다. AIDER 관련 벤치마크는 "diff" 형식을 사용하여 측정했습니다.

DeepSeek-R1 출력은 각 벤치마크에 대해 최대 32,768 토큰으로 제한했습니다.

### 기준 모델

DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, OpenAI-o1-1217을 포함한 여러 강력한 기준 모델들과 종합적인 평가를 수행했습니다. 중국 본토에서 OpenAI-o1-1217 API에 접근하기 어려워 공식 보고서를 기반으로 성능을 보고합니다. 증류된 모델들의 경우 오픈소스 모델인 QwQ-32B-Preview와도 비교했습니다.

### 평가 설정

모델의 최대 생성 길이를 32,768 토큰으로 설정했습니다. 긴 출력 추론 모델을 탐욕적 디코딩으로 평가하면 반복률이 높아지고 서로 다른 체크포인트 간에 상당한 변동성이 발생한다는 것을 발견했습니다. 따라서 기본적으로 pass@$k$ 평가를 사용하고 0이 아닌 온도를 사용하여 pass@1을 보고합니다.

구체적으로 샘플링 온도 $0.6$과 top-$p$ 값 $0.95$를 사용하여 각 질문에 대해 $k$개의 응답을 생성합니다(일반적으로 테스트 세트 크기에 따라 $4$와 $64$ 사이). 그런 다음 Pass@1은 다음과 같이 계산됩니다.

$$\text{pass@1} = \frac{1}{k}\sum_{i=1}^{k}p_{i}$$

여기서 $p_i$는 $i$번째 응답의 정확성을 나타냅니다. 이 방법은 더 신뢰할 수 있는 성능 추정치를 제공합니다. AIME 2024의 경우 64개 샘플을 사용한 합의(다수결 투표) 결과도 보고하며, 이를 $\text{cons}@64$로 표시합니다.

이러한 평가 방법론은 [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/pdf/2203.11171)에서 제안된 자기 일관성 개념과 연결됩니다. 해당 연구에서는 복잡한 추론 과제가 일반적으로 올바른 답으로 이어지는 여러 추론 경로를 허용한다는 직관을 활용했습니다. 다수결 투표를 통해 여러 추론 경로 중에서 가장 일관성 있는 답을 선택함으로써 단일 샘플의 확률적 변동성을 완화하고 더 안정적인 성능 측정을 가능하게 합니다.

## DeepSeek-R1 평가

| 벤치마크 (지표) | Claude-3.5-Sonnet-1022 | GPT-4o-0513 | DeepSeek-V3 | OpenAI-o1-mini | OpenAI-o1-1217 | DeepSeek-R1 |
|---|---|---|---|---|---|---|
| **아키텍처** | - | - | MoE | - | - | MoE |
| **활성화된 파라미터** | - | - | 37B | - | - | 37B |
| **전체 파라미터** | - | - | 671B | - | - | 671B |
| **영어** | | | | | | |
| MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | 91.8 | 90.8 |
| MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | 92.9 |
| MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | 84.0 |
| DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | 92.2 |
| IF-Eval (Prompt Strict) | 86.5 | 84.3 | 86.1 | 84.8 | - | 83.3 |
| GPQA Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | 75.7 | 71.5 |
| SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | 47.0 | 30.1 |
| FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | 82.5 |
| AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | 87.6 |
| ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | 92.3 |
| **코드** | | | | | | |
| LiveCodeBench (Pass@1-COT) | 38.9 | 32.9 | 36.2 | 53.8 | 63.4 | 65.9 |
| Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | 96.6 | 96.3 |
| Codeforces (Rating) | 717 | 759 | 1134 | 1820 | 2061 | 2029 |
| SWE Verified (Resolved) | 50.8 | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |
| Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | 61.7 | 53.3 |
| **수학** | | | | | | |
| AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | 79.8 |
| MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | 97.3 |
| CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | 78.8 |
| **중국어** | | | | | | |
| CLUE-WSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | 92.8 |
| C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | 91.8 |
| C-SimpleQA (Correct) | 55.4 | 58.7 | 68.0 | 40.3 | - | 63.7 |

MMLU, MMLU-Pro, GPQA Diamond과 같은 교육 지향적 지식 벤치마크에서 DeepSeek-R1은 DeepSeek-V3 대비 우수한 성능을 보여줍니다. 이러한 개선은 주로 STEM 관련 질문에서의 정확도 향상에 기인하며, 대규모 강화학습을 통해 상당한 성과를 달성했습니다. 또한 DeepSeek-R1은 긴 맥락 의존적 QA 과제인 [FRAMES](https://arxiv.org/pdf/2409.12941)에서 뛰어난 성능을 보여 강력한 문서 분석 능력을 입증했습니다. 이는 AI 기반 검색 및 데이터 분석 과제에서 추론 모델의 잠재력을 강조합니다.

사실적 벤치마크인 SimpleQA에서 DeepSeek-R1은 DeepSeek-V3를 능가하여 사실 기반 질의 처리 능력을 보여줍니다. OpenAI-o1이 GPT-4o를 능가하는 유사한 경향이 관찰됩니다. 하지만 DeepSeek-R1은 중국어 SimpleQA 벤치마크에서 DeepSeek-V3보다 낮은 성능을 보이는데, 이는 주로 안전 RL 이후 특정 질의에 대한 답변 거부 경향 때문입니다. 안전 RL 없이는 DeepSeek-R1이 70% 이상의 정확도를 달성할 수 있었습니다.

DeepSeek-R1은 모델의 형식 지시 준수 능력을 평가하도록 설계된 벤치마크인 [IF-Eval](https://arxiv.org/pdf/2311.07911)에서도 인상적인 결과를 제공합니다. 이러한 개선은 지도 미세 조정(SFT)과 RL 훈련의 최종 단계에서 지시 준수 데이터를 포함한 것과 연결될 수 있습니다.

더 나아가 AlpacaEval2.0과 ArenaHard에서 주목할 만한 성능이 관찰되어 DeepSeek-R1의 글쓰기 과제와 개방형 질의응답에서의 강점을 나타냅니다. DeepSeek-V3를 크게 능가하는 성능은 추론 능력뿐만 아니라 다양한 도메인에서 성능을 향상시키는 대규모 RL의 일반화 이점을 강조합니다.

또한 DeepSeek-R1이 생성하는 요약 길이는 간결하여 ArenaHard에서 평균 689 토큰, AlpacaEval 2.0에서 평균 2,218자를 기록했습니다. 이는 DeepSeek-R1이 GPT 기반 평가에서 길이 편향을 도입하지 않음을 나타내며, 여러 과제에 걸친 견고성을 더욱 확고히 합니다.

수학 과제에서 DeepSeek-R1은 OpenAI-o1-1217과 동등한 성능을 보여주며 다른 모델들을 큰 차이로 능가합니다. LiveCodeBench와 Codeforces와 같은 코딩 알고리즘 과제에서도 유사한 경향이 관찰되며, 추론 중심 모델들이 이러한 벤치마크를 지배하고 있습니다.

엔지니어링 지향적 코딩 과제에서는 OpenAI-o1-1217이 Aider에서 DeepSeek-R1을 능가하지만 SWE Verified에서는 비교할 만한 성능을 달성합니다. 현재 관련 RL 훈련 데이터의 양이 매우 제한적이므로 DeepSeek-R1의 엔지니어링 성능은 다음 버전에서 개선될 것으로 믿습니다.

## 증류된 모델 평가

| 모델 | AIME 2024 | MATH-500 | GPQA Diamond | LiveCodeBench | CodeForces |
|---|---|---|---|---|---|
| | pass@1 | cons@64 | pass@1 | pass@1 | pass@1 | rating |
| GPT-4o-0513 | 9.3 | 13.4 | 74.6 | 49.9 | 32.9 | 759 |
| Claude-3.5-Sonnet-1022 | 16.0 | 26.7 | 78.3 | 65.0 | 38.9 | 717 |
| OpenAI-o1-mini | 63.6 | 80.0 | 90.0 | 60.0 | 53.8 | 1820 |
| QwQ-32B-Preview | 50.0 | 60.0 | 90.6 | 54.5 | 41.9 | 1316 |
| DeepSeek-R1-Distill-Qwen-1.5B | 28.9 | 52.7 | 83.9 | 33.8 | 16.9 | 954 |
| DeepSeek-R1-Distill-Qwen-7B | 55.5 | 83.3 | 92.8 | 49.1 | 37.6 | 1189 |
| DeepSeek-R1-Distill-Qwen-14B | 69.7 | 80.0 | 93.9 | 59.1 | 53.1 | 1481 |
| DeepSeek-R1-Distill-Qwen-32B | 72.6 | 83.3 | 94.3 | 62.1 | 57.2 | 1691 |
| DeepSeek-R1-Distill-Llama-8B | 50.4 | 80.0 | 89.1 | 49.0 | 39.6 | 1205 |
| DeepSeek-R1-Distill-Llama-70B | 70.0 | 86.7 | 94.5 | 65.2 | 57.5 | 1633 |

위 표에서 보듯이 단순히 DeepSeek-R1의 출력을 증류하는 것만으로도 효율적인 DeepSeek-R1-7B(즉, DeepSeek-R1-Distill-Qwen-7B)가 GPT-4o-0513과 같은 비추론 모델들을 전반적으로 능가할 수 있게 됩니다. DeepSeek-R1-14B는 모든 평가 지표에서 QwQ-32B-Preview를 능가하며, DeepSeek-R1-32B와 DeepSeek-R1-70B는 대부분의 벤치마크에서 o1-mini를 크게 초과합니다.

이러한 결과는 증류의 강력한 잠재력을 보여줍니다. 특히 주목할 점은 DeepSeek-R1-Distill-Qwen-7B가 AIME 2024에서 55.5%를 달성하여 4배 더 큰 QwQ-32B-Preview(50.0%)를 능가한다는 것입니다. 이는 효과적인 증류 기법이 모델 크기의 한계를 극복할 수 있음을 보여줍니다.

또한 이러한 증류된 모델들에 RL을 적용하면 상당한 추가 성과를 얻을 수 있다는 것을 발견했습니다. 이는 더 많은 탐구가 필요한 영역이라고 믿으며, 따라서 여기서는 단순한 SFT 증류 모델의 결과만을 제시합니다.

증류 과정의 효과는 여러 측면에서 분석할 수 있습니다. 첫째, 모델 크기가 클수록 더 나은 증류 결과를 보여주는 경향이 있습니다. 1.5B 모델도 상당한 성능 향상을 보이지만, 32B와 70B 모델은 훨씬 더 인상적인 결과를 달성합니다. 둘째, Qwen과 Llama 기반 모델 모두에서 일관된 개선이 관찰되어 증류 방법의 일반성을 입증합니다.

특히 수학적 추론 능력에서 증류된 모델들의 성능이 두드러집니다. MATH-500에서 DeepSeek-R1-Distill-Qwen-32B는 94.3%를 달성하여 원래 GPT-4o-0513의 74.6%를 크게 능가합니다. 이는 DeepSeek-R1의 추론 패턴이 작은 모델에 효과적으로 전달되었음을 보여줍니다.

코딩 능력에서도 유사한 패턴이 관찰됩니다. Codeforces에서 DeepSeek-R1-Distill-Qwen-32B는 1691 등급을 달성하여 o1-mini의 1820에 근접한 성능을 보입니다. 이는 증류를 통해 복잡한 알고리즘적 추론 능력도 효과적으로 전달될 수 있음을 시사합니다.
## 토론

### 증류 대 강화학습

DeepSeek-R1의 개발 과정에서 중요한 질문이 제기되었습니다. 3.2절에서 확인할 수 있듯이 DeepSeek-R1을 증류하여 작은 모델이 인상적인 결과를 달성할 수 있었지만, 여전히 한 가지 의문이 남아있었습니다. 증류 없이 논문에서 논의된 대규모 강화학습 훈련을 통해 모델이 비교할 만한 성능을 달성할 수 있을까요?

이 질문에 답하기 위해 수학, 코드, STEM 데이터를 사용하여 Qwen-32B-Base에 대규모 강화학습 훈련을 수행했습니다. 10,000단계 이상 훈련한 결과 DeepSeek-R1-Zero-Qwen-32B가 탄생했습니다.

| Model | AIME 2024 | MATH-500 | GPQA Diamond | LiveCodeBench | pass@1 |
|-------|-----------|----------|--------------|---------------|---------|
| | pass@1 | cons@64 | pass@1 | pass@1 | |
| QwQ-32B-Preview | 50.0 | 60.0 | 90.6 | 54.5 | 41.9 |
| DeepSeek-R1-Zero-Qwen-32B | 47.0 | 60.0 | 91.6 | 55.0 | 40.2 |
| DeepSeek-R1-Distill-Qwen-32B | 72.6 | 83.3 | 94.3 | 62.1 | 57.2 |

실험 결과는 32B 기본 모델이 대규모 강화학습 훈련 후 QwQ-32B-Preview와 동등한 성능을 달성함을 보여줍니다. 하지만 DeepSeek-R1에서 증류된 DeepSeek-R1-Distill-Qwen-32B는 모든 벤치마크에서 DeepSeek-R1-Zero-Qwen-32B를 크게 능가했습니다.

이러한 결과로부터 두 가지 중요한 결론을 도출할 수 있습니다. 첫째, 더 강력한 모델을 작은 모델로 증류하는 것은 뛰어난 결과를 가져다주는 반면, 이 논문에서 언급된 대규모 강화학습에 의존하는 작은 모델들은 엄청난 계산 능력을 필요로 하며 증류의 성능에도 미치지 못할 수 있습니다. 둘째, 증류 전략이 경제적이고 효과적이지만, 지능의 경계를 넘어서는 발전을 위해서는 여전히 더 강력한 기본 모델과 더 대규모의 강화학습이 필요할 수 있습니다.

이러한 발견은 [Scaling Laws for Reward Model Overoptimization](https://arxiv.org/pdf/2210.10760)에서 제시된 보상 모델 최적화의 한계와 연결됩니다. 해당 연구에서는 프록시 보상 모델에 대한 과도한 최적화가 실제 성능 향상으로 이어지지 않을 수 있음을 보여주었습니다. 마찬가지로 작은 모델에서 대규모 강화학습을 수행하는 것은 계산 자원 대비 효율성이 떨어질 수 있으며, 더 큰 모델에서 학습된 패턴을 증류하는 것이 더 효과적인 접근법임을 시사합니다.

증류의 효과성은 여러 측면에서 분석할 수 있습니다. 증류 과정에서 DeepSeek-R1이 학습한 복잡한 추론 패턴들이 작은 모델로 효과적으로 전달됩니다. 이는 마치 숙련된 전문가가 초보자에게 문제 해결 방법을 체계적으로 가르치는 것과 유사합니다. 반면 작은 모델에서 처음부터 강화학습을 수행하는 것은 제한된 용량으로 인해 복잡한 추론 전략을 자율적으로 개발하기 어려울 수 있습니다.

### 실패한 시도들

DeepSeek-R1 개발 초기 단계에서 실패와 좌절도 경험했습니다. 이러한 실패 경험을 공유하여 통찰을 제공하고자 하지만, 이것이 이러한 접근법들이 효과적인 추론 모델을 개발할 수 없다는 것을 의미하지는 않습니다.

#### 프로세스 보상 모델 (PRM)

프로세스 보상 모델은 추론 과제를 해결하기 위한 더 나은 접근법으로 모델을 안내하는 합리적인 방법입니다. [Let's Verify Step by Step](https://arxiv.org/pdf/2305.20050)에서 제시된 바와 같이, PRM은 결과 기반 감독보다 훨씬 더 신뢰할 수 있는 보상 모델을 훈련할 수 있음을 보여주었습니다. 해당 연구에서는 각 추론 단계의 정확성을 예측하는 과정 감독이 최종 결과만을 평가하는 결과 감독보다 우수함을 입증했습니다.

하지만 실제로 PRM은 궁극적인 성공을 방해할 수 있는 세 가지 주요 한계점을 가지고 있습니다. 첫째, 일반적인 추론에서 세밀한 단계를 명시적으로 정의하는 것이 어렵습니다. 수학 문제와 달리 일반적인 추론 과제에서는 명확한 단계 구분이 모호할 수 있습니다. 둘째, 현재 중간 단계가 올바른지 판단하는 것은 도전적인 과제입니다. 모델을 사용한 자동 주석은 만족스러운 결과를 얻지 못할 수 있으며, 수동 주석은 확장에 도움이 되지 않습니다. 셋째, 모델 기반 PRM이 도입되면 필연적으로 보상 해킹으로 이어지며, 보상 모델을 재훈련하는 데 추가적인 훈련 자원이 필요하고 전체 훈련 파이프라인을 복잡하게 만듭니다.

[Solving math word problems with process- and outcome-based feedback](https://arxiv.org/pdf/2211.14275)에서는 과정 기반 피드백과 결과 기반 피드백의 트레이드오프를 분석했습니다. 해당 연구에서는 결과 기반 감독도 과정 기반 감독과 유사한 최종 답변 오류율을 달성할 수 있지만, 더 적은 라벨 감독이 필요함을 보여주었습니다. 이는 보상 모델이 과정 기반 피드백을 근사하는 방법을 학습할 수 있기 때문입니다.

결론적으로 PRM은 모델이 생성한 상위 N개 응답을 재순위화하거나 안내된 탐색을 지원하는 데 좋은 능력을 보여주지만, 실험에서 대규모 강화학습 과정 중에 도입하는 추가적인 계산 오버헤드에 비해 그 장점은 제한적이었습니다.

#### 몬테카를로 트리 탐색 (MCTS)

[AlphaGo](https://arxiv.org/pdf/1712.01815)와 [AlphaZero](https://arxiv.org/pdf/1712.01815)에서 영감을 받아 테스트 시간 계산 확장성을 향상시키기 위해 몬테카를로 트리 탐색을 사용하는 방법을 탐구했습니다. AlphaZero는 게임 규칙만으로 학습하여 체스, 쇼기, 바둑에서 초인적 성능을 달성했으며, 일반적인 강화학습의 힘을 보여주었습니다.

이 접근법은 답변을 더 작은 부분으로 나누어 모델이 해결책 공간을 체계적으로 탐색할 수 있도록 하는 것입니다. 이를 위해 탐색에 필요한 특정 추론 단계에 해당하는 여러 태그를 생성하도록 모델에 프롬프트를 제공했습니다. 훈련을 위해 먼저 수집된 프롬프트를 사용하여 사전 훈련된 가치 모델의 안내를 받는 MCTS를 통해 답을 찾습니다. 이후 결과로 나온 질문-답변 쌍을 사용하여 액터 모델과 가치 모델을 모두 훈련하고, 이 과정을 반복적으로 정제합니다.

하지만 이 접근법은 훈련을 확장할 때 여러 도전에 직면합니다. 첫째, 체스와 달리 탐색 공간이 상대적으로 잘 정의된 것과 달리, 토큰 생성은 기하급수적으로 더 큰 탐색 공간을 제시합니다. 이를 해결하기 위해 각 노드에 대한 최대 확장 제한을 설정했지만, 이는 모델이 지역 최적점에 갇히게 할 수 있습니다. 둘째, 가치 모델이 탐색 과정의 각 단계를 안내하므로 생성 품질에 직접적인 영향을 미칩니다. 세밀한 가치 모델을 훈련하는 것은 본질적으로 어려우며, 이는 모델이 반복적으로 개선하기 어렵게 만듭니다.

AlphaGo의 핵심 성공은 성능을 점진적으로 향상시키기 위해 가치 모델을 훈련하는 데 의존했지만, 토큰 생성의 복잡성으로 인해 이 원리를 우리의 설정에서 복제하기는 어려웠습니다. [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/pdf/2408.03314)에서도 유사한 도전을 확인할 수 있습니다. 해당 연구에서는 검증자 기반 탐색과 수정 기반 제안 분포의 효과가 프롬프트의 난이도에 크게 의존함을 보여주었습니다.

결론적으로 MCTS는 사전 훈련된 가치 모델과 함께 사용할 때 추론 중 성능을 향상시킬 수 있지만, 자기 탐색을 통해 모델 성능을 반복적으로 향상시키는 것은 여전히 상당한 도전으로 남아있습니다. 이는 토큰 생성 과제의 복잡성과 세밀한 가치 모델 훈련의 어려움 때문입니다.

이러한 실패 경험들은 추론 모델 개발에서 직면하는 근본적인 도전들을 보여줍니다. PRM과 MCTS 모두 이론적으로는 매력적인 접근법이지만, 실제 구현에서는 확장성, 계산 효율성, 그리고 훈련 복잡성 측면에서 상당한 한계를 보였습니다. 이러한 경험은 DeepSeek-R1의 최종 설계에서 더 단순하면서도 효과적인 접근법을 채택하게 된 배경이 되었습니다.
## 결론, 한계점, 그리고 향후 연구 방향

이 연구에서는 강화학습을 통해 모델의 추론 능력을 향상시키는 여정을 공유했습니다. DeepSeek-R1-Zero는 콜드 스타트 데이터에 의존하지 않는 순수한 강화학습 접근법을 나타내며, 다양한 과제에서 강력한 성능을 달성했습니다. DeepSeek-R1은 더욱 강력한 모델로, 콜드 스타트 데이터와 반복적인 강화학습 미세 조정을 함께 활용합니다. 궁극적으로 DeepSeek-R1은 다양한 과제에서 OpenAI-o1-1217과 비교할 만한 성능을 달성했습니다.

### 증류를 통한 소형 모델의 추론 능력 향상

연구진은 추론 능력을 작은 밀집 모델로 증류하는 방법을 더욱 탐구했습니다. DeepSeek-R1을 교사 모델로 사용하여 80만 개의 훈련 샘플을 생성하고, 여러 작은 밀집 모델을 미세 조정했습니다. 결과는 매우 고무적입니다. DeepSeek-R1-Distill-Qwen-1.5B는 수학 벤치마크에서 GPT-4o와 Claude-3.5-Sonnet을 능가하여 AIME에서 28.9%, MATH에서 83.9%를 달성했습니다.

이러한 증류 결과는 매우 인상적입니다. 1.5B라는 상대적으로 작은 모델이 훨씬 큰 상용 모델들을 능가한다는 것은 효과적인 지식 증류의 힘을 보여줍니다. 이는 마치 숙련된 전문가가 초보자에게 핵심 기술을 전수하여 단시간에 높은 수준의 능력을 갖추게 하는 것과 유사합니다. 다른 밀집 모델들도 인상적인 결과를 달성하여 동일한 기본 체크포인트를 기반으로 한 다른 지시 조정 모델들을 크게 능가했습니다.

### 향후 연구 방향

미래에는 DeepSeek-R1을 위한 다음 방향들에서 연구에 투자할 계획입니다.

**일반 능력 향상**: 현재 DeepSeek-R1의 능력은 함수 호출, 멀티턴 대화, 복잡한 역할 연기, JSON 출력과 같은 과제에서 DeepSeek-V3에 미치지 못합니다. 앞으로는 긴 체인 오브 소트가 이러한 분야의 과제를 향상시키는 데 어떻게 활용될 수 있는지 탐구할 계획입니다. 이는 추론 능력과 일반적인 언어 모델 능력 사이의 균형을 맞추는 중요한 도전입니다.

**언어 혼용 문제 해결**: DeepSeek-R1은 현재 중국어와 영어에 최적화되어 있어, 다른 언어로 된 질의를 처리할 때 언어 혼용 문제가 발생할 수 있습니다. 예를 들어, DeepSeek-R1은 질의가 영어나 중국어가 아닌 다른 언어로 되어 있어도 추론과 응답에 영어를 사용할 수 있습니다. 향후 업데이트에서 이러한 한계를 해결하는 것을 목표로 합니다. 이는 글로벌 사용자들에게 더 나은 경험을 제공하기 위해 필수적인 개선사항입니다.

**프롬프트 엔지니어링 최적화**: DeepSeek-R1을 평가할 때 프롬프트에 민감하다는 것을 관찰했습니다. 퓨 샷 프롬프팅은 일관되게 성능을 저하시킵니다. 따라서 사용자들이 최적의 결과를 위해 제로샷 설정에서 문제를 직접 설명하고 출력 형식을 지정하는 것을 권장합니다. 이러한 민감성은 모델이 특정한 추론 패턴에 최적화되어 있기 때문으로 보이며, 향후 더 견고한 프롬프트 처리 능력을 개발할 필요가 있습니다.

**소프트웨어 엔지니어링 과제 개선**: 긴 평가 시간으로 인해 강화학습 과정의 효율성에 영향을 미치기 때문에, 소프트웨어 엔지니어링 과제에서는 대규모 강화학습이 광범위하게 적용되지 않았습니다. 그 결과 DeepSeek-R1은 소프트웨어 엔지니어링 벤치마크에서 DeepSeek-V3 대비 큰 개선을 보여주지 못했습니다. 향후 버전에서는 소프트웨어 엔지니어링 데이터에 대한 거부 샘플링을 구현하거나 강화학습 과정 중 비동기 평가를 통합하여 효율성을 개선함으로써 이를 해결할 것입니다.

### 연구의 의의와 기여

이 연구는 여러 측면에서 중요한 의의를 가집니다. 첫째, 순수한 강화학습만으로도 언어 모델의 추론 능력을 크게 향상시킬 수 있음을 입증했습니다. 이는 지도 학습 데이터에 대한 의존도를 줄이고, 모델이 자율적으로 추론 전략을 개발할 수 있는 가능성을 보여줍니다.

둘째, 효과적인 증류 방법을 통해 작은 모델도 강력한 추론 능력을 가질 수 있음을 보여주었습니다. 이는 계산 자원이 제한된 환경에서도 고성능 추론 모델을 활용할 수 있는 길을 열어줍니다. 특히 1.5B 모델이 상용 대형 모델을 능가하는 결과는 모델 효율성 측면에서 중요한 돌파구입니다.

셋째, 강화학습 과정에서 모델이 자발적으로 발현시키는 정교한 추론 행동들은 인공지능의 자기 진화 가능성을 시사합니다. "아하 모멘트"와 같은 현상은 모델이 단순히 패턴을 학습하는 것을 넘어서 진정한 추론 능력을 개발할 수 있음을 보여줍니다.

### 한계점과 도전 과제

하지만 여전히 해결해야 할 한계점들이 존재합니다. 언어 혼용 문제는 다국어 환경에서의 사용성을 제한하며, 프롬프트 민감성은 실제 사용에서 일관성 있는 성능을 보장하기 어렵게 만듭니다. 또한 소프트웨어 엔지니어링과 같은 특정 도메인에서는 아직 충분한 개선을 보여주지 못했습니다.

이러한 한계점들은 향후 연구의 방향을 제시합니다. 더 견고하고 일반화된 추론 능력을 개발하고, 다양한 언어와 도메인에서 일관된 성능을 보장하는 것이 중요한 과제입니다. 또한 계산 효율성과 성능 사이의 균형을 더욱 개선하여 실용적인 응용에서의 활용도를 높이는 것도 필요합니다.

### 미래 전망

DeepSeek-R1의 성공은 강화학습 기반 추론 모델 개발의 새로운 장을 열었습니다. 향후 연구에서는 이러한 기반 위에서 더욱 발전된 추론 능력을 가진 모델들이 개발될 것으로 기대됩니다. 특히 증류 기법의 발전과 함께 더 작고 효율적이면서도 강력한 추론 능력을 가진 모델들이 등장할 가능성이 높습니다.

또한 다양한 도메인과 언어에서 일관된 성능을 보이는 범용 추론 모델의 개발도 중요한 연구 방향이 될 것입니다. 이는 인공지능이 인간 수준의 추론 능력에 더욱 가까워지는 중요한 단계가 될 것입니다.
이 섹션은 DeepSeek-R1 연구에 참여한 핵심 기여자들과 협력자들의 목록을 포함한 부록으로, 연구 개발 과정에서 각자의 역할에 따라 체계적으로 정리되어 있습니다. 하지만 이러한 기여자 목록과 감사 인사는 기술적 내용이 아닌 행정적 정보에 해당하므로, 본 기술 리뷰에서는 다루지 않습니다.

대신 이 연구의 기술적 성과와 의의를 종합적으로 정리하면, DeepSeek-R1은 강화학습을 통한 추론 능력 향상이라는 중요한 연구 방향에서 상당한 돌파구를 마련했습니다. 특히 순수한 강화학습만으로도 언어 모델이 자율적으로 정교한 추론 전략을 개발할 수 있음을 입증한 DeepSeek-R1-Zero의 성과는 인공지능 연구 분야에 새로운 가능성을 제시했습니다.

또한 효과적인 증류 기법을 통해 작은 모델들도 강력한 추론 능력을 갖출 수 있음을 보여준 것은 실용적 관점에서 매우 중요한 기여입니다. 1.5B 파라미터의 소형 모델이 훨씬 큰 상용 모델들을 능가하는 성능을 보인 것은 계산 효율성과 접근성 측면에서 혁신적인 발전이라 할 수 있습니다.

이러한 연구 성과는 다양한 분야의 전문가들이 협력하여 이룬 결과이며, 강화학습, 자연어처리, 수학적 추론, 소프트웨어 엔지니어링 등 여러 기술 영역의 융합을 통해 달성되었습니다. 앞으로 이 연구를 기반으로 더욱 발전된 추론 모델들이 개발될 것으로 기대되며, 특히 다국어 지원과 다양한 도메인에서의 일관된 성능 향상이 중요한 연구 과제가 될 것입니다.
- - -
### References
* [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/pdf/2501.12948v1)