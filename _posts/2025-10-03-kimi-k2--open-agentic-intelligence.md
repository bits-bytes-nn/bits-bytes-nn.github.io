---
layout: post
title: "Kimi K2: Open Agentic Intelligence"
date: 2025-07-28 05:35:43
author: "Moonshot AI"
categories: "Language Models"
tags: ["Mixture-of-Experts-Sparsity-Scaling-Law", "MuonClip-Optimizer", "QK-Clip-Attention-Stabilization", "Large-Scale-Agentic-Data-Synthesis", "Multi-Stage-Reinforcement-Learning-with-Self-Critique", "Verifiable-Rewards-Reinforcement-Learning", "Synthetic-Data-Rephrasing-for-Token-Efficiency", "Agentic-Intelligence-Framework", "Tool-Use-Emergence", "Unified-Paradigm-for-Reinforcement-Learning"]
cover: /assets/images/language-models.jpg
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

대규모 언어 모델(LLM)의 발전은 인공지능 분야에서 혁명적인 변화를 예고하고 있습니다. 특히 에이전틱 인텔리전스(Agentic Intelligence)라는 새로운 패러다임은 AI 시스템이 단순히 정적인 데이터를 모방하는 것을 넘어 복잡하고 동적인 환경에서 자율적으로 인지하고, 계획하고, 추론하며 행동할 수 있는 능력을 의미합니다. 기존 언어 모델들은 주로 정적인 데이터 학습에 집중했지만, 실제 세계의 복잡한 문제를 해결하기 위해서는 상호작용을 통해 학습하고 경험을 통해 적응할 수 있는 능력이 필수적입니다.

연구팀은 현재 대규모 언어 모델의 주요 한계를 인식했습니다. 고품질 인간 생성 데이터의 가용성이 점점 제한되고 있으며, 모델의 에이전틱 능력을 향상시키기 위해서는 토큰 효율성과 다양한 환경에서의 적응력을 극대화해야 합니다. 특히 도구 사용, 소프트웨어 개발, 실세계 자율성과 같은 영역에서 AI 에이전트의 능력을 확장하는 것이 이 연구의 핵심 동기였습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

Kimi K2는 에이전틱 인텔리전스를 위해 특별히 설계된 1조 파라미터 규모의 Mixture-of-Experts(MoE) 언어 모델입니다. 연구팀은 두 가지 핵심 혁신을 통해 모델의 성능을 획기적으로 향상시켰습니다. 첫째, MuonClip이라는 새로운 옵티마이저를 개발하여 훈련 불안정성 문제를 해결했습니다. 이 옵티마이저는 QK-Clip 기법을 통해 어텐션 로짓의 폭발을 제어하면서도 토큰 효율성을 유지합니다.

둘째, 대규모 에이전틱 데이터 합성 파이프라인을 도입하여 모델의 도구 사용 능력을 체계적으로 향상시켰습니다. 이 파이프라인은 시뮬레이션 및 실세계 환경을 통합하여 다양한 도구, 에이전트, 작업, 궤적을 생성합니다. 또한 검증 가능한 보상(RLVR)과 자기 비판 루브릭 보상 메커니즘을 결합한 강화학습 프레임워크를 설계하여 모델이 외부적으로 정의된 작업뿐만 아니라 자체 출력을 평가하면서 학습할 수 있도록 했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?

Kimi K2의 구현은 사전 훈련과 후훈련의 두 단계로 나뉩니다. 사전 훈련에서는 15.5조 개의 고품질 토큰을 사용하여 MuonClip 옵티마이저로 모델을 훈련했습니다. MuonClip은 QK-Clip 메커니즘을 통해 어텐션 로짓의 폭발을 제어하고, 데이터 재구성 전략을 통해 토큰 유틸리티를 개선합니다. 모델 아키텍처는 DeepSeek-V3와 유사한 Multi-head Latent Attention(MLA)을 가진 초희소 MoE 구조를 채택했으며, 384개의 전문가와 64개의 어텐션 헤드로 구성됩니다.

후훈련 단계에서는 지도 학습 파인튜닝과 강화학습을 결합했습니다. 대규모 에이전틱 데이터 합성 파이프라인을 통해 도구 사용 시나리오를 생성하고, 검증 가능한 보상 시스템과 자기 비판 루브릭을 활용하여 모델의 능력을 향상시켰습니다. 강화학습 인프라는 코로케이션 아키텍처, 효율적인 엔진 전환 메커니즘, 부분 롤아웃 기법 등을 포함하여 대규모 훈련을 지원합니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?

Kimi K2는 다양한 벤치마크에서 뛰어난 성능을 입증했습니다. Tau2-Bench에서 66.1점, ACEBench(En)에서 76.5점, SWE-Bench Verified에서 65.8점을 달성하여 대부분의 오픈소스 및 클로즈드소스 기준선을 능가했습니다. 코딩, 수학, 추론 작업에서도 LiveCodeBench v6에서 53.7점, AIME 2025에서 49.5점, GPQA-Diamond에서 75.1점을 기록했습니다.

이 연구는 에이전틱 인텔리전스 분야에 중요한 기여를 했습니다. 토큰 효율적인 훈련 방법, 대규모 데이터 합성 파이프라인, 그리고 강화학습을 통한 모델 능력 향상은 향후 AI 연구의 새로운 방향을 제시합니다. 특히 도구 사용, 소프트웨어 개발, 복잡한 추론 작업에서 AI 에이전트의 능력을 크게 확장했다는 점에서 의의가 있습니다. 연구팀은 기본 및 후훈련된 체크포인트를 오픈소스로 공개하여 커뮤니티가 에이전틱 인텔리전스 연구를 더욱 발전시킬 수 있도록 지원했습니다.
- - -
# Kimi K2: 오픈 에이전틱 인텔리전스

## 초록

Kimi K2는 320억 개의 활성화된 파라미터와 총 1조 개의 파라미터를 가진 Mixture-of-Experts(MoE) 대규모 언어 모델입니다. 이 연구에서는 기존 Muon 옵티마이저를 개선한 MuonClip 옵티마이저를 제안하며, 이는 새로운 QK-clip 기법을 통해 훈련 불안정성 문제를 해결하면서도 Muon의 뛰어난 토큰 효율성을 유지합니다.

MuonClip을 기반으로 K2는 15.5조 개의 토큰으로 사전 훈련되었으며, 단 한 번의 손실 스파이크도 발생하지 않았습니다. 후훈련 과정에서 K2는 대규모 에이전틱 데이터 합성 파이프라인과 결합된 강화학습 단계를 포함한 다단계 후훈련 프로세스를 거쳤습니다. 이 과정에서 모델은 실제 및 합성 환경과의 상호작용을 통해 능력을 향상시켰습니다.

Kimi K2는 오픈소스 비사고형 모델 중에서 최첨단 성능을 달성했으며, 특히 에이전틱 능력에서 강점을 보입니다. 주목할 만한 성과로는 Tau2-Bench에서 66.1점, ACEBench(En)에서 76.5점, SWE-Bench Verified에서 65.8점, SWE-Bench Multilingual에서 47.3점을 기록하여 비사고형 설정에서 대부분의 오픈소스 및 클로즈드소스 기준선을 능가했습니다.

![Kimi K2 주요 결과](https://arxiv.org/html/2507.20534/x2.png)

또한 K2는 코딩, 수학, 추론 작업에서도 강력한 능력을 보여주며, LiveCodeBench v6에서 53.7점, AIME 2025에서 49.5점, GPQA-Diamond에서 75.1점, OJBench에서 27.1점을 기록했습니다. 이 모든 결과는 확장된 사고 과정 없이 달성된 것입니다. 이러한 결과들은 Kimi K2를 현재까지 가장 유능한 오픈소스 대규모 언어 모델 중 하나로 위치시키며, 특히 소프트웨어 엔지니어링과 에이전틱 작업에서 뛰어난 성능을 보입니다.

## 서론

대규모 언어 모델(LLM)의 발전은 에이전틱 인텔리전스(Agentic Intelligence)라는 심오한 패러다임 전환을 겪고 있습니다. 에이전틱 인텔리전스는 모델이 복잡하고 동적인 환경에서 자율적으로 인지하고, 계획하고, 추론하고, 행동할 수 있는 능력을 의미합니다. 이러한 전환은 정적인 모방 학습에서 벗어나 상호작용을 통해 적극적으로 학습하고, 훈련 분포를 넘어서는 새로운 기술을 습득하며, 경험을 통해 행동을 적응시키는 모델로의 변화를 나타냅니다.

이러한 접근법은 AI 에이전트가 정적인 인간 생성 데이터의 한계를 넘어서서 자체적인 탐색과 활용을 통해 초인간적 능력을 획득할 수 있게 한다고 여겨집니다. 따라서 에이전틱 인텔리전스는 도구 사용, 소프트웨어 개발, 실세계 자율성 등 광범위한 영역에 걸쳐 차세대 파운데이션 모델의 핵심 능력으로 빠르게 부상하고 있습니다.

에이전틱 인텔리전스를 달성하는 것은 사전 훈련과 후훈련 모두에서 도전과제를 제시합니다. 사전 훈련은 제한된 고품질 데이터라는 제약 하에서 모델에 광범위한 범용 사전 지식을 부여해야 하며, 이는 토큰 효율성(토큰당 학습 신호)을 중요한 스케일링 계수로 만듭니다. 후훈련은 이러한 사전 지식을 실행 가능한 행동으로 변환해야 하지만, 다단계 추론, 장기 계획, 도구 사용과 같은 에이전틱 능력은 자연 데이터에서 드물고 확장하기에 비용이 많이 듭니다.

구조화되고 고품질인 에이전틱 궤적의 확장 가능한 합성과 선호도 및 자기 비판을 통합하는 일반적인 강화학습 기법이 이러한 격차를 해소하는 데 필수적입니다.

이 연구에서는 에이전틱 능력의 핵심 도전과제를 해결하고 그 경계를 확장하기 위해 의도적으로 설계된 1.04조 파라미터의 Mixture-of-Experts(MoE) LLM인 Kimi K2를 소개합니다. 우리의 기여는 사전 훈련과 후훈련 최전선 모두에 걸쳐 있습니다.

첫째, 토큰 효율적인 Muon 알고리즘과 QK-Clip이라는 안정성 향상 메커니즘을 통합한 새로운 옵티마이저인 MuonClip을 제시합니다. MuonClip을 사용하여 15.5조 토큰에 대해 단 한 번의 손실 스파이크도 없이 Kimi K2를 성공적으로 사전 훈련시켰습니다.

둘째, 시뮬레이션 및 실세계 환경을 통해 도구 사용 시연을 체계적으로 생성하는 대규모 에이전틱 데이터 합성 파이프라인을 도입합니다. 이 시스템은 다양한 도구, 에이전트, 작업, 궤적을 구성하여 대규모로 고충실도의 검증 가능하게 올바른 에이전틱 상호작용을 생성합니다.

셋째, 검증 가능한 보상(RLVR)과 자기 비판 루브릭 보상 메커니즘을 결합한 일반적인 강화학습 프레임워크를 설계합니다. 모델은 외부적으로 정의된 작업뿐만 아니라 자신의 출력을 평가하는 것으로부터도 학습하여, 정적 영역에서 개방형 영역으로 정렬을 확장합니다.

Kimi K2는 광범위한 에이전틱 및 최첨단 벤치마크에서 강력한 성능을 보여줍니다. Tau2-bench에서 66.1점, ACEBench(en)에서 76.5점, SWE-bench Verified에서 65.8점, SWE-bench Multilingual에서 47.3점을 달성하여 비사고형 평가 설정에서 대부분의 오픈 및 클로즈드 웨이트 기준선을 능가하며, Claude 4 Opus 및 Sonnet과의 격차를 줄였습니다.

코딩, 수학, 그리고 더 넓은 STEM 영역에서 Kimi K2는 LiveCodeBench v6에서 53.7점, OJBench에서 27.1점, AIME 2025에서 49.5점, GPQA-Diamond에서 75.1점을 달성하여 일반적인 작업에서의 능력을 더욱 부각시켰습니다. LMSYS Arena 리더보드(2025년 7월 17일)에서 Kimi K2는 3,000명 이상의 사용자 투표를 바탕으로 오픈소스 모델 중 1위, 전체 5위를 기록했습니다.

에이전틱 인텔리전스의 추가적인 발전을 촉진하기 위해, 우리는 기본 및 후훈련된 체크포인트를 오픈소스로 공개하여 커뮤니티가 대규모로 에이전틱 인텔리전스를 탐색하고, 개선하고, 배포할 수 있도록 합니다.
# 사전 훈련

Kimi K2의 기본 모델은 15.5조 개의 고품질 토큰으로 사전 훈련된 1조 파라미터 규모의 Mixture-of-Experts(MoE) 트랜스포머 모델입니다. 고품질 인간 데이터의 가용성이 점점 제한되고 있는 상황에서, 토큰 효율성이 대규모 언어 모델 스케일링의 핵심 계수로 부상하고 있습니다. 이를 해결하기 위해 토큰 효율성을 극대화하도록 명시적으로 설계된 사전 훈련 기법들이 도입되었습니다.

구체적으로, 토큰 효율적인 Muon 옵티마이저가 사용되었으며, QK-Clip의 도입을 통해 훈련 불안정성을 완화했습니다. 또한 사용 가능한 고품질 토큰에서 지능을 더욱 추출하기 위해 합성 데이터 생성이 통합되었습니다. 모델 아키텍처는 경험적 스케일링 법칙 분석에서 도출된 DeepSeek-V3와 유사한 Multi-head Latent Attention(MLA)을 가진 초희소 MoE를 따릅니다. 기반 인프라는 훈련 효율성과 연구 효율성을 모두 최적화하도록 구축되었습니다.

## MuonClip: 가중치 클리핑을 통한 안정적 훈련

Kimi K2는 가중치 감쇠와 일관된 업데이트 RMS 스케일링을 통합한 토큰 효율적인 Muon 옵티마이저를 사용하여 훈련되었습니다. 이전 연구인 Moonlight에서의 실험들은 동일한 계산 예산과 모델 크기, 따라서 동일한 양의 훈련 데이터 하에서 Muon이 AdamW를 상당히 능가한다는 것을 보여주었으며, 이는 대규모 언어 모델 훈련에서 토큰 효율성을 개선하는 효과적인 선택임을 입증했습니다.

### Muon 스케일링 시 훈련 불안정성

효율성에도 불구하고, Muon 훈련을 확장하면 폭발하는 어텐션 로짓으로 인한 훈련 불안정성이라는 도전과제가 드러납니다. 이는 실험에서 Muon에서는 더 자주 발생하지만 AdamW에서는 덜 발생하는 문제입니다. 기존의 완화 전략들은 불충분합니다. 예를 들어, 로짓 소프트 캡은 어텐션 로짓을 직접 클리핑하지만, 쿼리와 키 간의 내적은 캡핑이 적용되기 전에 여전히 과도하게 증가할 수 있습니다. 반면, Query-Key Normalization(QK-Norm)은 추론 중에 Key 행렬이 완전히 구체화되지 않기 때문에 Multi-head Latent Attention(MLA)에 적용할 수 없습니다.

### QK-Clip으로 Muon 제어하기

이 문제를 해결하기 위해 어텐션 로짓을 명시적으로 제한하는 새로운 가중치 클리핑 메커니즘인 QK-Clip이 제안되었습니다. QK-Clip은 업데이트 후 쿼리와 키 투영 가중치를 재스케일링하여 어텐션 로짓의 증가를 제한하는 방식으로 작동합니다.

트랜스포머 레이어의 입력 표현을 \\(\mathbf{X}\\)라고 하겠습니다. 각 어텐션 헤드 \\(h\\)에 대해, 쿼리, 키, 값 투영은 다음과 같이 계산됩니다.

\\[\mathbf{Q}^{h}=\mathbf{X}\mathbf{W}_{q}^{h},\quad\mathbf{K}^{h}=\mathbf{X}\mathbf{W}_{k}^{h},\quad\mathbf{V}^{h}=\mathbf{X}\mathbf{W}_{v}^{h}\\]

여기서 \\(\mathbf{W}_{q}, \mathbf{W}_{k}, \mathbf{W}_{v}\\)는 모델 파라미터입니다. 어텐션 출력은 다음과 같습니다.

\\[\mathbf{O}^{h}=\operatorname{softmax}\left(\frac{1}{\sqrt{d}}\mathbf{Q}^{h}\mathbf{K}^{h\top}\right)\mathbf{V}^{h}\\]

배치 \\(B\\)에서 소프트맥스에 대한 최대 입력인 헤드별 스칼라인 최대 로짓을 다음과 같이 정의합니다.

\\[S_{\max}^{h}=\frac{1}{\sqrt{d}}\max_{\mathbf{X}\in B}\max_{i,j}\mathbf{Q}_{i}^{h}\mathbf{K}_{j}^{h\top}\\]

여기서 \\(i, j\\)는 훈련 샘플 \\(\mathbf{X}\\)에서 서로 다른 토큰의 인덱스입니다.

QK-Clip의 핵심 아이디어는 \\(S_{\max}^{h}\\)가 목표 임계값 \\(\tau\\)를 초과할 때마다 \\(\mathbf{W}_{k}, \mathbf{W}_{q}\\)를 재스케일링하는 것입니다. 중요한 점은 이 연산이 현재 단계의 순방향/역방향 계산을 변경하지 않는다는 것입니다. 단지 최대 로짓을 가중치 증가를 제어하는 강도를 결정하는 안내 신호로 사용할 뿐입니다.

단순한 구현은 모든 헤드를 동시에 클리핑합니다.

\\[\mathbf{W}_{q}^{h}\leftarrow\gamma^{\alpha}\mathbf{W}_{q}^{h}\qquad\mathbf{W}_{k}^{h}\leftarrow\gamma^{1-\alpha}\mathbf{W}_{k}^{h}\\]

여기서 \\(\gamma=\min(1,\tau/S_{\max})\\)이고 \\(S_{\max}=\max_{h}S_{\max}^{h}\\)이며, \\(\alpha\\)는 일반적으로 0.5로 설정되어 쿼리와 키에 동등한 스케일링을 적용하는 균형 파라미터입니다.

그러나 실제로는 소수의 헤드만이 폭발하는 로짓을 보인다는 것이 관찰되었습니다. 모델 훈련에 대한 개입을 최소화하기 위해, 헤드별 스케일링 팩터 \\(\gamma_{h}=\min(1,\tau/S_{\max}^{h})\\)를 결정하고 헤드별 QK-Clip을 적용하기로 했습니다.

이러한 클리핑은 일반적인 Multi-head Attention(MHA)에서는 간단합니다. MLA의 경우, 공유되지 않은 어텐션 헤드 구성 요소에만 클리핑을 적용합니다.

- \\(\textbf{q}^{C}\\)와 \\(\textbf{k}^{C}\\) (헤드별 구성 요소): 각각 \\(\sqrt{\gamma_{h}}\\)로 스케일링
- \\(\textbf{q}^{R}\\) (헤드별 회전): \\(\gamma_{h}\\)로 스케일링
- \\(\textbf{k}^{R}\\) (공유 회전): 헤드 간 영향을 피하기 위해 그대로 유지

![MuonClip 훈련 과정에서의 최대 로짓 변화](https://arxiv.org/html/2507.20534/x3.png)

![Kimi K2 MuonClip 훈련의 최대 로짓](https://arxiv.org/html/2507.20534/x4.png)

위 그림들은 MuonClip의 효과를 보여줍니다. 왼쪽 그림에서는 중간 규모 훈련 실행 중 어텐션 로짓이 빠르게 1000을 초과하여 잠재적인 수치적 불안정성과 훈련 발산을 초래할 수 있음을 보여줍니다. 오른쪽 그림은 \\(\tau = 100\\)으로 설정된 MuonClip을 사용한 Kimi K2의 전체 훈련 실행에서의 최대 로짓을 보여줍니다. 최대 로짓은 빠르게 100의 캡핑된 값으로 증가하고, 훈련 단계의 약 30% 후에만 안정적인 범위로 감소하여 QK-Clip의 효과적인 조절 효과를 보여줍니다.

### MuonClip: 새로운 옵티마이저

가중치 감쇠, 일관된 RMS 매칭, QK-Clip을 가진 Muon을 단일 옵티마이저로 통합하여 MuonClip이라고 명명했습니다. 다음은 MuonClip 알고리즘의 상세한 구현입니다.

```python
# MuonClip 옵티마이저 알고리즘
def muonclip_step(weights, gradients, momentum, tau=100):
    """
    MuonClip 옵티마이저의 한 스텝 실행
    
    Args:
        weights: 모델 가중치 딕셔너리
        gradients: 그래디언트 딕셔너리  
        momentum: 모멘텀 상태
        tau: QK-Clip 임계값
    """
    
    # 1. Muon 옵티마이저 스텝
    for weight_name, W in weights.items():
        if W.dim() == 2:  # 2D 가중치 행렬만 처리
            n, m = W.shape
            
            # 모멘텀 업데이트
            M_t = mu * momentum[weight_name] + gradients[weight_name]
            momentum[weight_name] = M_t
            
            # Newton-Schulz 반복으로 전처리
            O_t = newton_schulz(M_t) * sqrt(max(n, m)) * 0.2
            
            # 가중치 업데이트 (가중치 감쇠 포함)
            weights[weight_name] = W - lr * (O_t + weight_decay * W)
    
    # 2. QK-Clip 적용
    for layer in attention_layers:
        for head_idx in range(num_heads):
            # 순방향 패스에서 이미 계산된 S_max^h 획득
            S_max_h = get_max_logit(layer, head_idx)
            
            if S_max_h > tau:
                gamma = tau / S_max_h
                
                # MLA 구성 요소별 스케일링
                layer.W_qc[head_idx] *= sqrt(gamma)  # 헤드별 쿼리 구성 요소
                layer.W_kc[head_idx] *= sqrt(gamma)  # 헤드별 키 구성 요소  
                layer.W_qr[head_idx] *= gamma        # 헤드별 회전 쿼리
                # W_kr은 공유되므로 건드리지 않음
```

여러 스케일링 실험을 통해 MuonClip의 효과를 입증했습니다. 먼저 바닐라 Muon을 사용하여 90억 개 활성화 파라미터와 530억 개 총 파라미터를 가진 중간 규모 MoE 모델을 훈련했습니다. 앞서 보여준 그림에서 볼 수 있듯이, 최대 어텐션 로짓이 빠르게 1000의 크기를 초과하여 Muon 훈련에서 어텐션 로짓 폭발이 이미 이 규모에서 명백함을 보여줍니다. 이 수준의 최대 로짓은 일반적으로 상당한 손실 스파이크와 때때로 발산을 포함한 훈련 중 불안정성을 초래합니다.

다음으로, QK-Clip이 모델 성능을 저하시키지 않으며 MuonClip 옵티마이저가 손실 궤적에 악영향을 주지 않으면서 Muon의 최적화 특성을 보존한다는 것을 확인했습니다. 실험 설계와 결과에 대한 자세한 논의는 부록 D에서 제공됩니다.

마지막으로, \\(\tau = 100\\)으로 설정된 MuonClip을 사용하여 대규모 MoE 모델인 Kimi K2를 훈련하고 전체 훈련 실행 동안 최대 어텐션 로짓을 모니터링했습니다. 처음에는 QK-Clip으로 인해 로짓이 100으로 캡핑됩니다. 훈련 과정에서 최대 로짓은 \\(\tau\\)에 대한 조정 없이 점진적으로 일반적인 작동 범위로 감소합니다. 중요한 점은 다음 그림에서 보여주듯이 훈련 손실이 부드럽고 안정적으로 유지되며 관찰 가능한 스파이크가 없다는 것입니다.

![Kimi K2의 단계별 훈련 손실 곡선](https://arxiv.org/html/2507.20534/x5.png)

위 그림은 스무딩이나 서브샘플링 없이 Kimi K2의 단계별 훈련 손실 곡선을 보여줍니다. 전체 훈련 과정에서 스파이크가 없음을 보여줍니다. 명확성을 위해 훈련 초기 부분은 생략되었습니다. 이는 MuonClip이 대규모 언어 모델 훈련에서 어텐션 역학에 대한 견고하고 확장 가능한 제어를 제공한다는 것을 검증합니다.

## 사전 훈련 데이터: 재구성을 통한 토큰 유틸리티 개선

사전 훈련에서 토큰 효율성은 훈련 중 소비되는 각 토큰에 대해 달성되는 성능 향상의 정도를 의미합니다. 토큰 유틸리티, 즉 각 토큰이 기여하는 효과적인 학습 신호를 증가시키는 것은 모델 업데이트에 대한 토큰당 영향을 향상시켜 토큰 효율성을 직접적으로 개선합니다. 이는 고품질 토큰의 공급이 제한적이고 최대한 활용되어야 할 때 특히 중요합니다.

토큰 유틸리티를 증가시키는 단순한 접근법은 동일한 토큰에 반복적으로 노출시키는 것이지만, 이는 과적합과 일반화 감소로 이어질 수 있습니다. Kimi K1.5에 비해 Kimi K2의 사전 훈련 데이터에서 핵심적인 발전은 토큰 유틸리티를 증가시키기 위한 합성 데이터 생성 전략의 도입입니다. 구체적으로, 상당한 과적합을 유발하지 않으면서 고품질 토큰의 볼륨을 증폭시키기 위해 신중하게 설계된 재구성 파이프라인이 사용되었습니다.

### 지식 데이터 재구성

자연적이고 지식 집약적인 텍스트에 대한 사전 훈련은 트레이드오프를 제시합니다. 단일 에포크는 포괄적인 지식 흡수에 불충분하지만, 다중 에포크 반복은 수익 감소와 과적합 위험 증가를 가져옵니다. 고품질 지식 토큰의 토큰 유틸리티를 개선하기 위해 다음 핵심 구성 요소로 구성된 합성 재구성 프레임워크가 제안되었습니다.

**스타일 및 관점 다양성 프롬프팅**: 사실적 무결성을 유지하면서 언어적 다양성을 향상시키기 위해 신중하게 설계된 다양한 프롬프트가 적용됩니다. 이러한 프롬프트는 대규모 언어 모델이 다양한 스타일과 서로 다른 관점에서 원본 텍스트의 충실한 재구성을 생성하도록 안내합니다.

**청크별 자기회귀 생성**: 긴 문서에서 전역적 일관성을 보존하고 정보 손실을 피하기 위해 청크 기반 자기회귀 재작성 전략이 채택됩니다. 텍스트는 세그먼트로 나뉘고, 개별적으로 재구성된 후, 다시 연결되어 완전한 구절을 형성합니다. 이 방법은 일반적으로 LLM에 존재하는 암묵적 출력 길이 제한을 완화합니다.

![자동회귀 청크별 재구성 파이프라인](https://arxiv.org/html/2507.20534/x6.png)

위 그림은 긴 입력 발췌문에 대한 자동회귀 청크별 재구성 파이프라인을 보여줍니다. 입력은 보존된 컨텍스트와 함께 더 작은 청크로 분할되고, 순차적으로 재작성된 후, 완전한 재작성된 구절로 연결됩니다.

**충실도 검증**: 원본과 재작성된 내용 간의 일관성을 보장하기 위해 각 재구성된 구절과 그 소스의 의미적 정렬을 비교하는 충실도 검사가 수행됩니다. 이는 훈련 전 초기 품질 제어 단계 역할을 합니다.

데이터 재구성을 다중 에포크 반복과 비교하기 위해 SimpleQA에서 해당 정확도를 테스트했습니다. K2의 초기 체크포인트로 실험하여 세 가지 훈련 전략을 평가했습니다. (1) 원본 데이터셋을 10 에포크 반복, (2) 데이터를 한 번 재구성하고 10 에포크 반복, (3) 데이터를 10번 재구성하고 단일 훈련 패스. 다음 표에서 보듯이, 정확도는 이러한 전략들에서 일관되게 향상되어 재구성 기반 증강의 효과를 입증합니다.

| 재구성 횟수 | 에포크 수 | SimpleQA 정확도 |
|-------------|-----------|-----------------|
| 0 (원본 위키텍스트) | 10 | 23.76 |
| 1 | 10 | 27.39 |
| 10 | 1 | 28.94 |

이 방법을 다른 대규모 지식 코퍼스로 확장했으며 유사하게 고무적인 결과를 관찰했고, 각 코퍼스는 최대 두 번 재구성되었습니다.

### 수학 데이터 재구성

수학적 추론 능력을 향상시키기 위해 SwallowMath에서 도입된 방법론에 따라 고품질 수학 문서를 "학습 노트" 스타일로 재작성했습니다. 또한 다른 언어의 고품질 수학 자료를 영어로 번역하여 데이터 다양성을 증가시켰습니다.

재구성된 데이터셋의 하위 집합에 대한 초기 실험은 유망한 결과를 보여주지만, 지속적인 스케일링을 위한 전략으로서 합성 데이터의 사용은 여전히 활발한 조사 영역입니다. 주요 도전과제에는 사실적 정확성을 손상시키지 않으면서 다양한 소스 도메인에 접근법을 일반화하는 것, 환각과 의도하지 않은 독성을 최소화하는 것, 대규모 데이터셋에 대한 확장성을 보장하는 것이 포함됩니다.

### 전체 사전 훈련 데이터

Kimi K2 사전 훈련 코퍼스는 웹 텍스트, 코드, 수학, 지식의 네 가지 주요 도메인에 걸쳐 15.5조 개의 큐레이션된 고품질 토큰으로 구성됩니다. 대부분의 데이터 처리 파이프라인은 Kimi K1.5에서 설명된 방법론을 따릅니다. 각 도메인에 대해 엄격한 정확성과 품질 검증을 수행하고 큐레이션된 데이터셋이 높은 다양성과 효과를 모두 달성하도록 목표 데이터 실험을 설계했습니다.
## 모델 아키텍처

Kimi K2는 320억 개의 활성화된 파라미터를 가진 1.04조 파라미터 규모의 Mixture-of-Experts(MoE) 트랜스포머 모델입니다. 이 아키텍처는 DeepSeek-V3와 유사한 설계를 따르며, Multi-head Latent Attention(MLA)을 어텐션 메커니즘으로 사용합니다. 모델의 은닉 차원은 7168이고 MoE 전문가 은닉 차원은 2048입니다.

스케일링 법칙 분석을 통해 희소성의 지속적인 증가가 상당한 성능 향상을 가져온다는 것이 밝혀졌으며, 이는 DeepSeek-V3의 256개에 비해 전문가 수를 384개로 증가시키는 동기가 되었습니다. 추론 중 계산 오버헤드를 줄이기 위해 어텐션 헤드 수를 DeepSeek-V3의 128개에서 64개로 줄였습니다.

다음 표는 Kimi K2와 DeepSeek-V3 간의 아키텍처 파라미터를 상세히 비교한 것입니다.

| 구성 요소 | DeepSeek-V3 | Kimi K2 | 변화량 |
|-----------|-------------|---------|--------|
| 레이어 수 | 61 | 61 | = |
| 총 파라미터 | 671B | 1.04T | ↑ 54% |
| 활성화된 파라미터 | 37B | 32.6B | ↓ 13% |
| 전문가 수 (총) | 256 | 384 | ↑ 50% |
| 토큰당 활성 전문가 | 8 | 8 | = |
| 공유 전문가 | 1 | 1 | = |
| 어텐션 헤드 | 128 | 64 | ↓ 50% |
| 밀집 레이어 수 | 3 | 1 | ↓ 67% |
| 전문가 그룹화 | Yes | No | - |

### 희소성 스케일링 법칙

Muon을 사용하여 Mixture-of-Experts(MoE) 모델 패밀리에 맞춤화된 희소성 스케일링 법칙을 개발했습니다. 희소성은 총 전문가 수와 활성화된 전문가 수의 비율로 정의됩니다. 신중하게 제어된 소규모 실험을 통해 고정된 활성화 파라미터 수(즉, 일정한 FLOP) 하에서 총 전문가 수를 증가시키는 것(즉, 희소성 증가)이 훈련 및 검증 손실을 일관되게 낮추어 전체 모델 성능을 향상시킨다는 것을 관찰했습니다.

![희소성 스케일링 법칙](https://arxiv.org/html/2507.20534/x7.png)

위 그림은 희소성 스케일링 법칙을 보여줍니다. 희소성이 증가하면 모델 성능이 향상됩니다. 활성화된 전문가 수를 8개로, 공유 전문가 수를 1개로 고정하고 총 전문가 수를 변화시켜 서로 다른 희소성 수준을 가진 모델들을 만들었습니다.

구체적으로, 계산 최적 희소성 스케일링 법칙 하에서 1.5의 동일한 검증 손실을 달성하기 위해 희소성 48은 희소성 수준 8, 16, 32에 비해 각각 1.69배, 1.39배, 1.15배의 FLOP을 감소시킵니다. 희소성 증가가 더 나은 성능으로 이어지지만, 이러한 이득은 인프라 복잡성 증가를 수반합니다. 모델 성능과 비용의 균형을 맞추기 위해 Kimi K2에서는 희소성 48을 채택하여 순방향 패스당 384개 전문가 중 8개를 활성화합니다.

### 어텐션 헤드 수

DeepSeek-V3는 메모리 대역폭을 더 잘 활용하고 계산 효율성을 향상시키기 위해 어텐션 헤드 수를 모델 레이어 수의 약 두 배로 설정합니다. 그러나 컨텍스트 길이가 증가함에 따라 어텐션 헤드 수를 두 배로 늘리면 상당한 추론 오버헤드가 발생하여 더 긴 시퀀스 길이에서 효율성이 감소합니다. 이는 효율적인 긴 컨텍스트 처리가 필수적인 에이전틱 애플리케이션에서 주요 제한사항이 됩니다.

예를 들어, 128k의 시퀀스 길이에서 총 전문가 수를 384개로 고정한 상태에서 어텐션 헤드 수를 64개에서 128개로 증가시키면 추론 FLOP이 83% 증가합니다. 이 설계의 영향을 평가하기 위해 다양한 훈련 FLOP 하에서 어텐션 헤드 수가 레이어 수와 같은 구성과 두 배인 구성을 비교하는 제어된 실험을 수행했습니다.

![어텐션 헤드 수에 따른 스케일링 곡선](https://arxiv.org/html/2507.20534/x8.png)

위 그림은 어텐션 헤드 수가 레이어 수와 같은 모델과 두 배인 모델의 스케일링 곡선을 보여줍니다. 어텐션 헤드 수를 두 배로 늘리면 검증 손실이 약 0.5%에서 1.2% 감소합니다.

동일 토큰 훈련 조건 하에서 어텐션 헤드를 두 배로 늘리는 것이 서로 다른 계산 예산에서 검증 손실에서 단지 미미한 개선(0.5%에서 1.2% 범위)만을 가져온다는 것을 관찰했습니다. 희소성 48이 이미 강력한 성능을 제공하는 상황에서, 어텐션 헤드를 두 배로 늘리는 것으로부터의 한계 이득은 추론 비용을 정당화하지 못합니다. 따라서 64개의 어텐션 헤드를 선택했습니다.

## 훈련 인프라

### 컴퓨팅 클러스터

Kimi K2는 NVIDIA H800 GPU가 장착된 클러스터에서 훈련되었습니다. H800 클러스터의 각 노드는 2TB RAM과 노드 내에서 NVLink 및 NVSwitch로 연결된 8개의 GPU를 포함합니다. 서로 다른 노드 간에는 8×400 Gbps RoCE 인터커넥트가 통신을 촉진하는 데 사용됩니다.

### 모델 스케일링을 위한 병렬화

대규모 언어 모델의 훈련은 종종 동적 자원 가용성 하에서 진행됩니다. 특정 자원량에서만 적용 가능한 하나의 병렬화 전략을 최적화하는 대신, 32의 배수인 임의의 노드 수에서 Kimi K2를 훈련할 수 있는 유연한 전략을 추구했습니다.

이 전략은 가상 스테이지를 가진 16-way Pipeline Parallelism(PP), 16-way Expert Parallelism(EP), ZeRO-1 Data Parallelism의 조합을 활용합니다. 이 설정 하에서 BF16으로 모델 파라미터를 저장하고 FP32로 그래디언트 누적 버퍼를 저장하는 것은 256개 GPU에 분산된 약 6TB의 GPU 메모리를 필요로 합니다.

옵티마이저 상태의 배치는 훈련 구성에 따라 달라집니다. 총 훈련 노드 수가 클 때는 옵티마이저 상태가 분산되어 디바이스당 메모리 사용량을 무시할 수 있는 수준으로 줄입니다. 총 훈련 노드 수가 적을 때(예: 32개)는 일부 옵티마이저 상태를 CPU로 오프로드할 수 있습니다.

이 접근법을 통해 소규모 및 대규모 실험 모두에 동일한 병렬화 구성을 재사용할 수 있으며, 각 GPU가 모든 상태에 대해 약 30GB의 GPU 메모리를 보유하도록 합니다. 나머지 GPU 메모리는 활성화에 사용됩니다.

**인터리브된 1F1B와 EP 통신 오버랩**: 웜업 마이크로 배치 수를 증가시켜 표준 인터리브된 1F1B 스케줄 하에서 EP all-to-all 통신을 계산과 오버랩할 수 있습니다. 이에 비해 DualPipe는 파라미터와 그래디언트에 필요한 메모리를 두 배로 늘려 이를 보상하기 위한 병렬화 증가를 필요로 합니다.

**더 작은 EP 크기**: 1F1B 단계에서 완전한 계산-통신 오버랩을 보장하기 위해 K2의 감소된 어텐션 계산 시간(DeepSeek-V3의 128개 헤드에 비해 64개 어텐션 헤드)은 EP 연산 시간을 최소화할 필요가 있습니다. 이는 가장 작은 실행 가능한 EP 병렬화 전략, 구체적으로 EP = 16을 채택함으로써 달성됩니다.

### 활성화 감소

파라미터, 그래디언트 버퍼, 옵티마이저 상태를 위한 공간을 예약한 후, 각 디바이스의 나머지 GPU 메모리는 전체 MoE 활성화를 보유하기에 불충분합니다. 특히 1F1B 웜업 단계에서 가장 큰 활성화를 누적하는 초기 파이프라인 스테이지에서 활성화 메모리가 제약 내에 맞도록 하기 위해 다음 기법들이 사용됩니다.

**선택적 재계산**: LayerNorm, SwiGLU, MLA 업-프로젝션을 포함한 저비용, 고메모리 사용량 스테이지에 재계산이 적용됩니다. 또한 훈련 중 활성화 메모리를 더욱 줄이기 위해 MoE 다운-프로젝션이 재계산됩니다.

**민감하지 않은 활성화를 위한 FP8 저장**: MoE 업-프로젝션과 SwiGLU의 입력은 FP32 스케일을 가진 1×128 타일에서 FP8-E4M3으로 압축됩니다. 소규모 실험에서는 측정 가능한 손실 증가가 없음을 보여줍니다.

**활성화 CPU 오프로드**: 나머지 모든 활성화는 CPU RAM으로 오프로드됩니다. 복사 엔진이 오프로드와 온로드를 스트리밍하여 계산 및 통신 커널과 오버랩시킵니다.

![서로 다른 PP 단계에서 오버랩된 계산, 통신 및 오프로딩](https://arxiv.org/html/2507.20534/x9.png)

위 그림은 서로 다른 파이프라인(PP) 단계에서 계산, 통신 및 오프로딩이 오버랩되는 시각화를 보여줍니다. 이 그림은 훈련 과정에서 다양한 활동(계산, 통신, 오프로딩)의 타임라인을 여러 PP 단계에 걸쳐 보여주며, 전체 훈련 효율성과 처리량을 최적화하는 데 도움이 되는 다양한 작업의 오버랩과 파이프라이닝을 설명합니다.

## 훈련 레시피

MuonClip 옵티마이저와 WSD 학습률 스케줄을 사용하여 4,096 토큰 컨텍스트 윈도우로 모델을 사전 훈련했으며, 총 15.5조 토큰을 처리했습니다. 처음 10조 토큰은 500 스텝 웜업 후 2e-4의 일정한 학습률로 훈련되었고, 이어서 5.5조 토큰은 2e-4에서 2e-5로의 코사인 감쇠로 훈련되었습니다. 가중치 감쇠는 전체적으로 0.1로 설정되었고, 전역 배치 크기는 6700만 토큰으로 유지되었습니다.

사전 훈련 말기에 어닐링 단계와 긴 컨텍스트 활성화 단계를 수행했습니다. 배치 크기는 6700만 토큰으로 일정하게 유지되었고, 학습률은 2e-5에서 7e-6으로 감쇠되었습니다. 이 단계에서 모델은 4k 시퀀스 길이로 4000억 토큰에 대해 훈련되었고, 이어서 32k 시퀀스 길이로 추가 600억 토큰에 대해 훈련되었습니다. 컨텍스트 윈도우를 128k로 확장하기 위해 YaRN 방법을 사용했습니다.
## 후훈련

Kimi K2의 후훈련 과정은 사전 훈련된 기본 모델을 실제 사용 가능한 에이전틱 인텔리전스로 변환하는 핵심 단계입니다. 이 과정은 지도 학습 기반 파인튜닝과 강화학습을 결합한 다단계 접근법을 통해 모델의 도구 사용 능력, 추론 능력, 그리고 사용자와의 상호작용 품질을 크게 향상시킵니다.

### 지도 학습 파인튜닝

후훈련 과정에서는 이전 연구에서 도출된 결론에 따라 Muon 옵티마이저를 사용합니다. 이는 Muon으로 사전 훈련된 체크포인트가 Muon 파인튜닝과 함께 사용될 때 최고의 성능을 보인다는 발견에 기반합니다. 

지도 학습 파인튜닝을 위해 다양한 도메인에 걸친 대규모 지시 튜닝 데이터셋을 구축했습니다. 이 과정은 두 가지 핵심 원칙에 의해 안내됩니다. 프롬프트 다양성 극대화와 높은 응답 품질 보장입니다. 이를 위해 서로 다른 작업 도메인에 맞춤화된 데이터 생성 파이프라인 모음을 개발했으며, 각 파이프라인은 인간 주석, 프롬프트 엔지니어링, 검증 프로세스의 조합을 활용합니다.

다양한 작업에 대한 후보 응답을 생성하기 위해 K1.5와 기타 사내 도메인 특화 전문가 모델들을 채택했으며, 이후 LLM 또는 인간 기반 판단자가 자동화된 품질 평가와 필터링을 수행합니다. 에이전틱 데이터의 경우, 다단계 상호작용 추론을 통해 모델에게 도구 사용 능력을 가르치는 데이터 합성 파이프라인을 구축했습니다.

#### 도구 사용 학습을 위한 대규모 에이전틱 데이터 합성

현대 LLM 에이전트의 핵심 능력은 익숙하지 않은 도구를 자율적으로 사용하고, 외부 환경과 상호작용하며, 추론, 실행, 오류 수정을 통해 행동을 반복적으로 개선하는 것입니다. 에이전틱 도구 사용 능력은 실세계 시스템과의 동적 상호작용이 필요한 복잡하고 다단계적인 작업을 해결하는 데 필수적입니다.

ACEBench와 τ-bench와 같은 최근 벤치마크들은 포괄적인 도구 사용 평가의 중요성을 강조했으며, ToolLLM과 ACEBench와 같은 프레임워크들은 수천 개의 도구를 효과적으로 사용하도록 모델을 가르치는 잠재력을 보여주었습니다. 그러나 이러한 능력을 대규모로 훈련하는 것은 상당한 도전과제를 제시합니다. 실세계 환경은 풍부하고 진정한 상호작용 신호를 제공하지만, 비용, 복잡성, 개인정보 보호 및 접근성 제약으로 인해 대규모로 구축하기 어려운 경우가 많습니다.

합성 데이터 생성에 관한 최근 연구들(AgentInstruct, Self-Instruct, StableToolBench, ZeroSearch)은 실세계 상호작용에 의존하지 않고 대규모 데이터를 생성하는 데 유망한 결과를 보여주었습니다. 이러한 발전과 ACEBench의 포괄적인 데이터 합성 프레임워크에서 영감을 받아, 실세계 도구 사용 시나리오를 대규모로 시뮬레이션하는 파이프라인을 개발했습니다. 이를 통해 수만 개의 다양하고 고품질인 훈련 예제를 생성할 수 있습니다.

![도구 사용을 위한 데이터 합성 파이프라인](https://arxiv.org/html/2507.20534/x10.png)

![에이전트 궤적 생성](https://arxiv.org/html/2507.20534/x11.png)

데이터 합성 파이프라인은 세 단계로 구성됩니다. 첫 번째 단계에서는 실세계 도구와 LLM 합성 도구 모두에서 도구 사양을 구축합니다. 두 번째 단계에서는 도구 저장소에서 샘플링된 각 도구 세트에 대해 해당 도구 세트를 사용할 에이전트와 일부 해당 작업을 생성합니다. 세 번째 단계에서는 각 에이전트와 작업에 대해 에이전트가 도구를 호출하여 작업을 완료하는 궤적을 생성합니다.

**도메인 진화와 도구 생성**

포괄적인 도구 저장소를 두 가지 상호 보완적인 접근법을 통해 구축했습니다. 첫째, GitHub 저장소에서 3000개 이상의 실제 MCP(Model Context Protocol) 도구를 직접 가져와 기존의 고품질 도구 사양을 활용했습니다. 둘째, 계층적 도메인 생성 프로세스를 통해 합성 도구를 체계적으로 진화시켰습니다. 핵심 카테고리(예: 금융 거래, 소프트웨어 애플리케이션, 로봇 제어)부터 시작하여 각 카테고리 내에서 여러 특정 애플리케이션 도메인을 진화시켰습니다. 그 다음 각 도메인에 대해 명확한 인터페이스, 설명, 운영 의미론을 가진 특화된 도구들을 합성했습니다. 이 진화 프로세스는 20,000개 이상의 합성 도구를 생성했습니다.

![실제 MCP 도구의 t-SNE 시각화](https://arxiv.org/html/2507.20534/x12.png)

![합성 도구의 t-SNE 시각화](https://arxiv.org/html/2507.20534/x13.png)

도구 컬렉션의 다양성을 t-SNE 임베딩을 통해 시각화한 결과, MCP와 합성 도구 모두가 도구 공간의 상호 보완적인 영역을 다루고 있음을 보여줍니다. 실세계 MCP 도구들은 원래 소스 카테고리를 기반으로 자연스러운 클러스터링을 보이며, 합성 도구들은 사전 정의된 도메인 카테고리로 체계적으로 구성되어 도구 공간의 체계적인 커버리지를 제공합니다.

**에이전트 다양화**

도구 저장소에서 다양한 도구 조합을 갖춘 다양한 시스템 프롬프트를 합성하여 수천 개의 서로 다른 에이전트를 생성했습니다. 이를 통해 다양한 능력, 전문 분야, 행동 패턴을 가진 다양한 에이전트 집단을 만들어 잠재적 사용 사례의 광범위한 커버리지를 보장했습니다.

**루브릭 기반 작업 생성**

각 에이전트 구성에 대해 간단한 것부터 복잡한 작업까지 다양한 작업을 생성했습니다. 각 작업은 성공 기준, 예상되는 도구 사용 패턴, 평가 체크포인트를 명시하는 명시적인 루브릭과 쌍을 이룹니다. 이러한 루브릭 기반 접근법은 에이전트 성능의 일관되고 객관적인 평가를 보장합니다.

**다중 턴 궤적 생성**

여러 구성 요소를 통해 현실적인 도구 사용 시나리오를 시뮬레이션했습니다. 사용자 시뮬레이션에서는 서로 다른 의사소통 스타일과 선호도를 가진 LLM 생성 사용자 페르소나가 에이전트와 다중 턴 대화에 참여하여 자연스러운 상호작용 패턴을 만들어냅니다.

도구 실행 환경은 정교한 도구 시뮬레이터(기능적으로 세계 모델과 동등)가 도구 호출을 실행하고 현실적인 피드백을 제공합니다. 시뮬레이터는 각 도구 실행 후 상태를 유지하고 업데이트하여 지속적인 효과를 가진 복잡한 다단계 상호작용을 가능하게 합니다. 또한 성공, 부분적 실패, 엣지 케이스를 포함한 다양한 결과를 생성하기 위해 제어된 확률성을 도입합니다.

**품질 평가와 필터링**

LLM 기반 판단자가 각 궤적을 작업 루브릭에 대해 평가합니다. 성공 기준을 충족하는 궤적만이 훈련을 위해 보존되어 작업 완료 전략의 자연스러운 변화를 허용하면서도 고품질 데이터를 보장합니다.

**실제 실행 환경과의 하이브리드 접근법**

시뮬레이션이 확장성을 제공하지만, 시뮬레이션 충실도의 고유한 한계를 인정합니다. 이를 해결하기 위해 진정성이 중요한 시나리오, 특히 코딩과 소프트웨어 엔지니어링 작업에서 시뮬레이션된 환경을 실제 실행 샌드박스로 보완했습니다. 이러한 실제 샌드박스는 실제 코드를 실행하고, 진정한 개발 환경과 상호작용하며, 테스트 스위트 통과율과 같은 객관적인 메트릭을 통해 실측 피드백을 제공합니다.

확장 가능한 시뮬레이션과 목표화된 실세계 실행을 결합한 이 하이브리드 파이프라인을 활용하여 커버리지와 진정성의 균형을 맞춘 다양하고 고품질인 도구 사용 시연을 생성합니다. 합성 데이터 생성의 규모와 자동화는 실제 실행 환경에서 제공되는 기반과 결합되어 품질 필터링 프로세스를 통해 대규모 거부 샘플링을 효과적으로 구현합니다.

이 고품질 합성 데이터를 지도 학습 파인튜닝에 사용할 때, 광범위한 실세계 애플리케이션에서 모델의 도구 사용 능력에 상당한 개선을 보여주었습니다. 이러한 접근법은 시뮬레이션의 확장성과 실제 실행의 진정성을 결합하여 에이전틱 능력 개발에 필요한 대규모 고품질 훈련 데이터를 제공하는 실용적인 솔루션을 제시합니다.
### 강화학습

Kimi K2의 후훈련 과정에서 강화학습은 지도 학습 파인튜닝을 넘어서 모델의 능력을 한층 더 발전시키는 핵심 구성 요소입니다. 이 단계에서는 검증 가능한 보상과 자기 비판 메커니즘을 결합한 확장 가능한 Gym 스타일 프레임워크를 도입하여 다양한 시나리오에서 강화학습을 수행합니다.

강화학습 프레임워크는 수학, STEM, 논리적 추론과 같은 객관적 작업에서는 검증 가능한 보상을 사용하고, 창작 글쓰기와 같은 주관적 선호도가 중요한 작업에서는 자기 비판 보상 메커니즘을 활용합니다. 이러한 이중 접근법을 통해 모델은 객관적으로 평가 가능한 영역과 주관적 판단이 필요한 영역 모두에서 성능을 향상시킬 수 있습니다.

#### 검증 가능한 보상

검증 가능한 보상 시스템은 객관적으로 평가할 수 있는 작업들에 대해 명확하고 일관된 피드백을 제공합니다. 이 시스템은 수학, STEM, 논리적 추론, 지시 따르기, 충실성, 코딩, 안전성 작업을 포괄하며, 각 도메인에 특화된 검증 메커니즘을 구축했습니다.

강화학습 데이터 준비 과정에서는 다양한 커버리지와 적당한 난이도라는 두 가지 핵심 원칙을 따릅니다. 다양한 커버리지는 각 도메인 내에서 광범위한 작업 유형과 시나리오를 포함하여 모델이 특정 패턴에 과적합되지 않도록 보장합니다. 적당한 난이도는 모델이 학습할 수 있을 만큼 도전적이면서도 완전히 해결 불가능하지 않은 작업들을 선별하여 효과적인 학습 신호를 제공합니다.

수학 도메인에서는 대수, 기하, 미적분, 확률론 등 다양한 수학 분야의 문제들을 포함하며, 각 문제에 대해 단계별 해결 과정과 최종 답안을 모두 평가합니다. STEM 영역에서는 물리학, 화학, 생물학의 개념 이해와 문제 해결 능력을 평가하며, 논리적 추론에서는 연역적 추론, 귀납적 추론, 인과관계 분석 등을 다룹니다.

코딩 작업에서는 하이브리드 검증 프레임워크를 사용합니다. 이는 자동화된 테스트 스위트 실행과 인간 전문가의 코드 품질 평가를 결합한 것으로, 기능적 정확성뿐만 아니라 코드의 가독성, 효율성, 유지보수성까지 종합적으로 평가합니다. 특히 실제 실행 환경에서의 테스트를 통해 코드가 실제로 작동하는지 확인하고, 엣지 케이스와 예외 상황에 대한 처리도 평가합니다.

안전성 작업에서는 모델이 유해한 콘텐츠를 생성하지 않고 윤리적 가이드라인을 준수하는지 평가합니다. 이를 위해 다양한 안전성 시나리오와 잠재적 위험 상황을 포함한 테스트 세트를 구축하고, 모델의 응답이 안전 기준을 충족하는지 자동화된 분류기와 인간 평가자를 통해 검증합니다.

#### 검증을 넘어서: 자기 비판 루브릭 보상

검증 가능한 보상이 객관적 작업에 효과적인 반면, 창작 글쓰기, 대화 품질, 창의적 문제 해결과 같은 주관적 영역에서는 다른 접근법이 필요합니다. 이를 위해 자기 비판 피드백을 통한 일반적인 강화학습 프레임워크를 도입했습니다.

이 프레임워크에서 K2는 자신의 출력을 평가하는 비판자 역할을 수행합니다. 모델은 핵심 루브릭과 규범적 루브릭을 사용하여 자신의 응답을 체계적으로 평가합니다. 핵심 루브릭은 명확성과 관련성, 대화의 유창성과 참여도, 객관적 근거 기반 상호작용과 같은 기본적인 품질 기준을 다룹니다.

규범적 루브릭은 보상 해킹을 방지하기 위해 설계된 것으로, 초기 칭찬 금지와 명시적 정당화 요구와 같은 규칙을 포함합니다. 이는 모델이 단순히 긍정적인 평가를 받기 위해 표면적인 개선을 하는 것을 방지하고, 실질적인 품질 향상에 집중하도록 유도합니다.

자기 비판 시스템은 검증 가능한 신호에 기반한 폐쇄 루프 비판자 개선을 통해 지속적으로 발전합니다. 모델의 자기 평가 능력은 객관적으로 검증 가능한 작업에서의 성능과 연결되어 보정되며, 이를 통해 주관적 영역에서도 신뢰할 수 있는 평가를 제공할 수 있게 됩니다.

이러한 자기 비판 메커니즘의 한계로는 확신 편향이 있습니다. 모델이 자기 자격 부여를 피하고 명확성을 선호하는 경향으로 인해 확신 있는 응답을 선호할 수 있으며, 이는 모호한 상황에서 적절한 인식론적 겸손을 억제할 수 있습니다. 이를 완화하기 위해 불확실성 표현과 다양한 관점 제시를 장려하는 추가적인 루브릭을 도입했습니다.

#### 강화학습 알고리즘

강화학습 구현에서는 토큰 효율성을 위한 예산 제어, 망각 방지를 위한 PTX 손실, 훈련 중 탐색과 활용의 균형을 맞추기 위한 온도 감쇠 스케줄을 포함한 정책 최적화 알고리즘을 사용합니다.

예산 제어 메커니즘은 강화학습 과정에서 사용되는 토큰 수를 효율적으로 관리합니다. 각 에피소드에서 생성되는 토큰 수를 모니터링하고, 학습 효과가 포화되는 지점에서 조기 종료를 통해 계산 자원을 절약합니다. 이는 특히 긴 시퀀스를 생성하는 에이전틱 작업에서 중요한 최적화입니다.

PTX(Pre-Training miXture) 손실은 강화학습 과정에서 모델이 사전 훈련에서 학습한 지식을 잊어버리는 것을 방지합니다. 이는 강화학습 목표와 사전 훈련 목표를 가중 조합하여 구현되며, 모델이 새로운 능력을 학습하면서도 기존의 언어 이해와 생성 능력을 유지하도록 보장합니다.

온도 감쇠 스케줄은 훈련 초기에는 높은 온도를 사용하여 탐색을 장려하고, 훈련이 진행됨에 따라 온도를 점진적으로 낮춰 활용에 집중하도록 합니다. 이러한 접근법은 모델이 다양한 전략을 탐색한 후 최적의 정책으로 수렴할 수 있게 합니다.

정책 최적화는 Proximal Policy Optimization(PPO) 알고리즘을 기반으로 하되, 에이전틱 작업의 특성에 맞게 수정되었습니다. 특히 다단계 추론과 도구 사용이 포함된 복잡한 작업에서는 중간 단계의 보상 신호를 활용하여 더 효과적인 학습이 가능하도록 했습니다.

### 강화학습 인프라

강화학습 훈련을 위한 인프라는 동기화된 강화학습 훈련을 위한 코로케이션 아키텍처, 효율적인 엔진 전환, 시스템 시작 최적화, 부분 롤아웃 기법을 통한 장기 에이전틱 작업 지원을 특징으로 합니다.

#### 코로케이션 아키텍처

하이브리드 코로케이션 아키텍처에서는 훈련과 추론 엔진이 워커를 공유하며, 중앙화된 컨트롤러가 엔진 간 데이터 생성과 파라미터 업데이트를 관리합니다. 이 설계는 자원 활용도를 최대화하면서도 훈련과 추론 작업 간의 효율적인 전환을 가능하게 합니다.

코로케이션 아키텍처의 핵심은 동적 자원 할당입니다. 강화학습 과정에서 데이터 생성 단계에서는 추론 엔진이 활성화되어 정책 롤아웃을 수행하고, 파라미터 업데이트 단계에서는 훈련 엔진이 활성화되어 그래디언트 계산과 가중치 업데이트를 수행합니다. 이러한 전환은 중앙 컨트롤러에 의해 조율되며, 각 단계 간의 동기화를 보장합니다.

#### 효율적인 엔진 전환

서로 다른 샤딩 패러다임 간의 파라미터 업데이트를 위한 분산 체크포인트 엔진 솔루션을 통해 Kimi K2의 전체 파라미터 업데이트를 30초 미만에 완료할 수 있습니다. 이는 파이프라인된 파라미터별 전송을 통해 달성됩니다.

엔진 전환 과정에서는 메모리 효율성과 전송 속도를 모두 고려한 최적화가 적용됩니다. 파라미터는 레이어별로 분할되어 전송되며, 각 레이어의 전송이 완료되는 즉시 다음 레이어의 전송이 시작되는 파이프라인 방식을 사용합니다. 이를 통해 전체 모델의 파라미터 전송 시간을 크게 단축할 수 있습니다.

#### 효율적인 시스템 시작

체크포인트 엔진을 사용한 최적화된 시작 프로세스는 디스크 IO를 최소화하고 단일 지점 실패에 대한 견고성을 제공하여 추론 복제본이 독립적으로 재시작할 수 있도록 합니다. 이는 대규모 분산 시스템에서 안정성과 가용성을 보장하는 데 중요합니다.

시스템 시작 최적화에는 계층적 체크포인트 로딩이 포함됩니다. 가장 중요한 파라미터부터 우선적으로 로드하여 모델이 부분적으로라도 빠르게 작동할 수 있도록 하고, 나머지 파라미터는 백그라운드에서 점진적으로 로드됩니다. 이러한 접근법은 시스템의 응답성을 크게 향상시킵니다.

#### 에이전틱 롤아웃

장기 다중 턴 에이전틱 작업의 도전과제를 무거운 환경 서비스, GPU 활용을 위한 동시 롤아웃, 부분 롤아웃 기법, 환경 통합을 위한 통합 Gym 스타일 인터페이스를 통해 해결합니다.

에이전틱 롤아웃에서는 복잡한 다단계 작업을 효율적으로 처리하기 위한 여러 기법이 사용됩니다. 부분 롤아웃 기법은 매우 긴 에피소드를 더 작은 세그먼트로 나누어 처리함으로써 메모리 사용량을 제어하고 중간 체크포인트를 통한 복구를 가능하게 합니다.

동시 롤아웃은 여러 환경에서 병렬로 에피소드를 실행하여 GPU 활용률을 최대화합니다. 각 환경은 독립적으로 작동하지만 중앙 스케줄러에 의해 조율되어 자원 경합을 방지하고 효율적인 배치 처리를 가능하게 합니다.

통합 Gym 스타일 인터페이스는 다양한 환경과 작업을 일관된 방식으로 처리할 수 있게 합니다. 이는 코딩 환경, 수학 문제 해결 환경, 도구 사용 환경 등 서로 다른 특성을 가진 환경들을 동일한 프레임워크 내에서 관리할 수 있게 하여 시스템의 확장성과 유지보수성을 크게 향상시킵니다.
강화학습은 지도 학습 파인튜닝보다 더 나은 토큰 효율성과 일반화 성능을 제공한다고 여겨집니다. K1.5 연구팀의 작업을 기반으로, K2에서는 작업 다양성과 훈련 FLOP 모두에서 강화학습을 확장했습니다. 이를 지원하기 위해 광범위한 시나리오에서 강화학습을 촉진하는 Gym과 유사한 확장 가능한 프레임워크를 개발했습니다.

이 프레임워크는 검증 가능한 보상을 가진 다수의 작업으로 확장되었습니다. 창작 글쓰기와 개방형 질문 답변과 같은 주관적 선호도에 의존하는 작업의 경우, 모델이 자신의 출력을 쌍별 비교를 통해 판단하는 자기 비판 보상을 도입했습니다. 이러한 접근법을 통해 다양한 도메인의 작업들이 모두 강화학습 패러다임의 혜택을 받을 수 있게 되었습니다.

### 검증 가능한 보상

검증 가능한 보상 시스템은 객관적으로 평가할 수 있는 작업들에 대해 명확하고 일관된 피드백을 제공합니다. 이 시스템은 수학, STEM, 논리적 추론, 복잡한 지시 따르기, 충실성, 코딩 및 소프트웨어 엔지니어링, 안전성 작업을 포괄하며, 각 도메인에 특화된 검증 메커니즘을 구축했습니다.

#### 수학, STEM 및 논리적 작업을 위한 Gym

수학, STEM, 논리적 추론 도메인에서 강화학습 데이터 준비는 다양한 커버리지와 적당한 난이도라는 두 가지 핵심 원칙을 따릅니다.

**다양한 커버리지**는 각 도메인 내에서 광범위한 작업 유형과 시나리오를 포함하여 모델이 특정 패턴에 과적합되지 않도록 보장합니다. 수학과 STEM 작업의 경우, 전문가 주석, 내부 QA 추출 파이프라인, 그리고 오픈 데이터셋을 조합하여 고품질 QA 쌍을 수집했습니다. 수집 과정에서 태깅 시스템을 활용하여 커버리지가 부족한 도메인의 범위를 의도적으로 증가시켰습니다.

논리적 작업의 경우, 데이터셋은 구조화된 데이터 작업(예: 다중 홉 테이블 추론, 교차 테이블 집계)과 논리 퍼즐(예: 24게임, 스도쿠, 수수께끼, 암호 산술, 모스 부호 해독)을 포함한 다양한 형식으로 구성됩니다.

**적당한 난이도**는 강화학습에서 효과적인 학습 신호를 제공하기 위한 핵심 요소입니다. 너무 쉬운 문제는 모델이 이미 잘 해결할 수 있어 학습 신호가 약하고, 너무 어려운 문제는 모델이 전혀 해결하지 못해 역시 학습 효과가 제한적입니다. 각 문제의 난이도는 SFT 모델의 pass@k 정확도를 사용하여 평가하고, 적당한 난이도를 가진 문제만을 선별했습니다.

#### 복잡한 지시 따르기

효과적인 지시 따르기는 명시적 제약 조건을 이해하는 것뿐만 아니라 암묵적 요구사항을 탐색하고, 엣지 케이스를 처리하며, 확장된 대화에서 일관성을 유지하는 것을 요구합니다. 이러한 도전과제를 자동화된 검증과 적대적 탐지를 결합한 하이브리드 검증 프레임워크와 확장 가능한 커리큘럼 생성 파이프라인을 통해 해결했습니다.

**하이브리드 규칙 검증**은 두 가지 검증 메커니즘을 구현합니다. 첫째, 검증 가능한 출력(예: 길이, 스타일 제약)을 가진 지시에 대해서는 코드 인터프리터를 통한 결정론적 평가를 수행합니다. 둘째, 제약 조건에 대한 미묘한 이해가 필요한 지시에 대해서는 LLM-as-judge 평가를 사용합니다.

모델이 실제 준수 없이 지시 이행을 주장하는 적대적 행동을 해결하기 위해, 이러한 기만적 주장을 특별히 탐지하는 추가적인 해킹 체크 레이어를 통합했습니다. 이는 모델이 표면적으로만 지시를 따르는 척하는 것을 방지하고 실질적인 준수를 보장합니다.

**다중 소스 지시 생성**은 포괄적인 커버리지를 보장하기 위해 세 가지 서로 다른 생성 전략을 사용합니다. 첫째, 데이터 팀이 개발한 전문가 제작 복잡 조건부 프롬프트와 루브릭을 활용합니다. 둘째, AutoIF에서 영감을 받은 에이전틱 지시 증강을 수행합니다. 셋째, 특정 실패 모드나 엣지 케이스를 탐사하는 추가 지시를 생성하는 데 특화된 파인튜닝된 모델을 사용합니다.

#### 충실성

충실성은 다중 턴 도구 사용, 자체 생성 추론 체인, 개방 환경 상호작용과 같은 시나리오에서 작동하는 에이전틱 모델에 필수적입니다. FACTS Grounding의 평가 프레임워크에서 영감을 받아, 자동화된 검증을 수행하는 문장 수준 충실성 판단 모델을 훈련했습니다.

이 판단 모델은 컨텍스트에서 뒷받침하는 증거 없이 사실적 주장을 하는 문장을 탐지하는 데 효과적입니다. 이는 보상 모델로 작동하여 전체적인 충실성 성능을 향상시키는 역할을 합니다. 모델이 주장하는 내용이 제공된 컨텍스트나 신뢰할 수 있는 소스에 의해 뒷받침되는지 확인함으로써, 환각이나 근거 없는 주장을 줄이고 신뢰할 수 있는 응답을 생성하도록 유도합니다.

#### 코딩 및 소프트웨어 엔지니어링

경쟁 수준의 프로그래밍 문제를 해결하는 능력을 향상시키기 위해, 오픈소스 데이터셋과 합성 소스 모두에서 문제와 그 판단기를 수집했습니다. 합성 데이터의 다양성과 보상 신호의 정확성을 보장하기 위해, 사전 훈련 데이터에서 검색한 고품질 인간 작성 단위 테스트를 통합했습니다.

소프트웨어 엔지니어링 작업의 경우, GitHub에서 대량의 풀 리퀘스트와 이슈를 수집하여 사용자 프롬프트/이슈와 실행 가능한 단위 테스트로 구성된 소프트웨어 개발 환경을 구축했습니다. 이 환경은 확장성과 보안을 위해 Kubernetes로 구동되는 견고한 샌드박스 인프라 위에 구축되었습니다.

![강화학습 인프라 아키텍처](https://arxiv.org/html/2507.20534/x14.png)

이 시스템은 안정적인 성능으로 10,000개 이상의 동시 샌드박스 인스턴스를 지원하여 경쟁적 코딩과 소프트웨어 엔지니어링 작업 모두에 이상적입니다. 실제 개발 환경을 시뮬레이션함으로써 모델이 현실적인 소프트웨어 개발 시나리오에서 학습할 수 있게 합니다.

#### 안전성

안전성 향상 작업은 폭력, 사기, 차별과 같은 주요 위험 카테고리를 포괄하는 인간이 큐레이션한 시드 프롬프트 세트로 시작됩니다. 정교한 탈옥 시도(예: 역할 연기, 문학적 서사, 학술적 담론)를 시뮬레이션하기 위해, 세 가지 핵심 구성 요소를 가진 자동화된 프롬프트 진화 파이프라인을 사용합니다.

**공격 모델**은 대상 LLM에서 안전하지 않은 응답을 유도하도록 설계된 적대적 프롬프트를 반복적으로 생성합니다. **대상 모델**은 이러한 프롬프트에 대한 응답을 생성하여 잠재적 취약점을 시뮬레이션합니다. **판단 모델**은 상호작용을 평가하여 적대적 프롬프트가 안전 메커니즘을 성공적으로 우회했는지 결정합니다.

각 상호작용은 작업별 루브릭을 사용하여 평가되며, 판단 모델이 이진 성공/실패 라벨을 제공할 수 있게 합니다. 이러한 체계적인 접근법을 통해 다양한 안전성 시나리오에서 모델의 견고성을 평가하고 개선할 수 있습니다.

### 검증을 넘어서: 자기 비판 루브릭 보상

검증 가능한 보상을 가진 작업을 넘어 모델 정렬을 확장하기 위해, 자기 비판 피드백을 통한 일반적인 강화학습 프레임워크를 도입했습니다. 이 접근법은 유용성, 창의성, 추론의 깊이, 사실성, 안전성을 포함한 미묘한 인간 선호도와 LLM을 정렬하도록 설계되었으며, 검증 가능한 시나리오에서 학습된 능력을 더 광범위한 주관적 작업으로 확장합니다.

프레임워크는 모델이 자신의 출력을 평가하여 선호도 신호를 생성하는 자기 비판 루브릭 보상 메커니즘을 사용하여 작동합니다. K2를 유능한 판단자로 부트스트랩하기 위해, 오픈소스와 사내 선호도 데이터셋의 혼합을 큐레이션하고 SFT 단계에서 비판 능력을 초기화했습니다.

#### 자기 비판 정책 최적화

학습 루프의 첫 번째 핵심 프로세스에서, K2 액터는 광범위한 사용 사례를 다루는 일반적인 프롬프트에 대한 응답을 생성합니다. 그 다음 K2 비판자는 루브릭의 조합에 대해 쌍별 평가를 수행하여 모든 결과를 순위를 매깁니다.

이 루브릭은 Kimi가 소중히 여기는 AI 어시스턴트의 기본 가치를 나타내는 핵심 루브릭, 보상 해킹을 제거하는 것을 목표로 하는 규범적 루브릭, 그리고 특정 지시적 맥락을 위해 데이터 팀이 제작한 인간 주석 루브릭을 포함합니다.

특정 루브릭이 필수로 지정될 수 있지만, K2는 이를 내부 사전과 비교하여 가중치를 부여할 수 있는 유연성을 유지합니다. 이러한 능력은 핵심 정체성과 일관성을 유지하면서 특정 지시에 적응하여, 진화하는 온-정책 행동과의 동적이고 지속적인 정렬을 가능하게 합니다.

#### 폐쇄 루프 비판자 개선 및 정렬

강화학습 훈련 중에 비판자 모델은 검증 가능한 신호를 사용하여 개선됩니다. 검증 가능한 보상 프롬프트에서 생성된 온-정책 롤아웃은 비판자를 지속적으로 업데이트하는 데 사용되며, 이는 RLVR에서 객관적 성능 신호를 평가 모델로 직접 증류하는 중요한 단계입니다.

이 전이 학습 프로세스는 더 주관적인 판단을 검증 가능한 데이터에 기반하여, 검증 가능한 작업에서의 성능 향상이 명시적 보상 신호가 없는 복잡한 작업에서 비판자의 판단을 향상시킬 수 있게 합니다.

이 폐쇄 루프 프로세스는 비판자가 정책의 진화와 보조를 맞춰 평가 기준을 지속적으로 재보정하도록 보장합니다. 주관적 평가를 검증 가능한 데이터에 기반함으로써, 프레임워크는 복잡하고 검증 불가능한 인간 목표와의 견고하고 확장 가능한 정렬을 가능하게 합니다.

결과적으로, 이러한 전체적인 정렬은 사용자 의도 이해, 창작 글쓰기, 복잡한 추론, 미묘한 언어 이해를 포함한 광범위한 도메인에서 포괄적인 성능 향상을 가져옵니다.

### 강화학습 알고리즘

K1.5에서 도입된 정책 최적화 알고리즘을 K2의 기반으로 채택했습니다. 각 문제 \\(x\\)에 대해, 이전 정책 \\(\pi_{\mathrm{old}}\\)에서 \\(K\\)개의 응답 \\(\{y_1, \ldots, y_k\}\\)를 샘플링하고, 다음 목적 함수에 대해 모델 \\(\pi_\theta\\)를 최적화합니다.

\\[L_{\mathrm{RL}}(\theta) = \mathbb{E}_{x \sim \mathcal{D}}\left[\frac{1}{K}\sum_{i=1}^{K}\left[\left(r(x,y_{i})-\bar{r}(x)-\tau\log\frac{\pi_{\theta}(y_{i}|x)}{\pi_{\mathrm{old}}(y_{i}|x)}\right)^{2}\right]\right]\\]

여기서 \\(\bar{r}(x) = \frac{1}{k}\sum_{i=1}^{k}r(x,y_{i})\\)는 샘플링된 응답의 평균 보상이고, \\(\tau > 0\\)은 안정적인 학습을 촉진하는 정규화 파라미터입니다. SFT에서와 마찬가지로, 이 목적 함수를 최소화하기 위해 Muon 옵티마이저를 사용합니다.

K2에서 강화학습 훈련을 더 광범위한 작업으로 확장함에 따라, 모든 도메인에서 일관된 성능 향상을 달성하는 것이 주요 도전과제입니다. 이를 해결하기 위해 강화학습 알고리즘에 여러 추가 사항을 도입했습니다.

#### 예산 제어

강화학습이 종종 모델 생성 응답의 길이를 상당히 증가시킨다는 것이 널리 관찰되었습니다. 더 긴 응답은 모델이 복잡한 추론 작업에서 향상된 성능을 위해 추가적인 테스트 시간 계산을 활용할 수 있게 하지만, 비추론 도메인에서는 그 이점이 추론 비용을 정당화하지 못하는 경우가 많습니다.

모델이 추론 예산을 적절히 분배하도록 장려하기 위해, 강화학습 훈련 전반에 걸쳐 샘플별 최대 토큰 예산을 시행합니다. 예산은 작업 유형에 따라 결정됩니다. 이 토큰 예산을 초과하는 응답은 잘리고 페널티가 부여되어, 모델이 지정된 제한 내에서 솔루션을 생성하도록 인센티브를 제공합니다.

경험적으로, 이 접근법은 모델의 토큰 효율성을 크게 향상시켜 모든 도메인에서 간결하면서도 효과적인 솔루션을 장려합니다.

#### PTX 손실

공동 강화학습 훈련 중 가치 있는 고품질 데이터의 잠재적 망각을 방지하기 위해, 손으로 선별된 고품질 샘플로 구성된 데이터셋을 큐레이션하고 보조 PTX 손실을 통해 강화학습 목적 함수에 통합했습니다.

이 전략은 고품질 데이터의 장점을 활용할 뿐만 아니라 훈련 체제에 명시적으로 존재하는 제한된 작업 세트에 과적합될 위험을 완화합니다. 이러한 증강은 더 광범위한 도메인에서 모델의 일반화를 상당히 개선합니다.

#### 온도 감쇠

창작 글쓰기와 복잡한 추론과 같은 작업에서, 훈련 초기 단계에서 높은 샘플링 온도를 통한 탐색 촉진이 중요하다는 것을 발견했습니다. 높은 온도는 모델이 다양하고 혁신적인 응답을 생성할 수 있게 하여 효과적인 전략의 발견을 촉진하고 차선책 솔루션으로의 조기 수렴 위험을 줄입니다.

그러나 훈련 후기 단계나 평가 중에 높은 온도를 유지하는 것은 과도한 무작위성을 도입하고 모델 출력의 신뢰성과 일관성을 손상시킬 수 있어 해로울 수 있습니다. 이를 해결하기 위해 훈련 전반에 걸쳐 탐색에서 활용으로 전환하는 온도 감쇠 스케줄을 사용합니다.

이 전략은 모델이 가장 유익할 때 탐색을 활용하면서 궁극적으로 안정적이고 고품질의 출력으로 수렴하도록 보장합니다.
강화학습 인프라는 Kimi K2의 대규모 강화학습 훈련을 지원하기 위한 핵심 시스템 구성 요소입니다. 이 인프라는 동기화된 강화학습 훈련을 위한 코로케이션 아키텍처, 효율적인 엔진 전환 메커니즘, 시스템 시작 최적화, 그리고 장기 에이전틱 작업을 위한 특화된 롤아웃 기법을 특징으로 합니다.

### 코로케이션 아키텍처

K1.5와 유사하게, 동기화된 강화학습 훈련을 위한 하이브리드 코로케이션 아키텍처를 채택했습니다. 이 아키텍처에서는 훈련 엔진과 추론 엔진이 동일한 워커에서 공존하며, 한 엔진이 활발히 작동할 때 다른 엔진은 GPU 자원을 해제하거나 오프로드하여 수용합니다.

강화학습 훈련의 각 반복에서, 중앙화된 컨트롤러가 먼저 추론 엔진을 호출하여 훈련을 위한 새로운 데이터를 생성합니다. 그 다음 훈련 엔진에 새로운 데이터에 대한 훈련을 알리고, 다음 반복을 위해 업데이트된 파라미터를 추론 엔진으로 전송합니다. 각 엔진은 처리량을 위해 크게 최적화되어 있습니다.

K2 모델의 규모로 확장됨에 따라, 엔진 전환과 장애 복구의 지연 시간이 상당해집니다. 이러한 측면에서 시스템 설계 고려사항을 제시합니다.

### 효율적인 엔진 전환

롤아웃 중에는 훈련 엔진의 파라미터가 DRAM으로 오프로드됩니다. 훈련 엔진을 가동하는 것은 H2D 전송의 간단한 단계입니다. 그러나 추론 엔진을 가동하는 것은 더 큰 도전과제입니다. 서로 다른 샤딩 패러다임을 가진 훈련 엔진에서 업데이트된 파라미터를 획득해야 하기 때문입니다.

![파라미터 업데이트를 위한 체크포인트 엔진 활용](https://arxiv.org/html/2507.20534/x14.png)

K2의 규모와 관련된 방대한 수의 디바이스를 고려할 때, 파라미터 재샤딩과 브로드캐스팅을 위해 네트워크 파일 시스템을 사용하는 것은 비현실적입니다. 오버헤드를 낮게 유지하는 데 필요한 총 대역폭은 초당 수 페타바이트에 달합니다.

이 도전과제를 해결하기 위해, 훈련 노드에 코로케이션된 분산 체크포인트 엔진을 개발하여 파라미터 상태를 관리합니다. 파라미터 업데이트를 수행하기 위해, 각 체크포인트 엔진 워커는 훈련 엔진에서 파라미터의 로컬 복사본을 획득한 다음, 모든 체크포인트 엔진 워커에 걸쳐 전체 파라미터 세트를 브로드캐스트합니다. 이후 추론 엔진은 체크포인트 엔진에서 필요한 파라미터 샤드만을 검색합니다.

1T 모델에 대해 이를 가능하게 하기 위해, 업데이트는 메모리 풋프린트를 최소화하는 파이프라인 방식으로 파라미터별로 수행됩니다. 각 추론 워커의 특정 샤딩 스킴에 관계없이 전체 클러스터에 걸쳐 전체 파라미터 세트를 브로드캐스트하기로 선택했습니다. 이는 이론적으로 최적인 접근법보다 몇 배 더 많은 데이터를 전송하지만, 훈련 엔진과 추론 엔진에 덜 침입적인 더 간단한 시스템 설계를 제공합니다.

동기화 오버헤드 감소와 더 높은 네트워크 대역폭 활용으로 인해 이 접근법이 필요한 것만 전송하는 방법보다 성능이 우수합니다. 시스템은 Kimi K2에 대한 전체 파라미터 업데이트를 30초 미만에 완료할 수 있으며, 이는 일반적인 강화학습 훈련 반복에서 무시할 수 있는 지속 시간입니다.

### 효율적인 시스템 시작

대규모 훈련은 시스템 장애가 발생하기 쉽기 때문에, Kimi K2와 같은 대형 모델에서는 시작 시간 최적화가 중요합니다. 훈련 엔진을 시작하기 위해, 각 훈련 워커가 디스크에서 파라미터의 일부 또는 전부를 선택적으로 읽고, 필요한 파라미터를 동료들에게 브로드캐스트하도록 합니다. 설계 목표는 모든 워커가 체크포인트를 집합적으로 한 번만 읽도록 보장하여 비용이 많이 드는 디스크 IO를 최소화하는 것입니다.

추론 엔진은 독립적인 복제본이므로, 이들 간에 추가적인 동기화 장벽을 도입하는 것을 피하고자 합니다. 따라서 시작을 위해 체크포인트 엔진을 재사용하기로 선택했습니다. 체크포인트 엔진이 훈련 엔진이 시작하는 방식과 유사하게 디스크에서 체크포인트를 집합적으로 읽도록 하고, 이전 섹션에서 소개한 접근법을 사용하여 초기화되지 않은 추론 엔진의 상태를 업데이트합니다.

전용 체크포인트 엔진을 활용함으로써, 추론 복제본이 다른 복제본과 통신하지 않고도 재시작할 수 있기 때문에 시스템이 단일 지점 장애에 대해서도 견고해집니다.

### 에이전틱 롤아웃

강화학습 인프라는 장기 다중 턴 에이전틱 작업의 훈련을 지원합니다. 롤아웃 중에 이러한 작업은 복잡한 환경 상호작용과 연장된 롤아웃 지속 시간과 같은 독특한 도전과제를 제시합니다. 이러한 문제를 완화하기 위한 몇 가지 최적화를 소개합니다.

환경의 다양성으로 인해, 특정 상호작용이 환경 피드백(예: 가상 머신이나 코드 인터프리터)을 기다리는 동안 차단되어 GPU가 유휴 상태가 될 수 있습니다. GPU 활용률을 최대화하기 위해 두 가지 전략을 사용합니다. 첫째, 무거운 환경을 더 쉽게 확장할 수 있는 전용 서비스로 배포합니다. 둘째, 특정 비용이 많이 드는 상호작용으로 인한 지연 시간을 상쇄하기 위해 많은 수의 동시 롤아웃을 사용합니다.

에이전틱 롤아웃의 또 다른 도전과제는 개별 롤아웃 궤적이 극도로 길 수 있다는 것입니다. 긴 꼬리 궤적이 전체 롤아웃 프로세스를 차단하는 것을 방지하기 위해, 부분 롤아웃 기법을 사용합니다. 이 전략은 긴 꼬리의 미완료 작업을 일시 중지하고 다음 강화학습 반복에서 재개할 수 있게 합니다.

연구 효율성을 향상시키기 위해, 새로운 환경의 통합을 간소화하는 OpenAI Gym 프레임워크에서 영감을 받은 통합 인터페이스도 설계했습니다. 향후 더 다양한 상호작용 환경으로 강화학습 인프라를 확장하기를 희망합니다.
# 평가

이 섹션에서는 Kimi-K2-Instruct의 후훈련 평가로 시작하여 Kimi-K2-Base의 능력에 대한 간략한 개요를 제공하고, 포괄적인 안전성 평가로 마무리합니다.

## 후훈련 평가

### 평가 설정

#### 벤치마크

Kimi-K2-Instruct는 다양한 영역에 걸쳐 평가되었습니다. 코딩 분야에서는 LiveCodeBench v6(2024년 8월부터 2025년 5월까지의 문제), OJBench, MultiPL-E, SWE-bench Verified, TerminalBench, Multi-SWE-bench, SWE-Lancer, PaperBench, Aider-Polyglot을 채택했습니다.

도구 사용 작업의 경우, 다중 턴 도구 호출 능력을 강조하는 \\(\tau^{2}\\)-Bench와 AceBench에서 성능을 평가했습니다. 추론 영역에서는 수학, 과학, 논리적 작업의 광범위한 범위를 포함했습니다. AIME 2024/2025, MATH-500, HMMT 2025, CNMO 2024, PolyMath-en, ZebraLogic, AutoLogi, GPQA-Diamond, SuperGPQA, Humanity's Last Exam(텍스트 전용)입니다.

장기 컨텍스트 능력은 장기 컨텍스트 검색을 위한 MRCR과 장기 컨텍스트 추론을 위한 DROP, FRAMES, LongBench v2에서 벤치마킹했습니다. 사실성의 경우 FACTS Grounding, Vectara Hallucination Leaderboard, FaithJudge를 평가했습니다. 마지막으로 일반적인 능력은 MMLU, MMLU-Redux, MMLU-Pro, IFEval, Multi-Challenge, SimpleQA, LiveBench(2024-11-25 기준)를 사용하여 평가했습니다.

#### 기준선

오픈소스와 독점 최첨단 모델 모두를 대상으로 벤치마킹했으며, 모든 후보는 테스트 시간 계산으로부터의 추가적인 이득을 제거하기 위해 비사고형 구성에서 평가되었습니다. 오픈소스 기준선으로는 DeepSeek-V3-0324와 Qwen3-235B-A22B가 있으며, 후자는 벤더가 권장하는 비사고형 체제에서 실행되었습니다. 독점 기준선으로는 Claude Sonnet 4, Claude Opus 4, GPT-4.1, Gemini 2.5 Flash Preview(2025-05-20)가 있으며, 각각 통합된 온도와 top-p 설정 하에서 공식 API를 통해 해당 비사고형 모드로 호출되었습니다.

#### 평가 구성

모든 실행은 비사고형 모드에서 모델을 쿼리합니다. 출력 토큰 길이는 SWE-bench Verified(Agentless)를 제외하고 모든 곳에서 8192 토큰으로 제한되며, 이는 16384로 증가됩니다. 문제당 분산이 높은 벤치마크의 경우, 안정적인 점수를 얻기 위해 \\(k\\)번 반복 샘플링을 채택하고 결과를 평균화하여 Avg@k로 표시합니다.

장기 컨텍스트 작업의 경우, 평가 중 컨텍스트 윈도우 크기를 128K 토큰으로 설정하고, 이 제한을 초과하는 입력은 윈도우 내에 맞도록 잘라냅니다. SWE-bench Verified는 두 가지 모드로 평가됩니다. 테스트 없는 단일 패치를 통한 에이전트리스 코딩(Acc)과 내부 검증기를 사용한 best-of-N 선택으로 단일 시도(Acc)와 다중 시도(Acc) 모두에서 bash/editor 도구를 통한 에이전틱 코딩입니다. SWE-bench Multilingual은 단일 시도 에이전틱 설정에서만 테스트됩니다. 일부 데이터 포인트는 평가 비용이 과도하게 높아 생략되었습니다.

![Kimi-K2-Instruct와 주요 오픈소스 및 독점 모델 간의 성능 비교](https://arxiv.org/html/2507.20534v1#S4.T3)

위 표는 다양한 작업에 걸친 Kimi-K2-Instruct와 주요 오픈소스 및 독점 모델 간의 성능 비교를 보여줍니다. 굵은 글씨는 전역 최첨단을 나타내고, 밑줄 친 굵은 글씨는 최고의 오픈소스 결과를 나타냅니다. *로 표시된 데이터 포인트는 모델의 기술 보고서나 블로그에서 직접 가져온 것입니다.

### 평가 결과

Kimi-K2-Instruct의 포괄적인 평가 결과가 표에 제시되어 있으며, 자세한 설명은 부록 C에서 제공됩니다. 아래에서는 네 가지 핵심 도메인에서의 주요 결과를 강조합니다.

#### 에이전틱 및 경쟁 코딩

Kimi-K2-Instruct는 실세계 SWE 작업에서 최첨단 오픈소스 성능을 보여줍니다. SWE-bench Verified(65.8%, 다중 시도 시 71.6%), SWE-bench Multilingual(47.3%), SWE-lancer(39.1%)에서 대부분의 기준선을 능가하며, Claude 4 Opus 및 Sonnet과의 격차를 상당히 줄였습니다. 경쟁 코딩 벤치마크(예: LiveCodeBench v6 53.7%, OJBench 27.1%)에서도 모든 모델 중 선두를 차지하여 난이도 수준에 걸친 실용적인 코딩 숙련도를 강조합니다.

#### 에이전틱 도구 사용

다중 턴 도구 사용 벤치마크에서 Kimi-K2-Instruct는 새로운 표준을 설정합니다. \\(\tau^{2}\\)-Bench에서 66.1 Pass@1, ACEBench에서 76.5를 달성하여 모든 기준선을 상당히 능가합니다. 이러한 결과는 도메인에 걸친 기반이 있고 제어되며 에이전트 주도적인 도구 오케스트레이션에서의 강점을 확인합니다.

#### 일반적인 능력

Kimi-K2-Instruct는 일반 지식, 수학, 지시 따르기, 장기 컨텍스트 작업에서 강력하고 균형 잡힌 성능을 보입니다. SimpleQA(31.0%), MMLU(89.5%), MMLU-Redux(92.7%)에서 오픈소스 동료들을 능가하고, 지시 벤치마크(IFEval: 89.8%, Multi-Challenge: 54.1%)에서 모든 모델을 선도합니다. 수학과 STEM에서는 최고 수준의 점수(AIME 2024: 69.6%, GPQA-Diamond: 75.1%)를 달성하고, 장기 컨텍스트 사실성과 검색(DROP: 93.5%, MRCR: 55.0%)에서 경쟁력을 유지합니다. 이러한 결과는 Kimi-K2-Instruct를 단기 및 장기 컨텍스트 설정 모두에서 균형 잡히고 유능한 일반주의자로 위치시킵니다.

#### 개방형 평가

LMSYS Arena 리더보드(2025년 7월 17일)에서 Kimi-K2-Instruct는 3,000명 이상의 사용자 투표를 바탕으로 오픈소스 모델 중 1위, 전체 5위를 기록했습니다. 다양하고 블라인드 프롬프트에 걸친 이러한 실세계 선호도 신호는 개방형 작업에서 고품질 응답을 생성하는 Kimi-K2의 강점을 강조합니다.

## 사전 훈련 평가

### 평가 설정

#### 벤치마크

다양한 능력 영역에 걸쳐 Kimi-K2-Base를 평가했습니다. 일반적인 능력의 경우, MMLU, MMLU-Pro, MMLU-Redux, BBH, TriviaQA, SuperGPQA, SimpleQA, HellaSwag, AGIEval, GPQA-Diamond, ARC-Challenge, WinoGrande에서 평가했습니다.

코딩 능력의 경우, EvalPlus(HumanEval, MBPP, HumanEval+, MBPP+의 평균), LiveCodeBench v6, CRUXEval을 사용했습니다. 수학적 추론의 경우, GSM8K, GSM8K-Platinum, MATH, CMATH를 활용했습니다. 중국어 능력의 경우, C-Eval, CMMLU, CSimpleQA에서 평가했습니다.

#### 기준선

주요 오픈소스 파운데이션 모델들과 벤치마킹했습니다. DeepSeek-V3-Base, Qwen2.5-72B-Base(Qwen3-235B-A22B-Base는 오픈소스가 아니며, Qwen 시리즈에서 가장 큰 오픈소스 기본 모델은 Qwen2.5-72B-Base입니다), Llama 4-Maverick(Llama 4-Behemoth도 오픈소스가 아닙니다). 모든 모델은 공정한 비교를 보장하기 위해 동일한 구성에서 평가되었습니다.

#### 평가 구성

MMLU, MMLU-Redux, GPQA-Diamond, HellaSwag, ARC-Challenge, C-Eval, CMMLU에 대해서는 퍼플렉시티 기반 평가를 사용했습니다. MMLU-Pro, SuperGPQA, TriviaQA, BBH, CSimpleQA, MATH, CMATH, GSM8K, GSM8K-Platinum, CRUXEval, LiveCodeBench, EvalPlus에 대해서는 생성 기반 평가를 사용했습니다. GPQA-Diamond에 내재된 높은 분산을 완화하기 위해 8번의 독립적인 실행에 걸친 평균 점수를 보고합니다. 모든 평가는 모든 모델에서 일관된 설정을 보장하기 위해 LM-Harness-Evaluation에서 파생된 내부 프레임워크를 사용하여 수행되었습니다.

### 평가 결과

다양한 평가 벤치마크에 걸친 Kimi-K2-Base와 주요 오픈소스 파운데이션 모델 간의 포괄적인 비교가 표에 제시되어 있습니다. 결과는 Kimi-K2-Base가 평가된 작업의 대부분에서 최첨단 성능을 달성하여 오픈소스 환경에서 선도적인 파운데이션 모델로 자리매김한다는 것을 보여줍니다.

#### 일반 언어 이해

Kimi-K2-Base는 12개의 영어 벤치마크 중 10개에서 최첨단 성능을 달성합니다. 주목할 만한 결과로는 MMLU(87.79%), MMLU-Pro(69.17%), MMLU-Redux(90.17%), SuperGPQA(44.67%), SimpleQA(35.25%)가 있으며, 모든 기준선을 상당히 능가합니다.

#### 코딩 능력

코딩 벤치마크에서 Kimi-K2-Base는 모든 메트릭에서 선도적인 성능으로 새로운 표준을 설정합니다. CRUXEval-I-cot에서 74.00%, CRUXEval-O-cot에서 83.50%, LiveCodeBench v6에서 26.29%, EvalPlus에서 80.33%를 달성하여 특히 단계별 추론이 필요한 시나리오에서 뛰어난 코드 생성 및 이해 능력을 보여줍니다.

#### 수학적 추론

Kimi-K2-Base는 뛰어난 수학적 능력을 보이며, 4개 벤치마크 중 3개에서 선도합니다. MATH(70.22%), GSM8K(92.12%), GSM8K-Platinum(94.21%). CMATH(90.26%)에서는 DeepSeek-V3-Base(90.53%)에 근소하게 뒤처지지만 경쟁력 있는 성능을 유지합니다. 이러한 결과는 다양한 난이도 수준에서 모델의 견고한 수학적 문제 해결 능력을 강조합니다.

#### 중국어 이해

모델은 뛰어난 다국어 능력을 보여주며, 모든 중국어 벤치마크에서 최첨단 결과를 달성합니다. C-Eval(92.50%), CMMLU(90.90%), CSimpleQA(77.57%). 이러한 결과는 Kimi-K2-Base를 다른 언어에서 강력한 성능을 유지하면서 중국어 이해를 위한 선도적인 모델로 확립합니다.

## 안전성 평가

### 실험 설정

다른 오픈소스 LLM들과 비교하여 Kimi K2에 대한 레드팀 평가를 수행했습니다. 평가는 유해한 콘텐츠, 개인정보 콘텐츠, 보안 콘텐츠를 포함한 다양한 공격 시나리오와 프롬프트 주입 및 반복적 탈옥과 같은 다양한 공격 전략을 다뤘습니다. 적대적 프롬프트를 생성하고 응답을 분석하기 위해 Promptfoo를 선택했습니다. 이를 통해 확장 가능한 방식으로 모델을 평가할 수 있습니다.

#### 모델 선택

Kimi K2를 세 개의 다른 오픈소스 LLM과 비교했습니다. DeepSeek-V3, DeepSeek-R1, Qwen3입니다.

#### Promptfoo 설정

평가된 플러그인과 전략이 표에 나열되어 있으며, 각 플러그인은 모든 전략과 쌍을 이루어 성능을 평가합니다.

대규모 언어 모델 추론의 고유한 비결정성을 고려하여, 단일 패스 출력은 변동성을 보일 수 있습니다. 이를 고려하기 위해 각 전략에 대해 플러그인당 3개의 공격 프롬프트를 생성했습니다.

#### 프롬프트 언어 설정

각 플러그인-전략 조합에 대한 언어 호환성을 사전 테스트했습니다. 일부 플러그인은 영어와 중국어를 모두 지원하는 반면, 다른 플러그인은 영어만 지원합니다. 둘 다 지원하는 조합의 경우, 각 언어로 3개의 프롬프트를 생성하여 조합당 총 6개의 프롬프트를 만들었습니다.

#### 수동 검토

평가 과정에 인간 검토를 통합했습니다. 주관성 문제를 최소화하기 위해 여러 라운드의 검토를 수행하고 일관성을 보장하고 판단의 변동성을 줄이기 위해 동일한 검토자가 주어진 테스트 세트 내의 모든 사례를 평가하도록 했습니다.

### 안전성 평가 결과

다양한 플러그인-전략 조합 하에서 서로 다른 모델의 통과율이 표에 제시되어 있습니다.

특정 평가 시나리오에 대한 목표화된 최적화 없이, 일부 복잡한 사례(예: Harmful-Iterative Jailbreak)의 통과율은 다른 모델에 비해 상대적으로 높았습니다. 서로 다른 공격 전략에서 모델들은 다양한 경향을 보였습니다. Base64 전략 하에서 통과율은 일반적으로 100%에 접근하거나 도달했으며, 이는 인코딩 변환이 모델의 기본 견고성에 최소한의 영향을 미쳤음을 시사합니다. 반대로 Crescendo 전략은 통과율의 일반적인 하락을 가져왔으며, 이는 더 강한 적대적 효과를 나타냅니다.

또한 복잡한 공격 전략이 항상 기본 프롬프트를 능가하지는 않습니다. 일부 원래 적대적 프롬프트는 여러 라운드의 변환 후 의도된 의미를 잃을 수 있어, 결과적인 모델 출력이 덜 의미 있게 됩니다.

#### 자동화된 레드팀의 한계

인간 검토의 개입으로 인해 평가 결과는 불가피하게 어느 정도의 주관성을 포함합니다. 또한 특정 플러그인 유형은 API 오용이나 외부 도구 호출을 포함하며, 이는 도구 호출 능력을 가진 에이전트 모델을 평가하는 데 더 적합합니다. 기본 LLM의 맥락에서 이러한 테스트는 제한적인 관련성을 가질 수 있습니다.
## 한계점

Kimi K2 모델의 내부 테스트를 통해 현재 버전에서 몇 가지 한계점이 확인되었습니다. 이러한 한계점들은 모델의 실제 적용 과정에서 나타나는 실질적인 문제들로, 향후 개선이 필요한 영역들을 명확히 보여줍니다.

### 어려운 추론 작업에서의 토큰 생성 문제

복잡한 추론 작업이나 명확하지 않은 도구 정의를 다룰 때 모델이 과도한 토큰을 생성하는 경향이 관찰되었습니다. 이는 특히 다단계 추론이 필요한 수학 문제나 복잡한 논리적 추론 과제에서 두드러지게 나타납니다. 모델이 문제를 해결하기 위해 필요 이상으로 긴 설명이나 중복적인 추론 과정을 생성하면서, 결과적으로 출력이 잘리거나 불완전한 도구 호출로 이어지는 경우가 발생합니다.

이러한 문제는 모델이 복잡한 문제에 직면했을 때 확신을 얻기 위해 더 많은 계산 자원을 사용하려는 경향과 관련이 있습니다. 하지만 이는 실제 사용 환경에서 응답 시간 지연과 계산 비용 증가를 초래할 수 있어 실용성을 저해하는 요소로 작용합니다.

### 불필요한 도구 사용으로 인한 성능 저하

특정 작업에서 도구 사용이 불필요하게 활성화될 경우 성능이 저하되는 현상이 확인되었습니다. 이는 모델이 단순한 텍스트 생성이나 기본적인 질문 답변으로 충분히 해결할 수 있는 작업에서도 복잡한 도구 호출 과정을 거치려 하면서 발생하는 문제입니다.

이러한 현상은 모델이 에이전틱 능력을 과도하게 활용하려는 경향에서 비롯되며, 작업의 복잡도와 필요한 도구 사용 수준을 정확히 판단하지 못하는 한계를 보여줍니다. 결과적으로 간단한 작업에서도 불필요한 계산 오버헤드가 발생하고, 응답의 정확성이나 효율성이 떨어질 수 있습니다.

### 소프트웨어 프로젝트 구축에서의 원샷 프롬프팅 한계

완전한 소프트웨어 프로젝트를 구축할 때 원샷 프롬프팅의 성공률이 에이전틱 코딩 프레임워크 하에서 K2를 사용하는 것만큼 좋지 않다는 점이 확인되었습니다. 이는 복잡한 소프트웨어 개발 작업에서 단일 프롬프트로는 모든 요구사항을 충족하는 완성도 높은 결과를 얻기 어렵다는 것을 의미합니다.

소프트웨어 프로젝트는 일반적으로 여러 파일, 다양한 기능, 복잡한 의존성 관계를 포함하므로, 단일 상호작용으로는 모든 측면을 고려한 완전한 구현을 생성하기 어렵습니다. 반면 에이전틱 프레임워크에서는 반복적인 개선과 다단계 검증 과정을 통해 더 나은 결과를 달성할 수 있습니다.

### 향후 개선 방향

연구팀은 이러한 한계점들을 해결하기 위해 지속적으로 노력하고 있으며, 향후 릴리스에서 개선된 버전을 제공할 예정입니다. 특히 토큰 생성 효율성 개선, 작업별 도구 사용 최적화, 그리고 복잡한 소프트웨어 개발 작업에서의 원샷 성능 향상에 중점을 두고 있습니다.

또한 사용자 커뮤니티로부터의 피드백을 적극적으로 수집하여 실제 사용 환경에서 발생하는 다양한 문제들을 파악하고 해결책을 모색하고 있습니다. 이러한 지속적인 개선 노력을 통해 Kimi K2가 더욱 실용적이고 효율적인 에이전틱 인텔리전스 모델로 발전할 수 있을 것으로 기대됩니다.

## 결론

Kimi K2는 에이전틱 인텔리전스를 위해 구축된 1조 파라미터 규모의 오픈 웨이트 Mixture-of-Experts 모델입니다. 이 연구에서는 토큰 효율적인 MuonClip 옵티마이저와 15.5조 토큰의 고품질 데이터셋을 활용하여 안정적이고 확장 가능한 사전 훈련을 달성했습니다.

후훈련 과정에서는 대규모 합성 도구 사용 데이터와 검증 가능한 보상 및 자기 비판 피드백을 모두 활용하는 통합 강화학습 프레임워크를 결합했습니다. 이러한 혁신적인 접근법을 통해 Kimi K2는 에이전틱 및 추론 벤치마크에서 새로운 최첨단 성능을 달성하며, 현재까지 가장 유능한 오픈 웨이트 대규모 언어 모델로 자리매김했습니다.

특히 Tau2-Bench에서 66.1점, ACEBench(En)에서 76.5점, SWE-Bench Verified에서 65.8점, Multi-SWE-bench에서 47.3점을 기록하여 비사고형 설정에서 대부분의 오픈소스 및 클로즈드소스 기준선을 능가했습니다. 또한 코딩, 수학, 추론 작업에서도 LiveCodeBench v6에서 53.7점, AIME 2025에서 49.5점, GPQA-Diamond에서 75.1점, OJBench에서 27.1점을 달성하여 광범위한 영역에서의 뛰어난 능력을 입증했습니다.

이러한 성과는 확장된 사고 과정 없이 달성된 것으로, 모델의 기본적인 추론 능력과 에이전틱 지능의 우수성을 보여줍니다. Kimi K2는 특히 소프트웨어 엔지니어링과 에이전틱 작업에서 탁월한 성능을 보이며, 실세계 응용에서의 실용성을 크게 향상시켰습니다.

## 감사의 말

SWE-bench Verified와 Multi-SWE-bench 실험 결과 평가에 도움을 준 OpenHands와 Multi-SWE-bench 팀의 귀중한 지원에 감사드립니다.
# 부록

이 부록에서는 Kimi K2 연구의 세부적인 기술적 내용과 보완 자료들을 제공합니다. 주요 내용으로는 연구 기여자 목록, 도구 호출을 위한 토큰 템플릿, 평가 세부사항, QK-Clip의 모델 품질에 대한 영향 분석, Muon의 로짓 폭발 취약성에 대한 이론적 설명, 강화학습을 위한 비판자 루브릭, 그리고 강화학습 훈련을 위한 엔진 전환 파이프라인이 포함됩니다.

## 기여자 목록

Kimi K2 연구에는 다양한 분야의 전문가들이 참여했으며, 모든 저자는 성(last name)의 알파벳 순서로 나열되어 있습니다. 별표(*)로 표시된 이름은 현재 팀에 소속되지 않은 인원을 나타냅니다. 이 연구는 대규모 언어 모델 개발의 복잡성과 다학제적 특성을 반영하여 광범위한 협력을 통해 수행되었습니다.

## 도구 호출을 위한 토큰 템플릿

Kimi K2의 도구 호출 시스템은 세 가지 핵심 구성 요소로 이루어진 토큰 구조를 사용합니다. 이는 도구 선언 메시지, 어시스턴트 메시지의 도구 호출 섹션, 그리고 도구 결과 메시지입니다.

### 도구 선언 메시지

도구 선언 메시지의 원시 토큰은 다음과 같은 형식으로 구성됩니다.

```
<|im_begin|> tool_declare <|im_middle|>
# Tools
{{ tool declaration content }}
<|im_end|>
```

파란색으로 강조된 부분은 특수 토큰을 나타내며, 녹색 부분인 괄호로 둘러싸인 부분은 도구 선언 내용입니다. 도구 선언 내용을 표현하기 위해 TypeScript를 사용하는데, 이는 간결한 언어이면서도 포괄적인 타입 시스템을 갖추고 있어 도구 파라미터의 타입과 제약 조건을 간단한 텍스트로 표현할 수 있기 때문입니다.

OpenAI의 채팅 완성 API와 호환되는 JSON 형식의 두 가지 간단한 도구 예제와 비교해보면, TypeScript로 정의된 동일한 도구들이 훨씬 더 간결함을 알 수 있습니다. 호환성을 향상시키기 위해 훈련 데이터의 일부에서는 JSON을 도구 선언 언어로 사용하여, 제3자 프레임워크가 추가 개발 없이도 도구 호출 스킴을 지원할 수 있도록 했습니다.

### 도구 호출 섹션

모델 응답 메시지의 도구 호출 섹션에 대한 토큰 템플릿은 다음과 같습니다.

```
<tool_call_section_begin|>
<|tool_call_begin|>
// call_id part
functions.{{tool name}}:{{counter}}
<|tool_arguments_begin|>
{{ json serialized call arguments }}
<|tool_call_end|>
<|tool_call_begin|>
// more tool calls
<|tool_call_end|>
<|tool_call_section_end|>
```

템플릿에서 보듯이, 단일 응답 턴에 여러 도구 호출을 배치하여 병렬 도구 호출을 지원합니다. 각 도구 호출은 고유한 호출 ID를 가지며, 이는 functions.{tool-name}:{counter} 형식으로 구성됩니다. 여기서 tool-name은 도구의 이름이고, counter는 대화에서 0부터 시작하는 모든 도구 호출의 자동 증가 카운터입니다.

추론 중에 모델이 때때로 예상치 못한 토큰을 생성하여 도구 호출을 파싱할 때 형식 오류가 발생할 수 있습니다. 이 문제를 해결하기 위해 lm-format-enforcer에서 영감을 받은 enforcer라는 제약 디코딩 모듈을 개발했습니다. `<tool_call_section_begin|>` 토큰이 생성되면, 이후의 도구 관련 토큰들이 사전 정의된 템플릿을 따르도록 보장하고, JSON 인수 문자열이 선언된 스키마를 따르도록 합니다.

### 도구 결과 메시지

도구 결과 메시지는 도구의 호출 ID와 해당 결과로 인코딩된 간단한 텍스트 메시지입니다.

```
<|im_begin|> tool <|im_middle|>
## Results of {{call_id}}
{{ execution result content }}
<|im_end|>
```

이러한 구조화된 토큰 템플릿을 통해 Kimi K2는 복잡한 다중 턴 도구 사용 시나리오에서도 일관되고 신뢰할 수 있는 도구 호출을 수행할 수 있습니다.

## QK-Clip이 모델 품질에 미치는 영향 분석

QK-Clip 설계는 최소 개입 원칙을 따릅니다. 즉, 필요할 때만 활성화되고 훈련이 안정화된 후에는 비활성화됩니다. 경험적 증거와 분석 결과는 모델 품질에 미치는 영향이 무시할 수 있을 정도임을 보여줍니다.

### 소규모 절제 실험

공격적인 임계값(τ = 30)을 사용하여 Muon에 QK-Clip을 적용한 소규모 설정에서의 검증 손실 비교를 통해, QK-Clip이 손실에 미치는 영향이 무시할 수 있을 정도임을 확인했습니다. 이는 어텐션 로짓을 제한하는 안전하고 효과적인 방법임을 나타냅니다.

0.5B 활성화 파라미터와 3B 총 파라미터를 가진 두 개의 소규모 MoE 모델을 훈련했습니다. 하나는 바닐라 Muon을 사용하고, 다른 하나는 낮은 클리핑 임계값(τ = 30)을 사용하는 MuonClip을 사용했습니다. 결과적으로 MuonClip을 적용해도 손실 곡선에 무시할 수 있을 정도의 영향만 있었으며, 이는 공격적인 클리핑도 MuonClip에서 수렴이나 훈련 역학을 손상시키지 않음을 나타냅니다.

또한 하위 작업에 대한 평가에서도 성능의 통계적으로 유의한 저하가 없음을 확인했습니다. 이러한 결과들은 MuonClip이 모델 품질을 손상시키지 않으면서 어텐션 로짓을 제한하는 안전하고 효과적인 방법임을 종합적으로 보여줍니다.

### 자기 비활성화

Kimi K2에서 QK-Clip은 일시적으로만 활성화되었습니다.

- 초기 70,000 스텝: 어텐션 헤드의 12.7%가 적어도 한 번은 QK-Clip을 트리거하여 S_max를 100으로 클램핑했습니다.
- 70,000 스텝 이후: 모든 헤드가 어느 시점에서 S_max를 100 아래로 줄여 QK-Clip을 비활성화했습니다.

QK-Clip이 활성화될 때는 다른 헤드에 대한 잠재적 과정규화를 최소화하기 위해 헤드별로(레이어별이 아닌) 적용됩니다. 훈련이 안정화된 후에는 QK-Clip이 비활성화되어 전혀 영향을 주지 않습니다.

## Muon이 로짓 폭발에 더 취약한 이유

로짓 폭발은 최대 사전 소프트맥스 어텐션 점수 \\(S_{max} = \max_{i,j}(q_i^{\vphantom{\top}} \cdot k_j)\\)가 훈련 중 무제한으로 증가할 때 발생합니다. \\(|q_i \cdot k_j| \leq \|q_i\| \|k_j\| \leq \|x_i\| \|x_j\| \|\mathbf{W}_q\| \|\mathbf{W}_k\|\\)이고 RMS-Norm이 \\(\|x_i\| \|x_j\|\\)를 제한된 상태로 유지하므로, 이 현상은 주로 \\(\mathbf{W}_q\\) 또는 \\(\mathbf{W}_k\\)의 증가하는 스펙트럼 노름에 의해 주도됩니다.

경험적으로 Muon이 로짓 폭발에 더 취약하다는 것을 발견했습니다. 이에 대한 가설을 아래에 제시합니다.

### 업데이트의 구조적 차이

Muon은 msign 연산에서 나오는 가중치 업데이트를 생성하므로, 업데이트 행렬의 모든 특이값이 동일하여 효과적인 랭크가 전체입니다. 반면, Adam이 생성하는 일반적인 업데이트 행렬은 편향된 스펙트럼을 보입니다. 즉, 몇 개의 큰 특이값이 지배적이고 효과적인 랭크가 낮습니다. Adam에 대한 이러한 낮은 랭크 가정은 새로운 것이 아니며, 고차 muP도 동일한 가정을 합니다.

이러한 현상은 16B Moonlight 모델에서 검증되었으며, Muon으로 훈련된 가중치가 Adam으로 훈련된 것보다 더 높은 특이값 엔트로피(즉, 더 높은 효과적인 랭크)를 보여 이론적 직관을 뒷받침합니다.

### SVD 공식화

단계 \\(t-1\\)에서 파라미터 행렬이 특이값 분해 \\(\mathbf{W}_{t-1} = \sum_i \sigma_i u_i v_i^{\top}\\)를 가진다고 하겠습니다. 업데이트 행렬을 \\(\Delta\mathbf{W}_t = \sum_j \bar{\sigma} \bar{u}_j \bar{v}_j^{\top}\\)로 쓸 수 있습니다.

다음 파라미터 업데이트는 \\(\mathbf{W}_t \leftarrow \sum_i \sigma_i u_i v_i^{\top} + \sum_j \bar{\sigma} \bar{u}_j \bar{v}_j^{\top}\\)입니다.

Muon에서는 가중치와 업데이트 모두 Adam보다 높은 효과적인 랭크를 가지므로, 특이벡터 쌍 \\(u_i v_i^{\top}\\)가 \\(\bar{u}_j \bar{v}_j^{\top}\\)와 정렬될 확률이 더 높다고 가설을 세웁니다. 이는 \\(\mathbf{W}_t\\)의 해당 특이값이 가산적으로 증가하게 할 수 있습니다.

### 어텐션 특화 증폭

어텐션 로짓은 이중선형 형태 \\(q_i \cdot k_j = (x_i \mathbf{W}_q) \cdot (x_j \mathbf{W}_k)\\)를 통해 계산됩니다. 곱 \\(\mathbf{W}_q \mathbf{W}_k^{\top}\\)는 스펙트럼 노름을 제곱하므로, 어느 행렬에서든 특이값 증가가 복합됩니다. Muon의 특이값 확대 경향은 따라서 로짓 폭발의 더 높은 위험으로 이어집니다.

이러한 이론적 분석을 통해 Muon이 Adam에 비해 어텐션 로짓 폭발에 더 취약한 이유를 이해할 수 있으며, 이는 QK-Clip과 같은 안정화 메커니즘의 필요성을 뒷받침합니다.
## 일반적인 강화학습을 위한 K2 비판자 루브릭

Kimi K2의 자기 비판 보상 메커니즘은 체계적인 루브릭 프레임워크를 통해 구현됩니다. 이 프레임워크는 핵심 루브릭과 규범적 루브릭으로 구성되어 있으며, 모델이 자신의 출력을 객관적이고 일관되게 평가할 수 있도록 설계되었습니다.

### 핵심 루브릭

핵심 루브릭은 응답 품질을 평가하는 기본적인 기준들을 정의합니다. 이러한 기준들은 Kimi가 추구하는 AI 어시스턴트의 핵심 가치를 반영합니다.

**명확성과 관련성**은 응답이 사용자의 의도를 완전히 다루면서도 간결한 정도를 평가합니다. 이 기준은 불필요한 세부사항을 제거하고, 중심 질문과 일치하며, 간단한 단락이나 압축된 목록과 같은 효율적인 형식을 사용하는 데 중점을 둡니다. 특별히 요구되지 않는 한 긴 항목화는 피해야 하며, 선택이 예상되는 경우 응답은 명확하게 단일하고 잘 정의된 답변을 제공해야 합니다.

**대화의 유창성과 참여도**는 단순한 질문-답변을 넘어서는 자연스럽고 흐름이 있는 대화에 대한 응답의 기여를 평가합니다. 이는 일관성 유지, 주제에 대한 적절한 참여 표시, 관련 관찰이나 통찰 제공, 적절할 때 건설적으로 대화를 안내하는 것, 후속 질문의 신중한 사용, 가상적이거나 개인적 유추 질문의 우아한 처리, 대화 맥락에 맞는 효과적인 톤 조정(예: 공감적, 공식적, 캐주얼)을 포함합니다.

**객관적이고 근거 기반 상호작용**은 사용자 요청의 실질적 내용에 집중하면서 객관적이고 근거 있는 톤을 유지하는 응답의 능력을 평가합니다. 이는 메타 논평(질문의 구조, 주제 조합, 인지된 이상함, 또는 상호작용 자체의 성격 분석)과 사용자나 그들의 입력에 대한 부당한 아첨이나 과도한 칭찬을 모두 피하는 것을 평가합니다. 우수한 응답은 대화 역학에 대한 논평이나 칭찬을 통해 호의를 얻으려는 시도보다는 직접적이고 작업 중심적인 지원을 우선시하면서 존중하지만 중립적으로 상호작용합니다.

### 규범적 루브릭

규범적 루브릭은 보상 해킹을 방지하기 위해 설계된 구체적인 규칙들을 포함합니다. 이러한 규칙들은 모델이 표면적인 개선을 통해 단순히 긍정적인 평가를 받으려 하는 것을 방지하고, 실질적인 품질 향상에 집중하도록 유도합니다.

**초기 칭찬 금지**는 응답이 사용자나 질문에 대한 칭찬으로 시작해서는 안 된다는 규칙입니다. 예를 들어 "아름다운 질문이네요", "좋은 질문입니다!"와 같은 표현을 사용해서는 안 됩니다. 이는 모델이 실질적인 내용 제공보다는 사용자를 기쁘게 하려는 시도에 집중하는 것을 방지합니다.

**명시적 정당화 금지**는 응답이 얼마나 좋은지 또는 사용자의 요청을 얼마나 성공적으로 충족했는지 설명하는 문장이나 절을 포함해서는 안 된다는 규칙입니다. 이는 단순히 내용을 설명하는 것과는 다릅니다. 모델이 자신의 성과를 자화자찬하는 것을 방지하여 더 객관적인 평가를 유도합니다.

### 한계점

이 평가 프레임워크의 잠재적 부작용 중 하나는 모호함이나 주관성을 포함하는 맥락에서도 확신 있고 단호해 보이는 응답을 선호할 수 있다는 것입니다. 이는 현재 루브릭의 두 가지 핵심 제약에서 비롯됩니다.

**자기 자격 부여 회피**는 규범적 규칙이 자기 평가, 명시적 면책 조항, 또는 헤징 언어(예: "이것이 정확하지 않을 수 있습니다", "제가 틀릴 수도 있습니다")를 금지한다는 것입니다. 이러한 문구들이 인식론적 겸손을 반영할 수 있지만, 종종 비정보적이거나 성과적인 것으로 처벌됩니다.

**명확성과 단일성에 대한 선호**는 루브릭이 사용자가 추천이나 설명을 요청할 때 직접적이고 결정적인 답변을 보상한다는 것입니다. 복잡하거나 개방형 시나리오에서는 이것이 적절히 신중하거나 다중 관점 응답을 억제할 수 있습니다.

결과적으로, 모델은 때때로 모호함, 뉘앙스, 또는 인식론적 겸손이 더 적절할 영역에서 확실성을 과장할 수 있습니다. 프레임워크의 향후 반복에서는 보정된 불확실성의 더 세밀한 처리를 통합할 수 있을 것입니다.

이러한 루브릭 시스템을 통해 Kimi K2는 객관적으로 검증 가능한 작업에서의 성능 향상이 명시적 보상 신호가 없는 복잡한 작업에서도 비판자의 판단을 향상시킬 수 있도록 하는 폐쇄 루프 비판자 개선을 달성합니다. 이는 주관적 평가를 검증 가능한 데이터에 기반함으로써, 복잡하고 검증 불가능한 인간 목표와의 견고하고 확장 가능한 정렬을 가능하게 합니다.

## 강화학습 훈련을 위한 엔진 전환 파이프라인

Kimi K2의 강화학습 인프라에서 핵심적인 기술적 도전과제 중 하나는 훈련 엔진과 추론 엔진 간의 효율적인 파라미터 업데이트입니다. 이를 해결하기 위해 정교한 파이프라인 설계가 개발되었습니다.

체크포인트 엔진은 각 GPU에서 세 개의 동일한 크기 디바이스 버퍼를 관리합니다. H2D 버퍼는 오프로드된 모델 파라미터를 로딩하는 데 사용되고, 두 개의 IPC 버퍼는 GPU 간 브로드캐스트를 위해 사용됩니다. IPC 버퍼는 추론 엔진과 공유되어 동일한 물리적 메모리에 직접 접근할 수 있게 합니다. 이 세 개의 버퍼를 통해 세 단계를 파이프라인으로 배열할 수 있습니다.

### 이론적 3단계 파이프라인

이론적으로 설계된 3단계 파이프라인은 다음과 같이 구성됩니다. H2D 단계에서는 최신 가중치의 샤드가 H2D 버퍼로 비동기적으로 복사됩니다. 브로드캐스트 단계에서는 복사가 완료되면 샤드가 하나의 IPC 버퍼로 복사되고 모든 디바이스에 브로드캐스트됩니다. 리로드 단계에서는 추론 엔진이 다른 IPC 버퍼에서 파라미터를 동시에 로드합니다.

### PCIe 포화로 인한 2단계 파이프라인

NVIDIA H800 클러스터에서는 동시 H2D와 브로드캐스트가 공유 PCIe 패브릭을 포화시켜 3단계를 순차적 절차로 축소시킵니다. 따라서 더 간단한 2단계 스킴을 채택했습니다. 첫 번째 단계에서는 모든 디바이스가 단일 동기 H2D 전송을 수행하고, 두 번째 단계에서는 브로드캐스트와 리로드가 병렬로 진행됩니다.

2단계 파이프라인은 여러 동기 H2D 복사 작업에 의해 제한될 수 있습니다. 하지만 대규모 디바이스에서는 모델이 작은 샤드로 분할되어 전체 파라미터 세트가 한 번의 전송으로 H2D 버퍼에 맞을 수 있어 오버헤드가 사라집니다.

H2D, 브로드캐스트, 리로드 가중치를 오버랩함으로써 훈련 엔진에서 모든 추론 엔진으로 가중치를 재샤딩하는 높은 대역폭을 얻을 수 있습니다. 이러한 파이프라인 설계를 통해 Kimi K2는 대규모 강화학습 훈련에서 효율적인 파라미터 업데이트를 달성할 수 있습니다.

이 파이프라인 시스템은 분산 훈련 환경에서 메모리 효율성과 전송 속도를 모두 고려한 최적화를 제공하며, 특히 1조 파라미터 규모의 모델에서 30초 미만의 전체 파라미터 업데이트를 가능하게 합니다.
- - -
### References
* [Kimi K2: Open Agentic Intelligence](http://arxiv.org/pdf/2507.20534v1)