---
layout: post
title: "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval"
date: 2024-01-31 18:30:21
author: "Stanford University"
categories: "Language-Models"
tags: ["Recursive-Abstractive-Processing", "Tree-Organized-Retrieval", "Hierarchical-Summarization", "Retrieval-Augmented-Language-Models", "Collapsed-Tree-Retrieval"]
cover: /assets/images/language-models.webp
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
검색 증강 언어 모델은 세계 지식의 변화에 적응하고 롱테일 지식을 통합할 수 있다는 장점이 있습니다. 그러나 기존의 검색 방식들은 검색 코퍼스에서 짧은 연속적인 텍스트 청크만을 검색하기 때문에 전체 문서의 맥락을 포괄적으로 이해하는 데 한계가 있었습니다. 특히 복잡한 다단계 추론이 필요하거나 문서 전체의 맥락을 이해해야 하는 질문에 대해서는 효과적으로 대응하지 못했습니다. 이러한 한계를 극복하고 더 효과적인 문서 이해와 정보 검색을 위한 새로운 접근 방식이 필요했습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
이 연구는 RAPTOR(Recursive Abstractive Processing for Tree-Organized Retrieval)라는 혁신적인 트리 기반 검색 시스템을 제안합니다. RAPTOR는 텍스트 청크를 재귀적으로 임베딩하고 클러스터링한 뒤 요약하여, 아래에서 위로 다양한 수준의 요약을 포함하는 계층적 트리 구조를 구성합니다. 이 구조는 문서의 전체적인 맥락부터 세부적인 정보까지 다양한 추상화 수준의 정보를 효과적으로 통합할 수 있게 해줍니다. 특히 트리 순회와 축소된 트리라는 두 가지 검색 메커니즘을 통해 질문의 특성에 따라 적절한 수준의 정보를 검색할 수 있습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
RAPTOR의 구현은 세 가지 주요 단계로 이루어집니다. 첫째, SBERT를 사용하여 텍스트 청크들을 임베딩합니다. 둘째, 가우시안 혼합 모델(GMM)을 사용하여 의미적으로 유사한 청크들을 클러스터링합니다. 셋째, GPT-3.5-turbo를 사용하여 각 클러스터의 요약을 생성합니다. 이 과정은 재귀적으로 반복되어 다층 트리 구조를 형성합니다. 검색 단계에서는 FAISS를 활용한 효율적인 유사도 검색을 통해 관련 정보를 빠르게 찾아낼 수 있습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
RAPTOR는 여러 벤치마크 데이터셋에서 기존 방법들을 크게 상회하는 성능을 보여주었습니다. 특히 GPT-4와 결합했을 때 QuALITY 데이터셋에서 82.6%의 정확도를 달성하여 이전 최고 성능인 62.3%를 크게 개선했으며, QASPER 데이터셋에서도 55.7%의 F-1 점수로 새로운 최고 성능을 기록했습니다. 이러한 결과는 계층적 문서 처리 방식의 효과성을 입증하며, 특히 복잡한 다단계 추론이 필요한 질문에서 RAPTOR의 우수성을 보여줍니다. 이는 검색 증강 언어 모델의 발전 방향을 제시하고, 더 효과적인 문서 이해와 정보 검색을 위한 새로운 패러다임을 제시합니다.
- - -
## RAPTOR: 재귀적 추상화 처리를 통한 트리 기반 검색

검색 증강 언어 모델은 세계의 변화에 더 잘 적응하고 롱테일 지식을 통합할 수 있다는 장점이 있습니다. 하지만 기존의 대부분 방법들은 검색 코퍼스에서 짧은 연속적인 텍스트 청크만을 검색하기 때문에 전체 문서 맥락을 포괄적으로 이해하는 데 한계가 있었습니다.

이러한 한계를 극복하기 위해 본 논문에서는 RAPTOR(Recursive Abstractive Processing for Tree-Organized Retrieval)라는 새로운 접근 방식을 제안합니다. RAPTOR는 텍스트 청크를 재귀적으로 임베딩하고 클러스터링한 뒤 요약하여, 아래에서 위로 다양한 수준의 요약을 포함하는 트리를 구성합니다. 추론 시에는 이 트리에서 검색을 수행하여 긴 문서에 걸쳐 있는 정보를 다양한 추상화 수준에서 통합할 수 있습니다.

통제된 실험을 통해 재귀적 요약을 활용한 RAPTOR의 검색 방식이 기존의 검색 증강 언어 모델들보다 여러 과제에서 상당한 성능 향상을 보여주었습니다. 특히 복잡한 다단계 추론이 필요한 질의응답 과제에서 최신 성능을 달성했습니다. 예를 들어 RAPTOR의 검색 방식을 GPT-4와 결합했을 때, QuALITY 벤치마크에서 기존 최고 성능 대비 절대 정확도를 20% 향상시켰습니다.

이러한 성과는 RAPTOR가 도입한 계층적 문서 처리 방식의 효과를 입증합니다. 기존 청크 기반 접근법들이 긴 문서를 다루는 데 어려움을 겪었던 것과 달리, RAPTOR의 트리 구조 기반 검색은 문서의 전체적인 맥락을 더 잘 파악하고 복잡한 추론 과제를 효과적으로 해결할 수 있음을 보여줍니다.

## 대규모 언어 모델의 지식 검색 및 활용

대규모 언어 모델(Large Language Models, LLMs)은 다양한 과제에서 인상적인 성능을 보여주며 혁신적인 도구로 자리매김하고 있습니다. Petroni와 연구진의 연구에서 보여주듯이, 모델의 규모가 커짐에 따라 LLM은 그 자체로 매우 효과적인 지식 저장소 역할을 할 수 있으며, 파라미터 내에 다양한 사실적 정보를 인코딩할 수 있습니다. Roberts와 연구진이 입증한 바와 같이, 하위 과제에 대한 미세 조정을 통해 모델의 성능을 더욱 향상시킬 수 있습니다.

하지만 아무리 큰 모델이라도 특정 과제에 필요한 도메인별 지식을 충분히 포함하지 못할 수 있으며, 세상이 계속 변화함에 따라 모델이 가진 사실 정보가 무효화될 수 있습니다. Lewis와 연구진, Mitchell과 연구진의 연구에서 지적했듯이, 방대한 텍스트 코퍼스를 통한 추가 미세 조정이나 편집을 통해 이러한 모델의 지식을 업데이트하는 것은 매우 어려운 과제입니다.

이에 대한 대안으로, Chen과 연구진과 Yu와 연구진이 개방형 질의응답 시스템에서 선구적으로 도입한 방식이 있습니다. 이는 텍스트를 문단 단위로 나눈 후 별도의 정보 검색 시스템에 인덱싱하는 방식입니다. Lewis와 연구진, Izacard와 연구진, Min과 연구진, Ram과 연구진의 연구에서 보여주듯이, 검색된 정보는 질문과 함께 LLM의 컨텍스트로 제공됩니다. 이러한 "검색 증강" 방식을 통해 특정 도메인의 최신 지식을 시스템에 쉽게 제공할 수 있으며, Akyurek와 연구진이 지적한 것처럼 불투명하고 출처를 추적하기 어려운 LLM의 파라메트릭 지식과 달리 해석 가능성과 출처 추적이 용이해집니다.

그러나 기존의 검색 증강 접근 방식에도 한계가 있습니다. 본 논문에서 다루고자 하는 주요 문제점은 대부분의 기존 방법들이 소수의 짧은 연속적인 텍스트 청크만을 검색한다는 것입니다. 이는 대규모 담화 구조를 표현하고 활용하는 능력을 제한합니다. 특히 Kočiskỳ와 연구진이 제시한 NarrativeQA 데이터셋과 같이 책 전체를 이해해야 하는 주제적 질문에서 이러한 한계가 두드러집니다. 예를 들어 "신데렐라는 어떻게 해피엔딩에 도달했나요?"라는 질문에 대해, 상위 k개의 짧은 연속 텍스트만으로는 충분한 맥락을 제공할 수 없습니다.

이러한 문제를 해결하기 위해, 본 논문에서는 텍스트의 상위 수준과 하위 수준 세부 사항을 모두 포착할 수 있는 트리 구조를 활용한 인덱싱 및 검색 시스템을 설계했습니다. RAPTOR는 텍스트 청크를 클러스터링하고, 이러한 클러스터의 텍스트 요약을 생성한 뒤, 이를 반복하여 아래에서 위로 트리를 생성합니다. 이러한 구조를 통해 RAPTOR는 다양한 수준의 텍스트 청크를 LLM의 컨텍스트에 로드하여 서로 다른 수준의 질문에 효과적이고 효율적으로 답변할 수 있습니다.
### RAPTOR의 주요 기술적 기여

본 논문의 핵심 기여는 서로 다른 규모의 컨텍스트를 검색 증강할 수 있도록 텍스트 요약을 활용하는 방식을 제안하고, 긴 문서 모음에서 그 효과성을 실험적으로 입증한 것입니다. UnifiedQA, GPT-3, GPT-4 세 가지 언어 모델을 사용한 통제된 실험에서 RAPTOR는 기존의 검색 증강 방식보다 우수한 성능을 보여주었습니다.

특히 GPT-4와 결합했을 때, 때로는 UnifiedQA와 결합했을 때도 세 가지 질의응답 과제에서 새로운 최고 성능을 달성했습니다. 이러한 과제들은 책과 영화에 대한 자유 형식 응답 질문(NarrativeQA), 전체 텍스트 NLP 논문에 대한 질문(QASPER), 중간 길이 지문 기반의 객관식 문제(QuALITY)를 포함합니다.

RAPTOR의 트리 구조 기반 검색 방식은 기존 접근법들의 한계를 효과적으로 극복합니다. 기존의 검색 증강 방식들이 짧은 연속적인 텍스트 청크만을 검색함으로써 전체적인 문서 구조를 파악하는 데 어려움을 겪었던 것과 달리, RAPTOR는 계층적 요약을 통해 문서의 다양한 수준의 정보를 효과적으로 통합할 수 있습니다.

이러한 혁신적인 접근 방식은 특히 "신데렐라는 어떻게 해피엔딩에 도달했나요?"와 같이 문서 전체의 맥락을 이해해야 하는 복잡한 질문에서 그 가치가 두드러집니다. RAPTOR는 문서의 여러 부분에 걸쳐 있는 정보를 다양한 추상화 수준에서 통합하여 보다 포괄적이고 정확한 답변을 제공할 수 있습니다.

### 검색의 필요성과 검색 방법의 발전

최근 하드웨어와 알고리즘의 발전으로 언어 모델이 처리할 수 있는 컨텍스트 길이가 크게 확장되었습니다. Liu와 연구진, Sun과 연구진의 연구에 따르면, 모델들은 긴 컨텍스트에서 성능이 저하되는 경향을 보이며, 특히 중요한 정보가 긴 컨텍스트 내에 포함되어 있을 때 이를 효과적으로 활용하지 못하는 것으로 나타났습니다. 또한 긴 컨텍스트를 사용하는 것은 실용적인 측면에서 비용이 많이 들고 처리 속도가 느리다는 단점이 있습니다. 이러한 이유로 지식 집약적 과제에서는 가장 관련성 높은 정보를 선별하는 것이 여전히 중요합니다.

검색 증강 언어 모델(RALMs)은 검색기, 리더, 그리고 전체 시스템 학습 등 다양한 구성 요소에서 발전을 이루어왔습니다. 검색 방법은 TF-IDF나 BM25와 같은 전통적인 용어 기반 기술에서 딥러닝 기반 전략으로 발전했으며, 최근에는 대규모 언어 모델의 광범위한 지식 기억 능력을 활용하여 검색기로 사용하는 연구도 진행되고 있습니다.

리더 구성 요소에서는 Izacard와 Grave가 제안한 Fusion-in-Decoder(FiD)가 주목할 만한 발전을 이루었습니다. FiD는 DPR과 BM25를 모두 활용하여 검색을 수행하고, 인코더에서 각 문단을 독립적으로 처리합니다. 또한 Borgeaud와 연구진이 제안한 RETRO는 교차 청크 어텐션과 청크 단위 검색을 활용하여 검색된 컨텍스트를 기반으로 텍스트를 생성합니다.

전체 시스템 학습 측면에서는 Atlas, REALM, RAG 등의 혁신적인 접근 방식이 제시되었습니다. Atlas는 검색기와 함께 인코더-디코더 모델을 미세 조정하는 방식을 채택했으며, REALM은 개방형 질의응답을 위해 양방향 마스크드 언어 모델을 미세 조정하는 방식을 사용합니다. RAG는 사전 학습된 시퀀스-투-시퀀스 모델과 신경망 검색기를 통합하는 방식을 제안했습니다.

Min과 연구진은 다중 답변 검색에서 문단의 다양성과 관련성을 처리하기 위한 트리 디코딩 알고리즘을 활용하는 Joint Passage Retrieval(JPR) 모델을 소개했습니다. Dense Hierarchical Retrieval(DHR)과 Hybrid Hierarchical Retrieval(HHR)은 문서와 문단 수준의 검색을 결합하고 희소 검색과 밀집 검색 방법을 통합하여 검색 정확도를 향상시켰습니다.

### 재귀적 요약을 통한 컨텍스트 처리

요약 기술은 문서의 압축된 관점을 제공하여 콘텐츠에 더 집중적으로 접근할 수 있게 해줍니다. Gao와 연구진이 제안한 요약/스니펫 모델은 문단의 요약과 스니펫을 활용하여 대부분의 데이터셋에서 정확도를 향상시켰지만, 때로는 정보 손실이 발생할 수 있는 압축 방식이라는 한계가 있습니다.

Wu와 연구진이 제안한 재귀-추상화 요약 모델은 작은 텍스트 청크를 요약하고 이를 통합하여 더 큰 섹션의 요약을 형성하는 과제 분해 방식을 사용합니다. 이 방법은 더 넓은 주제를 포착하는 데는 효과적이지만 세부적인 내용을 놓칠 수 있습니다. LlamaIndex는 이러한 문제를 완화하기 위해 유사한 방식으로 인접한 텍스트 청크를 요약하면서도 중간 노드를 유지하여 다양한 수준의 세부 정보를 보존합니다. 그러나 두 방법 모두 인접성에 기반한 그룹화나 요약 방식을 사용하기 때문에 텍스트 내의 원거리 상호 의존성을 놓칠 수 있습니다. 이는 RAPTOR를 통해 발견하고 그룹화할 수 있는 부분입니다.

### RAPTOR의 방법론 개요

RAPTOR는 긴 텍스트가 종종 하위 주제와 계층적 구조를 가진다는 아이디어에 기반하여, 의미적 깊이와 연결성을 효과적으로 처리하는 재귀적 트리 구조를 구축합니다. 이 트리 구조는 텍스트의 광범위한 주제적 이해와 세부적인 내용을 균형있게 다루며, 텍스트 내의 순서가 아닌 의미적 유사성을 기반으로 노드들을 그룹화할 수 있습니다.

RAPTOR 트리의 구축은 기존의 검색 증강 기법과 유사하게 검색 코퍼스를 100 토큰 길이의 짧은 연속 텍스트로 분할하는 것으로 시작됩니다. 문장이 100 토큰 제한을 초과할 경우, 문장 중간에서 잘라내는 대신 전체 문장을 다음 청크로 이동시킵니다. 이는 각 청크 내의 문맥적, 의미적 일관성을 보존하기 위한 것입니다.

이렇게 분할된 텍스트는 BERT 기반 인코더인 SBERT(multi-qa-mpnet-base-cos-v1)를 사용하여 임베딩됩니다. 이 청크들과 해당 SBERT 임베딩은 트리 구조의 리프 노드를 형성합니다. 유사한 텍스트 청크들을 그룹화하기 위해 클러스터링 알고리즘이 사용되며, 그룹화된 텍스트들은 언어 모델을 통해 요약됩니다. 이렇게 요약된 텍스트는 다시 임베딩되고, 더 이상의 클러스터링이 불가능해질 때까지 임베딩, 클러스터링, 요약의 사이클이 반복됩니다. 이 과정을 통해 원본 문서의 구조화된 다층 트리 표현이 생성됩니다.

RAPTOR의 중요한 특징 중 하나는 계산 효율성입니다. 시스템은 구축 시간과 토큰 사용량 측면에서 선형적으로 확장되어, 대규모의 복잡한 코퍼스를 처리하는 데 적합합니다. RAPTOR의 확장성에 대한 자세한 논의는 부록 A에서 확인할 수 있습니다.

이 트리 구조 내에서의 쿼리 처리를 위해 RAPTOR는 두 가지 전략을 도입합니다. 첫 번째는 트리 순회 방식으로, 트리를 계층별로 순회하면서 각 레벨에서 가장 관련성 높은 노드들을 선별하고 가지치기합니다. 두 번째는 축소된 트리 방식으로, 모든 계층의 노드들을 통합적으로 평가하여 가장 관련성 높은 노드들을 찾아냅니다.

### 클러스터링 알고리즘의 구조와 원리

RAPTOR 트리를 구축하는 데 있어 클러스터링은 핵심적인 역할을 수행합니다. 이 알고리즘은 텍스트 세그먼트들을 응집력 있는 그룹으로 조직화하여 후속 검색 프로세스를 효과적으로 지원합니다. 특히 주목할 만한 특징은 소프트 클러스터링 방식을 채택했다는 점입니다. 이는 노드가 여러 클러스터에 동시에 속할 수 있으며, 클러스터의 수를 미리 고정할 필요가 없다는 것을 의미합니다. 이러한 유연성은 개별 텍스트 세그먼트가 다양한 주제와 관련된 정보를 포함할 수 있으므로, 여러 요약에 포함될 필요가 있는 경우에 매우 중요합니다.

클러스터링 알고리즘은 가우시안 혼합 모델(Gaussian Mixture Models, GMMs)을 기반으로 합니다. GMM은 데이터 포인트들이 여러 가우시안 분포의 혼합으로부터 생성된다고 가정합니다. $N$개의 텍스트 세그먼트가 각각 $d$차원의 밀집 벡터 임베딩으로 표현될 때, 텍스트 벡터 $\mathbf{x}$가 $k^{th}$ 가우시안 분포에 속할 확률은 다음과 같이 표현됩니다.

$$ P(\mathbf{x}|k)=\mathcal{N}(\mathbf{x};\mathbf{\mu}_{k},\mathbf{\Sigma}_{k}) $$

전체 확률 분포는 다음과 같은 가중 조합으로 표현됩니다.

$$ P(\mathbf{x})=\sum_{k=1}^{K}\pi_{k}\mathcal{N}(\mathbf{x};\mathbf{\mu}_{k},\mathbf{\Sigma}_{k}) $$

여기서 $\pi_{k}$는 $k^{th}$ 가우시안 분포의 혼합 가중치를 나타냅니다.

벡터 임베딩의 높은 차원성은 전통적인 GMM에 있어 도전 과제가 됩니다. Aggarwal과 연구진이 지적했듯이, 고차원 공간에서는 거리 메트릭이 유사성을 측정하는 데 제대로 작동하지 않을 수 있습니다. 이 문제를 해결하기 위해 UMAP(Uniform Manifold Approximation and Projection)이라는 매니폴드 학습 기법을 차원 축소에 활용합니다. McInnes와 연구진이 개발한 UMAP에서 n_neighbors 파라미터는 지역 구조와 전역 구조의 보존 사이의 균형을 결정합니다.

알고리즘은 n_neighbors를 변화시켜 계층적 클러스터링 구조를 생성합니다. 먼저 전역 클러스터를 식별한 다음, 이러한 전역 클러스터 내에서 지역 클러스터링을 수행합니다. 이러한 2단계 클러스터링 프로세스는 텍스트 데이터 간의 광범위한 관계를 포착할 수 있습니다.

만약 지역 클러스터의 결합된 컨텍스트가 요약 모델의 토큰 임계값을 초과하는 경우, 알고리즘은 해당 클러스터 내에서 재귀적으로 클러스터링을 적용하여 컨텍스트가 토큰 임계값 내에 유지되도록 합니다.

최적의 클러스터 수를 결정하기 위해 베이지안 정보 기준(Bayesian Information Criterion, BIC)을 사용합니다. Schwarz가 제안한 BIC는 모델의 복잡성을 페널티로 부과하면서 동시에 적합도를 보상합니다. GMM의 BIC는 다음과 같이 계산됩니다.

$$ BIC=\ln(N)k-2\ln(\hat{L}) $$

여기서 $N$은 텍스트 세그먼트(데이터 포인트)의 수, $k$는 모델 파라미터의 수, $\hat{L}$은 모델의 최대화된 우도 함수 값입니다. GMM의 맥락에서 파라미터 수 $k$는 입력 벡터의 차원과 클러스터 수의 함수입니다.

BIC로 최적의 클러스터 수가 결정되면, Expectation-Maximization 알고리즘을 사용하여 GMM 파라미터(평균, 공분산, 혼합 가중치)를 추정합니다. GMM의 가우시안 가정이 텍스트 데이터의 희소하고 편향된 분포 특성과 완벽하게 일치하지 않을 수 있지만, 실험적 관찰 결과 이 목적에 효과적인 모델임이 확인되었습니다.

### 모델 기반 요약 처리

가우시안 혼합 모델(Gaussian Mixture Models)을 사용하여 노드들을 클러스터링한 후, 각 클러스터 내의 노드들은 언어 모델을 통해 요약됩니다. 이 단계에서는 대량의 텍스트를 간결하고 일관된 요약문으로 변환합니다. 본 연구에서는 gpt-3.5-turbo를 사용하여 요약을 생성했습니다. 요약 단계는 검색된 정보의 잠재적으로 큰 볼륨을 관리 가능한 크기로 압축합니다. 부록 C (#A3)에서는 요약으로 인한 압축에 대한 통계를, 부록 D (#A4)에서는 요약에 사용된 프롬프트를 제공합니다.

요약 모델은 일반적으로 신뢰할 수 있는 요약을 생성하지만, 집중적인 주석 연구를 통해 약 4%의 요약에서 경미한 환각(hallucination) 현상이 발생한다는 것이 밝혀졌습니다. 이러한 환각은 상위 노드로 전파되지 않았으며 질의응답 과제에 눈에 띄는 영향을 미치지 않았습니다. 환각 현상에 대한 심층 분석은 부록 E (#A5)를 참조하시기 바랍니다.

![쿼리 메커니즘](https://ar5iv.org//html/2401.18059/assets/images/querying.jpg)

위 그림은 트리 구조화된 지식 표현에서 관련 컨텍스트를 검색하는 두 가지 핵심 메커니즘을 보여줍니다. 트리 순회 방식은 루트 레벨에서 시작하여 쿼리 벡터와의 코사인 유사도를 기반으로 상위 k개의 노드를 검색하고, 재귀적으로 트리를 순회하며 가장 관련성 높은 노드들을 검색합니다. 축소된 트리 방식은 트리 구조를 단일 계층으로 평탄화하고 코사인 유사도를 기반으로 지정된 토큰 임계값까지 가장 관련성 높은 노드들을 직접 검색합니다.

이러한 검색 메커니즘은 수학적으로 다음과 같이 표현됩니다. 트리 순회에서는 각 레벨에서 상위 $$ k $$ (여기서는 상위 $$ 1 $$) 개의 노드를 선택하고, 이전 레이어의 상위 $$ k $$ 개 노드의 자식 노드들 중에서 다시 상위 $$ k $$ 개를 선택합니다. 축소된 트리에서는 트리를 단일 계층으로 변환하고 쿼리 벡터와의 코사인 유사도를 기반으로 토큰 임계값에 도달할 때까지 노드들을 검색합니다. 두 방식 모두에서 코사인 유사도 검색이 수행되는 노드들이 강조 표시되어 있습니다.

### RAPTOR의 쿼리 메커니즘

RAPTOR는 트리 순회(Tree Traversal)와 축소된 트리(Collapsed Tree)라는 두 가지 쿼리 메커니즘을 활용하여 다층 RAPTOR 트리에서 관련 정보를 검색합니다. 이러한 메커니즘들은 각각 고유한 장단점을 가지고 있으며, 모든 노드는 SBERT를 사용하여 임베딩됩니다.

트리 순회 방식은 먼저 루트 노드 계층에서 시작하여 쿼리 임베딩과의 코사인 유사도를 기반으로 상위 $k$개의 가장 관련성 높은 노드를 선택합니다. 이후 선택된 노드들의 자식 노드들을 다음 계층에서 고려하며, 다시 쿼리 벡터와의 코사인 유사도를 기준으로 상위 $k$개의 노드를 선택합니다. 이 과정은 리프 노드에 도달할 때까지 반복되며, 최종적으로 선택된 모든 노드의 텍스트가 연결되어 검색된 컨텍스트를 형성합니다.

![쿼리 메커니즘](https://ar5iv.org//html/2401.18059/assets/images/beam_and_collapsed.png)

축소된 트리 방식은 다층 트리를 단일 계층으로 평탄화하여 모든 노드를 동시에 고려하는 더 단순한 방법을 제공합니다. 이 방식에서는 먼저 전체 RAPTOR 트리를 단일 계층으로 축소하여 모든 계층의 노드를 포함하는 집합 $C$를 생성합니다. 그 다음, 쿼리 임베딩과 축소된 집합 $C$의 모든 노드 임베딩 간의 코사인 유사도를 계산합니다. 마지막으로, 쿼리와의 코사인 유사도가 가장 높은 상위 $k$개의 노드를 선택하되, 모델의 입력 제한을 초과하지 않도록 사전 정의된 최대 토큰 수에 도달할 때까지 노드를 결과 집합에 추가합니다.

QASPER 데이터셋의 20개 스토리에 대한 실험 결과, 축소된 트리 방식이 일관되게 더 우수한 성능을 보여주었습니다. 이는 축소된 트리 검색이 트리 순회보다 더 큰 유연성을 제공하기 때문입니다. 즉, 모든 노드를 동시에 검색함으로써 주어진 질문에 대해 적절한 상세도 수준의 정보를 검색할 수 있습니다. 반면, 트리 순회에서는 동일한 $d$와 $k$ 값을 사용할 때 트리의 각 계층에서 선택되는 노드의 비율이 일정하므로, 질문과 관계없이 상위 수준의 주제 정보와 세부 정보의 비율이 동일하게 유지됩니다.

축소된 트리 방식의 한 가지 단점은 트리의 모든 노드에 대해 코사인 유사도 검색을 수행해야 한다는 것입니다. 하지만 이는 FAISS와 같은 고속 $k$-최근접 이웃 라이브러리를 활용하여 효율적으로 처리할 수 있습니다. 실험 결과를 바탕으로, 저자들은 2000 토큰을 최대값으로 하는 축소된 트리 방식을 주요 결과 도출에 사용했으며, 이는 약 상위 20개의 노드를 검색하는 것과 동일합니다. 토큰 기반 접근 방식을 사용함으로써 노드별로 토큰 수가 다를 수 있는 상황에서도 컨텍스트가 모델의 제한을 초과하지 않도록 보장할 수 있습니다.

UnifiedQA 모델을 사용한 실험에서는 UnifiedQA의 최대 컨텍스트 길이가 512 토큰이므로 400 토큰의 컨텍스트를 제공했으며, RAPTOR와 베이스라인 모델들에 동일한 양의 토큰을 제공했습니다.
### RAPTOR의 정성적 분석

RAPTOR의 검색 프로세스가 기존의 Dense Passage Retrieval(DPR) 방식과 비교하여 어떤 이점을 제공하는지 이해하기 위해 정성적 분석을 수행했습니다. 이 분석은 1500단어 분량의 신데렐라 동화를 사용하여 주제적이고 다단계적인 질문들을 중심으로 진행되었습니다.

RAPTOR의 트리 기반 검색은 질문의 상세도 수준에 맞춰 트리의 서로 다른 계층에서 노드를 선택할 수 있습니다. 예를 들어, "신데렐라는 어떻게 해피엔딩에 도달했나요?"와 같은 고수준의 주제적 질문에 대해서는 상위 계층의 요약 노드들을 선택하여 전체적인 스토리 맥락을 파악할 수 있습니다. 반면 "신데렐라가 무도회에서 입은 드레스의 색깔은 무엇인가요?"와 같은 구체적인 질문에 대해서는 하위 계층의 상세 노드들을 선택하여 정확한 정보를 제공할 수 있습니다.

이러한 계층적 접근 방식은 DPR과 같은 기존의 평면적 검색 방식보다 더 포괄적이고 맥락에 맞는 정보를 제공할 수 있습니다. DPR은 문서를 고정된 길이의 청크로 나누고 각 청크를 독립적으로 처리하기 때문에, 여러 부분에 걸쳐 있는 정보나 문서의 전체적인 맥락을 파악하는 데 어려움이 있습니다.

RAPTOR의 검색 시스템은 FAISS를 활용한 효율적인 유사도 검색을 통해 실용적인 성능도 확보했습니다. FAISS는 대규모 벡터 집합에서 빠른 최근접 이웃 검색을 가능하게 하는 라이브러리로, 특히 축소된 트리 방식에서 모든 노드에 대한 유사도 계산을 효율적으로 처리할 수 있게 해줍니다.

정성적 분석의 자세한 결과와 RAPTOR와 DPR이 특정 질문들에 대해 검색한 텍스트의 구체적인 예시들은 논문의 부록 G에서 확인할 수 있습니다. 이 분석을 통해 RAPTOR의 계층적 검색 방식이 문서의 다양한 추상화 수준에서 정보를 효과적으로 통합하여 더 정확하고 포괄적인 답변을 제공할 수 있음을 확인했습니다.

### 실험 데이터셋과 평가 방법

RAPTOR의 성능은 세 가지 주요 질의응답 데이터셋을 통해 평가되었습니다. 첫 번째로 NarrativeQA 데이터셋은 책과 영화 대본을 기반으로 한 질의응답 쌍으로 구성되어 있으며, 총 1,572개의 문서를 포함합니다. NarrativeQA-Story 과제는 전체 내러티브에 대한 포괄적인 이해를 요구하므로, 문학 도메인에서 모델의 긴 텍스트 이해 능력을 평가하는 데 적합합니다. 이 데이터셋에서는 BLEU(B-1, B-4), ROUGE(R-L), METEOR(M) 메트릭을 사용하여 성능을 측정했습니다.

두 번째로 QASPER 데이터셋은 1,585개의 NLP 논문에 걸쳐 5,049개의 질문을 포함하고 있으며, 각 질문은 전체 텍스트에 포함된 정보를 탐색하도록 설계되었습니다. QASPER의 답변 유형은 답변 가능/불가능, 예/아니오, 추상적, 추출적으로 분류되며, 표준 F1 점수를 사용하여 정확도를 측정합니다.

![쿼리 프로세스 시각화](https://ar5iv.org//html/2401.18059/assets/images/qualitative_querying.png)

마지막으로 QuALITY 데이터셋은 객관식 문제로 구성되어 있으며, 각 문제는 평균 약 5,000 토큰 길이의 문맥 구절을 동반합니다. 이 데이터셋은 질의응답 과제를 위해 전체 문서에 대한 추론을 요구하므로, 중간 길이 문서에서 RAPTOR의 검색 시스템 성능을 측정하는 데 적합합니다. 특히 QuALITY-HARD라는 도전적인 하위 집합을 포함하고 있는데, 이는 대다수의 인간 평가자들이 빠른 설정에서 잘못 답변한 문제들로 구성되어 있습니다. 전체 테스트 세트와 HARD 하위 집합 모두에 대한 정확도를 보고했습니다.

### 통제된 기준 비교 실험

먼저 UnifiedQA 3B를 리더로 사용하고, SBERT, BM25, DPR을 임베딩 모델로 사용하여 RAPTOR 트리 구조의 유무에 따른 통제된 비교를 세 가지 데이터셋(QASPER, NarrativeQA, QuALITY)에서 수행했습니다. 실험 결과는 RAPTOR가 어떤 검색기와 결합하더라도 모든 데이터셋에서 해당 검색기의 단독 성능을 일관되게 능가한다는 것을 보여줍니다.

SBERT와 RAPTOR의 조합이 가장 우수한 성능을 보여주었기 때문에, 이후의 모든 실험에서는 이 조합을 사용했습니다. GPT-3, GPT-4, UnifiedQA 세 가지 언어 모델을 사용하여 RAPTOR를 BM25, DPR과 비교한 결과, QASPER 데이터셋에서 RAPTOR는 모든 언어 모델에서 일관되게 더 나은 성능을 보여주었습니다. RAPTOR의 F-1 Match 점수는 GPT-3, GPT-4, UnifiedQA를 사용했을 때 각각 53.1%, 55.7%, 36.6%를 기록했으며, 이는 DPR보다 각각 1.8, 2.7, 4.5 포인트, BM25보다 6.5, 5.5, 10.2 포인트 높은 수치입니다.
### 실험 결과 분석

QASPER 데이터셋에서 RAPTOR가 더 우수한 성능을 보인 것은 NLP 논문 내의 정보를 종합하는 특성을 고려할 때 자연스러운 결과입니다. RAPTOR의 상위 계층 요약 노드들은 단순히 상위 k개의 가장 유사한 원본 텍스트 청크만을 추출하는 기존 방식과 달리, 독립적으로는 정답을 포함하지 않을 수 있는 정보들을 통합하여 더 포괄적인 맥락을 제공할 수 있기 때문입니다.

QuALITY 데이터셋에서도 RAPTOR는 GPT-3를 사용했을 때 62.4%의 정확도를 달성하여 DPR과 BM25 대비 각각 2%와 5.1%의 성능 향상을 보여주었습니다. UnifiedQA를 사용한 경우에도 유사한 경향이 관찰되어 DPR과 BM25 대비 각각 2.7%와 6.7%의 성능 향상을 기록했습니다.

NarrativeQA 데이터셋에서는 RAPTOR가 여러 메트릭에서 우수한 성능을 보여주었습니다. ROUGE-L 점수에서 BM25와 DPR 대비 각각 7.3과 2.7 포인트 높은 성능을 달성했으며, BLEU-1, BLEU-4, METEOR 메트릭에서도 BM25 대비 1.7에서 5.8 포인트, DPR 대비 0.7에서 2.1 포인트의 성능 향상을 보여주었습니다.

### 최신 시스템과의 비교

통제된 비교 실험을 넘어 RAPTOR의 성능을 다른 최신 시스템들과 비교했습니다. QASPER 데이터셋에서 RAPTOR는 GPT-4와 결합하여 55.7%의 F-1 점수를 기록하며 CoLT5 XL의 53.9% 성능을 뛰어넘어 새로운 최고 성능을 달성했습니다.

QuALITY 데이터셋에서는 RAPTOR와 GPT-4의 조합이 82.6%의 정확도로 이전 최고 성능인 62.3%를 크게 상회하는 결과를 보여주었습니다. 특히 주목할 만한 점은 QuALITY-HARD 하위 집합에서 CoLISA 대비 21.5%의 큰 성능 향상을 달성했다는 것입니다. 이는 인간 평가자들이 정확한 답변을 위해 텍스트를 다시 읽거나 복잡한 추론이 필요한 까다로운 질문들에 대해서도 RAPTOR가 효과적으로 작동함을 보여줍니다.

NarrativeQA 데이터셋에서는 RAPTOR와 UnifiedQA의 조합이 새로운 최고 METEOR 점수를 기록했습니다. Wu와 연구진이 제안한 재귀적 요약 모델과 비교했을 때, RAPTOR는 모든 메트릭에서 더 우수한 성능을 보여주었습니다. Wu와 연구진의 모델이 트리 구조의 최상위 루트 노드의 요약에만 의존하는 반면, RAPTOR는 중간 계층과 클러스터링 접근 방식을 활용하여 일반적인 주제부터 구체적인 세부 사항까지 다양한 수준의 정보를 포착할 수 있어 전반적으로 더 강력한 성능을 보여줍니다.
### 트리 구조의 기여도 분석

RAPTOR의 트리 구조가 검색 능력에 미치는 영향을 정량적, 정성적으로 분석했습니다. 상위 노드들이 문서의 전반적인 이해나 다단계 추론이 필요한 쿼리를 처리하는 데 중요한 역할을 할 것이라는 가설을 검증하기 위해 QuALITY 데이터셋의 스토리들을 사용하여 실험을 진행했습니다.

각 스토리에 대해 RAPTOR 트리를 구축한 후, 검색 과정에서 서로 다른 계층의 하위 집합만을 사용하도록 제한하여 성능을 측정했습니다. 예를 들어, 리프 노드와 각 상위 계층에서 독점적으로 검색을 수행하거나, 계층들의 서로 다른 연속적인 하위 집합에서 검색을 수행했습니다.

한 스토리에 대한 구체적인 실험 결과를 보면, 모든 계층을 활용한 전체 트리 검색이 특정 계층에만 집중한 검색 전략들보다 우수한 성능을 보여주었습니다. 이는 RAPTOR 트리 구조의 중요성을 잘 보여줍니다. 원본 텍스트와 상위 수준의 요약을 모두 검색에 활용함으로써, RAPTOR는 상위 수준의 주제적 질문부터 세부적인 질문까지 더 넓은 범위의 질문을 효과적으로 처리할 수 있습니다.

예를 들어 "이야기의 중심 주제는 무엇인가요?"와 같은 고수준의 질문에는 상위 계층의 요약 노드가 더 유용한 정보를 제공할 수 있고, "주인공이 입은 옷의 색깔은 무엇인가요?"와 같은 구체적인 질문에는 리프 노드의 상세 정보가 더 적합할 수 있습니다. RAPTOR의 계층적 구조는 이러한 다양한 수준의 질문에 대해 적절한 계층의 정보를 선택적으로 활용할 수 있게 해줍니다.

실험 결과에서 특히 주목할 만한 점은 단일 계층만을 사용했을 때의 성능이 57.9%였던 반면, 두 계층을 사용했을 때는 63.1%, 세 계층을 모두 사용했을 때는 73.68%까지 성능이 향상되었다는 것입니다. 이는 서로 다른 계층의 정보가 상호 보완적으로 작용하여 더 정확한 답변을 생성하는 데 기여한다는 것을 보여줍니다.

## 결론

본 논문에서는 대규모 언어 모델의 파라메트릭 지식을 다양한 추상화 수준의 맥락 정보로 보강하는 새로운 트리 기반 검색 시스템인 RAPTOR를 제시했습니다. RAPTOR는 재귀적 클러스터링과 요약 기법을 활용하여 검색 코퍼스의 여러 섹션에 걸친 정보를 종합할 수 있는 계층적 트리 구조를 생성합니다. 쿼리 단계에서는 이 트리 구조를 활용하여 더 효과적인 검색을 수행할 수 있습니다.

통제된 실험을 통해 RAPTOR가 기존의 전통적인 검색 방법들을 능가할 뿐만 아니라, 여러 질의응답 과제에서 새로운 성능 기준을 수립했음을 입증했습니다. 특히 GPT-4와 결합했을 때 QuALITY 데이터셋에서 82.6%의 정확도를 달성하여 이전 최고 성능인 62.3%를 크게 상회했으며, QASPER 데이터셋에서도 55.7%의 F-1 점수로 새로운 최고 성능을 기록했습니다.

RAPTOR의 성공은 계층적 문서 처리 방식의 효과성을 입증합니다. 기존의 청크 기반 접근법들이 긴 문서의 전체적인 맥락을 파악하는 데 어려움을 겪었던 것과 달리, RAPTOR의 트리 구조 기반 검색은 문서의 다양한 추상화 수준에서 정보를 효과적으로 통합하여 복잡한 추론 과제를 해결할 수 있음을 보여줍니다. 이는 특히 문서 전체의 맥락을 이해해야 하는 복잡한 질문이나 다단계 추론이 필요한 과제에서 두드러진 장점을 보여줍니다.

### 재현성 검증 및 구현 세부사항

RAPTOR 실험에서는 질의응답과 요약 과제를 위해 네 가지 언어 모델을 활용했습니다. 질의응답 과제에는 GPT-3와 GPT-4를, 요약 과제에는 GPT-3.5-turbo를 사용했습니다. gpt-3, gpt-4, gpt-3.5-turbo 모델들은 OpenAI API를 통해 접근할 수 있으며, 질의응답 과제에 사용된 UnifiedQA는 Hugging Face에서 공개적으로 사용할 수 있습니다.

평가에 사용된 세 가지 데이터셋인 QuALITY, QASPER, NarrativeQA는 모두 공개적으로 접근 가능합니다. QuALITY 데이터셋은 중간 길이의 텍스트에 대한 독해력을 평가하기 위한 데이터셋으로, GitHub을 통해 제공됩니다. QASPER는 NLP 논문들에 대한 전문적인 질의응답 데이터셋으로, Allen AI의 웹사이트에서 얻을 수 있습니다. NarrativeQA는 책과 영화 스크립트에 대한 질의응답 데이터셋으로, DeepMind의 GitHub 저장소에서 제공됩니다. 이러한 데이터셋들의 공개적 접근성은 본 연구에서 수행된 검색 및 질의응답 실험의 재현성을 보장합니다.

RAPTOR의 소스 코드는 [GitHub](https://github.com/parthsarthi03/raptor)을 통해 공개될 예정입니다. 이 코드베이스에는 트리 구축 알고리즘, 검색 메커니즘, 그리고 다양한 언어 모델과의 통합을 위한 인터페이스가 포함되어 있습니다. 코드는 모듈화된 구조로 설계되어 있어, 연구자들이 자신의 필요에 맞게 시스템의 각 구성 요소를 수정하거나 확장할 수 있습니다.

![Tree construction process](https://ar5iv.org//html/2401.18059/assets/x1.png)

RAPTOR의 트리 구축 과정은 위 그림과 같이 시각화할 수 있습니다. 이 프로세스는 텍스트를 의미적으로 유사한 청크들로 클러스터링하고, 각 클러스터에 대한 요약을 생성하여 계층적 구조를 형성합니다. 이러한 구조는 효율적인 정보 검색을 가능하게 하며, 특히 긴 문서에서 관련 정보를 찾는 데 효과적입니다.

구현의 핵심 구성요소로는 SBERT를 사용한 텍스트 임베딩, GMM(Gaussian Mixture Model) 기반 클러스터링, 그리고 GPT 모델을 활용한 요약 생성이 있습니다. 각 구성요소는 모듈화되어 있어 다른 임베딩 모델이나 클러스터링 알고리즘으로 쉽게 대체할 수 있습니다. 이러한 모듈성은 시스템의 유연성을 높이고 다양한 실험 설정을 가능하게 합니다.

- - -
### References
* [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](http://arxiv.org/pdf/2401.18059v1)
