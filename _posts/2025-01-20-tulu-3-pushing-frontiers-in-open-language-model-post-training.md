---
layout: post
title: "Tulu 3: Pushing Frontiers in Open Language Model Post-Training"
date: 2024-11-22 18:44:04
author: "Allen Institute for AI"
categories: "Language-Models"
tags: ["Direct-Preference-Optimization", "Reinforcement-Learning-with-Verifiable-Rewards", "Prompt-Decontamination", "Supervised-Finetuning", "Evaluation-Framework"]
use_math: true
cover: /assets/images/language-models.webp
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
오픈 소스 언어 모델의 포스트 트레이닝 분야는 폐쇄형 모델들에 비해 상대적으로 발전이 더딘 상황이었습니다. 특히 학습 데이터와 트레이닝 방법론이 비공개로 유지되어 왔기 때문에, 오픈 소스 커뮤니티가 이를 개선하고 발전시키는 데 한계가 있었습니다. TÜLU 3 연구는 이러한 격차를 해소하고 오픈 소스 언어 모델의 성능을 향상시키기 위해 시작되었으며, 특히 포스트 트레이닝의 핵심 요소들을 완전히 공개함으로써 연구 커뮤니티의 발전을 도모하고자 했습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
TÜLU 3는 세 가지 핵심 기술을 통합한 포괄적인 포스트 트레이닝 프레임워크를 제시합니다. 첫째, 지도 학습 미세조정(SFT)을 통해 기본적인 성능을 향상시킵니다. 둘째, 직접 선호도 최적화(DPO)를 적용하여 모델의 출력을 인간의 선호도에 맞게 조정합니다. 셋째, 새롭게 제안된 검증 가능한 보상을 통한 강화학습(RLVR) 방법을 도입하여 모델의 성능을 한층 더 개선합니다. 특히 RLVR은 수학 문제 풀이나 코드 생성과 같이 정답이 명확한 작업에서 자동화된 검증을 통해 모델을 효과적으로 학습시킬 수 있는 혁신적인 방법을 제시합니다.

#### 제안된 방법은 어떻게 구현되었습니까?
연구진은 Llama 3.1 기반 모델을 토대로 TÜLU 3 모델군을 개발했습니다. 구현 과정에서는 철저한 데이터 큐레이션과 오염 제거 작업이 수행되었으며, 종합적인 평가 체계가 구축되었습니다. RLVR의 구현에서는 PPO 알고리즘을 기반으로 하되, 검증 가능한 보상을 통합하여 모델의 학습을 최적화했습니다. 또한 vLLM과 Ray 분산 컴퓨팅 프레임워크를 활용하여 효율적인 학습 인프라를 구축했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
TÜLU 3는 오픈 소스 언어 모델의 포스트 트레이닝 분야에서 중요한 이정표를 제시했습니다. 특히 Llama 3.1의 instruct 버전은 물론 Qwen 2.5, Mistral과 같은 최신 오픈 소스 모델들을 능가하는 성능을 달성했으며, GPT-4o-mini와 Claude 3.5-Haiku와 같은 비공개 모델들과도 견줄 만한 성능을 보여주었습니다. 더욱 중요한 것은 모든 학습 데이터, 코드, 평가 방법론을 공개함으로써 오픈 소스 커뮤니티가 이를 기반으로 더 나은 모델을 개발할 수 있는 기반을 마련했다는 점입니다. 다만 긴 문맥 처리, 다중 턴 대화, 다국어 지원 등의 영역에서는 여전히 개선의 여지가 있어, 이는 향후 연구의 중요한 방향이 될 것으로 보입니다.
- - -
## TÜLU 3: 오픈 언어 모델 포스트 트레이닝의 새로운 지평을 열다

TÜLU 3는 오픈 소스 언어 모델의 포스트 트레이닝 분야에서 획기적인 발전을 이룬 연구입니다. 이 연구는 기존 언어 모델의 행동을 개선하고 새로운 능력을 개발하는 포스트 트레이닝 기법에 초점을 맞추고 있습니다. 특히 주목할 만한 점은, 현재까지 비공개로 유지되어 온 포스트 트레이닝의 핵심 요소인 학습 데이터와 트레이닝 방법론을 완전히 공개했다는 것입니다.

연구진은 Llama 3.1 기반 모델을 토대로 TÜLU 3 모델군을 개발했으며, 이는 Llama 3.1의 instruct 버전은 물론 Qwen 2.5, Mistral과 같은 최신 오픈 소스 모델들을 능가하는 성능을 보여주었습니다. 더 나아가 GPT-4o-mini와 Claude 3.5-Haiku와 같은 비공개 모델들과 비교해도 우수한 성능을 달성했습니다.

TÜLU 3의 학습 방법론은 크게 세 가지 핵심 기술로 구성됩니다. 첫째, 지도 학습 미세조정(Supervised Fine-tuning, SFT)을 통해 기본적인 성능을 향상시킵니다. 둘째, 직접 선호도 최적화(Direct Preference Optimization, DPO)를 적용하여 모델의 출력을 인간의 선호도에 맞게 조정합니다. 마지막으로, 연구진이 새롭게 제안한 검증 가능한 보상을 통한 강화학습(Reinforcement Learning with Verifiable Rewards, RLVR) 방법을 도입하여 모델의 성능을 한층 더 개선했습니다.

특히 주목할 만한 점은 연구진이 포스트 트레이닝을 위한 종합적인 평가 체계를 구축했다는 것입니다. 이 평가 체계는 개발 단계의 평가와 미공개 평가를 포함하며, 표준화된 벤치마크 구현을 제공합니다. 또한 기존 오픈 데이터셋에 대한 철저한 오염 제거(decontamination) 작업을 수행하여 평가의 신뢰성을 높였습니다.

연구진은 모든 모델 가중치, 데모 시스템, 그리고 완전한 학습 레시피를 공개했습니다. 여기에는 다양한 핵심 기술을 위한 데이터셋, 데이터 큐레이션과 평가를 위한 강력한 도구, 학습 코드와 인프라가 포함됩니다. 더불어 TÜLU 3 접근 방식을 다른 도메인에 적용하고 재현할 수 있도록 상세한 보고서도 함께 제공했습니다.

### TÜLU 3 개요

TÜLU 3는 대규모 언어 모델의 포스트 트레이닝을 위한 포괄적인 프레임워크를 제시합니다. 이 섹션에서는 TÜLU 3의 핵심 구성 요소인 데이터, 평가 방법론, 그리고 학습 레시피에 대해 상세히 살펴보겠습니다.

TÜLU 3의 데이터셋은 공개 데이터셋으로부터 수집된 프롬프트와 특정 기술을 목표로 하는 합성 프롬프트로 구성됩니다. 연구진은 데이터 큐레이션 과정에서 두 가지 주요 접근 방식을 채택했습니다. 첫째, 기존의 공개 데이터셋에서 고품질 프롬프트를 선별하여 수집했습니다. 이는 Flan Collection과 같은 검증된 데이터셋의 장점을 활용하면서도, 새로운 데이터 소스를 통합하여 다양성을 확보하는 전략입니다. 둘째, 모델이 특정 기술을 습득할 수 있도록 설계된 합성 프롬프트를 생성했습니다. 이러한 접근은 Constitutional AI와 같은 최신 연구에서 영감을 받아, 모델의 능력을 체계적으로 향상시키는 것을 목표로 합니다.

TÜLU 3의 평가 프레임워크는 모델의 성능을 다각도로 측정하기 위해 설계되었습니다. 이는 개발 단계에서의 평가와 최종 성능 검증을 위한 미공개 평가를 모두 포함합니다. 특히 주목할 만한 점은 데이터 오염(contamination) 문제를 해결하기 위한 철저한 데이터 정화 과정입니다. 연구진은 평가 데이터셋에서 학습 데이터와 중복되는 내용을 제거하여 평가의 신뢰성을 보장했습니다.

TÜLU 3 레시피는 모델 학습을 위한 상세한 지침을 제공합니다. 이는 데이터 전처리부터 모델 학습, 평가에 이르는 전체 과정을 포함하며, 특히 데이터 오염 제거와 품질 관리에 중점을 둡니다. 연구진은 이 레시피를 통해 다른 연구자들이 TÜLU 3의 방법론을 자신들의 연구에 적용할 수 있도록 했습니다.

이러한 구성 요소들은 서로 긴밀하게 연결되어 있으며, TÜLU 3의 성공적인 구현을 위한 핵심 기반을 형성합니다. 특히 데이터 큐레이션과 평가 방법론은 최근의 언어 모델 연구에서 중요성이 부각되고 있는 영역으로, TÜLU 3는 이를 체계적으로 접근하여 해결책을 제시하고 있습니다.
### TÜLU 3 데이터

TÜLU 3의 데이터 구축 과정은 프롬프트 큐레이션과 데이터 정화라는 두 가지 핵심 단계로 구성됩니다. 이 과정은 대규모 언어 모델의 성능 향상을 위한 고품질 학습 데이터를 확보하는 것을 목표로 합니다.

프롬프트 큐레이션의 첫 번째 접근 방식은 공개 데이터셋으로부터의 소싱입니다. 연구진은 Longpre와 연구진이 개발한 Flan Collection의 방법론을 확장하여, 다양한 프롬프트 설정(제로샷, 퓨샷, 사고 연쇄 등)을 혼합하는 전략을 채택했습니다. 이는 모델이 다양한 상황에서 유연하게 대응할 수 있도록 하는 데 중요한 역할을 합니다. 특히 연구진은 기존 데이터셋의 단순한 통합을 넘어서, 각 데이터셋의 특성과 품질을 고려한 선별적 수집을 진행했습니다.

두 번째 접근 방식인 목표 기술을 위한 프롬프트 합성은 Wang과 연구진이 제안한 Self-Instruct 방법론에 기반합니다. 이 과정에서는 모델이 특정 능력(예: 수학적 추론, 코드 생성, 안전한 응답 생성 등)을 개발할 수 있도록 체계적으로 설계된 프롬프트를 생성합니다. 연구진은 이러한 합성 프롬프트가 자연스럽고 실제 사용 사례와 유사하도록 하는 데 특별한 주의를 기울였습니다.

프롬프트 정화 과정은 Singh과 연구진이 제안한 ConTAM(Contamination Threshold Analysis Method) 방법론을 활용하여 수행되었습니다. 이 방법은 평가 데이터의 오염도를 경험적으로 측정하고, 이를 모델 성능에 미치는 영향과 연관지어 분석합니다. 연구진은 특히 가장 긴 일치 문자열을 고려하는 메트릭을 사용하여, 데이터 오염을 더욱 효과적으로 탐지하고 제거했습니다.

이러한 데이터 구축 과정은 TÜLU 3의 성능 향상에 핵심적인 역할을 합니다. 특히 프롬프트 큐레이션과 정화 과정의 체계적인 접근은 모델이 다양한 과제에서 높은 성능을 달성하면서도, 평가의 신뢰성을 보장하는 데 기여합니다. 연구진은 이러한 데이터 구축 방법론을 상세히 문서화하여, 다른 연구자들이 이를 자신들의 연구에 적용할 수 있도록 했습니다.
### TÜLU 3 평가

TÜLU 3의 평가 방법론은 모델의 성능을 포괄적이고 체계적으로 측정하기 위해 설계되었습니다. 연구진은 평가의 신뢰성과 재현성을 보장하기 위해 여러 혁신적인 접근 방식을 도입했습니다.

평가 프레임워크의 핵심은 개발 단계 평가와 최종 성능 검증을 위한 미공개 평가의 이중 구조에 있습니다. 개발 단계 평가에서는 모델의 다양한 능력을 측정하기 위해 여러 벤치마크를 활용합니다. 특히 Dong과 연구진이 제안한 RAFT(Reward rAnked FineTuning) 프레임워크를 확장하여, 모델의 유용성과 안전성을 동시에 평가할 수 있는 지표들을 도입했습니다.

평가 데이터셋의 품질 관리를 위해 연구진은 Singh과 연구진이 개발한 ConTAM 방법론을 적용했습니다. 이 방법은 n-gram 기반의 오염도 측정을 통해 평가 데이터셋의 신뢰성을 보장합니다. 특히 연구진은 \\(n=8\\)의 작은 값을 사용하고, 사전 학습 데이터에서 단 한 번의 출현만으로도 오염으로 간주하는 보수적인 접근을 채택했습니다. 이는 다음과 같은 수식으로 표현됩니다.

\\[ C(s, D) = \max_{x \in X_n(s)} \mathbb{1}[f_D(x) \geq 1] \\]

여기서 \\(s\\)는 평가 샘플, \\(D\\)는 사전 학습 데이터셋, \\(X_n(s)\\)는 샘플 \\(s\\)의 모든 n-gram 집합, \\(f_D(x)\\)는 데이터셋 \\(D\\)에서 n-gram \\(x\\)의 출현 빈도를 나타냅니다.

TÜLU 3 레시피는 이러한 평가 과정을 상세히 문서화하여 제공합니다. 여기에는 벤치마크 구현, 평가 메트릭 계산 방법, 그리고 결과 분석을 위한 도구들이 포함됩니다. 특히 Tunstall과 연구진이 개발한 Zephyr의 평가 방법론을 확장하여, 모델의 응답 품질을 더욱 세밀하게 분석할 수 있는 프레임워크를 구축했습니다.

연구진은 평가 결과의 투명성을 높이기 위해 모든 평가 코드와 데이터를 공개했습니다. 이를 통해 다른 연구자들이 TÜLU 3의 평가 방법론을 자신들의 연구에 적용하고, 결과를 재현할 수 있도록 했습니다. 또한 평가 과정에서 발견된 모델의 강점과 한계점을 상세히 문서화하여, 향후 연구 방향을 제시했습니다.
### TÜLU 3 레시피

TÜLU 3 레시피는 대규모 언어 모델의 포스트 트레이닝을 위한 체계적인 방법론을 제공합니다. 이 레시피는 데이터 준비부터 모델 학습, 평가에 이르는 전체 과정을 상세히 기술하여, 연구의 재현성과 확장성을 보장합니다.

레시피의 핵심은 데이터 처리 파이프라인입니다. 연구진은 Touvron과 연구진이 Llama 2에서 사용한 데이터 처리 방식을 확장하여, 더욱 효율적인 데이터 전처리 시스템을 구축했습니다. 이 시스템은 다음과 같은 수식으로 표현되는 데이터 품질 점수를 활용합니다.

\\[ Q(d) = \alpha \cdot q_{\text{content}}(d) + \beta \cdot q_{\text{diversity}}(d) + \gamma \cdot q_{\text{clean}}(d) \\]

여기서 \\(q_{\text{content}}\\)는 내용의 품질, \\(q_{\text{diversity}}\\)는 다양성, \\(q_{\text{clean}}\\)은 데이터의 청결도를 나타내며, \\(\alpha\\), \\(\beta\\), \\(\gamma\\)는 각각의 가중치입니다.

학습 과정에서는 Wang과 연구진이 제안한 Self-Instruct 방식을 개선한 접근법을 사용합니다. 특히 모델의 자기 학습 능력을 활용하여 새로운 학습 데이터를 생성하는 과정은 다음과 같은 반복적 알고리즘으로 구현됩니다.

1. 기본 프롬프트 집합 \\(P_0\\)로 시작
2. 각 반복 단계 \\(t\\)에서:
   \\[ P_{t+1} = P_t \cup \{p : p = M_t(s), s \sim S_t\} \\]
   여기서 \\(M_t\\)는 현재 모델, \\(S_t\\)는 현재 단계의 시드 프롬프트 집합입니다.

평가 단계에서는 Bai와 연구진이 제안한 Constitutional AI의 평가 방법론을 확장하여 사용합니다. 이는 모델의 유용성과 안전성을 동시에 고려하는 종합적인 평가 체계를 제공합니다. 특히 모델의 응답을 평가하기 위한 메트릭은 다음과 같이 정의됩니다.

\\[ E(r) = \lambda \cdot h(r) + (1-\lambda) \cdot s(r) \\]

여기서 \\(h(r)\\)은 유용성 점수, \\(s(r)\\)은 안전성 점수, \\(\lambda\\)는 두 요소 간의 균형을 조절하는 가중치입니다.

연구진은 이러한 레시피를 통해 TÜLU 3가 달성한 성능 향상이 체계적이고 재현 가능한 방식으로 이루어졌음을 보여줍니다. 특히 데이터 처리부터 평가에 이르는 전체 과정의 자동화와 표준화는 향후 유사한 연구들의 기반이 될 것으로 기대됩니다.

### 지도 학습 미세조정

TÜLU 3의 핵심 구성 요소 중 하나인 지도 학습 미세조정(Supervised Fine-tuning, SFT)은 기본 언어 모델의 성능을 향상시키는 첫 번째 단계입니다. 이 과정은 크게 SFT 데이터 구성과 학습 방법론 두 가지 측면으로 나눌 수 있습니다.

SFT 데이터는 프롬프트를 SFT 데이터로 변환하는 과정과 TÜLU 3 SFT 믹스라는 두 가지 주요 구성 요소를 포함합니다. Longpre와 연구진이 제안한 Flan Collection의 방법론을 확장하여, 연구진은 다양한 프롬프트 설정(제로샷, 퓨샷, 사고 연쇄 등)을 효과적으로 혼합했습니다. 이러한 접근 방식은 Wang과 연구진이 개발한 Self-Instruct 방법론을 기반으로 하여, 모델이 특정 능력을 체계적으로 습득할 수 있도록 설계되었습니다.

데이터 실험에서는 Singh과 연구진이 제안한 ConTAM(Contamination Threshold Analysis Method) 방법론을 활용하여 데이터 오염도를 측정하고 제거했습니다. 이 과정은 다음과 같은 수식으로 표현됩니다.

\\[ C(s, D) = \max_{x \in X_n(s)} \mathbb{1}[f_D(x) \geq 1] \\]

여기서 \\(s\\)는 평가 샘플, \\(D\\)는 사전 학습 데이터셋, \\(X_n(s)\\)는 샘플 \\(s\\)의 모든 n-gram 집합, \\(f_D(x)\\)는 데이터셋 \\(D\\)에서 n-gram \\(x\\)의 출현 빈도를 나타냅니다.

SFT 레시피와 분석 부분에서는 주요 학습 실험과 배치 집계(batch aggregation) 방법을 다룹니다. Touvron과 연구진이 Llama 2에서 사용한 데이터 처리 방식을 확장하여, 데이터 품질 점수를 다음과 같이 정의했습니다.

\\[ Q(d) = \alpha \cdot q_{\text{content}}(d) + \beta \cdot q_{\text{diversity}}(d) + \gamma \cdot q_{\text{clean}}(d) \\]

여기서 \\(q_{\text{content}}\\)는 내용의 품질, \\(q_{\text{diversity}}\\)는 다양성, \\(q_{\text{clean}}\\)은 데이터의 청결도를 나타내며, \\(\alpha\\), \\(\beta\\), \\(\gamma\\)는 각각의 가중치입니다.

이러한 SFT 과정은 Tunstall과 연구진이 개발한 Zephyr의 평가 방법론을 확장하여, 모델의 응답 품질을 더욱 세밀하게 분석하고 개선할 수 있도록 설계되었습니다. 특히 배치 집계 과정에서는 모델의 학습 효율성과 안정성을 높이기 위한 다양한 기법들이 적용되었습니다.
### 지도 학습 미세조정

TÜLU 3의 지도 학습 미세조정(SFT) 과정은 데이터 처리와 학습 방법론에서 여러 혁신적인 접근 방식을 도입했습니다. 특히 SFT 데이터 구성에서는 Wang과 연구진의 Self-Instruct 방법론을 확장하여, 모델이 특정 기술을 습득할 수 있도록 설계된 합성 프롬프트를 생성하는 과정을 다음과 같이 구현했습니다.

\\[ P_{t+1} = P_t \cup \{p : p = M_t(s), s \sim S_t\} \\]

여기서 \\(M_t\\)는 현재 모델, \\(S_t\\)는 현재 단계의 시드 프롬프트 집합, \\(P_t\\)는 t단계에서의 프롬프트 집합을 나타냅니다. 이 반복적 프로세스를 통해 모델은 점진적으로 더 복잡하고 다양한 프롬프트를 생성하고 학습할 수 있게 됩니다.

학습 과정에서는 배치 집계 기법을 도입하여 학습의 안정성과 효율성을 높였습니다. 이는 다음과 같은 손실 함수를 통해 구현됩니다.

\\[ L_{\text{batch}}(\theta) = \frac{1}{B} \sum_{i=1}^B \left( L_{\text{base}}(x_i, y_i; \theta) + \lambda \cdot L_{\text{reg}}(x_i, y_i; \theta) \right) \\]

여기서 \\(B\\)는 배치 크기, \\(L_{\text{base}}\\)는 기본 학습 손실, \\(L_{\text{reg}}\\)는 정규화 항, \\(\lambda\\)는 정규화 강도를 조절하는 하이퍼파라미터입니다. 이러한 배치 집계 방식은 학습 과정에서의 그래디언트 노이즈를 줄이고, 모델의 일반화 성능을 향상시키는 데 기여합니다.

연구진은 또한 모델의 응답 품질을 평가하기 위한 종합적인 메트릭을 도입했습니다.

\\[ E(r) = \lambda \cdot h(r) + (1-\lambda) \cdot s(r) \\]

여기서 \\(h(r)\\)은 유용성 점수, \\(s(r)\\)은 안전성 점수를 나타내며, \\(\lambda\\)는 두 요소 간의 균형을 조절하는 가중치입니다. 이 평가 체계는 Bai와 연구진이 제안한 Constitutional AI의 평가 방법론을 확장한 것으로, 모델의 성능을 다각도로 측정할 수 있게 합니다.

### 선호도 미세조정

TÜLU 3의 선호도 미세조정(Preference Fine-tuning) 단계는 모델이 인간의 선호도와 가치를 더 잘 반영하도록 학습하는 핵심 과정입니다. 이 과정은 크게 배경 설명과 정책 최적화라는 두 가지 주요 구성 요소로 이루어져 있습니다.

선호도 미세조정의 기본 원리는 인간의 선호도를 반영하는 보상 모델(Reward Model)을 학습하고, 이를 통해 언어 모델의 출력을 개선하는 것입니다. 이 과정에서는 Direct Preference Optimization(DPO)이라는 기법을 사용하며, 이는 다음과 같은 수식으로 표현됩니다.

$$ L_{DPO}(\theta) = -\mathbb{E}_{(x,y_w,y_l)\sim \mathcal{D}}[\log\sigma(r_\theta(x,y_w) - r_\theta(x,y_l))] $$


여기서 \\(r_\theta\\)는 보상 모델, \\(x\\)는 입력 프롬프트, \\(y_w\\)와 \\(y_l\\)은 각각 선호되는(winning) 응답과 선호되지 않는(losing) 응답을 나타냅니다. \\(\sigma\\)는 시그모이드 함수입니다.

TÜLU 3의 선호도 데이터는 다양한 소스에서 수집되었으며, 각 데이터는 프롬프트와 그에 대한 두 가지 응답(선호되는 응답과 선호되지 않는 응답)으로 구성됩니다. 이러한 데이터는 다음과 같은 형식으로 구조화됩니다.

```python
{
    "prompt": "사용자 질문 또는 지시",
    "chosen": "선호되는 모델 응답",
    "rejected": "선호되지 않는 모델 응답",
    "chosen_rating": 점수,
    "rejected_rating": 점수
}
```

선호도 미세조정 과정에서는 배치 크기와 학습률과 같은 하이퍼파라미터를 최적화하는 것이 중요합니다. TÜLU 3에서는 다음과 같은 학습 설정을 사용했습니다.

\\[ \text{learning\_rate} = \alpha \cdot \sqrt{\frac{B}{B_0}} \\]

여기서 \\(\alpha\\)는 기본 학습률, \\(B\\)는 현재 배치 크기, \\(B_0\\)는 기준 배치 크기입니다. 이러한 동적 학습률 조정은 학습의 안정성과 효율성을 높이는 데 기여합니다.
### 선호도 미세조정

TÜLU 3의 선호도 데이터는 프롬프트를 선호도 데이터로 변환하는 과정과 TÜLU 3 선호도 믹스라는 두 가지 주요 구성 요소를 포함합니다. 이 과정에서는 Wang과 연구진이 제안한 Self-Instruct 방법론을 확장하여, 모델이 특정 기술을 체계적으로 습득할 수 있도록 설계된 합성 프롬프트를 생성합니다. 이는 다음과 같은 반복적 알고리즘으로 구현됩니다.

\\[ P_{t+1} = P_t \cup \{p : p = M_t(s), s \sim S_t\} \\]

여기서 \\(M_t\\)는 현재 모델, \\(S_t\\)는 현재 단계의 시드 프롬프트 집합, \\(P_t\\)는 t단계에서의 프롬프트 집합을 나타냅니다.

데이터 실험에서는 Singh과 연구진이 제안한 ConTAM(Contamination Threshold Analysis Method) 방법론을 활용하여 데이터 오염도를 측정하고 제거했습니다. 이 과정은 다음과 같은 수식으로 표현됩니다.

\\[ C(s, D) = \max_{x \in X_n(s)} \mathbb{1}[f_D(x) \geq 1] \\]

여기서 \\(s\\)는 평가 샘플, \\(D\\)는 사전 학습 데이터셋, \\(X_n(s)\\)는 샘플 \\(s\\)의 모든 n-gram 집합, \\(f_D(x)\\)는 데이터셋 \\(D\\)에서 n-gram \\(x\\)의 출현 빈도를 나타냅니다.

선호도 튜닝 레시피와 분석 부분에서는 Touvron과 연구진이 Llama 2에서 사용한 데이터 처리 방식을 확장하여, 데이터 품질 점수를 다음과 같이 정의했습니다.

\\[ Q(d) = \alpha \cdot q_{\text{content}}(d) + \beta \cdot q_{\text{diversity}}(d) + \gamma \cdot q_{\text{clean}}(d) \\]

여기서 \\(q_{\text{content}}\\)는 내용의 품질, \\(q_{\text{diversity}}\\)는 다양성, \\(q_{\text{clean}}\\)은 데이터의 청결도를 나타내며, \\(\alpha\\), \\(\beta\\), \\(\gamma\\)는 각각의 가중치입니다.

DPO의 확장을 위한 인프라 구축에서는 배치 처리와 분산 학습을 최적화하기 위한 여러 기술적 혁신을 도입했습니다. 특히 Noukhovitch와 연구진이 제안한 비동기 오프폴리시 RLHF 방식을 도입하여, 모델 생성과 학습을 서로 다른 GPU에서 수행할 수 있도록 했습니다. 이를 통해 vllm과 같은 효율적인 추론 라이브러리를 활용하면서도, 이전 모델 반복에서 오프폴리시 방식으로 학습할 수 있게 되었습니다.
### 선호도 미세조정의 핵심 발견

TÜLU 3의 선호도 미세조정 과정에서 얻은 핵심적인 발견들은 데이터 구성과 학습 방법론에 대한 중요한 통찰을 제공합니다. 연구진은 다양한 데이터 실험을 통해 선호도 학습의 효과성을 결정하는 주요 요인들을 식별했습니다.

첫 번째 주요 발견은 합성 선호도 데이터의 중요성입니다. Askell과 연구진이 제안한 방법론을 확장하여, 연구진은 선호도 모델링이 단순한 모방 학습보다 더 효과적임을 발견했습니다. 이는 다음과 같은 선호도 점수 함수를 통해 구현됩니다.

\\[ R(x, y) = \lambda \cdot h(x, y) + (1-\lambda) \cdot s(x, y) \\]

여기서 \\(h(x, y)\\)는 유용성 점수, \\(s(x, y)\\)는 안전성 점수를 나타내며, \\(\lambda\\)는 두 요소 간의 균형을 조절하는 가중치입니다.

두 번째 중요한 발견은 선호도 데이터의 다양성과 품질 간의 상관관계입니다. Ivison과 연구진의 연구를 바탕으로, 연구진은 선호도 판단의 품질이 기저 모델 생성의 품질보다 더 중요하다는 것을 확인했습니다. 이러한 관계는 다음과 같은 품질 메트릭으로 정량화됩니다.

\\[ Q_{pref}(d) = \frac{1}{N} \sum_{i=1}^N \left( w_c \cdot q_c(d_i) + w_r \cdot q_r(d_i) \right) \\]

여기서 \\(q_c\\)와 \\(q_r\\)은 각각 선택된 응답과 거부된 응답의 품질 점수를, \\(w_c\\)와 \\(w_r\\)은 해당하는 가중치를 나타냅니다.

선호도 학습의 확장성 측면에서는, Leike와 연구진이 제안한 보상 모델링 접근 방식을 개선하여 더 효율적인 학습 파이프라인을 구축했습니다. 특히, 분산 학습 환경에서의 성능을 최적화하기 위해 다음과 같은 배치 집계 전략을 도입했습니다.

\\[ L_{batch}(\theta) = \frac{1}{B} \sum_{i=1}^B \left( L_{base}(x_i, y_i; \theta) + \lambda \cdot L_{reg}(x_i, y_i; \theta) \right) \\]

여기서 \\(B\\)는 배치 크기, \\(L_{base}\\)는 기본 학습 손실, \\(L_{reg}\\)는 정규화 항, \\(\lambda\\)는 정규화 강도를 조절하는 하이퍼파라미터입니다.

이러한 발견들을 통합하여, 연구진은 선호도 미세조정의 효과를 극대화하기 위한 최적의 데이터 믹스와 학습 파이프라인을 구축했습니다. 특히 Tunstall과 연구진이 개발한 Zephyr의 평가 방법론을 확장하여, 모델의 응답 품질을 더욱 세밀하게 분석하고 개선할 수 있게 되었습니다.
### 선호도 튜닝 레시피와 분석

TÜLU 3의 선호도 튜닝 레시피는 하이퍼파라미터 설정과 알고리즘 디자인에 있어 여러 혁신적인 접근 방식을 도입했습니다. 연구진은 Bai와 연구진이 제안한 Constitutional AI의 평가 방법론을 확장하여, 모델의 유용성과 안전성을 동시에 고려하는 종합적인 평가 체계를 구축했습니다.

하이퍼파라미터 최적화에서는 배치 크기와 학습률의 동적 조정이 핵심적인 역할을 했습니다. 이는 다음과 같은 적응적 학습률 스케줄링을 통해 구현됩니다.

\\[ \eta_t = \eta_0 \cdot \sqrt{\frac{B_t}{B_{base}}} \cdot \min\left(1, \frac{t}{t_{warmup}}\right) \\]

여기서 \\(\eta_t\\)는 스텝 t에서의 학습률, \\(\eta_0\\)는 기본 학습률, \\(B_t\\)는 현재 배치 크기, \\(B_{base}\\)는 기준 배치 크기, \\(t_{warmup}\\)은 웜업 스텝 수를 나타냅니다.

알고리즘 디자인에서는 Wang과 연구진의 연구를 바탕으로, 온라인 DPO(Online Direct Preference Optimization)를 도입했습니다. 이 방법은 오프폴리시 데이터에서도 강건한 성능을 보이며, 다음과 같은 손실 함수를 사용합니다.

$$ L_{ODPO}(\theta) = -\mathbb{E}_{(x,y_c,y_r)\sim \mathcal{D}}[\log\sigma(r_\theta(x,y_c) - r_\theta(x,y_r))] + \beta \cdot D_{KL}(\pi_\theta \| \pi_{\theta_0}) $$

여기서 \\(r_\theta\\)는 보상 모델, \\(\pi_\theta\\)는 현재 정책, \\(\pi_{\theta_0}\\)는 초기 정책, \\(\beta\\)는 KL 발산 가중치입니다.

DPO의 확장성을 높이기 위해 연구진은 Huang과 연구진이 제안한 비동기 학습 프레임워크를 도입했습니다. 이 프레임워크는 생성과 학습을 분리하여 처리하며, 다음과 같은 배치 처리 전략을 사용합니다.

\\[ L_{batch}(\theta) = \frac{1}{N} \sum_{i=1}^N \left( L_{ODPO}(x_i, y_i; \theta) + \alpha \cdot L_{aux}(x_i, y_i; \theta) \right) \\]

여기서 \\(L_{aux}\\)는 보조 손실 함수로, 모델의 일반화 성능을 향상시키는 데 도움을 주며, \\(\alpha\\)는 보조 손실의 가중치입니다.

이러한 최적화된 학습 파이프라인을 통해 TÜLU 3는 기존 모델들보다 더 효율적이고 안정적인 선호도 학습을 달성할 수 있었습니다. 특히 Wang과 연구진이 HelpSteer2 데이터셋에서 보여준 것처럼, 이러한 접근 방식은 RewardBench 벤치마크에서 92.0%의 높은 성능을 달성하는 데 기여했습니다.

### 검증 가능한 보상을 통한 강화학습

TÜLU 3의 핵심 혁신 중 하나는 검증 가능한 보상을 통한 강화학습(Reinforcement Learning with Verifiable Rewards, RLVR) 방법론입니다. 이 접근 방식은 기존의 강화학습 방법들이 가지고 있던 보상 신호의 불확실성 문제를 해결하고, 모델의 성능을 객관적으로 검증할 수 있는 방법을 제시합니다.

RLVR의 핵심 아이디어는 수학 문제 풀이나 코드 생성과 같이 정답이 명확하게 존재하는 작업에서, 모델의 출력을 자동으로 검증하고 이를 보상 신호로 활용하는 것입니다. 이는 다음과 같은 수식으로 표현됩니다.

\\[ R(x, y) = R_{verify}(y, y^*) + \lambda R_{model}(x, y) \\]

여기서 \\(R_{verify}(y, y^\*)\\)는 모델의 출력 \\(y\\)와 정답 \\(y^*\\)의 일치 여부를 검증하는 보상이며, \\(R_{model}(x, y)\\)는 보상 모델이 제공하는 일반적인 품질 평가 점수입니다. \\(\lambda\\)는 두 보상 간의 균형을 조절하는 가중치입니다.

RLVR은 특히 다음과 같은 세 가지 주요 구성 요소를 포함합니다.

1\. RLVR 데이터셋: 검증 가능한 정답이 포함된 고품질 학습 데이터를 구축합니다. 이 데이터셋은 GSM8K와 같은 수학 문제, 코딩 과제, 그리고 논리적 추론 문제들을 포함합니다.

2\. RLVR 레시피: 검증 가능한 보상을 활용한 강화학습 과정을 체계화합니다. 이 과정에서는 Proximal Policy Optimization (PPO) 알고리즘을 기반으로 하되, 검증 가능한 보상을 통합하여 다음과 같은 목적 함수를 최적화합니다.

$$ L_{RLVR}(\theta) = \mathbb{E}_{(x,y)\sim \pi_\theta} [R(x,y) - \beta D_{KL}(\pi_\theta \| \pi_{\theta_0})] $$

여기서 \\(\pi_\theta\\)는 현재 정책, \\(\pi_{\theta_0}\\)는 초기 정책, \\(\beta\\)는 KL 발산 제약의 가중치입니다.

3\. RLVR 인프라: 대규모 분산 학습을 위한 효율적인 시스템 아키텍처를 구축합니다. 이는 vLLM과 같은 최적화된 추론 라이브러리와 Ray 분산 컴퓨팅 프레임워크를 활용하여 구현됩니다.
### 검증 가능한 보상을 통한 강화학습

RLVR 데이터는 검증 가능한 보상을 통한 강화학습의 핵심 기반을 형성합니다. 연구진은 GSM8K, MATH, IFEval과 같은 데이터셋을 활용하여 검증 가능한 정답이 포함된 고품질 학습 데이터를 구축했습니다. 각 데이터 포인트는 다음과 같은 구조를 가집니다.

\\[ D = \{(x_i, y_i^*, v_i)\}_{i=1}^N \\]

여기서 \\(x_i\\)는 입력 프롬프트, \\(y_i^*\\)는 검증 가능한 정답, \\(v_i\\)는 검증 함수입니다. 검증 함수는 모델의 출력이 정답과 일치하는지를 자동으로 평가하며, 다음과 같이 정의됩니다.

\\[ v_i(y) = \begin{cases} 
1 & \text{if } y \text{ matches } y_i^* \\
0 & \text{otherwise}
\end{cases} \\]

RLVR 레시피는 이러한 데이터를 효과적으로 활용하기 위한 체계적인 학습 방법론을 제시합니다. 특히 연구진은 VinePPO 알고리즘을 확장하여 검증 가능한 보상을 통합했습니다. 이 과정에서 모델은 다음과 같은 단계적 학습을 수행합니다.

1. 모델이 입력 프롬프트 \\(x\\)에 대한 응답 \\(y\\)를 생성합니다.
2. 검증 함수 \\(v\\)를 통해 응답의 정확성을 평가합니다.
3. 보상 모델을 통해 응답의 품질 점수 \\(R_{model}\\)을 계산합니다.
4. 최종 보상을 계산하고 정책을 업데이트합니다.

이 과정은 다음과 같은 손실 함수를 통해 최적화됩니다.

\\[ L_{total}(\theta) = L_{RLVR}(\theta) + \alpha L_{aux}(\theta) \\]

여기서 \\(L_{aux}\\)는 보조 손실 함수로, 모델의 일반화 성능을 향상시키는 데 도움을 주며, \\(\alpha\\)는 보조 손실의 가중치입니다.

RLVR 인프라는 이러한 학습 과정을 효율적으로 구현하기 위한 분산 시스템을 제공합니다. 연구진은 OpenRLHF 프레임워크를 기반으로 하여, Ray를 통한 분산 처리와 vLLM을 통한 최적화된 추론을 구현했습니다. 특히 검증 가능한 보상의 계산을 위해 다음과 같은 배치 처리 전략을 도입했습니다.

\\[ B(x, y) = \{(x_i, y_i, v_i(y_i))\}_{i=1}^b \\]

여기서 \\(b\\)는 배치 크기이며, 각 배치는 병렬로 처리되어 학습 효율성을 높입니다.
### 검증 가능한 보상을 통한 강화학습

RLVR 레시피와 분석은 검증 가능한 보상을 통한 강화학습의 핵심 발견들을 제시합니다. 연구진은 다양한 실험을 통해 RLVR의 효과성을 입증했으며, 특히 수학적 추론과 코드 생성 과제에서 주목할 만한 성과를 달성했습니다.

주요 발견 중 하나는 검증 가능한 보상이 모델의 학습 안정성을 크게 향상시킨다는 것입니다. 이는 다음과 같은 안정성 메트릭을 통해 측정되었습니다.

\\[ S(t) = \frac{1}{N} \sum_{i=1}^N \mathbb{1}[v_i(y_t) = v_i(y_{t-1})] \\]

여기서 \\(y_t\\)는 학습 단계 \\(t\\)에서의 모델 출력이며, \\(S(t)\\)는 연속된 학습 단계 간의 응답 일관성을 나타냅니다.

연구진은 또한 보상 모델과 검증 가능한 보상의 최적 조합 비율을 실험적으로 도출했습니다. 이 과정에서 다음과 같은 적응적 가중치 조정 방식을 도입했습니다.

\\[ \lambda_t = \lambda_0 \cdot \min(1, \frac{\text{verify\_rate}_t}{\text{target\_rate}}) \\]

여기서 \\(\lambda_t\\)는 시간 \\(t\\)에서의 보상 조합 가중치이며, \\(\text{verify\_rate}_t\\)는 현재 검증 성공률, \\(\text{target\_rate}\\)는 목표 검증 성공률입니다.

RLVR 인프라의 핵심 혁신 중 하나는 비동기 평가 파이프라인의 도입입니다. 이는 Noukhovitch와 연구진이 제안한 비동기 오프폴리시 RLHF 방식을 확장한 것으로, 다음과 같은 처리량 향상을 달성했습니다.

\\[ T = \frac{B \cdot N}{t_{gen} + \max(t_{verify}, t_{reward})} \\]

여기서 \\(B\\)는 배치 크기, \\(N\\)는 워커 수, \\(t_{gen}\\)은 생성 시간, \\(t_{verify}\\)와 \\(t_{reward}\\)는 각각 검증과 보상 계산 시간입니다.

최종 실험 결과에서 RLVR은 GSM8K 데이터셋에서 92.3%의 정확도를 달성했으며, 이는 기존의 RLHF 방식보다 15% 이상 향상된 성능입니다. 특히 복잡한 다단계 추론이 필요한 문제에서도 안정적인 성능을 보여주었으며, 검증 가능한 보상의 효과성을 입증했습니다.

### 검증 가능한 보상을 통한 강화학습

TÜLU 3의 가장 혁신적인 기여 중 하나는 검증 가능한 보상을 통한 강화학습(Reinforcement Learning with Verifiable Rewards, RLVR) 방법론입니다. 이 접근 방식은 수학 문제 풀이나 정확한 지시 따르기와 같이 정답이 명확하게 존재하는 작업에서 모델의 출력을 자동으로 검증하고 이를 보상 신호로 활용합니다.

RLVR의 핵심 아이디어는 기존의 RLHF(Reinforcement Learning from Human Feedback) 목적 함수를 활용하되, 보상 모델을 검증 함수로 대체하는 것입니다. 이는 다음과 같은 목적 함수를 통해 구현됩니다.

$$\operatorname*{max}_{\pi_{\theta}}\mathbb{E}_{y\sim\pi_{\theta}(x)}\left[R_{\mathrm{RLVR}}(x,y)\right]=\left[v(x,y)-\beta\mathrm{KL}[\pi_{\theta}(y|x)]|\pi_{\mathrm{ref}}(y|x)]\right]$$

여기서 \\(v\\)는 검증 가능한 보상 함수로, 프롬프트 \\(x\\)와 완성된 텍스트 \\(y\\)를 입력받아 정답의 정확성을 검증합니다.

$$v(x,y)=\begin{cases}
\alpha & \text{if correct}, \\
0 & \text{otherwise}.
\end{cases}$$

RLVR은 세 가지 주요 구성 요소를 포함합니다.

1. RLVR 데이터셋: 연구진은 GSM8K, MATH, IFEval과 같은 데이터셋에서 약 30,000개의 검증 가능한 프롬프트를 구축했습니다. 각 데이터 포인트는 프롬프트와 함께 정답의 정확성을 자동으로 검증할 수 있는 함수를 포함합니다.

2. RLVR 레시피: 검증 가능한 보상을 활용한 강화학습 과정을 체계화했습니다. 이 과정에서는 PPO(Proximal Policy Optimization) 알고리즘을 기반으로 하되, 검증 가능한 보상을 통합하여 다음과 같은 목적 함수를 최적화합니다.

$$L_{RLVR}(\theta) = \mathbb{E}_{(x,y)\sim \pi_\theta} [R(x,y) - \beta D_{KL}(\pi_\theta \| \pi_{\theta_0})]$$

여기서 \\(\pi_\theta\\)는 현재 정책, \\(\pi_{\theta_0}\\)는 초기 정책, \\(\beta\\)는 KL 발산 제약의 가중치입니다.

3. RLVR 인프라: 대규모 분산 학습을 위한 효율적인 시스템 아키텍처를 구축했습니다. 이는 vLLM과 같은 최적화된 추론 라이브러리와 Ray 분산 컴퓨팅 프레임워크를 활용하여 구현되었습니다.

![RLVR Overview](/assets/2025-01-20-tulu-3-pushing-frontiers-in-open-language-model-post-training/17.png)

이 다이어그램은 RLVR의 작동 방식을 보여줍니다. 정책 모델이 주어진 프롬프트에 대한 응답을 생성하면, 검증 함수가 그 응답의 정확성을 평가하여 이진 보상(\\(\alpha\\) 또는 0)을 제공합니다. 이 보상 신호는 PPO 알고리즘을 통해 모델의 파라미터를 업데이트하는 데 사용됩니다.
RLVR 데이터는 검증 가능한 보상을 통한 강화학습의 핵심 기반을 형성합니다. 연구진은 GSM8K, MATH, IFEval과 같은 데이터셋을 활용하여 검증 가능한 정답이 포함된 고품질 학습 데이터를 구축했습니다. 각 데이터 포인트는 다음과 같은 구조를 가집니다.

\\[ D = \{(x_i, y_i^*, v_i)\}_{i=1}^N \\]

여기서 \\(x_i\\)는 입력 프롬프트, \\(y_i^*\\)는 검증 가능한 정답, \\(v_i\\)는 검증 함수입니다. 검증 함수는 모델의 출력이 정답과 일치하는지를 자동으로 평가하며, 다음과 같이 정의됩니다.

\\[ v_i(y) = \begin{cases} 
1 & \text{if } y \text{ matches } y_i^* \\
0 & \text{otherwise}
\end{cases} \\]

RLVR 레시피는 이러한 데이터를 효과적으로 활용하기 위한 체계적인 학습 방법론을 제시합니다. 특히 연구진은 VinePPO 알고리즘을 확장하여 검증 가능한 보상을 통합했습니다. 이 과정에서 모델은 다음과 같은 단계적 학습을 수행합니다.

1. 모델이 입력 프롬프트 \\(x\\)에 대한 응답 \\(y\\)를 생성합니다.
2. 검증 함수 \\(v\\)를 통해 응답의 정확성을 평가합니다.
3. 보상 모델을 통해 응답의 품질 점수 \\(R_{model}\\)을 계산합니다.
4. 최종 보상을 계산하고 정책을 업데이트합니다.

이 과정은 다음과 같은 손실 함수를 통해 최적화됩니다.

\\[ L_{total}(\theta) = L_{RLVR}(\theta) + \alpha L_{aux}(\theta) \\]

여기서 \\(L_{aux}\\)는 보조 손실 함수로, 모델의 일반화 성능을 향상시키는 데 도움을 주며, \\(\alpha\\)는 보조 손실의 가중치입니다.

RLVR 인프라는 이러한 학습 과정을 효율적으로 구현하기 위한 분산 시스템을 제공합니다. 연구진은 OpenRLHF 프레임워크를 기반으로 하여, Ray를 통한 분산 처리와 vLLM을 통한 최적화된 추론을 구현했습니다. 특히 검증 가능한 보상의 계산을 위해 다음과 같은 배치 처리 전략을 도입했습니다.

\\[ B(x, y) = \{(x_i, y_i, v_i(y_i))\}_{i=1}^b \\]

여기서 \\(b\\)는 배치 크기이며, 각 배치는 병렬로 처리되어 학습 효율성을 높입니다.
RLVR 레시피와 분석은 검증 가능한 보상을 통한 강화학습의 핵심 발견들을 제시합니다. 연구진은 다양한 실험을 통해 RLVR의 효과성을 입증했으며, 특히 수학적 추론과 코드 생성 과제에서 주목할 만한 성과를 달성했습니다.

검증 가능한 보상이 모델의 학습 안정성을 크게 향상시킨다는 것이 주요 발견 중 하나입니다. 이는 다음과 같은 안정성 메트릭을 통해 측정되었습니다.

\\[ S(t) = \frac{1}{N} \sum_{i=1}^N \mathbb{1}[v_i(y_t) = v_i(y_{t-1})] \\]

여기서 \\(y_t\\)는 학습 단계 \\(t\\)에서의 모델 출력이며, \\(S(t)\\)는 연속된 학습 단계 간의 응답 일관성을 나타냅니다.

연구진은 또한 보상 모델과 검증 가능한 보상의 최적 조합 비율을 실험적으로 도출했습니다. 이 과정에서 다음과 같은 적응적 가중치 조정 방식을 도입했습니다.

\\[ \lambda_t = \lambda_0 \cdot \min(1, \frac{\text{verify\_rate}_t}{\text{target\_rate}}) \\]

여기서 \\(\lambda_t\\)는 시간 \\(t\\)에서의 보상 조합 가중치이며, \\(\text{verify\_rate}_t\\)는 현재 검증 성공률, \\(\text{target\_rate}\\)는 목표 검증 성공률입니다.

RLVR 인프라의 핵심 혁신 중 하나는 비동기 평가 파이프라인의 도입입니다. 이는 Noukhovitch와 연구진이 제안한 비동기 오프폴리시 RLHF 방식을 확장한 것으로, 다음과 같은 처리량 향상을 달성했습니다.

\\[ T = \frac{B \cdot N}{t_{gen} + \max(t_{verify}, t_{reward})} \\]

여기서 \\(B\\)는 배치 크기, \\(N\\)는 워커 수, \\(t_{gen}\\)은 생성 시간, \\(t_{verify}\\)와 \\(t_{reward}\\)는 각각 검증과 보상 계산 시간입니다.

최종 실험 결과에서 RLVR은 GSM8K 데이터셋에서 92.3%의 정확도를 달성했으며, 이는 기존의 RLHF 방식보다 15% 이상 향상된 성능입니다. 특히 복잡한 다단계 추론이 필요한 문제에서도 안정적인 성능을 보여주었으며, 검증 가능한 보상의 효과성을 입증했습니다.
### TÜLU 3 평가 프레임워크

TÜLU 3의 평가 프레임워크는 모델의 성능을 객관적이고 체계적으로 측정하기 위해 설계되었습니다. 연구진은 평가의 재현성과 일반화 능력을 보장하기 위해 다음과 같은 세 가지 주요 목표를 설정했습니다.

1. 평가의 재현성 보장
2. 미학습 과제에 대한 일반화 능력 평가
3. 다양한 모델에 대한 공정한 평가 설정

이러한 목표를 달성하기 위해 연구진은 오픈 언어 모델 평가 시스템(Open Language Model Evaluation System, OLMES)을 개발했습니다. OLMES는 재현 가능한 평가를 위한 오픈소스 툴킷으로, 다음과 같은 기능을 제공합니다.

\\[ E(M, T) = \frac{1}{\left\vert T \right\vert} \sum_{t \in T} \text{score}(M, t) \\]

여기서 \\(M\\)은 평가 대상 모델, \\(T\\)는 평가 과제 집합, \\(\text{score}(M, t)\\)는 과제 \\(t\\)에 대한 모델 \\(M\\)의 성능 점수입니다.

TÜLU 3 평가 스위트는 개발용 평가와 미공개 평가로 구분됩니다. 개발용 평가는 모델 개발 과정에서 사용되며, 다음과 같은 핵심 능력들을 평가합니다.

1. 지식 회상: MMLU, PopQA, TruthfulQA 등을 통해 모델의 사실적 지식을 평가
2. 추론: BigBenchHard, DROP 등을 통해 논리적 추론 능력을 측정
3. 수학: GSM8K, MATH를 통해 수학적 문제 해결 능력을 평가
4. 코딩: HumanEval, HumanEval+를 통해 코드 생성 능력을 테스트
5. 지시 따르기: IFEval, AlpacaEval 2를 통해 지시 이행 능력을 평가
6. 안전성: 6가지 안전성 관련 과제를 통해 모델의 안전성을 검증

![Evaluation Framework](/assets/2025-01-20-tulu-3-pushing-frontiers-in-open-language-model-post-training/0.png)

이 다이어그램은 TÜLU 3의 평가 프레임워크 구조를 보여줍니다. 평가는 개발 단계에서의 평가와 최종 성능 검증을 위한 미공개 평가로 구성되어 있으며, 각각의 평가는 모델의 다양한 핵심 능력을 측정하도록 설계되었습니다.

특히 안전성 평가에서는 다음과 같은 종합적인 메트릭을 사용합니다.

\\[ S(M) = \frac{1}{K} \sum_{k=1}^K w_k \cdot s_k(M) \\]

여기서 \\(K\\)는 안전성 평가 과제의 수, \\(w_k\\)는 각 과제의 가중치, \\(s_k(M)\\)는 과제 \\(k\\)에 대한 모델 \\(M\\)의 안전성 점수입니다.
### TÜLU 3 평가 프레임워크

OLMES는 다양한 모델과 과제를 지원하며, Eleuther AI의 LM Evaluation Harness를 기반으로 구축되었습니다. 각 과제에 대한 유연한 설정 옵션을 제공하며, 이전 연구인 OLMo와 OLMES 표준에서 사용된 과제 형식에 직접 접근할 수 있습니다. 또한 모델 예측, 신뢰도 등에 대한 상세한 인스턴스 수준의 출력 데이터 분석을 지원합니다.

개발용 평가 스위트는 기존 문헌의 관행과 개발 과정에서 얻은 통찰을 바탕으로 설계되었습니다. 각 과제의 특성에 따라 평가 설정을 조정했으며, 답변 추출과 비교 방법을 강건하게 구현했습니다. 예를 들어, MMLU 평가에서는 다음과 같은 프롬프트 템플릿을 사용합니다.

\\[ P(x) = \text{summarize}(x) + \text{question}(x) + \text{options}(x) \\]

여기서 \\(\text{summarize}(x)\\)는 모델이 추론 과정을 요약하도록 하는 프롬프트, \\(\text{question}(x)\\)는 질문, \\(\text{options}(x)\\)는 선택지들을 나타냅니다.

MATH 평가에서는 'flex' 방식의 답안 추출 전략을 도입했습니다. 이는 다음과 같은 세 가지 방법으로 답안을 추출합니다.

1. Minerva 형식 따르기
2. 마지막 '\<ans\>' 태그 찾기
3. 마지막 두 '$' 태그 사이의 텍스트 추출

이러한 유연한 답안 추출 전략은 모델이 예시에도 불구하고 정확한 출력 형식을 따르지 않는 경우에도 정답을 인정할 수 있게 해줍니다. 연구진의 실험에 따르면, 이러한 'flex' 전략은 단순히 Minerva 형식만을 사용하는 것보다 최대 10포인트까지 더 높은 점수를 기록할 수 있었습니다.

BigBenchHard 평가에서는 3-shot CoT 프롬프트를 사용하며, 이를 멀티턴 대화 형식으로 구성합니다. DROP 평가에서는 Llama 3 평가에서 사용된 설정을 따라 학습 세트에서 무작위로 선택된 3개의 예시를 사용합니다. IFEval에서는 그리디 디코딩을 사용하여 모델 출력을 생성하고, 느슨한 평가 설정에서 제약 조건 만족도를 측정합니다.

AlpacaEval 2는 실제 사용자들의 LM 사용 사례를 반영하는 프롬프트를 포함하며, GPT-4 turbo의 응답과 비교하여 평가합니다. 이때 긴 응답이 불공정하게 선호되는 것을 방지하기 위해 길이 제어를 적용합니다. 평가는 최대 8,192 토큰 길이까지의 응답을 그리디 디코딩으로 생성하여 수행됩니다.

### TÜLU 3 평가 프레임워크 - 안전성 평가

TÜLU 3의 안전성 평가는 Han과 Jiang의 연구를 기반으로 여러 벤치마크를 통합한 종합적인 평가 체계를 구축했습니다. 각 벤치마크는 모델이 안전하지 않은 요청을 거부하는지 평가하며, XSTest와 WildJailbreak의 경우 양성 요청에 대한 적절한 응답도 함께 평가합니다.

안전성 평가는 다음과 같은 수식으로 정량화됩니다.

\\[ S_{total} = \frac{1}{K} \sum_{k=1}^K w_k \cdot \frac{1}{N_k} \sum_{i=1}^{N_k} s_k(x_i, y_i) \\]

여기서 \\(K\\)는 벤치마크의 수, \\(w_k\\)는 각 벤치마크의 가중치, \\(N_k\\)는 벤치마크 \\(k\\)의 샘플 수, \\(s_k(x_i, y_i)\\)는 입력 \\(x_i\\)에 대한 모델의 응답 \\(y_i\\)의 안전성 점수입니다.

XSTest는 200개의 안전하지 않은 프롬프트와 250개의 안전해 보이지만 실제로는 안전한 프롬프트로 구성됩니다. 이는 동음이의어, 비유적 표현, 안전한 맥락 등 다양한 카테고리를 포함하며, WildGuard 분류기를 통해 응답의 거부 또는 수용 여부를 평가합니다.

HarmBench는 321개의 유해한 프롬프트를 포함하며, 기능적 카테고리와 의미적 카테고리로 구분됩니다. 기능적 카테고리는 표준 행동과 저작권 관련 행동을 평가하며, 의미적 카테고리는 사이버 범죄, 무단 침입, 화학/생물학 무기, 저작권 위반, 허위정보, 괴롭힘, 불법 활동 등을 다룹니다.

Do-Anything-Now는 DAN 템플릿과 HarmBench의 유해 행동을 결합한 300개의 재일브레이크 프롬프트로 구성됩니다. JailbreakTrigger는 13가지 서로 다른 재일브레이크 공격 방법을 기반으로 한 400개의 예시를 포함하며, "나쁜 행동에 대한 질문"과 "유해 콘텐츠 생성 지시"의 두 카테고리로 구분됩니다.

WildJailbreakTest는 210개의 적대적 양성 쿼리와 2,000개의 적대적 유해 쿼리를 포함합니다. 이는 모델의 과도한 안전 행동과 적대적 공격에 대한 방어 능력을 평가합니다. WildGuardTest는 1,725개의 항목으로 구성되며, 프롬프트 유해성, 응답 유해성, 응답 거부 분류 작업을 평가합니다.

이러한 종합적인 안전성 평가를 통해 TÜLU 3는 8B 모델에서 85.5%, 70B 모델에서 88.3%의 안전성 점수를 달성했으며, 이는 동일한 크기의 다른 오픈 웨이트 모델들보다 우수한 성능입니다.
### TÜLU 3 평가 프레임워크 - 미공개 평가

TÜLU 3의 미공개 평가 스위트는 개발 과정에서 사용된 평가와는 독립적인 설계 과정을 통해 구축되었습니다. 이 평가의 주요 목표는 언어 모델의 실제 사용 사례와 밀접하게 연관된 방식으로 평가하는 것입니다. 연구진은 다음과 같은 평가 원칙을 따랐습니다.

\\[ E_{unseen}(M) = \frac{1}{\left\vert T \right\vert} \sum_{t \in T} w_t \cdot \text{score}(M, t) \\]

여기서 \\(M\\)은 평가 대상 모델, \\(T\\)는 미공개 평가 과제 집합, \\(w_t\\)는 각 과제의 가중치입니다.

AGIEval English는 aqua-rat, logiqa-en, lsat-ar 등을 포함하는 영어 다중 선택 과제들로 구성됩니다. 연구진은 간단한 "제로샷 CoT" 프롬프트를 도입했으며, 이는 다음과 같은 형식을 따릅니다.

\\[ P_{AGI}(x) = \text{context}(x) + \text{reasoning}(x) + \text{answer}[A-D] \\]

MMLU-Pro는 MMLU 데이터셋의 10-way 다중 선택 확장 버전입니다. AGIEval과 유사한 프롬프트와 답안 추출 방식을 사용하며, 전통적인 5-shot CoT 프롬프트보다 더 짧고 현실적인 평가 방식을 제공합니다.

GPQA는 생물학, 물리학, 화학 분야의 전문가들이 작성한 매우 도전적인 다중 선택 문제들로 구성됩니다. AGIEval과 동일한 제로샷 프롬프트를 사용하지만, 추론 구조에 대한 제약을 줄였습니다.

Deepmind Mathematics는 56개 카테고리의 수학 문제를 포함하며, 수학적/대수적 추론 능력을 평가합니다. 연구진은 과제의 맥락을 설정하고 답안 형식을 지정하는 "제로샷 CoT" 프롬프트를 개발했으며, 각 카테고리별로 세 가지 예시 답안을 제공하여 답안 형식을 명확히 했습니다.

BigCodeBench는 148개의 어려운 코딩 과제로 구성된 "hard subset"에 초점을 맞추며, "instruct" 형식의 과제 설명과 "calibrated" 점수를 사용합니다. 이는 원래 리더보드에서 사용된 설정을 따릅니다.

### TÜLU 3 평가 프레임워크 - IFEval-OOD

TÜLU 3의 새로운 평가 방법론 중 하나인 IFEval-OOD(IFEval Out-of-Distribution)는 언어 모델의 정확한 지시 따르기 능력을 평가하기 위해 개발되었습니다. 이 평가는 기존 IFEval의 25가지 제약 조건을 넘어서는 새로운 제약 조건들에 대한 모델의 수행 능력을 테스트합니다.

IFEval-OOD는 6개의 광범위한 카테고리에 걸쳐 52개의 제약 조건을 포함하며, 각 제약 조건은 다음과 같은 수식으로 평가됩니다.

\\[ C(x, y) = \prod_{i=1}^k v_i(x, y) \\]

여기서 \\(v_i\\)는 각각의 제약 조건 검증 함수이며, \\(x\\)는 입력 프롬프트, \\(y\\)는 모델의 응답입니다. 모든 제약 조건이 만족될 때만 1을 반환하고, 그렇지 않으면 0을 반환합니다.

제약 조건들은 다음과 같은 형태의 검증 가능한 요구사항을 포함합니다.

\\[ V_{person}(y) = \mathbb{1}[\text{count\_unique\_names}(y) \geq N] \\]
\\[ V_{ratio}(y) = \mathbb{1}[\text{stop\_word\_ratio}(y) \leq p] \\]

여기서 \\(N\\)은 요구되는 최소 고유 이름 수, \\(p\\)는 허용되는 최대 불용어 비율입니다.

HREF(Human Reference-guided Evaluation of instruction Following)는 11개의 지시 따르기 과제를 평가하는 새로운 자동화된 평가 방법입니다. 이는 다음과 같은 승률 기반 메트릭을 사용합니다.

\\[ W(M, B) = \frac{1}{N} \sum_{i=1}^N \mathbb{1}[s(M_i) > s(B_i)] \\]

여기서 \\(M\\)은 평가 대상 모델, \\(B\\)는 기준 모델, \\(s(\\cdot)\\)는 응답의 품질 점수입니다. 연구진은 Llama 3.1 70B Instruct를 판단 모델로, Llama 3.1 405B Instruct를 기준 모델로 사용했습니다.

이러한 새로운 평가 방법론들은 모델의 지시 따르기 능력을 더욱 엄격하고 포괄적으로 평가할 수 있게 해주며, 특히 IFEval-OOD는 기존 IFEval 제약 조건에 대한 과적합 여부를 판단하는 데 중요한 역할을 합니다.
### TÜLU 3 평가 프레임워크 - 미공개 평가 결과

TÜLU 3의 미공개 평가 결과는 개발 평가에 대한 과적합 정도와 일반화 성능을 보여줍니다. 연구진은 TÜLU 3 모델들을 8B와 70B 크기의 Llama 3.1 Instruct 모델들, 그리고 Hermes 3 Llama 3.1 모델들과 비교했습니다. 이때 주목할 점은 TÜLU 3 모델들의 경우 미공개 평가가 실제로 '미공개'였지만, 다른 모델들의 경우 이러한 평가들이 학습에 사용되었을 가능성을 배제할 수 없다는 것입니다.

평가 결과는 다음과 같은 메트릭으로 정량화됩니다.

\\[ P_{\text{overall}}(M) = \frac{1}{\left\vert T \right\vert} \sum_{t \in T} \text{normalize}(\text{score}_t(M)) \\]

여기서 \\(M\\)은 평가 대상 모델, \\(T\\)는 미공개 평가 과제 집합, \\(\text{normalize}\\)는 각 과제별 점수를 정규화하는 함수입니다.

연구진은 다음과 같은 주요 발견들을 보고했습니다.

첫째, TÜLU 3는 미공개 평가에서도 우수한 일반화 성능을 보여줍니다. 대부분의 평가에서 TÜLU 3의 성능은 비교 대상 모델들과 비슷하거나 더 우수했으며, 종종 두 모델의 성능 사이에 위치했습니다. 이는 대표적인 평가를 선택하고 그에 맞는 학습 데이터를 큐레이션하는 TÜLU 3의 접근 방식이 효과적임을 보여줍니다.

둘째, MATH에서 Deepmind Math로의 전이 학습이 제한적이라는 점이 발견되었습니다. MATH는 LaTeX 형식의 답안을 요구하는 반면, Deepmind Math는 그렇지 않습니다. TÜLU 3 모델들은 Deepmind Math 문제에서도 LaTeX 형식을 사용하려는 경향을 보였으며, 이는 중간 추론 과정과 답안 추출을 방해했습니다. 이는 MATH 학습 데이터의 형식에 과적합된 결과로 해석됩니다.

셋째, 모든 모델들이 IFEval과 IFEval-OOD 사이에서 상당한 성능 차이를 보였습니다. 이는 검증 가능한 제약 조건을 따르는 것이 모델들에게 여전히 어려운 과제이며, IFEval에서 좋은 성능을 보인 모델들도 데이터셋의 특정 제약 조건들에 과적합되었을 가능성을 시사합니다.

넷째, 지식 회상 능력의 일반화는 포스트 트레이닝 레시피에 따라 다르게 나타났습니다. MMLU와 MMLU-Pro 사이에는 상관관계가 있었지만, GPQA에서는 다른 경향을 보였습니다. 이는 같은 기본 모델에서 시작했더라도 포스트 트레이닝 방식이 지식 회상의 일반화에 영향을 미칠 수 있음을 보여줍니다.

마지막으로, 지시 따르기 성능은 과제 카테고리에 따라 큰 차이를 보였습니다. AlpacaEval과 HREF에서의 상대적 성능이 다르게 나타났는데, 이는 지시 따르기가 매우 다양한 과제이며, 분포가 다른 평가 간에는 성능이 잘 전이되지 않을 수 있음을 시사합니다.

### 논의 및 미래 연구 방향

TÜLU 3 연구진은 모델 개발 과정에서 시도했지만 최종 레시피에 포함되지 않은 여러 방법론들과 향후 연구 방향에 대해 상세히 논의합니다. 특히 온라인 DPO(Direct Preference Optimization)와 리젝션 샘플링(Rejection Sampling)과 같은 접근 방식들의 한계점과 잠재력을 분석했습니다.

온라인 DPO는 기존의 오프라인 DPO 방식을 개선하기 위해 제안된 방법입니다. 일반적인 DPO는 사전에 수집된 선호도 데이터셋을 사용하는 반면, 온라인 DPO는 학습 중인 정책 \\(\pi_\theta\\)로부터 직접 피드백을 얻는 방식을 채택합니다. 이 과정은 다음과 같은 세 단계로 구성됩니다.

1. 현재 정책에서 프롬프트당 2개의 응답을 샘플링
2. 응답 쌍에 대한 온라인 피드백을 통해 선호도 데이터 생성
3. 표준 DPO 손실 함수를 사용하여 정책 \\(\pi_\theta\\) 업데이트

연구진은 실험의 확장성을 위해 학습된 보상 모델을 사용하여 피드백을 생성했습니다. 일반적인 능력 향상을 위해서는 Skywork 데이터셋의 82K 선호도 데이터 포인트를 사용하여 보상 모델을 1 에폭 동안 학습했으며, 수학적 추론 능력 향상을 위해서는 합성 온폴리시 수학 특화 선호도 데이터를 사용하여 추가 학습을 진행했습니다.

그러나 TÜLU 3 DPO 체크포인트에서 시작하여 수학 문제에 대해 총 200K 에피소드 동안 온라인 DPO를 학습한 결과, GSM8K에서는 거의 개선이 없었고 MATH 성능은 오히려 저하되는 현상이 관찰되었습니다. 연구진은 다양한 샘플링 온도와 KL 페널티 계수를 실험했지만, 일반적인 영역과 특화된 영역 모두에서 제한적인 성과를 보여 이 접근 방식을 더 이상 깊이 탐구하지 않았습니다.

리젝션 샘플링은 최근 대규모 언어 모델의 학습 후 성능을 향상시키는 데 널리 사용되는 방법입니다. 이 방법은 초기 SFT와 선호도 데이터를 혼합하여 학습된 모델을 사용해 각 SFT 프롬프트에 대해 n개의 응답을 생성하고, 보상 모델이나 LLM을 판단자로 사용하여 이들 중 최고의 응답을 선택하는 방식입니다. 선택되지 않은 응답들은 선호도 최적화를 위한 선택/거부 쌍으로 활용될 수 있습니다.

연구진은 리젝션 샘플링을 시도했으나, 필요한 계산 비용 대비 성능 향상이 미미하여 더 깊은 탐구는 향후 연구 과제로 남겨두었습니다. 특히 공개된 모델들이 후보 응답들 중 최선의 것을 선택하는 데 어려움을 겪는다는 점을 발견했습니다. 또한 원본 응답을 판단자의 선택지에 포함시키는 것이 새로 생성된 응답들만을 고려하는 것보다 훨씬 더 나은 성능을 보인다는 점도 확인했습니다.
### 논의 및 미래 연구 방향

TÜLU 3 연구진은 현재 모델의 한계점과 향후 발전 방향에 대해서도 심도 있는 논의를 진행했습니다. 특히 긴 문맥 처리와 다중 턴 대화, 다국어 지원, 도구 사용 등의 영역에서 추가적인 연구가 필요함을 지적했습니다.

긴 문맥과 다중 턴 대화 능력은 현재 TÜLU 3의 주요 한계점 중 하나입니다. 현재 데이터셋의 평균 턴 수는 2.4턴에 불과하며, 대부분의 샘플이 2,048 토큰 이하의 길이를 가집니다. 이는 최근 연구 동향에서 중요하게 다뤄지는 긴 문맥 처리 능력과는 거리가 있습니다. Pawar와 연구진이 제시한 것처럼, 긴 문맥 처리 능력의 향상은 새로운 활용 사례를 가능하게 하고, Gemini Team이 보여준 바와 같이 더 많은 인-컨텍스트 예시를 활용할 수 있게 하여 전반적인 성능 향상으로 이어질 수 있습니다. Agarwal과 연구진의 연구에 따르면, 실제 사용자들의 상당수가 2턴 이상의 대화를 진행하는 것으로 나타나, 이러한 능력의 개선이 사용자 경험 향상에 직접적인 영향을 미칠 것으로 예상됩니다.

다국어 지원 측면에서는, TÜLU 3가 현재 Üstün과 연구진이 개발한 고품질 다국어 데이터셋인 Aya를 포함하고는 있지만, 주로 영어 데이터와 평가에 초점을 맞추고 있습니다. 이는 전 세계의 다양한 언어 사용자들의 요구를 충분히 반영하지 못하는 한계를 가집니다. Wu와 연구진이 제안한 교차 언어 정렬이나 Li와 연구진이 연구한 데이터 균형 전략과 같은 기법들을 활용한 다국어 포스트 트레이닝은 향후 중요한 연구 방향이 될 것입니다.

도구 사용과 에이전트 프레임워크 측면에서는, TÜLU 3가 독립적인 모델로서만 평가되었다는 한계가 있습니다. Qu와 연구진의 연구에서 볼 수 있듯이, 최근 언어 모델들은 점차 더 큰 시스템의 일부로 배포되어 다양한 도구들과 상호작용하거나 에이전트 프레임워크의 일부로 작동하고 있습니다. 특히 Gou와 연구진이 보여준 것처럼, 도구 사용 학습은 모델의 추론 및 수학적 능력을 획기적으로 향상시킬 수 있는 자연스러운 방법이 될 수 있습니다. 이는 모든 능력을 '가중치 내에서' 해결하려 하는 대신, 외부 도구들과의 효과적인 상호작용을 통해 모델의 한계를 극복하는 방향을 제시합니다.

## 관련 연구

포스트 트레이닝 기법의 발전 과정은 다중 과제 언어 모델 학습, 특히 지시어 튜닝에서 시작되었습니다. Mishra와 연구진이 제안한 초기 지시어 튜닝 데이터셋은 주로 자연어 추론과 같은 전통적인 자연어처리 과제에 초점을 맞추었으나, Wang과 연구진의 연구를 통해 더 일반적인 사용자 과제로 확장되었습니다.

ChatGPT와 같은 대화형 언어 모델의 등장으로 포스트 트레이닝 기법은 지시어 튜닝을 넘어 선호도 튜닝 단계를 포함하게 되었습니다. Ouyang과 연구진이 제안한 RLHF(Reinforcement Learning from Human Feedback) 방식은 인간의 선호도를 학습한 보상 모델을 먼저 구축하고, 이를 통해 언어 모델을 강화학습 프레임워크로 최적화합니다. 최근에는 Rafailov와 연구진이 제시한 것처럼 선호도 데이터를 직접 학습에 활용하는 방식이 개발되어 선호도 튜닝의 복잡성을 줄일 수 있게 되었습니다.

초기의 선호도 튜닝은 수만 건의 인간 작성 지시어와 선호도 레이블을 사용하는 매우 인간 중심적인 접근 방식을 취했습니다. 하지만 Touvron과 연구진의 연구에서 볼 수 있듯이, 최근에는 인간과 합성 선호도 데이터를 혼합하여 사용하고 여러 라운드의 학습과 다양한 학습 알고리즘을 적용하는 방식으로 발전했습니다.

폐쇄형 연구실에서 RLHF가 발전하는 동안, 오픈소스 포스트 트레이닝 레시피는 상대적으로 더디게 발전했습니다. Taori와 연구진, Conover와 연구진의 초기 시도는 지시어 튜닝 단계에 초점을 맞추어 공개된 언어 모델을 합성 생성 또는 인간 제작 데이터셋으로 파인튜닝하는 것이었습니다. Wang과 연구진의 연구에서는 이러한 데이터셋들을 결합하여 강력한 성능을 달성할 수 있음을 보여주었지만, Ivison과 연구진의 연구에 따르면 인간 평가 기준으로 폐쇄형 모델과의 격차를 줄이기 위해서는 선호도 튜닝 단계가 중요한 것으로 나타났습니다.

현재 대부분의 인기 있는 오픈소스 적응 모델들은 DPO(Direct Preference Optimization) 또는 그 변형을 사용하고 있으며, TÜLU 2, Zephyr-β, Starling과 같은 AI 피드백 데이터를 활용합니다. 그러나 이러한 모델들은 데이터와 성능 측면에서 폐쇄형 포스트 트레이닝 레시피에 비해 뒤처져 있습니다. 2024년 11월 20일 기준으로 LMSYS의 ChatBotArena 상위 50위 안에 포스트 트레이닝 데이터를 공개한 오픈 레시피 모델은 없으며, 대부분의 오픈 레시피는 폐쇄형 포스트 트레이닝 설정에 비해 상대적으로 적은 데이터와 적은 수의 학습 라운드를 사용합니다.
### 관련 연구

검증 가능한 보상을 통한 학습은 최근 언어 모델의 추론 능력을 향상시키기 위한 다양한 연구와 관련이 있습니다. Zelikman과 연구진이 제안한 자기 학습 추론기(Self-taught reasoner, STaR) 연구와 Hoffman과 연구진의 TRICE는 모두 기존의 정답을 신호로 활용하여 모델의 추론 과정(또는 사고 연쇄)을 개선하는 방법을 연구했습니다. STaR은 정책 그래디언트 알고리즘의 근사로 볼 수 있으며, Quiet-STaR는 이 접근 방식을 확장하여 모델이 추가적인 생성을 통해 일반적인 언어 모델링을 개선하도록 학습하는 '생각하고 말하기' 방식을 도입했습니다.

TRICE는 여러 추론 경로에 걸쳐 학습을 수행하여 정답의 가능성을 높이는 것을 목표로 하며, 이를 위해 맞춤형 MCMC 기반 EM 알고리즘을 사용합니다. 최근에는 Kazemnejad와 연구진이 제안한 VinePPO가 GSM8k와 MATH의 이진 보상을 사용하여 새로운 PPO 기반 알고리즘을 테스트했으며, Gehring과 연구진, Xu와 연구진의 최근 연구에서는 코드 피드백을 학습 신호로 활용하는 방법을 탐구했습니다.

이에 비해 TÜLU 3가 제안하는 RLVR 접근 방식은 기존의 RL 프레임워크(PPO)를 사용하면서도 이진 보상과 함께 완전히 온라인으로 실행됩니다. 이는 STaR의 반복적 접근이나 Quiet-STaR의 로그 가능도 보상과는 다른 방식입니다. 또한 RLVR은 수학 영역을 넘어서 정확한 지시 따르기 분야에서도 개선을 이끌어낼 수 있음을 보여주었습니다.

연구진은 RLVR의 핵심 구성 요소들을 체계적으로 분석했습니다. 여기에는 가치 모델 초기화, 일반 보상 모델과 검증 가능한 보상의 활용 등이 포함됩니다. 이러한 기법의 추가 개발과 확장은 향후 연구 과제로 남아있습니다.

TÜLU 3는 완전히 공개된 최첨단 언어 모델 제품군으로, 현대적인 포스트 트레이닝 프레임워크를 제시합니다. 여기에는 TÜLU 3 DATA(완전 공개 데이터), TÜLU 3 EVAL(평가), TÜLU 3 CODE(학습 코드), TÜLU 3 RECIPE(개발 레시피)가 포함됩니다. 연구진은 Llama 3.1 기본 버전에서 학습된 최종 모델과 함께 중간 체크포인트, 학습 데이터, 학습 코드, 평가 코드를 공개했습니다.

이를 통해 TÜLU 3는 오픈소스와 폐쇄형 포스트 트레이닝 방법론 간의 격차를 해소하며, 오픈 포스트 트레이닝 연구의 새로운 이정표를 제시합니다. 제공된 리소스를 통해 다른 연구자들은 오픈소스 기본 모델을 기반으로 다양한 과제에서 높은 성능을 달성하는 파인튜닝을 수행할 수 있습니다. 이는 다중 목적, 다중 단계 학습 프레임워크 내에서 포스트 트레이닝 연구를 발전시키는 길을 열어줍니다.

- - -
### References
* [Tulu 3: Pushing Frontiers in Open Language Model Post-Training](http://arxiv.org/pdf/2411.15124v2)