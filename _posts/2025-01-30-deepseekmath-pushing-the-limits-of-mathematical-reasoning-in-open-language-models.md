---
layout: post
title: "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
date: 2024-02-05 18:55:32
author: "DeepSeek-AI"
categories: "Language-Models"
tags: ["DeepSeekMath-Corpus", "Group-Relative-Policy-Optimization", "Iterative-Reinforcement-Learning", "Large-Scale-Reinforcement-Learning-on-Base-Model", "Reasoning-Oriented-Reinforcement-Learning"]
use_math: true
cover: /assets/images/language-models.webp
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
수학적 추론은 복잡한 구조와 엄격한 논리적 사고를 요구하는 특성으로 인해 언어 모델에게 큰 도전 과제가 되어왔습니다. GPT-4나 Gemini-Ultra와 같은 최신 비공개 모델들이 수학 문제 해결에서 인상적인 성능을 보여주고 있지만, 공개된 오픈소스 모델들은 이들과 상당한 성능 차이를 보이고 있었습니다. 이러한 격차를 줄이고 수학적 추론 능력이 향상된 오픈소스 언어 모델을 개발하는 것이 본 연구의 주요 동기가 되었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
DeepSeekMath는 두 가지 핵심적인 혁신을 제시합니다. 첫째, Common Crawl에서 1,200억 개의 수학 관련 토큰을 추출하는 체계적인 데이터 선택 파이프라인을 구축했습니다. 이는 fastText 기반 분류기를 활용하여 고품질의 수학 콘텐츠를 식별하고 정제하는 반복적 과정을 통해 이루어졌습니다. 둘째, PPO의 변형인 Group Relative Policy Optimization (GRPO)를 도입하여 크리틱 모델 없이도 효율적인 강화학습을 가능하게 했습니다. 이를 통해 메모리 사용량을 크게 줄이면서도 수학적 추론 능력을 효과적으로 향상시켰습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
구현은 세 단계로 진행되었습니다. 먼저 DeepSeek-Coder-Base-v1.5 7B를 기반으로 대규모 수학 코퍼스를 통해 사전 학습을 진행했습니다. 이어서 체인 오브 소트, 프로그램 오브 소트, 도구 통합 추론 데이터를 활용한 지도 학습 파인튜닝을 수행했습니다. 마지막으로 GRPO를 적용하여 모델의 수학적 추론 능력을 더욱 향상시켰습니다. GRPO는 그룹 통계를 활용하여 기준선을 추정함으로써 크리틱 모델 없이도 효율적인 강화학습을 가능하게 했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
DeepSeekMath는 MATH 벤치마크에서 51.7%의 정확도를 달성하며 모든 공개 모델들의 성능을 크게 앞섰고, GPT-4와 Gemini-Ultra의 성능에 근접했습니다. 특히 주목할 만한 점은 7B 규모의 모델로 이러한 성과를 달성했다는 것입니다. 이는 효과적인 데이터 선택과 학습 방법을 통해 모델 규모의 한계를 상당 부분 극복할 수 있음을 보여줍니다. 또한 코드 학습이 수학적 추론 능력 향상에 긍정적인 영향을 미친다는 것과 arXiv 논문이 예상과 달리 큰 효과를 보이지 않는다는 흥미로운 발견도 있었습니다. 이러한 결과들은 향후 수학적 언어 모델 개발에 있어 중요한 지침이 될 것으로 기대됩니다.
- - -
### DeepSeekMath: 수학적 추론의 한계를 넓히는 오픈 언어 모델

수학적 추론은 복잡하고 구조화된 특성으로 인해 언어 모델에게 상당한 도전 과제를 제시합니다. 본 논문에서는 DeepSeekMath 7B를 소개합니다. 이 모델은 DeepSeek-Coder-Base-v1.5 7B를 기반으로 Common Crawl에서 추출한 1,200억 개의 수학 관련 토큰과 함께 자연어 및 코드 데이터를 활용하여 추가 사전 학습을 진행했습니다.

![MATH 벤치마크 성능 비교](/assets/2025-01-30-deepseekmath-pushing-the-limits-of-mathematical-reasoning-in-open-language-models/0.png)

위 그래프는 외부 도구나 투표 기법을 사용하지 않고도 DeepSeekMath 7B가 경쟁 수준의 MATH 벤치마크에서 51.7%라는 인상적인 점수를 달성했음을 보여줍니다. 이는 Gemini-Ultra와 GPT-4의 성능 수준에 근접한 결과입니다. 특히 주목할 만한 점은 DeepSeekMath 7B에서 64개의 샘플에 대한 자기 일관성(self-consistency) 평가를 수행했을 때 MATH 벤치마크에서 60.9%의 성능을 달성했다는 것입니다.

DeepSeekMath의 수학적 추론 능력은 두 가지 핵심 요소에 기인합니다. 첫째, 세심하게 설계된 데이터 선택 파이프라인을 통해 공개적으로 사용 가능한 웹 데이터의 잠재력을 효과적으로 활용했습니다. 둘째, Proximal Policy Optimization (PPO)의 변형인 Group Relative Policy Optimization (GRPO)를 도입하여 수학적 추론 능력을 향상시키는 동시에 PPO의 메모리 사용량을 최적화했습니다.

이러한 혁신적인 접근 방식을 통해 DeepSeekMath는 기존의 오픈 소스 언어 모델들이 달성하지 못했던 수준의 수학적 추론 능력을 보여주고 있으며, 이는 인공지능의 수학적 문제 해결 능력을 한 단계 발전시켰음을 의미합니다.

### 대규모 언어 모델을 통한 수학적 추론의 혁신

대규모 언어 모델(LLM)은 수학적 추론 분야에서 획기적인 발전을 이루어냈습니다. Hendrycks와 연구진이 개발한 정량적 추론 벤치마크와 Trinh과 연구진의 기하학적 추론 벤치마크에서 괄목할 만한 성과를 보여주었으며, 복잡한 수학 문제 해결에서도 인간을 효과적으로 지원하고 있습니다. 하지만 GPT-4나 Gemini-Ultra와 같은 최신 모델들은 공개적으로 사용할 수 없으며, 현재 이용 가능한 오픈소스 모델들은 이들과 상당한 성능 차이를 보이고 있습니다.

본 연구에서는 DeepSeekMath를 소개합니다. 이는 오픈소스 모델들의 수학적 능력을 크게 향상시키고 학술적 벤치마크에서 GPT-4에 근접한 성능을 보여주는 특화된 언어 모델입니다. 이를 위해 Common Crawl에서 fastText 기반 분류기를 사용하여 1,200억 개의 고품질 수학 토큰으로 구성된 DeepSeekMath Corpus를 구축했습니다. 초기에는 OpenWebMath의 사례들을 긍정적 샘플로, 다양한 웹 페이지들을 부정적 샘플로 활용하여 분류기를 학습시켰습니다. 이후 이 분류기를 통해 Common Crawl에서 추가적인 긍정 사례들을 발굴하고, 이를 인간 주석을 통해 정제한 뒤, 향상된 데이터셋으로 분류기를 다시 학습시켰습니다.

평가 결과는 이 대규모 코퍼스의 우수한 품질을 입증합니다. DeepSeekMath-Base 7B 모델은 GSM8K에서 64.2%, MATH 데이터셋에서 36.2%의 성능을 달성하여 Minerva 540B를 능가했습니다. 또한 DeepSeekMath Corpus가 다국어를 지원하기 때문에 중국어 수학 벤치마크에서도 성능 향상이 관찰되었습니다. 우리는 이러한 수학적 데이터 처리 경험이 연구 커뮤니티에 중요한 시작점이 될 것이며, 향후 더 많은 발전 가능성이 있다고 믿습니다.

DeepSeekMath-Base는 DeepSeek-Coder-Base-v1.5 7B를 기반으로 초기화되었는데, 이는 일반적인 언어 모델보다 코드 학습 모델에서 시작하는 것이 더 효과적이라는 관찰에 기반합니다. 또한 수학 학습이 MMLU와 BBH 벤치마크에서도 모델의 성능을 향상시킨다는 점이 주목할 만합니다. 이는 수학적 능력뿐만 아니라 일반적인 추론 능력도 함께 강화된다는 것을 보여줍니다.

사전 학습 이후에는 체인 오브 소트, 프로그램 오브 소트, 도구 통합 추론 데이터를 활용하여 수학적 지시어 튜닝을 적용했습니다. 그 결과로 나온 DeepSeekMath-Instruct 7B는 모든 7B 규모의 경쟁 모델들을 능가하고 70B 규모의 오픈소스 지시어 튜닝 모델들과 비견할 만한 성능을 보여줍니다.
### 강화학습을 통한 수학적 추론 능력 향상

DeepSeekMath는 Proximal Policy Optimization(PPO)의 변형인 Group Relative Policy Optimization(GRPO)를 도입했습니다. GRPO는 기존 PPO의 크리틱 모델을 제거하고 대신 그룹 점수에서 기준선을 추정함으로써 학습 리소스를 크게 절감했습니다. 영어 지시어 튜닝 데이터의 일부만을 사용하여 GRPO를 적용한 결과, 이미 강력한 성능을 보여준 DeepSeekMath-Instruct의 성능을 더욱 향상시켰습니다. 도메인 내 과제(GSM8K: 82.9% → 88.2%, MATH: 46.8% → 51.7%)와 도메인 외 수학 과제(예: CMATH: 84.6% → 88.8%) 모두에서 상당한 개선을 이루어냈습니다.

### 통합 패러다임을 통한 강화학습 기법의 이해

연구진은 Rejection Sampling Fine-Tuning(RFT), Direct Preference Optimization(DPO), PPO, GRPO와 같은 다양한 방법들을 이해하기 위한 통합 패러다임을 제시했습니다. 이 통합된 관점에서 볼 때, 이러한 방법들은 모두 직접적이거나 단순화된 강화학습 기법으로 개념화될 수 있습니다. 연구진은 온라인 대 오프라인 학습, 결과 대 과정 감독, 단일 턴 대 반복적 강화학습 등 광범위한 실험을 수행하여 이 패러다임의 핵심 요소들을 심도 있게 조사했습니다.

### 강화학습의 효과성 분석

연구진은 강화학습이 지시어 튜닝 모델의 성능을 향상시키는 이유를 분석하고, 이 통합 패러다임을 기반으로 더 효과적인 강화학습을 달성하기 위한 잠재적 방향을 제시했습니다. 특히 GRPO의 도입은 기존 PPO 방식의 계산 비용을 크게 줄이면서도 수학적 추론 능력을 효과적으로 향상시킬 수 있음을 보여주었습니다. 이는 강화학습이 수학적 추론 과제에서 모델의 성능을 개선하는 데 매우 효과적인 도구가 될 수 있음을 시사합니다.

### DeepSeekMath의 주요 기여

DeepSeekMath는 대규모 수학 사전 학습과 강화학습의 탐구 및 분석이라는 두 가지 핵심 영역에서 중요한 기여를 했습니다. 먼저 수학 사전 학습 측면에서, 공개적으로 접근 가능한 Common Crawl 데이터에서 수학적 목적으로 활용할 수 있는 가치 있는 정보를 발견했습니다. 세심하게 설계된 데이터 선택 파이프라인을 통해 수학적 내용이 포함된 웹 페이지에서 1,200억 개의 토큰으로 구성된 고품질 데이터셋인 DeepSeekMath Corpus를 구축했습니다. 이는 Lewkowycz와 연구진이 Minerva에서 사용한 수학 웹 페이지의 7배, 최근 공개된 Paster와 연구진의 OpenWebMath의 9배에 달하는 규모입니다.

DeepSeekMath-Base 7B 모델은 Minerva 540B와 비슷한 성능을 달성했는데, 이는 수학적 추론 능력에 있어 파라미터 수가 유일한 핵심 요소가 아님을 보여줍니다. 고품질 데이터로 사전 학습된 더 작은 모델도 강력한 성능을 달성할 수 있다는 점이 입증되었습니다. 또한 수학 학습 실험을 통해 코드 학습을 수학 학습 전에 진행하면 도구 사용 여부와 관계없이 수학 문제 해결 능력이 향상된다는 것을 발견했습니다. 이는 코드 학습이 추론 능력을 향상시키는지에 대한 오랜 의문에 대한 부분적인 답을 제시합니다.

강화학습 측면에서는 Group Relative Policy Optimization(GRPO)이라는 효율적이고 효과적인 강화학습 알고리즘을 도입했습니다. GRPO는 크리틱 모델을 사용하지 않고 대신 그룹 점수에서 기준선을 추정함으로써 Proximal Policy Optimization(PPO)에 비해 학습 리소스를 크게 절감했습니다. 지시어 튜닝 데이터만을 사용하여 DeepSeekMath-Instruct 모델의 성능을 크게 향상시켰으며, 강화학습 과정에서 도메인 외 성능도 향상되는 것을 관찰했습니다.

연구진은 RFT, DPO, PPO, GRPO와 같은 다양한 방법들을 이해하기 위한 통합 패러다임을 제시했습니다. 온라인과 오프라인 학습, 결과와 과정 감독, 단일 턴과 반복적 강화학습 등 광범위한 실험을 수행하여 이 패러다임의 핵심 요소들을 심도 있게 조사했습니다. 이를 통해 강화학습의 효과성 이면의 이유를 탐구하고, 대규모 언어 모델의 더 효과적인 강화학습을 달성하기 위한 여러 잠재적 방향을 제시했습니다.

### 평가 지표와 벤치마크 분석

DeepSeekMath의 성능 평가는 영어와 중국어 수학 추론, 형식 수학, 자연어 이해, 추론, 코드 생성 등 다양한 영역에서 이루어졌습니다. 영어 수학 추론 평가에서는 GSM8K, MATH, SAT, OCW Courses, MMLU-STEM과 같은 초등학교부터 대학 수준까지의 수학 문제를 포함하는 벤치마크를 사용했습니다. 중국어 평가에는 MGSM-zh, CMATH, Gaokao-MathCloze, Gaokao-MathQA 등의 벤치마크가 활용되었습니다.

평가는 두 가지 주요 측면에서 진행되었습니다. 첫째, 모델이 외부 도구 없이 자체적으로 텍스트 기반 해결책을 생성하는 능력을 평가했고, 둘째, Python을 활용한 문제 해결 능력을 측정했습니다. 영어 벤치마크에서 DeepSeekMath-Base는 비공개 모델인 Minerva 540B와 대등한 성능을 보여주었으며, 수학 사전 학습 여부와 관계없이 Mistral 7B와 Llemma-34B를 포함한 모든 오픈소스 기반 모델들을 큰 차이로 앞섰습니다.

특히 주목할 만한 점은 중국어 벤치마크에서의 우수한 성능입니다. 이는 기존 연구들과 달리 영어로만 된 수학 사전 학습 데이터에 국한되지 않고, 고품질의 비영어권 데이터도 포함시켰기 때문입니다. 수학적 지시어 튜닝과 강화학습을 적용한 DeepSeekMath-Instruct와 DeepSeekMath-RL은 더욱 향상된 성능을 보여주었으며, 특히 오픈소스 커뮤니티에서는 처음으로 경쟁 수준의 MATH 데이터셋에서 50% 이상의 정확도를 달성했습니다.

형식 수학 분야에서는 Isabelle 증명 보조기를 사용하여 miniF2F 데이터셋에서 비형식적 정리를 형식적 증명으로 변환하는 과제를 수행했습니다. DeepSeekMath-Base는 이 퓨 샷 자동 형식화 과제에서도 뛰어난 성능을 보여주었습니다.

자연어 이해와 추론 능력을 종합적으로 평가하기 위해 57개의 다양한 주제를 다루는 MMLU 벤치마크와 다단계 추론이 필요한 23개의 도전적인 과제로 구성된 BIG-Bench Hard(BBH)에서도 평가를 진행했습니다. 코드 생성 능력은 HumanEval과 MBPP 벤치마크를 통해 검증했습니다. 특히 주목할 만한 점은 수학 사전 학습이 언어 이해와 추론 성능 모두에 긍정적인 영향을 미쳤다는 것입니다.

### 수학적 사전 학습을 통한 모델의 기초 강화

DeepSeekMath의 수학적 사전 학습은 Common Crawl에서 추출한 1,200억 개의 토큰으로 구성된 고품질 수학 코퍼스를 기반으로 합니다. 이는 Minerva에서 사용된 수학 웹 페이지의 7배, OpenWebMath의 9배에 달하는 규모입니다. 이러한 대규모 데이터셋을 구축하기 위해 연구진은 fastText 기반의 분류기를 활용했습니다.

분류기 학습은 두 단계로 진행되었습니다. 첫 단계에서는 OpenWebMath의 사례들을 긍정적 샘플로, 다양한 웹 페이지들을 부정적 샘플로 활용하여 초기 분류기를 학습시켰습니다. 두 번째 단계에서는 이 분류기를 Common Crawl에 적용하여 추가적인 긍정 사례들을 발굴했고, 이를 인간 주석을 통해 정제한 후 향상된 데이터셋으로 분류기를 재학습시켰습니다.

이러한 접근 방식의 효과는 평가 결과를 통해 명확히 드러났습니다. DeepSeekMath-Base 7B 모델은 GSM8K에서 64.2%, MATH 데이터셋에서 36.2%의 성능을 달성하여 Minerva 540B를 능가했습니다. 특히 주목할 만한 점은 DeepSeekMath Corpus가 다국어를 지원한다는 것입니다. 이로 인해 중국어 수학 벤치마크에서도 상당한 성능 향상이 관찰되었습니다.

수학 사전 학습의 또 다른 중요한 발견은 코드 학습 모델을 기반으로 시작하는 것이 일반적인 언어 모델보다 더 효과적이라는 점입니다. 이러한 이유로 DeepSeekMath-Base는 DeepSeek-Coder-Base-v1.5 7B를 초기 모델로 선택했습니다. 더불어 수학 학습이 MMLU와 BBH 벤치마크에서도 모델의 성능을 향상시킨다는 점이 확인되었는데, 이는 수학적 능력이 일반적인 추론 능력 향상에도 기여한다는 것을 시사합니다.

이러한 수학적 사전 학습의 성공은 두 가지 핵심 요소에 기인합니다. 첫째, 세심하게 설계된 데이터 선택 파이프라인을 통해 공개적으로 사용 가능한 웹 데이터의 잠재력을 최대한 활용했습니다. 둘째, 코드 학습 모델을 기반으로 한 접근 방식이 수학적 추론 능력 향상에 효과적이라는 것을 입증했습니다. 이러한 발견들은 향후 수학적 언어 모델 개발에 있어 중요한 지침이 될 것으로 기대됩니다.

### DeepSeekMath 코퍼스의 데이터 수집과 정제 과정

![데이터 수집 파이프라인](/assets/2025-01-30-deepseekmath-pushing-the-limits-of-mathematical-reasoning-in-open-language-models/1.png)

DeepSeekMath 코퍼스는 Common Crawl에서 수학 관련 데이터를 체계적으로 수집하고 정제하는 반복적 파이프라인을 통해 구축되었습니다. 이 과정은 고품질의 수학 관련 데이터셋을 시드 코퍼스로 활용하여 시작되며, 이러한 접근 방식은 코딩과 같은 다른 도메인에도 적용할 수 있습니다.

초기 시드 코퍼스로는 Paster와 연구진이 개발한 OpenWebMath를 선택했습니다. 이 고품질 수학 웹 텍스트 컬렉션을 기반으로 fastText 모델을 학습시켜 더 많은 OpenWebMath와 유사한 수학 웹 페이지를 찾아내는 데 활용했습니다. 구체적으로, 시드 코퍼스에서 무작위로 선택한 50만 개의 데이터 포인트를 긍정 학습 예시로, Common Crawl에서 추가로 50만 개의 웹 페이지를 부정 예시로 사용했습니다.

fastText 모델 학습에는 오픈소스 라이브러리를 사용했으며, 다음과 같은 주요 파라미터를 설정했습니다.
- 벡터 차원: 256
- 학습률: 0.1
- 단어 n-gram의 최대 길이: 3
- 단어 출현 최소 횟수: 3
- 학습 에포크 수: 3

원본 Common Crawl의 크기를 줄이기 위해 URL 기반 중복 제거와 근접 중복 제거 기술을 적용하여 400억 개의 HTML 웹 페이지로 축소했습니다. 이후 학습된 fastText 모델을 사용하여 중복이 제거된 Common Crawl에서 수학 관련 웹 페이지를 추출했습니다. 저품질 콘텐츠를 필터링하기 위해 fastText 모델이 예측한 점수를 기준으로 페이지들의 순위를 매기고, 상위 랭킹의 페이지들만 보존했습니다. 보존할 데이터의 양은 상위 400억, 800억, 1,200억, 1,600억 토큰에 대한 사전 학습 실험을 통해 평가되었으며, 첫 번째 반복에서는 상위 400억 토큰을 유지하기로 결정했습니다.

첫 번째 데이터 수집 반복 이후에도 많은 수학 웹 페이지가 수집되지 않은 상태로 남아있었는데, 이는 주로 fastText 모델이 충분한 다양성을 갖지 못한 긍정 예시 세트로 학습되었기 때문입니다. 이를 해결하기 위해 추가적인 수학 관련 소스를 식별하여 시드 코퍼스를 확장하고, 이를 통해 fastText 모델을 최적화했습니다.

구체적으로, 전체 Common Crawl을 서로 다른 도메인으로 구성했으며, 도메인은 동일한 기본 URL을 공유하는 웹 페이지들로 정의했습니다. 각 도메인에 대해 첫 번째 반복에서 수집된 웹 페이지의 비율을 계산했습니다. 웹 페이지의 10% 이상이 수집된 도메인을 수학 관련 도메인(예: mathoverflow.net)으로 분류했습니다.

이후 이러한 식별된 도메인 내에서 수학적 콘텐츠와 관련된 URL들을 수동으로 주석 처리했습니다(예: mathoverflow.net/questions). 이러한 URL들과 연결되어 있지만 아직 수집되지 않은 웹 페이지들을 시드 코퍼스에 추가했습니다. 이 접근 방식을 통해 더 많은 긍정 예시를 수집할 수 있었고, 이를 바탕으로 다음 반복에서 더 많은 수학 데이터를 찾아낼 수 있는 개선된 fastText 모델을 학습시킬 수 있었습니다.

네 번의 데이터 수집 반복 후, 최종적으로 3,550만 개의 수학 웹 페이지를 수집했으며, 이는 총 1,200억 개의 토큰에 해당합니다. 네 번째 반복에서 거의 98%의 데이터가 이미 세 번째 반복에서 수집되었음을 확인하고 데이터 수집을 종료했습니다.

벤치마크 오염을 방지하기 위해 Guo와 연구진의 방법을 따라 GSM8K, MATH와 같은 영어 수학 벤치마크와 CMATH, AGIEval과 같은 중국어 벤치마크의 문제나 답변이 포함된 웹 페이지를 필터링했습니다. 필터링 기준은 다음과 같습니다.
- 평가 벤치마크의 어떤 하위 문자열과 정확히 일치하는 10-gram 문자열을 포함하는 텍스트 세그먼트는 수학 학습 코퍼스에서 제거됩니다.
- 10-gram보다 짧지만 최소 3-gram 이상인 벤치마크 텍스트의 경우, 정확한 매칭을 사용하여 오염된 웹 페이지를 필터링합니다.

### DeepSeekMath 코퍼스의 품질 검증

DeepSeekMath 코퍼스의 품질을 검증하기 위해 최근 공개된 여러 수학 학습 코퍼스와의 비교 실험을 진행했습니다. 비교 대상으로는 MathPile(89억 토큰), OpenWebMath(136억 토큰), 그리고 Proof-Pile-2(519억 토큰)가 선정되었습니다. MathPile은 교과서, 위키피디아, ProofWiki, CommonCrawl, StackExchange, arXiv 등 다양한 소스에서 수집된 데이터셋으로, 특히 arXiv가 전체의 85% 이상을 차지합니다. OpenWebMath는 수학적 내용을 중심으로 필터링된 CommonCrawl 데이터이며, Proof-Pile-2는 OpenWebMath, AlgebraicStack(103억 토큰의 수학 코드), arXiv 논문(280억 토큰)으로 구성되어 있습니다.

실험은 DeepSeek LLM과 동일한 프레임워크를 공유하는 13억 개의 파라미터를 가진 일반 사전 학습 언어 모델을 기반으로 진행되었습니다. 각 수학 코퍼스에 대해 1,500억 토큰의 학습을 수행했으며, 효율적이고 경량화된 HAI-LLM 학습 프레임워크를 사용했습니다. 학습 과정에서는 AdamW 옵티마이저를 사용했으며, β₁ = 0.9, β₂ = 0.95, weight_decay = 0.1의 파라미터를 적용했습니다. 학습률은 2,000 스텝의 웜업 후 최대값에 도달하고, 학습 과정의 80% 지점에서 31.6%로, 90% 지점에서 최대값의 10.0%로 감소하는 다단계 스케줄을 따랐습니다. 최대 학습률은 5.3e-4로 설정했으며, 4K 컨텍스트 길이에서 4M 토큰의 배치 크기를 사용했습니다.

평가 결과는 DeepSeekMath 코퍼스의 우수성을 세 가지 측면에서 입증했습니다. 첫째, 퓨 샷 체인 오브 소트 프롬프팅을 사용한 8개의 수학 벤치마크 평가에서 DeepSeekMath 코퍼스로 학습된 모델이 가장 높은 성능을 보여주었습니다. 특히 GSM8K에서 23.8%, MATH에서 13.6%, MMLU STEM에서 56.3%의 정확도를 달성했으며, 이는 다른 코퍼스들과 비교했을 때 상당한 성능 향상을 보여줍니다.

둘째, DeepSeekMath 코퍼스는 영어와 중국어를 중심으로 한 다국어 수학 콘텐츠를 포함하고 있습니다. 실험 결과에 따르면, DeepSeekMath 코퍼스로 학습된 모델은 영어와 중국어 수학 추론 과제 모두에서 성능이 향상되었습니다. 반면, 주로 영어 중심인 기존의 수학 코퍼스들은 중국어 수학 추론에서 제한적인 개선을 보이거나 오히려 성능이 저하되는 현상을 보였습니다.

마지막으로, DeepSeekMath 코퍼스는 기존 수학 코퍼스들보다 몇 배 더 큰 규모를 자랑합니다. 학습 곡선을 분석한 결과, DeepSeekMath 코퍼스로 학습된 DeepSeek-LLM 1.3B는 더 가파른 학습 곡선과 지속적인 성능 향상을 보여주었습니다. 반면, 기준 코퍼스들은 상대적으로 작은 규모로 인해 학습 과정에서 여러 번 반복되었고, 그 결과 모델 성능이 빠르게 정체되는 현상이 관찰되었습니다.

### DeepSeekMath-Base 7B의 학습과 평가

DeepSeekMath-Base 7B는 강력한 수학적 추론 능력을 갖춘 기반 모델로, DeepSeek-Coder-Base-v1.5 7B를 초기 모델로 사용하여 5,000억 개의 토큰으로 학습되었습니다. 학습 데이터는 DeepSeekMath Corpus에서 56%, AlgebraicStack에서 4%, arXiv에서 10%, GitHub 코드에서 20%, 그리고 영어와 중국어로 된 Common Crawl의 자연어 데이터에서 10%로 구성되었습니다. 학습 과정에서는 앞서 설명한 설정을 기반으로 하되, 최대 학습률을 4.2e-4로 설정하고 1,000만 토큰의 배치 크기를 사용했습니다.

### 수학적 문제 해결 능력 평가

DeepSeekMath-Base 7B의 수학적 능력은 세 가지 주요 측면에서 평가되었습니다. 첫째, 외부 도구 없이 단계별 수학적 추론을 통한 문제 해결 능력, 둘째, 도구를 활용한 수학 문제 해결 능력, 셋째, 형식적 정리 증명 능력입니다. 

![수학 벤치마크 성능 비교](/assets/2025-01-30-deepseekmath-pushing-the-limits-of-mathematical-reasoning-in-open-language-models/2.png)

영어와 중국어로 된 8개의 벤치마크에서 퓨 샷 체인 오브 소트 프롬프팅을 사용하여 평가한 결과, DeepSeekMath-Base 7B는 모든 오픈소스 기반 모델들을 능가하는 성능을 보여주었습니다. 특히 GSM8K에서 64.2%, MATH 데이터셋에서 36.2%의 정확도를 달성하여, 널리 사용되는 일반 모델인 Mistral 7B와 최근 공개된 Proof-Pile-2로 학습된 Llemma 34B를 크게 앞섰습니다. 주목할 만한 점은 경쟁 수준의 MATH 데이터셋에서 기존 오픈소스 기반 모델들보다 10% 이상 높은 절대적 성능을 보여주었으며, PaLM을 기반으로 수학 텍스트로 추가 학습된 77배 더 큰 비공개 기반 모델인 Minerva 540B의 성능도 뛰어넘었다는 것입니다.

### 도구 활용 수학 문제 해결

프로그램 오브 소트 프롬프팅을 사용하여 GSM8K와 MATH 데이터셋에서 도구를 활용한 수학적 추론 능력을 평가했습니다. 모델은 각 문제를 해결하기 위해 math와 sympy 같은 라이브러리를 활용할 수 있는 Python 프로그램을 작성하도록 프롬프팅되었으며, 프로그램의 실행 결과가 답안으로 평가되었습니다. 평가 결과, DeepSeekMath-Base 7B는 이전 최고 성능을 보여준 Llemma 34B의 성능을 뛰어넘었습니다.

### 형식 수학과 자연어 이해력

형식 수학 분야에서는 Jiang과 연구진이 제안한 비형식-형식 증명 변환 과제를 수행했습니다. 이는 비형식적 명제, 그에 대응하는 형식적 명제, 그리고 비형식적 증명을 바탕으로 형식적 증명을 생성하는 과제입니다. 올림피아드 수준의 형식 수학을 다루는 miniF2F 벤치마크에서 평가를 진행했으며, 퓨 샷 프롬프팅을 통해 각 문제에 대한 Isabelle 형식 증명을 생성했습니다. Jiang과 연구진의 방법을 따라 모델이 증명 스케치를 생성하고, 자동화된 증명기인 Sledgehammer를 활용하여 누락된 세부 사항을 채우는 방식을 사용했습니다.
### 자연어 이해와 추론 능력 평가

DeepSeekMath-Base 7B의 자연어 이해 능력은 MMLU 벤치마크에서, 추론 능력은 BBH에서, 그리고 프로그래밍 능력은 HumanEval과 MBPP에서 평가되었습니다. 평가 결과는 모델이 이전 버전인 DeepSeek-Coder-Base-v1.5와 비교하여 MMLU와 BBH에서 상당한 성능 향상을 보여주었습니다. 특히 MMLU에서 54.9%, BBH에서 59.5%의 정확도를 달성하여, 수학 학습이 언어 이해와 추론 능력 향상에도 긍정적인 영향을 미쳤음을 입증했습니다.

### 프로그래밍 능력 유지

코드 토큰을 지속적으로 학습에 포함시킴으로써, DeepSeekMath-Base 7B는 HumanEval에서 40.9%, MBPP에서 52.6%의 성능을 보여주며 DeepSeek-Coder-Base-v1.5의 코딩 능력을 효과적으로 유지했습니다. 이는 수학적 추론 능력을 향상시키면서도 기존의 프로그래밍 역량을 보존할 수 있다는 것을 보여줍니다.

### 종합적 성능 분석

전반적으로 DeepSeekMath-Base 7B는 일반 모델인 Mistral 7B와 비교하여 세 가지 추론 및 코딩 벤치마크에서 월등한 성능을 보여주었습니다. 이는 수학적 학습이 모델의 전반적인 추론 능력을 향상시키는 데 기여했음을 시사합니다. 특히 주목할 만한 점은 수학 학습이 단순히 수학적 문제 해결 능력만을 향상시킨 것이 아니라, 자연어 이해와 일반적인 추론 능력까지 함께 발전시켰다는 것입니다.

이러한 결과는 DeepSeekMath-Base 7B가 수학적 추론, 자연어 이해, 프로그래밍이라는 세 가지 핵심 영역에서 균형 잡힌 성능을 달성했음을 보여줍니다. 특히 수학 학습을 통해 얻은 추론 능력의 향상이 다른 영역의 성능 저하 없이 이루어졌다는 점에서 의미가 있습니다.

### 지도 학습 기반 파인튜닝

DeepSeekMath는 영어와 중국어를 아우르는 수학 문제 해결을 위해 체계적인 지도 학습 기반 파인튜닝을 진행했습니다. 이 과정에서 다양한 수학 분야의 문제들을 체인 오브 소트(CoT), 프로그램 오브 소트(PoT), 그리고 도구 통합 추론 형식으로 구성된 해답과 쌍을 이루도록 구성했습니다. 총 77만 6천 개의 학습 데이터를 구축했으며, 이는 다음과 같은 특징을 가집니다.

영어 수학 데이터셋의 경우, GSM8K와 MATH 문제들에 도구 통합 해답을 주석으로 달았으며, MathInstruct의 일부와 Lila-OOD의 학습 세트를 활용했습니다. 이 데이터셋들은 대수학, 확률론, 정수론, 미적분학, 기하학 등 광범위한 수학 분야를 포괄합니다. 특히 Chen과 연구진이 제안한 프로그램 오브 소트 방식과 Gou와 연구진이 개발한 도구 통합 추론 방식을 적극 활용하여 문제 해결 과정을 체계화했습니다.

중국어 수학 데이터셋은 K-12 수준의 수학 문제들을 수집했으며, 선형 방정식을 포함한 76개의 세부 주제를 다룹니다. 각 문제에 대해 체인 오브 소트와 도구 통합 추론 형식으로 해답을 주석으로 달았습니다.

### DeepSeekMath-Instruct 7B의 학습과 평가

DeepSeekMath-Base를 기반으로 수학적 지시어 튜닝을 진행했습니다. 학습 과정에서는 최대 4천 토큰의 컨텍스트 길이까지 학습 예시들을 무작위로 연결했으며, 256의 배치 크기와 5e-5의 고정 학습률을 사용하여 500 스텝 동안 학습을 진행했습니다.

모델의 수학적 성능은 도구 사용 여부에 따라 두 가지 방식으로 평가되었으며, 영어와 중국어로 된 4개의 정량적 추론 벤치마크를 사용했습니다. 평가 대상이 된 주요 모델들은 다음과 같습니다.

비공개 모델로는 GPT-4와 GPT-4 Code Interpreter, Gemini Ultra와 Pro, Inflection-2, Grok-1, 그리고 중국 기업들이 최근 공개한 Baichuan-3와 GLM-4가 포함됩니다. 이들은 대부분 일반적인 목적으로 설계되었으며, 다양한 정렬 절차를 거쳤습니다.

공개 모델로는 DeepSeek-LLM-Chat 67B, Qwen 72B, SeaLLM-v2 7B, ChatGLM3 6B와 같은 일반 모델들과 함께, InternLM2-Math 20B, Math-Shepherd-Mistral 7B, WizardMath 시리즈, MetaMath 70B, ToRA 34B, MAmmoTH 70B와 같이 수학 능력이 강화된 모델들이 포함됩니다.

평가 결과, 도구 사용이 제한된 환경에서 DeepSeekMath-Instruct 7B는 뛰어난 단계별 추론 능력을 보여주었습니다. 특히 경쟁 수준의 MATH 데이터셋에서 모든 공개 모델들과 Inflection-2, Gemini Pro를 포함한 대다수의 비공개 모델들을 9% 이상의 절대적인 차이로 앞섰습니다. 이는 상당히 큰 규모의 모델들(예: Qwen 72B)이나 수학에 특화된 강화학습을 적용한 모델들(예: WizardMath-v1.1 7B)과 비교해도 우수한 성능입니다. DeepSeekMath-Instruct는 MATH 데이터셋에서 중국의 비공개 모델인 GLM-4와 Baichuan-3와 대등한 성능을 보였지만, GPT-4와 Gemini Ultra에는 미치지 못했습니다.

도구 사용이 허용된 평가 환경에서는 자연어 추론과 프로그램 기반 도구 사용을 통합한 문제 해결 방식을 적용했습니다. 이 경우 DeepSeekMath-Instruct 7B는 MATH 데이터셋에서 60%에 근접하는 정확도를 달성하여 모든 기존 공개 모델들을 능가했습니다. 다른 벤치마크들에서도 10배 더 큰 규모의 이전 최고 성능 모델인 DeepSeek-LLM-Chat 67B와 대등한 성능을 보여주었습니다.

### 강화학습을 통한 수학적 추론 능력 향상

DeepSeekMath는 Proximal Policy Optimization(PPO)의 변형인 Group Relative Policy Optimization(GRPO)를 도입하여 강화학습을 통해 수학적 추론 능력을 향상시켰습니다. GRPO는 기존 PPO의 크리틱 모델을 제거하고 대신 그룹 점수에서 기준선을 추정함으로써 학습 리소스를 크게 절감했습니다. 

PPO는 안정적인 정책 최적화를 위해 다음과 같은 목적 함수를 사용합니다.

\\[L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min\\left(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\right)\\right]\\]

여기서 \\(r_t(\\theta) = \\pi_\\theta(a_t \vert s_t) / \\pi_{\\theta_\\text{old}}(a_t \vert s_t)\\)는 새로운 정책과 이전 정책의 확률 비율이며, \\(\\hat{A}_t\\)는 추정된 이점 함수, \\(\\epsilon\\)은 정책 업데이트의 최대 허용 범위를 제어하는 클리핑 파라미터입니다.

GRPO는 이러한 PPO의 기본 구조를 유지하면서도, 크리틱 모델을 제거하고 그룹 점수에서 기준선을 추정하는 방식으로 계산 효율성을 개선했습니다. 영어 지시어 튜닝 데이터의 일부만을 사용하여 GRPO를 적용한 결과, DeepSeekMath-Instruct의 성능이 크게 향상되었습니다. GSM8K에서는 82.9%에서 88.2%로, MATH에서는 46.8%에서 51.7%로 성능이 개선되었으며, 중국어 수학 과제인 CMATH에서도 84.6%에서 88.8%로 성능이 향상되었습니다.

### 통합 패러다임을 통한 강화학습 기법의 이해

연구진은 Rejection Sampling Fine-Tuning(RFT), Direct Preference Optimization(DPO), PPO, GRPO와 같은 다양한 방법들을 이해하기 위한 통합 패러다임을 제시했습니다. 이러한 방법들은 모두 직접적이거나 단순화된 강화학습 기법으로 개념화될 수 있습니다. 

특히 GRPO는 PPO의 복잡성을 줄이면서도 효과적인 학습을 가능하게 합니다. 크리틱 모델을 제거하고 그룹 점수를 활용함으로써, 계산 비용을 크게 절감하면서도 모델의 수학적 추론 능력을 효과적으로 향상시킬 수 있었습니다. 이는 강화학습이 수학적 추론 과제에서 모델의 성능을 개선하는 데 매우 효과적인 도구가 될 수 있음을 보여줍니다.
### 강화학습 기법의 수학적 기초와 구현

GRPO의 핵심 혁신은 크리틱 모델을 제거하고 그룹 점수를 활용하여 기준선을 추정하는 방식에 있습니다. 이는 다음과 같은 수학적 공식으로 표현됩니다.

\\[L^{GRPO}(\\theta) = \\mathbb{E}\_g \\left[ \\sum_{(s,a) \\in g} \\min\\left(r(\\theta)\\hat{A}_g, \\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_g\\right)\\right]\\]

여기서 \\(g\\)는 그룹을 나타내며, \\(\\hat{A}_g\\)는 그룹 내 샘플들의 평균 보상에서 전체 그룹의 평균 보상을 뺀 값으로 계산됩니다.

\\[\\hat{A}\_g = \\frac{1}{ \left\vert g \right\vert} \\sum\_{(s,a) \\in g} r(s,a) - \\frac{1}{ \left\vert G \right\vert} \\sum_{g' \\in G} \\frac{1}{ \left\vert g' \right\vert} \\sum_{(s,a) \\in g'} r(s,a)\\]

### 강화학습의 효율성 개선

GRPO는 PPO의 주요 장점을 유지하면서도 계산 효율성을 크게 개선했습니다. 일반적인 PPO에서는 크리틱 모델을 학습시키기 위해 추가적인 계산 자원이 필요하지만, GRPO는 그룹 통계를 활용함으로써 이러한 오버헤드를 제거했습니다. 이는 다음과 같은 이점을 제공합니다.

1. 메모리 효율성: 크리틱 모델을 저장할 필요가 없어 메모리 사용량이 감소합니다.
2. 학습 안정성: 그룹 통계를 사용함으로써 보상 추정의 분산이 감소합니다.
3. 계산 속도: 크리틱 모델의 학습과 추론 과정이 제거되어 전체적인 학습 속도가 향상됩니다.

### 강화학습을 통한 도메인 일반화

GRPO를 통한 강화학습은 도메인 내 과제뿐만 아니라 도메인 외 과제에서도 성능 향상을 보여주었습니다. 이는 모델이 학습한 수학적 추론 능력이 일반화되어 다양한 문제 유형에 적용될 수 있음을 시사합니다. 특히 중국어 수학 과제에서의 성능 향상은 언어의 경계를 넘어서는 수학적 추론 능력의 전이를 보여주는 중요한 증거입니다.

### 강화학습 기반 수학적 추론의 미래 방향

연구진은 강화학습을 통한 수학적 추론 능력 향상의 가능성을 더욱 탐구하기 위해 여러 방향을 제시했습니다. 특히 그룹 기반 보상 추정 방식의 확장, 다중 에이전트 학습 구조의 도입, 그리고 커리큘럼 학습과의 통합 등이 주요 연구 방향으로 제시되었습니다. 이러한 접근은 수학적 추론 능력을 더욱 향상시키는 동시에 계산 효율성을 개선할 수 있는 잠재력을 가지고 있습니다.

### 그룹 상대 정책 최적화를 통한 강화학습

강화학습은 지도 학습 파인튜닝 단계 이후 대규모 언어 모델의 수학적 추론 능력을 더욱 향상시키는 데 효과적임이 입증되었습니다. 본 섹션에서는 효율적이고 효과적인 강화학습 알고리즘인 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)를 소개합니다.

#### PPO에서 GRPO로의 발전

근접 정책 최적화(Proximal Policy Optimization, PPO)는 대규모 언어 모델의 강화학습 파인튜닝 단계에서 널리 사용되는 액터-크리틱 강화학습 알고리즘입니다. PPO는 다음과 같은 대리 목적 함수를 최대화하여 언어 모델을 최적화합니다.

\\[{\mathcal{J}}\_{PPO}(\theta)=\mathbb{E}\_{[q\sim P(Q),o\sim\pi_{\theta_{old}}(O\vert q)]}\frac{1}{\left\vert o\right\vert}\sum_{t=1}^{\left\vert o\right\vert}\min\left[\frac{\pi_{\theta}(o_{t}\vert q,o_{<t})}{\pi_{\theta_{old}}(o_{t}\vert q,o_{<t})}A_{t},\text{clip}\left(\frac{\pi_{\theta}(o_{t}\vert q,o_{<t})}{\pi_{\theta_{old}}(o_{t}\vert q,o_{<t})},1-\epsilon,1+\epsilon\right)A_{t}\right]\\]

여기서 \\(\pi_\theta\\)와 \\(\pi_{\theta_{old}}\\)는 각각 현재와 이전 정책 모델을 나타내며, \\(q\\)와 \\(o\\)는 질문 데이터셋과 이전 정책 \\(\pi_{\theta_{old}}\\)에서 샘플링된 질문과 출력을 의미합니다. \\(\epsilon\\)은 PPO에서 학습 안정성을 위해 도입된 클리핑 관련 하이퍼파라미터입니다. \\(A_t\\)는 이점(advantage)으로, 일반화된 이점 추정(Generalized Advantage Estimation, GAE)을 적용하여 계산됩니다.

![PPO와 GRPO 비교](/assets/2025-01-30-deepseekmath-pushing-the-limits-of-mathematical-reasoning-in-open-language-models/3.png)

PPO에서는 보상 \\(\{r_t\}\\)와 학습된 가치 함수 \\(V_\psi\\)를 기반으로 이점을 계산합니다. 또한 보상 모델의 과도한 최적화를 방지하기 위해 각 토큰에서 참조 모델로부터의 KL 페널티를 보상에 추가합니다.

\\[r_t = r_\varphi(q,\sigma_{\leq t}) - \beta\log\frac{\pi_\theta(\sigma_t \vert q,\sigma_{\leq t})}{\pi_{ref}(\sigma_t \vert q,\sigma_{\leq t})}\\]

여기서 \\(r_\varphi\\)는 보상 모델, \\(\pi_{ref}\\)는 일반적으로 초기 지도 학습 모델인 참조 모델, \\(\beta\\)는 KL 페널티의 계수입니다.

PPO에서 사용되는 가치 함수는 일반적으로 정책 모델과 비슷한 크기의 또 다른 모델이므로 상당한 메모리와 계산 부담을 초래합니다. 또한 강화학습 훈련 중에 가치 함수는 이점 계산에서 분산 감소를 위한 기준선으로 사용되지만, 대규모 언어 모델의 맥락에서는 보상 모델이 일반적으로 마지막 토큰에만 점수를 할당하므로 각 토큰에서 정확한 가치 함수를 학습하는 것이 복잡해질 수 있습니다.

이러한 문제를 해결하기 위해 GRPO를 제안합니다. GRPO는 PPO에서와 같은 추가적인 가치 함수 근사를 필요로 하지 않으며, 대신 동일한 질문에 대해 생성된 여러 출력의 평균 보상을 기준선으로 사용합니다.

### DeepSeekMath-RL의 학습과 평가 과정

DeepSeekMath-RL은 DeepSeekMath-Instruct 7B를 기반으로 강화학습을 적용하여 개발되었습니다. 학습 데이터는 SFT 데이터에서 GSM8K와 MATH 관련 체인 오브 소트 형식의 문제 약 14만 4천 개로 구성되었으며, 다른 SFT 문제들은 강화학습 단계에서의 벤치마크 영향을 조사하기 위해 제외되었습니다.

보상 모델은 Wang과 연구진의 방법론을 따라 구축되었으며, DeepSeekMath-Base 7B를 기반으로 2e-5의 학습률로 초기 학습되었습니다. GRPO(Group Relative Policy Optimization) 적용 시에는 정책 모델의 학습률을 1e-6으로 설정하고, KL 계수는 0.04로 지정했습니다. 각 문제에 대해 64개의 출력을 샘플링하며, 최대 길이는 1,024 토큰으로 제한하고 학습 배치 크기는 1,024로 설정했습니다. 정책 모델은 각 탐색 단계 이후 단 한 번의 업데이트만 수행합니다.

DeepSeekMath-RL 7B의 평가는 DeepSeekMath-Instruct 7B와 동일한 방식으로 진행되었습니다. 체인 오브 소트 추론을 사용한 GSM8K와 MATH는 도메인 내 과제로, 다른 모든 벤치마크는 도메인 외 과제로 분류됩니다. 평가 결과, DeepSeekMath-RL 7B는 체인 오브 소트 추론을 통해 GSM8K에서 88.2%, MATH에서 51.7%의 정확도를 달성했습니다. 이는 7B에서 70B 범위의 모든 오픈소스 모델들과 대부분의 비공개 모델들의 성능을 뛰어넘는 결과입니다.

특히 주목할 만한 점은 DeepSeekMath-RL 7B가 DeepSeekMath-Instruct 7B에서 시작하여 GSM8K와 MATH의 체인 오브 소트 형식 지시어 튜닝 데이터만으로 학습되었음에도 불구하고, 모든 평가 지표에서 DeepSeekMath-Instruct 7B의 성능을 능가했다는 것입니다. 이는 강화학습의 효과성을 명확하게 보여주는 결과입니다.

### 사전 학습과 강화학습에서의 발견

#### 사전 학습 과정의 교훈

사전 학습 과정에서 얻은 중요한 통찰을 공유하고자 합니다. 특별한 언급이 없는 한, 2.2.1절에서 설명한 학습 설정을 따르며, DeepSeekMath 코퍼스는 데이터 수집의 두 번째 반복에서 얻은 890억 토큰의 데이터셋을 사용했습니다.

#### 코드 학습이 수학적 추론 능력을 향상시키는 효과

코드 학습이 추론 능력을 향상시킨다는 가설은 널리 알려져 있지만 아직 충분히 검증되지 않았습니다. 본 연구에서는 수학적 영역에서 이 가설을 부분적으로 검증했습니다. 코드 학습은 도구 사용 여부와 관계없이 모델의 수학적 추론 능력을 향상시키는 것으로 나타났습니다.

코드 학습이 수학적 추론에 미치는 영향을 연구하기 위해 다음과 같은 2단계 학습과 1단계 학습 설정으로 실험을 진행했습니다.

2단계 학습:
- 코드 학습 4,000억 토큰 → 수학 학습 1,500억 토큰: DeepSeek-LLM 1.3B를 4,000억 개의 코드 토큰으로 학습한 후 1,500억 개의 수학 토큰으로 학습
- 일반 학습 4,000억 토큰 → 수학 학습 1,500억 토큰: 대조 실험으로, 코드 토큰 대신 DeepSeek-AI가 구축한 대규모 일반 코퍼스에서 샘플링한 일반 토큰을 사용하여 수학적 추론 능력 향상에 있어 코드 토큰의 장점을 조사

1단계 학습:
- 수학 학습 1,500억 토큰: DeepSeek-LLM 1.3B를 1,500억 개의 수학 토큰으로 학습
- 코드와 수학 혼합 학습 (4,000억 코드 토큰 + 1,500억 수학 토큰): 코드 학습 후 수학 학습을 진행하면 코딩 성능이 저하되는 문제가 있습니다. 코드 토큰을 수학 토큰과 혼합하여 1단계 학습을 진행할 경우에도 수학적 추론 능력이 향상되는지, 그리고 치명적 망각 문제가 완화되는지 조사했습니다.

실험 결과는 표 6과 표 7에서 확인할 수 있습니다. 코드 학습은 2단계 학습과 1단계 학습 설정 모두에서 프로그램 기반 수학적 추론 능력을 향상시켰습니다. 2단계 학습 설정에서는 코드 학습만으로도 Python을 사용한 GSM8K와 MATH 문제 해결 능력이 크게 향상되었으며, 두 번째 단계의 수학 학습을 통해 추가적인 성능 향상을 달성했습니다.

흥미로운 점은 1단계 학습 설정에서 코드 토큰과 수학 토큰을 혼합하여 학습했을 때, 2단계 학습에서 발생하는 치명적 망각 문제가 효과적으로 완화되었고, 코딩 능력과 프로그램 기반 수학적 추론 능력(표 6) 모두에서 시너지 효과를 보였다는 것입니다.

코드 학습은 도구 없이도 수학적 추론 능력을 향상시킵니다. 2단계 학습 설정에서 초기 코드 학습 단계만으로도 적절한 수준의 성능 향상이 관찰되었습니다. 또한 이는 후속 수학 학습의 효율성을 높여 최종적으로 최고의 성능을 달성하는 데 기여했습니다. 그러나 코드 토큰과 수학 토큰을 혼합한 1단계 학습은 도구 없는 수학적 추론 능력을 다소 저하시켰습니다. 이는 DeepSeek-LLM 1.3B가 규모의 한계로 인해 코드와 수학 데이터를 동시에 완전히 학습하지 못했기 때문일 것으로 추정됩니다.
#### arXiv 논문이 수학적 추론 능력 향상에 미치는 영향

arXiv 논문은 일반적으로 수학 사전 학습 데이터의 중요한 구성 요소로 여겨져 왔습니다. Azerbayev와 연구진, Lewkowycz와 연구진, Polu와 Sutskever, Wang과 연구진 등의 연구에서도 이를 활용했습니다. 하지만 수학적 추론 능력 향상에 대한 arXiv 논문의 영향을 상세히 분석한 연구는 많지 않았습니다.

우리의 실험 결과는 다소 직관에 반하는 결과를 보여줍니다. arXiv 논문이 수학적 추론 능력 향상에 큰 효과가 없다는 것입니다. DeepSeek-LLM 1.3B와 DeepSeek-Coder-Base-v1.5 7B를 대상으로, 서로 다른 처리 과정을 거친 arXiv 코퍼스를 사용하여 실험을 진행했습니다.

- MathPile: 89억 토큰 규모의 코퍼스로, 85% 이상이 과학 논문으로 구성되어 있으며 정제와 필터링을 위한 휴리스틱 규칙을 적용했습니다.
- ArXiv-RedPajama: 서문, 주석, 매크로, 참고문헌이 제거된 280억 토큰 규모의 arXiv LaTeX 파일 전체입니다.

각 arXiv 코퍼스에 대해 DeepSeek-LLM 1.3B는 1,500억 토큰, DeepSeek-Coder-Base-v1.5 7B는 400억 토큰의 학습을 진행했습니다. 실험 결과, arXiv 논문만으로 구성된 코퍼스로 학습했을 때 두 모델 모두 다양한 수학 벤치마크에서 눈에 띄는 성능 향상을 보이지 않았거나 오히려 성능이 저하되었습니다. 이는 GSM8K와 MATH 같은 정량적 추론 데이터셋, MMLU-STEM과 같은 객관식 문제, 그리고 miniF2F와 같은 형식 수학 등 다양한 복잡도의 벤치마크에서 일관되게 나타났습니다.

하지만 이러한 결론은 다음과 같은 제한 사항을 고려해야 합니다.

1. 본 연구에서 다루지 않은 특정 수학 관련 과제, 예를 들어 정리의 비형식화(형식적 진술이나 증명을 비형식적 버전으로 변환)에 대한 arXiv 토큰의 영향
2. 다른 유형의 데이터와 결합했을 때 arXiv 토큰의 효과
3. 더 큰 규모의 모델에서 arXiv 논문의 이점이 나타날 가능성

이러한 측면들은 추가적인 연구가 필요한 영역으로 남아있습니다.
### 강화학습의 통찰과 발견

#### 통합 패러다임을 향한 여정

이 섹션에서는 지도 학습(SFT), 거부 샘플링 파인튜닝(RFT), DPO, PPO, GRPO와 같은 다양한 학습 방법을 분석하기 위한 통합 패러다임을 제시하고, 이를 바탕으로 실험을 진행한 결과를 공유합니다.

일반적으로 학습 방법의 파라미터 \\(\theta\\)에 대한 그래디언트는 다음과 같이 표현됩니다.

\\[\\nabla_\\theta J_A(\\theta) = \\mathbb{E}\_{(q,o)\\sim D} \\left[\\frac{1}{ \\left\\vert o \right\vert} \\sum_{t=1}^{ \left\vert o\right\vert} GC(q,o,t,\\pi_{rf}) \\nabla_\\theta \\log \\pi_\\theta(o_t \vert q,o_{<t})\\right]\\]

여기서 세 가지 핵심 요소를 확인할 수 있습니다.
1. 데이터 소스 D: 학습 데이터를 결정합니다.
2. 보상 함수 \\(\\pi_{rf}\\): 학습 보상 신호의 원천입니다.
3. 알고리즘 A: 학습 데이터와 보상 신호를 처리하여 데이터에 대한 페널티나 강화의 크기를 결정하는 그래디언트 계수 GC를 생성합니다.

![강화학습 그래디언트 계산](/assets/2025-01-30-deepseekmath-pushing-the-limits-of-mathematical-reasoning-in-open-language-models/4.png)

대표적인 방법들을 이러한 통합 패러다임으로 분석해보면:

지도 학습(SFT)은 사람이 선택한 SFT 데이터로 사전 학습된 모델을 파인튜닝합니다. 거부 샘플링 파인튜닝(RFT)은 SFT 모델에서 샘플링한 출력을 기반으로 SFT 질문들에 대해 모델을 추가로 파인튜닝하며, 답변의 정확성을 기준으로 출력을 필터링합니다.

![학습 방법별 성능 비교](/assets/2025-01-30-deepseekmath-pushing-the-limits-of-mathematical-reasoning-in-open-language-models/5.png)

데이터 소스에 대한 관찰 결과, 온라인 샘플링과 오프라인 샘플링으로 구분할 수 있습니다. 온라인 샘플링은 실시간 학습 정책 모델의 탐색 결과에서 학습 데이터를 얻는 반면, 오프라인 샘플링은 초기 SFT 모델의 샘플링 결과에서 데이터를 얻습니다. RFT와 DPO는 오프라인 방식을 따르고, Online RFT와 GRPO는 온라인 방식을 따릅니다.

![반복적 강화학습 성능](/assets/2025-01-30-deepseekmath-pushing-the-limits-of-mathematical-reasoning-in-open-language-models/6.png)

실험 결과에 따르면, Online RFT가 두 벤치마크에서 RFT를 크게 앞섰습니다. 특히 학습 초기에는 RFT와 비슷한 성능을 보이다가 후반부에서 절대적인 우위를 보였습니다. 이는 초기에는 액터와 SFT 모델이 매우 유사하여 샘플링된 데이터의 차이가 미미하지만, 후반부에서는 액터에서 샘플링된 데이터가 더 큰 차이를 보이며 실시간 데이터 샘플링의 장점이 두드러지기 때문입니다.
### 강화학습의 통찰과 발견

#### 그래디언트 계수의 영향과 특성

그래디언트 계수는 알고리즘이 보상 신호를 처리하여 모델 파라미터 업데이트의 크기를 결정하는 핵심 요소입니다. 실험에서는 보상 함수를 '규칙'과 '모델' 두 가지로 구분했습니다. 규칙은 답변의 정확성을 기준으로 응답의 품질을 판단하며, 모델은 규칙 판단을 기반으로 학습된 보상 모델을 통해 각 응답에 점수를 부여합니다.

방정식 10과 21을 통해 GRPO와 Online RFT의 주요 차이점을 확인할 수 있습니다. GRPO는 보상 모델이 제공하는 보상값에 따라 그래디언트 계수를 조정하는 특징이 있습니다. 이를 통해 응답의 품질에 따라 차등적인 강화와 페널티를 적용할 수 있습니다. 반면 Online RFT는 이러한 기능이 없어 잘못된 응답에 대한 페널티를 부과하지 않으며, 정답인 응답들에 대해서도 동일한 강도로 강화를 적용합니다.

![Maj@K와 Pass@K 성능 비교](/assets/2025-01-30-deepseekmath-pushing-the-limits-of-mathematical-reasoning-in-open-language-models/7.png)

실험 결과는 GRPO가 Online RFT를 능가하는 것을 보여주며, 이는 긍정적 및 부정적 그래디언트 계수를 조정하는 것의 효율성을 입증합니다. 또한 GRPO+PS가 GRPO+OS보다 우수한 성능을 보여주어, 단계별로 세분화된 그래디언트 계수를 사용하는 것이 유리함을 시사합니다.

#### 강화학습의 작동 원리 분석

본 연구에서는 지시어 튜닝 데이터의 일부를 기반으로 강화학습을 수행했으며, 이는 지시어 튜닝 모델의 성능을 크게 향상시켰습니다. 강화학습이 효과적인 이유를 더 깊이 이해하기 위해 지시어 모델과 강화학습 모델의 Pass@K와 Maj@K 정확도를 두 벤치마크에서 평가했습니다.

실험 결과에 따르면, 강화학습은 Pass@K는 향상시키지 않으면서 Maj@K의 성능을 개선했습니다. 이는 강화학습이 기본적인 능력을 향상시키기보다는 TopK 응답들 중에서 올바른 응답의 비중을 높임으로써 전반적인 성능을 개선한다는 것을 시사합니다. 이는 Wang과 연구진이 지도 학습 모델에서 발견한 추론 과제의 정렬 문제와 일맥상통하며, Song과 연구진, Wang과 연구진, Yuan과 연구진이 제안한 선호도 정렬 전략들을 통해 지도 학습 모델의 추론 성능을 향상시킬 수 있다는 연구 결과와도 부합합니다.
### 더 효과적인 강화학습을 위한 방향성

#### 데이터 소스의 중요성

데이터 소스는 모든 학습 방법의 기초가 되는 원자재입니다. 강화학습의 맥락에서 데이터 소스는 정책 모델에서 샘플링한 출력이 있는 레이블이 없는 질문들을 의미합니다. 본 연구에서는 지시어 튜닝 단계의 질문들과 단순한 핵심 샘플링만을 사용했는데, 이것이 우리의 강화학습 파이프라인이 Maj@K 성능만 향상시킨 잠재적 원인일 수 있습니다. 

향후에는 분포 외 질문 프롬프트에 대한 강화학습 파이프라인을 탐구하고, Yao와 연구진이 제안한 트리 검색 방법을 기반으로 한 고급 샘플링 전략을 적용할 계획입니다. 또한 Kwon과 연구진, Leviathan과 연구진, Xia와 연구진이 연구한 효율적인 추론 기술도 정책 모델의 탐색 효율성을 결정하는 매우 중요한 역할을 합니다.

#### 알고리즘의 발전 방향

알고리즘은 데이터와 보상 신호를 처리하여 모델 파라미터를 업데이트하기 위한 그래디언트 계수를 생성합니다. 이전 방정식을 기반으로 볼 때, 현재의 모든 방법들은 보상 함수의 신호를 완전히 신뢰하여 특정 토큰의 조건부 확률을 증가시키거나 감소시킵니다. 하지만 극도로 복잡한 과제에서는 보상 신호가 항상 신뢰할 수 있다고 보장할 수 없습니다.

예를 들어, Lightman과 연구진이 주의 깊게 주석을 단 PRM800K 데이터셋조차도 약 20%의 잘못된 주석을 포함하고 있습니다. 이러한 맥락에서 노이즈가 있는 보상 신호에 대해 강건한 강화학습 알고리즘을 탐구할 필요가 있습니다. Burns와 연구진이 제안한 WEAK-TO-STRONG 정렬 방법과 같은 접근이 학습 알고리즘에 근본적인 변화를 가져올 것으로 기대됩니다.

#### 보상 함수의 혁신

강화학습에서 보상 함수는 일반적으로 신경망 보상 모델입니다. 보상 모델과 관련하여 세 가지 중요한 발전 방향이 있습니다.

첫째, 보상 모델의 일반화 능력을 향상시키는 것입니다. 보상 모델은 분포 외 질문과 고급 디코딩 출력을 효과적으로 처리할 수 있어야 합니다. 그렇지 않으면 강화학습이 언어 모델의 분포를 안정화시키는 데 그칠 뿐, 근본적인 능력을 향상시키지 못할 수 있습니다.

둘째, 보상 모델의 불확실성을 반영하는 것입니다. 이러한 불확실성은 약한 보상 모델과 약-강 학습 알고리즘 사이의 연결 다리 역할을 할 수 있습니다.

셋째, Lightman과 연구진, Wang과 연구진이 연구한 것처럼 추론 과정에 대해 세분화된 학습 신호를 제공할 수 있는 고품질 프로세스 보상 모델을 효율적으로 구축하는 것입니다.

### 결론과 한계점, 그리고 향후 연구 방향

DeepSeekMath는 경쟁 수준의 MATH 벤치마크에서 모든 오픈소스 모델들을 능가하는 성능을 달성하며 비공개 모델들의 성능에 근접하는 결과를 보여주었습니다. DeepSeek-Coder-v1.5 7B를 기반으로 초기화된 이 모델은 Common Crawl에서 추출한 1,200억 개의 수학 토큰을 포함하여 총 5,000억 토큰에 대한 지속적인 학습을 진행했습니다.

광범위한 실험 연구를 통해 웹 페이지가 고품질 수학 데이터의 잠재력을 가지고 있음을 확인했으며, 예상과 달리 arXiv가 기대했던 만큼의 효과를 보여주지 않았다는 점이 주목할 만합니다. 연구진은 근접 정책 최적화(PPO)의 변형인 그룹 상대 정책 최적화(GRPO)를 도입했는데, 이는 적은 메모리 사용량으로도 수학적 추론 능력을 크게 향상시킬 수 있었습니다. 실험 결과는 DeepSeekMath-Instruct 7B가 이미 높은 벤치마크 점수를 달성한 상태에서도 GRPO가 효과적으로 작동함을 보여주었습니다.

또한 연구진은 다양한 강화학습 방법들을 이해하기 위한 통합 패러다임을 제시하고, 더 효과적인 강화학습을 위한 여러 잠재적 방향을 제시했습니다. 하지만 DeepSeekMath가 정량적 추론 벤치마크에서 인상적인 점수를 달성했음에도 불구하고, 기하학과 정리 증명 분야에서는 비공개 모델들에 비해 상대적으로 약한 성능을 보였습니다. 예를 들어, 실험 과정에서 삼각형과 타원 관련 문제들을 효과적으로 처리하지 못했는데, 이는 사전 학습과 파인튜닝 과정에서의 데이터 선택 편향을 시사합니다.

모델 규모의 제한으로 인해 DeepSeekMath는 GPT-4와 비교했을 때 퓨 샷 능력에서도 열세를 보였습니다. GPT-4는 퓨 샷 입력을 통해 성능을 향상시킬 수 있었지만, DeepSeekMath는 제로샷과 퓨 샷 평가에서 비슷한 수준의 성능을 보여주었습니다.

향후 연구 방향으로는 더 높은 품질의 사전 학습 코퍼스를 구축하기 위한 데이터 선택 파이프라인의 개선과 함께, 앞서 논의된 방향들을 바탕으로 대규모 언어 모델의 더 효과적인 강화학습 방법을 탐구할 계획입니다.

### 강화학습 방법론의 수학적 분석

강화학습 방법론의 수학적 기초를 이해하기 위해 다양한 학습 방법들의 목적 함수와 그래디언트를 상세히 분석해보겠습니다. 먼저 지도 학습 파인튜닝(Supervised Fine-tuning, SFT)의 목적 함수는 다음과 같이 정의됩니다.

\\[\\mathcal{F}\_{SFT}(\\theta)=\\mathbb{E}[q,o\\sim P_{sft}(Q,O)]\\left(\\frac{1}{\\left\vert o\\right\vert}\\sum_{t=1}^{\\left\vert o\\right\vert}\\log\\pi_\\theta(o_t \vert q,o_{ct})\\right)\\]

여기서 \\(P_{sft}(Q,O)\\)는 SFT 데이터셋의 분포를, \\(\\pi_\\theta\\)는 모델의 정책을, \\(q\\)는 질문을, \\(o\\)는 출력을 나타냅니다. SFT의 그래디언트는 다음과 같이 계산됩니다.

\\[\\nabla_\\theta\\mathcal{S}\_{SFT}=\\mathbb{E}[q,o\\sim P_{sft}(Q,O)]\\left(\\frac{1}{\\left\vert o\\right\vert}\\sum_{t=1}^{\\left\vert o\\right\vert}\\nabla_\\theta\\log\\pi_\\theta(o_t \vert q,o_{ct})\\right)\\]

거부 샘플링 파인튜닝(Rejection Sampling Fine-tuning, RFT)은 SFT 모델에서 샘플링한 여러 출력들 중 정답인 것들만 선택하여 학습하는 방식입니다. RFT의 목적 함수는 다음과 같습니다.

\\[\\mathcal{J}\_{RFT}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(Q),o\\sim\\pi_{sft}(O \vert q)]\\left(\\frac{1}{\\left\vert o\\right\vert}\\sum_{t=1}^{\\left\vert o\\right\vert}\\mathbb{I}[o]\\nabla_\\theta\\log\\pi_\\theta(o_t \vert q,o_{ct})\\right)\\]

온라인 RFT는 실시간 정책 모델 \\(\\pi_\\theta\\)에서 출력을 샘플링한다는 점에서 RFT와 차이가 있습니다. 온라인 RFT의 그래디언트는 다음과 같이 표현됩니다.

\\[\\nabla_\\theta\\mathcal{S}\_{OnRFT}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(Q),o\\sim\\pi_\\theta(O \vert q)]\\left(\\frac{1}{\\left\vert o\\right\vert}\\sum_{t=1}^{\\left\vert o\\right\vert}\\mathbb{I}[o]\\nabla_\\theta\\log\\pi_\\theta(o_t \vert q,o_{ct})\\right)\\]

직접 선호도 최적화(Direct Preference Optimization, DPO)는 선호도 학습을 위한 방법으로, 다음과 같은 목적 함수를 가집니다.

\\[\\mathcal{J}\_{DPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(Q),o^+,o^-\\sim\\pi_{sft}(O \vert q)]\\log\\sigma\\left(\\beta\\frac{1}{\\left\vert o^+\\right\vert}\\sum_{t=1}^{\\left\vert o^+\\right\vert}\\log\\frac{\\pi_\\theta(o_t^+ \vert q,o_{ct}^+)}{\\pi_{ref}(o_t^+ \vert q,o_{ct}^+)}-\\beta\\frac{1}{\\left\vert o^-\\right\vert}\\sum_{t=1}^{\\left\vert o^-\\right\vert}\\log\\frac{\\pi_\\theta(o_t^- \vert q,o_{ct}^-)}{\\pi_{ref}(o_t^- \vert q,o_{ct}^-)}\\right)\\]

근접 정책 최적화(Proximal Policy Optimization, PPO)는 안정적인 정책 학습을 위해 다음과 같은 목적 함수를 사용합니다.

\\[\\mathcal{J}\_{PPO}(\\theta)=\\mathbb{E}[q\\sim P_{sft}(Q),o\\sim\\pi_{\\theta_{old}}(O \vert q)]\\sum_{t=1}^{\\left\vert o\\right\vert}\\min\\left(\\frac{\\pi_\\theta(o_t \vert q,o_{ct})}{\\pi_{\\theta_{old}}(o_t \vert q,o_{ct})}A_t,\\text{clip}\\left(\\frac{\\pi_\\theta(o_t \vert q,o_{ct})}{\\pi_{\\theta_{old}}(o_t \vert q,o_{ct})},1-\\epsilon,1+\\epsilon\\right)A_t\\right)\\]

여기서 \\(A_t\\)는 일반화된 이점 추정(Generalized Advantage Estimation)을 통해 계산된 이점값입니다.

- - -
### References
* [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](http://arxiv.org/pdf/2402.03300v3)