---
layout: post
title: "LLaMA: Open and Efficient Foundation Language Models"
date: 2023-02-27 17:11:15
author: "Meta AI"
categories: "Language-Models"
tags: ["Open-and-Efficient-Foundation-Language-Models", "Optimal-Model/Data-Scaling-Up-Allocation", "Rotary-Positional-Embeddings", "SwiGLU-Activation-Function", "Responsible-AI-Evaluation"]
cover: /assets/images/language-models.webp
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
대규모 언어 모델(LLM)의 발전은 주로 비공개 데이터셋과 막대한 컴퓨팅 자원을 보유한 대기업들에 의해 주도되어 왔습니다. 이는 학계와 소규모 연구 기관들의 LLM 연구 참여를 제한하는 중요한 장벽이 되어왔습니다. 메타 AI 연구진은 이러한 한계를 극복하고자, 공개적으로 접근 가능한 데이터만을 사용하여 최고 수준의 성능을 달성할 수 있는 효율적인 언어 모델을 개발하고자 했습니다. 특히 Hoffmann의 연구에서 제시된 "더 큰 모델보다 더 많은 데이터로 학습된 작은 모델이 더 효율적일 수 있다"는 통찰을 검증하고 발전시키고자 했습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
LLaMA는 기존 접근법과 달리 모델 크기와 학습 데이터 양 사이의 최적 균형을 찾는데 중점을 두었습니다. 연구진은 7B부터 65B 파라미터에 이르는 다양한 규모의 모델을 개발하면서, 일반적으로 사용되는 것보다 훨씬 더 많은 토큰(1.4T)으로 학습을 진행했습니다. 또한 공개적으로 이용 가능한 데이터만을 엄선하여 사용함으로써, 모델의 투명성과 재현가능성을 보장했습니다. 특히 주목할 만한 점은 CommonCrawl, Wikipedia, arXiv 등 다양한 도메인의 데이터를 세심하게 필터링하고 전처리하여 고품질의 학습 데이터셋을 구축했다는 것입니다.

#### 제안된 방법은 어떻게 구현되었습니까?
LLaMA의 구현은 최신 트랜스포머 아키텍처를 기반으로 하되, 여러 가지 효율적인 최적화 기법을 도입했습니다. 사전 정규화, SwiGLU 활성화 함수, RoPE(회전 위치 임베딩) 등을 적용하여 모델의 성능과 학습 효율성을 개선했습니다. 학습 과정에서는 AdamW 옵티마이저와 코사인 학습률 스케줄링을 사용했으며, 효율적인 구현을 위해 xformers 라이브러리의 최적화된 어텐션 메커니즘을 활용했습니다. 특히 2048개의 A100 GPU를 사용하여 GPU당 약 380 토큰/초의 처리 속도를 달성했으며, 이를 통해 1.4T 토큰의 학습을 약 21일 만에 완료할 수 있었습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
이 연구의 가장 중요한 의의는 공개 데이터만으로도 최고 수준의 언어 모델을 개발할 수 있다는 것을 입증했다는 점입니다. 특히 13B 파라미터 모델이 GPT-3(175B)를 능가하고, 65B 모델이 PaLM-540B와 견줄만한 성능을 보였다는 것은 매우 중요한 발견입니다. 이는 효율적인 모델 설계와 데이터 선택이 단순한 모델 크기 확장보다 더 중요할 수 있다는 것을 시사합니다. 또한 작은 규모의 모델들이 단일 GPU에서도 실행 가능하다는 점은 LLM 연구의 민주화에 크게 기여할 것으로 기대됩니다. 다만 모델이 여전히 편향성과 유해성을 보이는 것으로 나타나, 이러한 문제들의 해결을 위한 추가 연구의 필요성도 제기되었습니다.
- - -
## LLaMA: 개방형 고효율 파운데이션 언어 모델

### 서론

메타 AI 연구진이 개발한 LLaMA는 7B부터 65B 파라미터에 이르는 파운데이션 언어 모델 시리즈입니다. 이 연구는 공개적으로 접근 가능한 데이터셋만을 사용하여 최첨단 성능의 언어 모델을 학습할 수 있다는 것을 입증했습니다. 특히 주목할 만한 점은 13B 파라미터 규모의 LLaMA 모델이 175B 파라미터의 GPT-3를 대부분의 벤치마크에서 능가했으며, 65B 파라미터 모델은 Chinchilla-70B와 PaLM-540B와 같은 최고 수준의 모델들과 견줄 만한 성능을 보여주었다는 것입니다.

대규모 언어 모델(LLM)은 텍스트 지시사항이나 퓨 샷 샘플 만으로도 새로운 과제를 수행할 수 있는 퓨 샷 능력을 보여왔습니다. Kaplan과 연구진의 연구에 따르면, 이러한 퓨 샷 특성은 모델이 충분한 규모에 도달했을 때 처음 나타나기 시작했습니다. 이는 더 큰 모델이 더 나은 성능을 보일 것이라는 가정 하에 모델 규모를 확장하는 연구로 이어졌습니다.

그러나 Hoffmann과 연구진의 최근 연구는 주어진 계산 예산 내에서 최고의 성능은 가장 큰 모델이 아닌, 더 많은 데이터로 학습된 작은 모델에서 달성된다는 것을 보여주었습니다. 특히 추론 단계에서의 계산 비용을 고려할 때, 목표 성능 수준에 도달하기 위해서는 큰 모델을 학습하는 것보다 작은 모델을 더 오래 학습하는 것이 궁극적으로 더 효율적일 수 있습니다. 예를 들어, Hoffmann과 연구진은 10B 모델을 200B 토큰으로 학습하는 것을 권장했지만, 본 연구에서는 7B 모델이 1T 토큰 이상에서도 계속해서 성능이 향상되는 것을 발견했습니다.

본 연구의 핵심은 일반적으로 사용되는 것보다 더 많은 토큰으로 학습하여 다양한 추론 예산에서 최상의 성능을 달성할 수 있는 언어 모델 시리즈를 개발하는 것입니다. 특히 주목할 만한 점은 LLaMA-13B가 10배 더 작은 규모임에도 불구하고 대부분의 벤치마크에서 GPT-3를 능가한다는 것입니다. 이는 단일 GPU에서도 실행할 수 있어 LLM 연구의 접근성을 높일 수 있습니다.

본 연구는 공개적으로 이용 가능한 데이터만을 사용하여 모델을 학습했다는 점에서 기존의 GPT-3, PaLM, Chinchilla와 차별화됩니다. 이는 모델의 오픈소스화를 가능하게 하며, 기존 모델들이 비공개이거나 문서화되지 않은 데이터("Books - 2TB" 또는 "Social media conversations" 등)에 의존하는 것과는 대조적입니다. OPT, GPT-NeoX, BLOOM, GLM과 같은 오픈소스 모델들이 존재하지만, PaLM-62B나 Chinchilla와 경쟁할 만한 수준의 성능을 보여주지는 못했습니다.

### 학습 접근 방식

LLaMA의 학습 방법론은 Brown과 연구진이 GPT-3에서 제시한 방식과 Chowdhery와 연구진이 PaLM에서 사용한 접근법을 기반으로 하며, 특히 Hoffmann과 연구진이 제시한 Chinchilla 스케일링 법칙에서 큰 영감을 받았습니다. 이 접근법의 핵심은 대규모 트랜스포머 모델을 표준 옵티마이저를 사용하여 방대한 양의 텍스트 데이터로 학습시키는 것입니다.

이러한 방법론적 선택은 최근의 연구 결과들을 세심하게 반영한 것입니다. 특히 Hoffmann과 연구진의 연구는 주어진 계산 예산 내에서 최적의 성능을 달성하기 위해서는 모델 크기와 학습 데이터 양 사이의 균형이 중요하다는 것을 보여주었습니다. 이들의 연구에 따르면, 모델 크기를 두 배로 늘릴 때마다 학습 데이터도 같은 비율로 증가시켜야 최적의 성능을 얻을 수 있습니다.

LLaMA의 학습 방식은 이러한 통찰을 실제 구현에 적용한 것으로, 특히 PaLM에서 사용된 트랜스포머 아키텍처의 효율적인 학습 방법과 GPT-3의 대규모 언어 모델 학습 경험을 결합했습니다. 표준 옵티마이저를 사용한다는 것은 복잡한 학습 기법을 도입하지 않고도 효과적인 학습이 가능하다는 것을 의미하며, 이는 모델의 확장성과 재현성을 높이는데 기여합니다.

### 사전 학습 데이터

LLaMA의 학습 데이터셋은 다양한 도메인을 포괄하는 여러 공개 데이터 소스의 조합으로 구성되어 있습니다. 이 데이터셋은 공개적으로 접근 가능하고 오픈소스 라이선스와 호환되는 데이터만을 엄선하여 구성되었습니다. 전체 학습 데이터에서 각 소스가 차지하는 비중을 살펴보면 다음과 같습니다.

영어 CommonCrawl이 전체 데이터의 67%를 차지하며, 이는 2017년부터 2020년까지의 5개 덤프에서 추출되었습니다. Wenzek과 연구진이 개발한 CCNet 파이프라인을 통해 이 데이터를 전처리했는데, 이 과정에서 라인 레벨 중복 제거, fastText 선형 분류기를 이용한 언어 식별, n-gram 언어 모델을 통한 저품질 콘텐츠 필터링이 수행되었습니다. 또한 위키피디아에서 참조로 사용된 페이지와 무작위로 샘플링된 페이지를 구분하는 선형 모델을 학습시켜, 참조로 분류되지 않은 페이지들은 제외했습니다.

C4 데이터셋은 전체의 15%를 차지하며, 초기 실험에서 다양한 전처리된 CommonCrawl 데이터셋의 사용이 성능 향상에 도움이 된다는 관찰에 기반하여 포함되었습니다. C4의 전처리 과정도 중복 제거와 언어 식별 단계를 포함하지만, CCNet과의 주요 차이점은 품질 필터링 방식에 있습니다. C4는 문장부호의 존재나 웹페이지의 단어 및 문장 수와 같은 휴리스틱에 주로 의존합니다.

GitHub 데이터는 4.5%를 차지하며, Google BigQuery에서 제공하는 공개 GitHub 데이터셋에서 Apache, BSD, MIT 라이선스로 배포된 프로젝트만을 선별했습니다. 라인 길이나 영숫자 문자 비율 등의 휴리스틱을 기반으로 저품질 파일을 필터링했고, 정규 표현식을 사용하여 헤더와 같은 상용구를 제거했습니다. 최종적으로 파일 레벨에서 정확한 매칭을 통한 중복 제거를 수행했습니다.

위키피디아 데이터도 4.5%를 차지하며, 2022년 6월부터 8월까지의 덤프에서 라틴 문자나 키릴 문자를 사용하는 20개 언어(bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk)를 포함합니다. 하이퍼링크, 주석, 기타 서식 상용구를 제거하는 전처리 과정을 거쳤습니다.

Gutenberg 프로젝트와 Books3 데이터는 전체 학습 데이터의 4.5%를 차지합니다. Gutenberg 프로젝트는 저작권이 만료된 공공 도메인의 도서들을 포함하고 있으며, Books3는 대규모 언어 모델 학습을 위해 공개적으로 제공되는 ThePile 데이터셋의 일부입니다. 이 데이터에 대해서는 도서 수준에서 중복 제거를 수행했으며, 90% 이상의 내용이 중복되는 도서들은 제외했습니다.

arXiv 데이터는 2.5%를 차지하며, 과학 논문의 LaTeX 파일을 처리하여 포함시켰습니다. Lewkowycz와 연구진의 접근 방식을 따라 첫 번째 섹션 이전의 모든 내용과 참고문헌을 제거했습니다. 또한 .tex 파일에서 주석을 제거하고, 논문 간의 일관성을 높이기 위해 사용자가 작성한 정의와 매크로를 인라인으로 확장했습니다.

Stack Exchange 데이터는 2%를 차지하며, 컴퓨터 과학부터 화학까지 다양한 분야를 다루는 고품질 질문과 답변 웹사이트의 덤프를 포함합니다. 28개의 가장 큰 웹사이트의 데이터를 선별했으며, 텍스트에서 HTML 태그를 제거하고 답변을 점수(높은 순에서 낮은 순)에 따라 정렬했습니다.

토크나이저로는 SentencePiece 구현체를 사용하여 바이트 페어 인코딩(BPE) 알고리즘을 적용했습니다. 특히 모든 숫자를 개별 자릿수로 분할하고, 알 수 없는 UTF-8 문자를 분해할 때는 바이트 단위로 대체하는 방식을 채택했습니다. 토크나이즈 후 전체 학습 데이터셋은 약 1.4T 토큰을 포함하며, 위키피디아와 도서 도메인을 제외한 대부분의 학습 데이터는 학습 과정에서 한 번만 사용되었습니다. 위키피디아와 도서 도메인의 경우 약 2회의 에포크를 수행했습니다.

### 아키텍처

LLaMA는 Vaswani와 연구진이 제안한 트랜스포머 아키텍처를 기반으로 하며, PaLM과 같은 최신 언어 모델들에서 사용된 다양한 개선사항들을 통합했습니다. 기존 트랜스포머 아키텍처에서 주요하게 변경된 사항들을 살펴보면 다음과 같습니다.

먼저, GPT-3에서 영감을 받은 사전 정규화(Pre-normalization) 방식을 도입했습니다. 이는 학습 안정성을 향상시키기 위해 트랜스포머의 각 서브레이어의 출력이 아닌 입력을 정규화하는 방식입니다. 정규화 함수로는 Zhang과 Sennrich가 제안한 RMSNorm을 사용했습니다. RMSNorm은 평균과 분산을 모두 사용하는 LayerNorm과 달리 RMS(Root Mean Square) 통계량만을 사용하여 계산 효율성을 높였습니다.

두 번째로, PaLM에서 영감을 받아 ReLU 비선형성을 Shazeer가 제안한 SwiGLU 활성화 함수로 대체했습니다. 이는 모델의 성능을 향상시키기 위한 선택이었으며, PaLM에서 사용된 \\(4d\\) 대신 \\(\frac{2}{3}4d\\) 차원을 사용했습니다. 이러한 차원 축소는 계산 효율성과 성능 사이의 균형을 고려한 선택입니다.

마지막으로, GPTNeo의 접근 방식을 따라 절대 위치 임베딩을 제거하고 대신 Su와 연구진이 제안한 회전 위치 임베딩(RoPE)을 네트워크의 각 층에 추가했습니다. RoPE는 위치 정보를 회전 행렬을 통해 인코딩하며, 상대적 위치 의존성을 명시적으로 모델링할 수 있다는 장점이 있습니다.

![Training Loss Graph](https://ar5iv.org//html/2302.13971/assets/x1.png)

위 그래프는 7B, 13B, 33B, 65B 파라미터를 가진 네 가지 LLaMA 모델의 학습 손실을 보여줍니다. 33B와 65B 모델은 1.4T 토큰으로 학습되었으며, 작은 모델들은 1.0T 토큰으로 학습되었습니다. 모든 모델은 4M 토큰의 배치 크기로 학습되었습니다. 그래프에서 볼 수 있듯이, 더 큰 모델들(33B와 65B)이 더 많은 데이터로 학습되었을 때 더 작은 모델들(7B와 13B)에 비해 더 낮은 학습 손실을 달성했습니다. 이는 모델 크기와 데이터 규모가 증가할수록 성능이 향상된다는 것을 보여줍니다.

### 옵티마이저

LLaMA 모델의 학습에는 AdamW 옵티마이저가 사용되었습니다. AdamW는 Loshchilov와 Hutter가 제안한 옵티마이저로, 기존 Adam 옵티마이저의 가중치 감쇠(weight decay) 방식을 개선한 알고리즘입니다. AdamW의 주요 하이퍼파라미터는 \\(\beta_1 = 0.9\\)와 \\(\beta_2 = 0.95\\)로 설정되었습니다. 여기서 \\(\beta_1\\)은 1차 모멘트 추정치의 지수 감소율을, \\(\beta_2\\)는 2차 모멘트 추정치의 지수 감소율을 제어합니다.

학습률은 코사인 스케줄링을 통해 조정되었습니다. 이 스케줄링 방식에서는 최종 학습률이 최대 학습률의 10%가 되도록 설정되었습니다. 이는 학습 과정에서 학습률이 점진적으로 감소하여 모델이 안정적으로 수렴할 수 있도록 돕습니다.

가중치 감쇠는 0.1로 설정되었으며, 이는 모델의 과적합을 방지하고 일반화 성능을 향상시키는데 중요한 역할을 합니다. 또한 그래디언트 클리핑 값을 1.0으로 설정하여 그래디언트 폭주 문제를 방지하고 학습의 안정성을 확보했습니다.

학습 초기에는 2,000 스텝의 웜업 기간을 두었습니다. 이 기간 동안 학습률이 점진적으로 증가하여 모델이 안정적으로 학습을 시작할 수 있도록 했습니다. 학습률과 배치 크기는 모델의 크기에 따라 다르게 설정되었습니다.

이러한 옵티마이저 설정은 AdamW의 장점을 최대한 활용하면서도 대규모 언어 모델의 특성을 고려한 것입니다. 특히 가중치 감쇠를 옵티마이저의 업데이트 단계와 분리함으로써, 적응적 학습률 방법에서도 효과적인 정규화가 이루어질 수 있도록 했습니다. 코사인 학습률 스케줄링과 함께 이러한 설정은 모델이 효과적으로 학습되면서도 안정적으로 수렴할 수 있도록 지원합니다.

### 효율적인 구현

LLaMA 모델의 학습 속도를 향상시키기 위해 여러 가지 최적화 기법이 적용되었습니다. 첫 번째로, 메모리 사용량과 실행 시간을 줄이기 위해 인과적 멀티헤드 어텐션의 효율적인 구현을 도입했습니다. xformers 라이브러리에서 제공하는 이 구현은 Rabe와 Staats의 연구에서 영감을 받았으며, Dao와 연구진이 제안한 역전파 방식을 활용합니다. 이 최적화의 핵심은 언어 모델링 작업의 인과적 특성으로 인해 마스킹되는 키/쿼리 점수를 계산하지 않고, 어텐션 가중치를 저장하지 않는 것입니다.

학습 효율성을 더욱 향상시키기 위해, 역전파 과정에서 재계산되는 활성화값의 양을 줄였습니다. 구체적으로, 선형 계층의 출력과 같이 계산 비용이 높은 활성화값들을 저장하는 방식을 채택했습니다. 이는 PyTorch의 자동 미분 기능을 사용하는 대신 트랜스포머 계층의 역전파 함수를 수동으로 구현함으로써 달성되었습니다. 이러한 최적화의 이점을 최대한 활용하기 위해, Korthikanti와 연구진이 제시한 모델 및 시퀀스 병렬화를 통해 모델의 메모리 사용량을 줄였습니다.

또한 GPU 간의 네트워크 통신(`all_reduce` 연산으로 인한)과 활성화값 계산을 최대한 중첩시켜 처리했습니다. 이러한 최적화 기법들의 결과로, 65B 파라미터 모델을 학습할 때 80GB RAM을 탑재한 2048개의 A100 GPU에서 GPU당 약 380 토큰/초의 처리 속도를 달성했습니다. 이는 1.4T 토큰으로 구성된 데이터셋에 대한 학습을 약 21일 만에 완료할 수 있게 해주었습니다.

이러한 구현의 효율성은 Rabe와 Staats의 연구에서 제시된 메모리 효율적인 어텐션 알고리즘에 기반합니다. 이들의 연구는 소프트맥스 분모의 나눗셈을 어텐션 계산의 마지막 단계까지 지연시키는 간단하면서도 강력한 기법을 도입했습니다. 이를 통해 쿼리당 상수의 메모리 오버헤드로 어텐션을 계산할 수 있게 되었고, 표준 구현에서 필요한 중간 어텐션 점수를 저장하기 위한 \\(\mathcal{O}(n)\\)의 메모리 요구사항을 크게 줄일 수 있었습니다.

### 주요 실험 결과

LLaMA 모델의 성능을 평가하기 위해 제로 샷과 퓨 샷 과제를 포함한 총 20개의 벤치마크를 사용했습니다. 제로 샷 평가에서는 과제에 대한 텍스트 설명과 테스트 샘플을 제공하고, 모델이 자유로운 생성이나 제안된 답변의 순위를 매기는 방식으로 진행되었습니다. 퓨 샷 평가의 경우 1개에서 64개 사이의 과제 샘플과 테스트 샘플을 제공하여 모델이 답변을 생성하거나 다양한 옵션의 순위를 매기도록 했습니다.

성능 비교를 위해 비공개 언어 모델인 GPT-3, Gopher, Chinchilla, PaLM과 오픈소스 모델인 OPT, GPT-J, GPT-Neo를 대상으로 선정했습니다. 또한 OPT-IML과 Flan-PaLM과 같은 지시어 튜닝 모델과도 간단한 비교를 수행했습니다.

자유 형식 생성 과제와 객관식 과제에서 모델의 성능을 평가했습니다. 객관식 과제에서는 주어진 맥락을 기반으로 가장 적절한 답변을 선택하는 것이 목표였습니다. 대부분의 경우 답변의 문자 수로 정규화된 우도를 기준으로 답변을 선택했으며, OpenBookQA와 BoolQ와 같은 특정 데이터셋에서는 Brown과 연구진의 방식을 따라 "Answer:"를 맥락으로 주었을 때의 우도로 정규화된 우도로 사용했습니다.

### 상식 추론 능력

상식 추론 능력을 평가하기 위해 8개의 표준 벤치마크를 사용했습니다. BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC easy/challenge, OpenBookQA. 이러한 데이터셋들은 Cloze 스타일 과제와 Winograd 스타일 과제, 그리고 객관식 문제 풀이를 포함합니다. 언어 모델링 커뮤니티의 관행에 따라 제로 샷 설정에서 평가를 진행했습니다.

실험 결과, LLaMA-65B는 BoolQ를 제외한 모든 벤치마크에서 Chinchilla-70B의 성능을 능가했습니다. 또한 BoolQ와 WinoGrande를 제외한 모든 벤치마크에서 PaLM-540B보다 우수한 성능을 보였습니다. 특히 주목할 만한 점은 LLaMA-13B 모델이 10배 더 작은 크기임에도 불구하고 대부분의 벤치마크에서 GPT-3를 능가했다는 것입니다.

### 클로즈 북 문답 능력

Natural Questions와 TriviaQA 두 가지 클로즈 북 문답 벤치마크에서 성능을 평가했습니다. 두 벤치마크 모두에서 정확한 일치(exact match) 성능을 측정했으며, 근거 문서에 접근할 수 없는 클로즈 북 설정에서 평가를 진행했습니다. LLaMA-65B는 제로 샷과 퓨 샷 설정 모두에서 최고 수준의 성능을 달성했습니다. 더욱 중요한 점은 LLaMA-13B가 5-10배 더 작은 크기임에도 불구하고 GPT-3와 Chinchilla와 비슷한 수준의 성능을 보였다는 것입니다. 이 모델은 추론 시 단일 V100 GPU에서도 실행이 가능합니다.

![성능 비교 그래프](https://ar5iv.org//html/2302.13971/assets/x2.png)

위 그래프는 LLaMA의 다양한 버전(7B, 13B, 33B, 65B 파라미터)과 Chinchilla 모델의 문답 및 상식 추론 과제에서의 학습 중 성능 변화를 보여줍니다. 대체로 더 큰 모델이 더 나은 성능을 보이며, 특히 65B 파라미터 모델이 가장 우수한 결과를 달성했습니다. 이는 모델 규모가 이러한 추론 과제에서 중요한 역할을 한다는 것을 시사합니다.
### 독해 능력

RACE 독해 벤치마크를 통해 모델의 독해 능력을 평가했습니다. 이 데이터셋은 중국 학생들을 위한 영어 독해 시험에서 수집된 것으로, 중학교와 고등학교 수준의 문제들로 구성되어 있습니다. Brown과 연구진의 평가 방식을 따라 실험을 진행한 결과, LLaMA-65B는 PaLM-540B와 비슷한 수준의 성능을 보였으며, LLaMA-13B는 GPT-3보다 몇 퍼센트 더 높은 정확도를 달성했습니다.

### 수학적 추론 능력

MATH와 GSM8k 두 가지 수학적 추론 벤치마크에서 모델의 성능을 평가했습니다. MATH는 LaTeX로 작성된 12K개의 중고등학교 수학 문제로 구성되어 있으며, GSM8k는 중학교 수준의 수학 문제들을 포함합니다. PaLM과 Minerva 모델과의 비교 실험을 수행했는데, Minerva는 ArXiv와 수학 웹페이지에서 추출한 38.5B 토큰으로 파인튜닝된 PaLM 모델 시리즈입니다. 반면 PaLM이나 LLaMA는 수학 데이터에 대한 파인튜닝을 거치지 않았습니다.

실험에서는 maj1@k 방식을 사용했는데, 이는 각 문제에 대해 \\(k\\)개의 샘플을 생성하고 다수결 투표를 수행하는 방식입니다. GSM8k에서 LLaMA-65B는 수학 데이터로 파인튜닝되지 않았음에도 불구하고 Minerva-62B의 성능을 능가했습니다.

### 코드 생성 능력

HumanEval과 MBPP 두 벤치마크를 통해 자연어 설명으로부터 코드를 생성하는 능력을 평가했습니다. 두 과제 모두에서 모델은 프로그램에 대한 간단한 설명과 입출력 샘플을 받습니다. HumanEval의 경우 함수 시그니처도 제공되며, 설명과 테스트가 독스트링 형태로 포함된 자연스러운 코드 형식으로 프롬프트가 구성됩니다.

실험 결과를 보면, 비슷한 파라미터 수를 가진 모델들 중에서 LLaMA는 LaMDA나 PaLM과 같은 일반 모델들보다 우수한 성능을 보였습니다. 13B 이상의 파라미터를 가진 LLaMA 모델들은 HumanEval과 MBPP 모두에서 LaMDA 137B의 성능을 능가했으며, LLaMA 65B는 더 오래 학습된 PaLM 62B의 성능도 뛰어넘었습니다.

코드 생성 실험에서 pass@1 결과는 온도 0.1로 샘플링하여 얻었으며, pass@100과 pass@80 메트릭은 온도 0.8로 측정했습니다. Chen과 연구진의 방식을 따라 pass@k의 편향되지 않은 추정치를 계산했습니다. 코드에 특화된 토큰으로 파인튜닝하면 성능을 더욱 향상시킬 수 있는데, 예를 들어 PaLM-Coder는 HumanEval에서 PaLM의 pass@1 점수를 26.2%에서 36%로 향상시켰습니다.
### 대규모 다중 과제 언어 이해 능력

MMLU(Massive Multitask Language Understanding) 벤치마크에서 모델의 성능을 평가했습니다. 이 벤치마크는 인문학, STEM, 사회과학을 포함하는 다양한 영역의 지식을 평가합니다. 5-샷 설정에서 평가를 진행한 결과, LLaMA-65B는 평균적으로 Chinchilla-70B와 PaLM-540B보다 몇 퍼센트 낮은 성능을 보였으며, 이는 대부분의 영역에서 일관되게 나타났습니다.

이러한 성능 차이의 잠재적인 원인으로는 사전 학습 데이터의 구성을 들 수 있습니다. LLaMA의 학습 데이터 중 도서와 학술 논문(ArXiv, Gutenberg, Books3)의 비중이 177GB에 불과한 반면, 다른 모델들은 최대 2TB의 도서 데이터를 사용했습니다. 이러한 대량의 도서 데이터 사용이 Gopher가 GPT-3보다 이 벤치마크에서 우수한 성능을 보인 이유일 수 있으며, 다른 벤치마크에서는 비슷한 수준의 성능을 보였다는 점이 이를 뒷받침합니다.

### 학습 중 성능 변화 분석

학습 과정에서 몇 가지 문답과 상식 추론 벤치마크에 대한 모델의 성능을 추적했습니다. 대부분의 벤치마크에서 성능은 꾸준히 향상되었으며, 모델의 학습 퍼플렉시티와 상관관계를 보였습니다. 그러나 SIQA와 WinoGrande는 예외적인 패턴을 보였습니다.

특히 SIQA에서는 성능의 변동성이 매우 크게 나타났는데, 이는 이 벤치마크의 신뢰성에 의문을 제기하게 만드는 결과입니다. WinoGrande의 경우, 성능이 학습 퍼플렉시티와 상관성이 높지 않았으며, LLaMA-33B와 LLaMA-65B가 학습 과정에서 비슷한 수준의 성능을 보였습니다.

### 지시어 파인튜닝 효과

비록 LLaMA-65B의 기본 버전이 이미 기본적인 지시사항을 따를 수 있지만, 소량의 지시어 데이터로 파인튜닝을 수행하면 MMLU에서의 성능이 향상되고 지시사항을 따르는 능력이 더욱 개선됨을 확인했습니다. Chung과 연구진의 프로토콜을 따라 단일 실험을 진행하여 LLaMA-I 모델을 학습했습니다.

실험 결과, LLaMA-I(65B)는 MMLU에서 68.9%의 성능을 달성했으며, 이는 기존의 중간 규모 지시어 파인튜닝 모델들의 성능을 뛰어넘는 수준입니다. 그러나 이는 여전히 GPT code-davinci-002의 77.4%에는 미치지 못하는 수준입니다. 이러한 결과는 간단한 지시어 파인튜닝 접근법만으로도 상당한 성능 향상을 달성할 수 있음을 보여줍니다.

### 편향성, 유해성 및 허위정보 평가

대규모 언어 모델은 학습 데이터에 존재하는 편향성을 재생산하고 증폭시키며, 유해하거나 공격적인 내용을 생성할 수 있다는 우려가 제기되어 왔습니다. LLaMA 모델의 학습 데이터셋이 웹에서 수집된 데이터를 상당 부분 포함하고 있다는 점을 고려할 때, 이러한 잠재적 위험성을 평가하는 것이 매우 중요합니다. 연구진은 LLaMA-65B 모델의 유해성을 평가하기 위해 유해 콘텐츠 생성과 고정관념 탐지에 초점을 맞춘 다양한 벤치마크 테스트를 수행했습니다.

#### RealToxicityPrompts 평가

언어 모델이 생성할 수 있는 유해 콘텐츠의 범위는 매우 광범위하며, 이는 포괄적인 평가를 어렵게 만듭니다. RealToxicityPrompts 벤치마크는 약 100k개의 프롬프트로 구성되어 있으며, 모델이 이에 대한 응답을 생성하면 PerspectiveAPI를 통해 유해성 점수가 자동으로 평가됩니다.

연구진은 각 프롬프트에 대해 그리디 디코딩을 사용하여 응답을 생성했으며, 유해성 점수는 0(무해)에서 1(유해) 사이의 값으로 측정되었습니다. 실험 결과, 모델의 크기가 증가할수록 유해성도 증가하는 경향을 보였으며, 특히 "respectful" 프롬프트에서 이러한 현상이 두드러졌습니다. 이는 이전 연구에서도 관찰된 현상이지만, Hoffmann과 연구진의 연구에서는 Chinchilla와 Gopher 사이에서 이러한 차이가 발견되지 않았습니다. 이는 모델 크기와 유해성의 관계가 동일한 모델 계열 내에서만 적용될 수 있음을 시사합니다.

#### CrowS-Pairs를 통한 편향성 평가

CrowS-Pairs 데이터셋을 사용하여 성별, 종교, 인종/피부색, 성적 지향, 연령, 국적, 장애, 외모, 사회경제적 지위 등 9가지 범주에서 모델의 편향성을 평가했습니다. 각 샘플은 고정관념을 반영하는 문장과 반고정관념적 문장으로 구성되어 있으며, 제로샷 설정에서 두 문장의 퍼플렉시티를 측정하여 모델이 고정관념적 문장을 선호하는 정도를 평가했습니다.

GPT-3 및 OPT-175B와의 비교 결과, LLaMA는 전반적으로 약간 더 나은 성능을 보였습니다. 그러나 종교 범주에서는 OPT-175B보다 10% 더 높은 편향성을 보였으며, 연령과 성별 범주에서도 상당한 편향성이 관찰되었습니다. 이러한 편향성은 여러 단계의 필터링에도 불구하고 CommonCrawl 데이터에서 기인한 것으로 추정됩니다.
#### WinoGender를 통한 성별 편향성 심층 분석

성별 범주의 편향성을 더 자세히 조사하기 위해 WinoGender 벤치마크를 활용했습니다. WinoGender는 Winograd 스키마를 기반으로 한 공지시어(co-reference) 해결 데이터셋으로, 대명사의 성별이 모델의 공지시어 해결 성능에 미치는 영향을 평가합니다. 각 문장은 "직업", "참여자", "대명사" 세 가지 요소를 포함하며, 대명사가 직업이나 참여자 중 어느 것을 지칭하는지를 문맥에 따라 판단하는 것이 목표입니다.

예를 들어 "간호사가 환자에게 그/그녀/그들의 근무 시간이 한 시간 후에 끝난다고 알렸다"라는 문장에서 "그/그녀/그들의"가 간호사를 지칭하는지 환자를 지칭하는지를 모델이 판단해야 합니다. 평가는 "her/her/she", "his/him/he", "their/them/someone"의 세 가지 대명사 유형에 대해 수행되었으며, 모델은 "간호사"와 "환자" 중 어느 것이 대명사의 선행사인지를 퍼플렉시티 비교를 통해 결정합니다.

실험 결과, LLaMA 모델은 "their/them/someone" 대명사에 대해 "her/her/she"와 "his/him/he" 대명사보다 훨씬 더 나은 공지시어 해결 성능을 보였습니다. 이는 모델이 성별 편향성을 가지고 있음을 시사하는 것으로, 특히 "her/her/she"와 "his/him/he" 대명사의 경우 문장의 맥락보다는 직업의 다수 성별을 기반으로 공지시어를 해결하는 경향이 있음을 보여줍니다.

#### TruthfulQA를 통한 진실성 평가

TruthfulQA는 모델의 진실성, 즉 주장이 참인지를 식별하는 능력을 측정하는 벤치마크입니다. 여기서 "참"은 신념 체계나 전통의 맥락이 아닌 "실제 세계에 대한 문자 그대로의 진실"을 의미합니다. 이 벤치마크는 38개 카테고리를 포함하며, 모델이 허위 정보나 거짓 주장을 생성할 위험을 평가하기 위해 설계되었습니다.

실험 결과, LLaMA 모델은 GPT-3와 비교했을 때 진실성과 정보성 모두에서 더 높은 점수를 기록했습니다. 그러나 여전히 정답률이 낮아 모델이 부정확한 답변을 생성할 가능성이 있음을 보여줍니다. 특히 LLaMA-65B의 경우 진실성 점수는 0.57, 진실성과 정보성을 모두 만족하는 답변의 비율은 0.53으로, 이는 모델이 상당한 비율로 부정확하거나 허위인 정보를 생성할 수 있음을 시사합니다.

### 탄소 발자국 분석

LLaMA 모델 학습의 환경적 영향을 정량적으로 평가하기 위해 Wu와 연구진이 제안한 에너지 소비량과 탄소 배출량 계산 방법론을 적용했습니다. 에너지 소비량은 다음 공식을 통해 와트시(Wh) 단위로 계산됩니다.

$$ \textrm{Wh}=\textrm{GPU-h} \times (\textrm{GPU power consumption}) \times \textrm{PUE} $$

여기서 PUE(Power Usage Effectiveness)는 1.1로 설정되었습니다. 이는 데이터센터의 전력 효율성을 나타내는 지표입니다.

탄소 배출량의 공정한 비교를 위해 데이터센터의 위치에 관계없이 미국 전역 평균 탄소 집약도 계수인 0.385 kg CO₂eq/KWh를 적용했습니다. 이를 통해 메가와트시(MWh)를 이산화탄소 등가량(tCO₂eq)으로 변환하는 공식은 다음과 같습니다.

$$ \textrm{tCO}_2\textrm{eq}=\textrm{MWh} \times 0.385 $$

이러한 표준화된 방법론을 통해 다른 대규모 언어 모델들과의 공정한 비교가 가능해졌습니다. 예를 들어, BLOOM은 0.057 kg CO₂eq/KWh의 그리드 배출량을 가진 환경에서 학습되어 27 tCO₂eq를 배출했으며, OPT는 0.231 kg CO₂eq/KWh의 환경에서 82 tCO₂eq를 배출했습니다.

OPT의 경우 공개된 학습 로그를 통해 992개의 A100-80GB GPU를 34일 동안 사용한 것으로 추정됩니다. LLaMA 모델 시리즈 개발을 위해서는 2048개의 A100-80GB GPU를 약 5개월 동안 사용했으며, 이는 설정된 가정하에서 약 2,638 MWh의 에너지를 소비하고 총 1,015 tCO₂eq의 탄소를 배출한 것으로 계산됩니다.

연구진은 이러한 상당한 환경적 비용에도 불구하고, 이미 학습된 모델을 공개함으로써 향후 추가적인 탄소 배출을 줄일 수 있을 것으로 기대합니다. 특히 일부 작은 규모의 모델들은 단일 GPU에서도 실행이 가능하여, 환경적 영향을 최소화하면서도 효과적으로 활용될 수 있습니다.

### 관련 연구

언어 모델은 단어, 토큰 또는 문자의 시퀀스에 대한 확률 분포를 나타냅니다. Shannon이 1948년과 1951년에 제시한 이 개념은 다음 토큰 예측이라는 형태로 자연어 처리의 핵심 문제로 자리잡았습니다. Turing이 1950년 "모방 게임"을 통해 기계 지능을 측정하는 방법을 제안한 이후, 언어 모델링은 인공지능의 발전을 측정하는 벤치마크로 제안되어 왔습니다.

전통적인 언어 모델은 n-gram 카운트 통계를 기반으로 했으며, 희소 이벤트의 추정을 개선하기 위해 다양한 스무딩 기법이 제안되었습니다. 지난 20년 동안 신경망이 언어 모델링 과제에 성공적으로 적용되었는데, 이는 피드포워드 모델에서 시작하여 순환 신경망과 LSTM으로 발전했습니다. 최근에는 셀프 어텐션을 기반으로 하는 트랜스포머 네트워크가 특히 장거리 의존성을 포착하는 데 있어 중요한 개선을 이끌어냈습니다.

모델과 데이터셋 크기 모두에서 언어 모델의 스케일링에는 오랜 역사가 있습니다. Brants와 연구진은 2조 토큰으로 학습된 언어 모델이 기계 번역의 품질 향상에 도움이 된다는 것을 보여주었습니다. 이 연구는 단순한 스무딩 기법인 'Stupid Backoff'를 사용했지만, 이후 Heafield와 연구진은 Kneser-Ney 스무딩을 웹 규모의 데이터에 적용하는 방법을 보여주었습니다. 이를 통해 CommonCrawl의 9,750억 토큰에 대해 5-gram 모델을 학습할 수 있었고, 5,000억 개의 n-gram을 포함하는 모델이 만들어졌습니다.

Chelba와 연구진은 언어 모델의 발전을 측정하기 위한 대규모 학습 데이터셋인 One Billion Word 벤치마크를 도입했습니다. 신경망 언어 모델의 맥락에서, Jozefowicz와 연구진은 LSTM을 10억 파라미터로 확장하여 Billion Word 벤치마크에서 최고 성능을 달성했습니다. 이후 트랜스포머의 스케일링은 많은 자연어 처리 과제에서 성능 향상을 이끌어냈습니다. 주목할 만한 모델로는 BERT, GPT-2, Megatron-LM, T5가 있습니다.

GPT-3는 1,750억 파라미터를 가진 모델로 중요한 돌파구를 마련했습니다. 이는 Jurassic-1, Megatron-Turing NLG, Gopher, Chinchilla, PaLM, OPT, GLM과 같은 대규모 언어 모델 시리즈로 이어졌습니다. Hestness와 연구진, Rosenfeld와 연구진은 딥러닝 모델의 스케일링이 성능에 미치는 영향을 연구하여 모델 크기, 데이터셋 크기, 시스템 성능 사이에 멱법칙 관계가 존재함을 보여주었습니다. Kaplan과 연구진은 트랜스포머 기반 언어 모델에 대한 구체적인 멱법칙을 도출했으며, 이는 나중에 Hoffmann과 연구진에 의해 데이터셋 스케일링 시 학습률 스케줄을 조정하는 방식으로 개선되었습니다. 마지막으로 Wei와 연구진은 스케일링이 대규모 언어 모델의 능력에 미치는 영향을 연구했습니다.

### 결론

본 연구에서는 최신 파운데이션 모델들과 경쟁력을 갖추면서도 공개적으로 배포 가능한 일련의 언어 모델을 선보였습니다. 특히 주목할 만한 점은 LLaMA-13B가 GPT-3보다 10배 이상 작은 크기임에도 불구하고 더 우수한 성능을 보였으며, LLaMA-65B는 Chinchilla-70B와 PaLM-540B와 견줄 만한 성능을 달성했다는 것입니다.

이전 연구들과는 달리, 본 연구는 독점적인 데이터셋에 의존하지 않고도 공개적으로 이용 가능한 데이터만으로 최고 수준의 성능을 달성할 수 있다는 것을 입증했습니다. 연구진은 이러한 모델들을 연구 커뮤니티에 공개함으로써 대규모 언어 모델의 발전을 가속화하고, 유해성과 편향성과 같은 알려진 문제점들을 개선하기 위한 노력을 지원할 수 있기를 기대합니다.

또한 Chung과 연구진의 연구 결과와 마찬가지로, 이러한 모델들을 지시어로 파인튜닝했을 때 유망한 결과를 얻을 수 있음을 확인했습니다. 연구진은 향후 연구에서 이 부분을 더욱 깊이 탐구할 계획입니다. 마지막으로, 모델의 규모를 확장하면서 지속적인 성능 향상이 관찰되었기 때문에, 연구진은 앞으로 더 큰 사전 학습 코퍼스로 더 큰 모델을 공개할 계획입니다.

- - -
### References
* [LLaMA: Open and Efficient Foundation Language Models](http://arxiv.org/pdf/2302.13971v1)
