---
layout: post
title: "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
date: 2025-01-22 15:19:35
author: "DeepSeek AI"
categories: "Reinforcement-Learning"
tags: ["Large-Scale-Reinforcement-Learning-on-Base-Model", "Reasoning-Oriented-Reinforcement-Learning", "Distillation-of-Reasoning-Capability", "Reinforcement-Learning-with-Cold-Start", "Rejection-Sampling-and-Supervised-Fine-Tuning"]
use_math: true
cover: "/assets/images/language-models.webp"
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
대규모 언어 모델(LLMs)의 발전 과정에서 사후 학습(post-training)이 중요한 구성 요소로 부각되고 있습니다. 특히 추론 능력 향상과 관련하여, OpenAI의 o1 시리즈가 Chain-of-Thought(CoT) 추론을 통해 획기적인 성능 향상을 이루었지만, 이를 효과적으로 구현하는 것은 여전히 주요 과제로 남아있었습니다. 기존 연구들은 프로세스 기반 보상 모델, 강화학습, 몬테카를로 트리 탐색 등 다양한 접근 방식을 시도했으나, OpenAI의 o1 시리즈에 견줄 만한 일반적인 추론 성능을 달성하지 못했습니다. 이에 연구진은 순수 강화학습(RL)을 활용하여 언어 모델의 추론 능력을 향상시키는 새로운 접근 방식을 탐구하고자 했습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
연구진은 두 가지 주요 모델을 제시했습니다. 첫째, DeepSeek-R1-Zero는 지도 학습 미세조정 없이 순수하게 강화학습만을 통해 학습된 모델입니다. 이는 AlphaGo Zero와 유사하게, 인간의 직접적인 지도 없이도 뛰어난 추론 능력을 획득할 수 있음을 보여줍니다. 둘째, DeepSeek-R1은 '콜드 스타트' 단계를 도입하고 다단계 학습 방식을 적용하여 DeepSeek-R1-Zero의 한계를 극복했습니다. 또한, 연구진은 이러한 모델의 지식을 더 작은 규모의 모델로 전달하기 위한 효과적인 증류 방법론도 개발했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
구현은 네 단계로 이루어졌습니다. 첫째, 수천 개의 콜드 스타트 데이터로 DeepSeek-V3-Base 모델을 미세조정했습니다. 둘째, 추론 중심의 강화학습을 수행했습니다. 셋째, RL 체크포인트에서 거부 샘플링을 통해 새로운 SFT 데이터를 생성하고, 이를 DeepSeek-V3의 지도 학습 데이터와 결합하여 모델을 재학습했습니다. 마지막으로, 모든 시나리오의 프롬프트를 고려한 추가 RL 과정을 수행했습니다. 특히 Group Relative Policy Optimization(GRPO)을 강화학습 알고리즘으로 채택하여 컴퓨팅 자원을 효율적으로 사용했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
이 연구는 두 가지 중요한 의미를 가집니다. 첫째, 순수 강화학습만으로도 언어 모델의 추론 능력을 크게 향상시킬 수 있다는 것을 입증했습니다. DeepSeek-R1은 AIME 2024, MATH-500, GPQA Diamond 등 다양한 추론 과제에서 OpenAI-o1-1217와 대등한 성능을 달성했습니다. 둘째, 증류를 통해 작은 모델도 강력한 추론 능력을 가질 수 있다는 것을 보여주었습니다. 특히 14B 규모의 증류 모델이 32B 규모의 QwQ-32B-Preview를 능가한 것은, 모델의 크기보다 학습 방법이 더 중요할 수 있다는 것을 시사합니다. 이러한 발견들은 향후 언어 모델의 발전 방향에 중요한 통찰을 제공합니다.
- - -
## DeepSeek-R1: 강화학습을 통한 언어 모델의 추론 능력 향상

DeepSeek-AI 연구진이 2025년 1월에 발표한 이 논문은 대규모 언어 모델의 추론 능력을 획기적으로 향상시키는 새로운 접근 방식을 소개합니다. 연구진은 두 가지 주요 모델인 DeepSeek-R1-Zero와 DeepSeek-R1을 개발했습니다.

DeepSeek-R1-Zero는 기존의 지도 학습 미세조정(Supervised Fine-tuning, SFT) 없이 순수하게 강화학습만을 통해 학습된 모델입니다. 이는 AlphaGo Zero가 바둑에서 보여준 것과 유사하게, 인간의 직접적인 지도 없이도 뛰어난 능력을 획득할 수 있다는 것을 언어 모델 분야에서 입증한 사례입니다. 이 모델은 강화학습을 통해 자연스럽게 다양한 추론 능력을 발현했지만, 가독성이 떨어지고 여러 언어가 혼합되는 등의 문제점도 나타났습니다.

이러한 한계를 극복하고 추론 성능을 더욱 향상시키기 위해 연구진은 DeepSeek-R1을 개발했습니다. 이 모델은 강화학습 이전에 '콜드 스타트' 단계를 도입하고 다단계 학습 방식을 적용했습니다. 성능 평가 결과를 보여주는 그래프를 살펴보면, DeepSeek-R1은 OpenAI-o1-1217와 비교했을 때 대등한 수준의 성능을 보여주고 있습니다. 특히 AIME 2024, Codeforces, GPQA Diamond, MATH-500, MMLU, SWE-bench Verified 등 다양한 추론 과제에서 우수한 성능을 달성했습니다.

연구진은 더 나아가 DeepSeek-R1의 지식을 더 작은 규모의 모델로 전달하기 위해 Qwen과 Llama 아키텍처를 기반으로 한 6개의 경량 모델(1.5B, 7B, 8B, 14B, 32B, 70B)을 개발했습니다. 이러한 지식 증류 과정을 통해 대규모 모델의 추론 능력을 유지하면서도 더 효율적으로 활용할 수 있는 방안을 제시했습니다.

이 연구는 강화학습만으로도 언어 모델의 추론 능력을 크게 향상시킬 수 있다는 것을 보여주었으며, 콜드 스타트와 같은 혁신적인 방법론을 통해 기존의 한계를 극복할 수 있다는 점에서 큰 의의가 있습니다.

### 서론

최근 대규모 언어 모델(Large Language Models, LLMs)은 인공일반지능(Artificial General Intelligence, AGI)을 향해 빠르게 진화하고 있습니다. 이러한 발전 과정에서 사후 학습(post-training)이 전체 학습 파이프라인의 중요한 구성 요소로 부각되고 있습니다. 사후 학습은 사전 학습에 비해 상대적으로 적은 컴퓨팅 자원으로도 추론 능력을 향상시키고, 사회적 가치와의 정렬을 이루며, 사용자 선호도에 맞게 적응하는 데 효과적입니다.

추론 능력 향상과 관련하여, OpenAI의 o1 시리즈 모델은 Chain-of-Thought(CoT) 추론 과정의 길이를 늘리는 방식으로 추론 시간 확장성을 도입한 최초의 모델입니다. 이 접근 방식은 수학, 코딩, 과학적 추론 등 다양한 추론 과제에서 상당한 성능 향상을 이루어냈습니다. 하지만 테스트 시간 확장성을 효과적으로 구현하는 것은 여전히 연구 커뮤니티의 주요 과제로 남아있습니다.

이전 연구들은 프로세스 기반 보상 모델, 강화학습, 몬테카를로 트리 탐색, 빔 서치 등 다양한 접근 방식을 탐구했지만, OpenAI의 o1 시리즈 모델에 견줄 만한 일반적인 추론 성능을 달성하지는 못했습니다. 본 논문에서는 순수 강화학습(RL)을 활용하여 언어 모델의 추론 능력을 향상시키는 첫 걸음을 내딛고자 합니다. 연구진의 목표는 지도 학습 데이터 없이도 순수 RL 과정을 통해 자체적으로 진화할 수 있는 LLM의 잠재력을 탐구하는 것입니다.

구체적으로, DeepSeek-V3-Base를 기본 모델로 사용하고 GRPO를 RL 프레임워크로 채택하여 모델의 추론 성능을 향상시켰습니다. 학습 과정에서 DeepSeek-R1-Zero는 자연스럽게 다양한 강력하고 흥미로운 추론 행동을 발현했습니다. 수천 번의 RL 단계를 거친 후, DeepSeek-R1-Zero는 추론 벤치마크에서 뛰어난 성능을 보여주었습니다. 예를 들어, AIME 2024에서의 pass@1 점수가 15.6%에서 71.0%로 향상되었고, 다수결 투표를 적용했을 때는 86.7%까지 개선되어 OpenAI-o1-0912의 성능과 비슷한 수준에 도달했습니다.

하지만 DeepSeek-R1-Zero는 가독성이 떨어지고 언어가 혼합되는 등의 문제에 직면했습니다. 이러한 문제를 해결하고 추론 성능을 더욱 향상시키기 위해, 연구진은 소량의 콜드 스타트 데이터와 다단계 학습 파이프라인을 도입한 DeepSeek-R1을 개발했습니다. 이 과정은 다음과 같은 단계로 구성됩니다.

1. 수천 개의 콜드 스타트 데이터를 수집하여 DeepSeek-V3-Base 모델을 미세조정
2. DeepSeek-R1-Zero와 같은 추론 중심의 RL 수행
3. RL 과정이 수렴에 가까워지면 RL 체크포인트에서 거부 샘플링을 통해 새로운 SFT 데이터 생성
4. 글쓰기, 사실 기반 QA, 자기 인식 등 DeepSeek-V3의 지도 학습 데이터와 결합하여 DeepSeek-V3-Base 모델 재학습
5. 새로운 데이터로 미세조정한 후, 모든 시나리오의 프롬프트를 고려한 추가 RL 과정 수행

이러한 단계를 거쳐 OpenAI-o1-1217와 대등한 성능을 보이는 DeepSeek-R1 체크포인트를 얻었습니다.
연구진은 DeepSeek-R1의 지식을 더 작은 크기의 밀집 모델로 전달하는 증류(distillation) 실험도 진행했습니다. Qwen2.5-32B를 기본 모델로 사용했을 때, DeepSeek-R1에서 직접 증류하는 방식이 해당 모델에 RL을 적용하는 것보다 더 나은 성능을 보였습니다. 이는 큰 기본 모델이 발견한 추론 패턴이 추론 능력 향상에 매우 중요하다는 것을 보여줍니다. 연구진은 이러한 증류 모델들을 Qwen과 Llama 시리즈를 기반으로 오픈소스로 공개했습니다.

특히 주목할 만한 점은 증류된 14B 모델이 최신 오픈소스 모델인 QwQ-32B-Preview를 큰 차이로 능가했다는 것입니다. 또한 증류된 32B와 70B 모델은 밀집 모델들 중에서 추론 벤치마크에서 새로운 기록을 수립했습니다.

본 연구의 주요 기여는 크게 두 가지 영역으로 나눌 수 있습니다.

첫째, 사후 학습 분야에서 기본 모델에 대한 대규모 강화학습을 성공적으로 구현했습니다. 연구진은 지도 학습 미세조정(SFT)을 예비 단계로 사용하지 않고 직접 RL을 기본 모델에 적용했습니다. 이 접근 방식을 통해 모델은 복잡한 문제를 해결하기 위한 Chain-of-Thought(CoT)를 자발적으로 탐색할 수 있었고, 그 결과로 DeepSeek-R1-Zero가 개발되었습니다. 이 모델은 자체 검증, 성찰, 긴 CoT 생성과 같은 능력을 보여주며 연구 커뮤니티에 중요한 이정표를 제시했습니다. 특히 지도 학습 없이도 순수한 RL만으로 LLM의 추론 능력을 향상시킬 수 있다는 것을 검증한 최초의 공개 연구라는 점에서 의의가 있습니다.

둘째, 증류 분야에서 작은 모델도 강력한 성능을 발휘할 수 있다는 것을 입증했습니다. 큰 모델의 추론 패턴을 작은 모델로 증류할 수 있으며, 이는 작은 모델에 직접 RL을 적용하는 것보다 더 나은 성능을 보인다는 것을 실험적으로 증명했습니다. 공개된 DeepSeek-R1과 그 API는 향후 연구 커뮤니티가 더 나은 소형 모델을 개발하는 데 도움이 될 것으로 기대됩니다.
DeepSeek-R1의 평가 결과는 다양한 분야에서 모델의 우수한 성능을 입증했습니다. 추론 과제에서 DeepSeek-R1은 AIME 2024에서 79.8%의 Pass@1 점수를 달성하여 OpenAI-o1-1217을 약간 상회하는 성능을 보였습니다. MATH-500에서는 97.3%라는 인상적인 점수를 기록하며 OpenAI-o1-1217과 대등한 수준의 성능을 보여주었고, 다른 모델들을 크게 앞섰습니다.

코딩 관련 과제에서도 DeepSeek-R1은 뛰어난 성과를 거두었습니다. Codeforces에서 2,029의 Elo 레이팅을 획득하여 인간 참가자의 96.3%를 능가하는 전문가 수준의 실력을 입증했습니다. 실제 엔지니어링 관련 과제에서는 DeepSeek-V3보다 약간 더 나은 성능을 보여, 실제 개발 작업에서도 유용하게 활용될 수 있음을 시사했습니다.

지식 기반 평가에서도 DeepSeek-R1은 탁월한 성과를 보여주었습니다. MMLU에서 90.8%, MMLU-Pro에서 84.0%, GPQA Diamond에서 71.5%를 달성하며 DeepSeek-V3를 크게 앞섰습니다. 이러한 점수는 OpenAI-o1-1217보다는 약간 낮지만, 다른 비공개 모델들을 능가하는 수준으로, 교육적 과제에서의 경쟁력을 입증했습니다. 사실 기반 벤치마크인 SimpleQA에서도 DeepSeek-R1은 DeepSeek-V3를 뛰어넘는 성능을 보였습니다.

또한 DeepSeek-R1은 창의적 글쓰기, 일반 질의응답, 편집, 요약 등 다양한 과제에서도 우수한 성능을 보여주었습니다. AlpacaEval 2.0에서 87.6%의 길이 제어 승률을, ArenaHard에서 92.3%의 승률을 달성하며 시험 외의 일반적인 질의에도 지능적으로 대응할 수 있는 강력한 능력을 보여주었습니다. 특히 긴 문맥 이해가 필요한 과제에서 DeepSeek-R1은 DeepSeek-V3를 크게 앞서는 성능을 보여, 긴 문맥 벤치마크에서 탁월한 성과를 거두었습니다.

### 접근 방식

DeepSeek-R1의 핵심적인 기술적 혁신은 대규모 강화학습을 통해 언어 모델의 추론 능력을 향상시키는 것입니다. 기존 연구들이 대량의 지도 학습 데이터에 의존했던 것과 달리, 본 연구는 지도 학습 미세조정(Supervised Fine-tuning, SFT) 없이도 강화학습만으로 모델의 추론 능력을 크게 향상시킬 수 있다는 것을 입증했습니다.

이러한 접근은 [Silver와 연구진](https://arxiv.org/abs/1712.01815)이 AlphaZero에서 보여준 자가 학습 방식에서 영감을 받았습니다. AlphaZero가 바둑과 체스에서 인간의 지식 없이도 자가 학습을 통해 최고 수준의 성능을 달성한 것처럼, DeepSeek-R1-Zero는 언어 모델 분야에서 유사한 혁신을 이루어냈습니다.

연구진은 크게 세 가지 주요 접근 방식을 제시했습니다.

첫째, DeepSeek-R1-Zero는 기본 모델에 직접 강화학습을 적용하여 지도 학습 없이도 추론 능력을 향상시킬 수 있음을 보여줍니다. 이는 [Gao와 연구진](https://arxiv.org/abs/2210.10760)이 제시한 보상 모델 과적합 문제를 해결하면서도 효과적인 학습이 가능하다는 것을 입증합니다.

둘째, DeepSeek-R1은 수천 개의 Chain-of-Thought(CoT) 예제로 미세조정된 체크포인트에서 시작하여 강화학습을 적용합니다. [Wang과 연구진](https://arxiv.org/abs/2203.11171)이 제안한 자기 일관성(Self-Consistency) 방법을 확장하여, 다양한 추론 경로를 탐색하고 최적의 해결책을 찾아내는 능력을 향상시켰습니다.

셋째, DeepSeek-R1의 추론 능력을 더 작은 밀집 모델로 전달하는 증류 과정을 수행합니다. [Lightman과 연구진](https://arxiv.org/abs/2305.20050)이 제시한 단계별 검증 방식을 활용하여, 큰 모델의 추론 패턴을 효과적으로 작은 모델에 전달할 수 있음을 보여줍니다.

이러한 접근 방식은 [Xin과 연구진](https://arxiv.org/abs/2408.08152)이 DeepSeek-Prover에서 보여준 것처럼, 강화학습과 증류를 통해 모델의 추론 능력을 체계적으로 향상시킬 수 있다는 것을 입증합니다. 특히 DeepSeek-R1은 지도 학습에 의존하지 않고도 강력한 추론 능력을 획득할 수 있다는 점에서 큰 의의가 있습니다.

### DeepSeek-R1-Zero: 기본 모델에 대한 강화학습

DeepSeek-R1-Zero는 지도 학습 데이터 없이 순수한 강화학습만을 통해 언어 모델의 추론 능력을 향상시키는 혁신적인 접근 방식을 제시합니다. Silver와 연구진이 AlphaZero에서 보여준 것처럼, 인간의 지식 없이도 자가 학습을 통해 뛰어난 성능을 달성할 수 있다는 아이디어에서 영감을 받았습니다.

강화학습 알고리즘으로는 Group Relative Policy Optimization(GRPO)를 채택했습니다. GRPO는 기존 PPO(Proximal Policy Optimization)와 달리 정책 모델과 동일한 크기의 critic 모델을 사용하지 않고, 대신 그룹 점수를 통해 기준값을 추정합니다. 이는 학습에 필요한 컴퓨팅 자원을 크게 절감할 수 있는 장점이 있습니다.

GRPO의 작동 방식을 수학적으로 살펴보면, 각 질문 \\(q\\)에 대해 이전 정책 \\(\pi_{\theta_{old}}\\)에서 출력 그룹 \\(\{o_1, o_2, \cdots, o_G\}\\)을 샘플링합니다. 그리고 다음과 같은 목적 함수를 최대화하여 정책 모델 \\(\pi_\theta\\)를 최적화합니다.

$$ J_{GRPO}(\theta) = \mathbb{E}[q \sim P(Q), \{o_i\}_{i=1}^{G} \sim \pi_{\theta_{old}}(O|q)] \frac{1}{G}\sum_{i=1}^{G}\left(\min\left[\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}A_i,\text{clip}\left(\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)},1-\varepsilon,1+\varepsilon\right)A_i\right]-\beta D_{KL}\left[\pi_{\theta}||\pi_{ref}\right]\right) $$

여기서 \\(\varepsilon\\)과 \\(\beta\\)는 하이퍼파라미터이며, \\(A_i\\)는 각 그룹 내 출력에 해당하는 보상 \\(\{r_1, r_2, \cdots, r_G\}\\)을 사용하여 계산되는 이점(advantage)입니다.

$$ A_i = \frac{r_i - \text{mean}(\{r_1,r_2,\cdots,r_G\})}{\text{std}(\{r_1,r_2,\cdots,r_G\})} $$

보상 모델링에서는 두 가지 유형의 보상을 사용합니다. 첫째, 정확도 보상은 응답의 정확성을 평가합니다. 예를 들어, 수학 문제의 경우 모델이 지정된 형식(예: 박스 안)으로 최종 답을 제공하도록 하여 규칙 기반으로 정확성을 검증할 수 있습니다. LeetCode 문제의 경우에는 미리 정의된 테스트 케이스를 통해 컴파일러가 피드백을 생성합니다.

둘째, 형식 보상은 모델이 사고 과정을 특정 태그('\<think\>' 및 '\</think\>') 사이에 배치하도록 강제합니다. 연구진은 신경망 보상 모델을 사용하지 않았는데, 이는 대규모 강화학습 과정에서 보상 해킹(reward hacking)이 발생할 수 있고, 보상 모델을 재학습하는 데 추가 자원이 필요하며 전체 학습 파이프라인이 복잡해질 수 있기 때문입니다.
DeepSeek-R1-Zero의 학습을 위해 연구진은 기본 모델이 지정된 지침을 따르도록 하는 간단한 템플릿을 설계했습니다. 이 템플릿은 모델이 먼저 추론 과정을 생성한 후 최종 답안을 제시하도록 구조화되어 있습니다. 연구진은 의도적으로 반영적 추론이나 특정 문제 해결 전략을 강제하는 등의 내용 관련 제약을 두지 않았습니다. 이는 강화학습 과정에서 모델의 자연스러운 발전을 정확하게 관찰하기 위함입니다.

성능 측면에서 DeepSeek-R1-Zero는 AIME 2024 벤치마크에서 놀라운 발전을 보여주었습니다. 강화학습이 진행됨에 따라 모델의 성능은 꾸준히 향상되어, pass@1 점수가 초기 15.6%에서 71.0%로 크게 증가했습니다. 이는 OpenAI-o1-0912와 비슷한 수준의 성능입니다. 다수결 투표 방식을 적용했을 때는 성능이 86.7%까지 향상되어 OpenAI-o1-0912를 능가했습니다.

![DeepSeek-R1-Zero의 학습 중 AIME 정확도](/assets/2025-01-23-deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning/1.png)

DeepSeek-R1-Zero의 자기 진화 과정은 특히 주목할 만합니다. 위 그래프에서 볼 수 있듯이, 모델의 사고 시간은 학습이 진행됨에 따라 지속적으로 증가했습니다. 이는 외부의 조정 없이 모델 내부에서 자연스럽게 발생한 발전입니다. 모델은 수백에서 수천 개의 추론 토큰을 생성하면서 복잡한 추론 과제를 해결하는 능력을 자연스럽게 습득했습니다.

![DeepSeek-R1-Zero의 응답 길이 변화](/assets/2025-01-23-deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning/2.png)

특히 흥미로운 점은 모델이 보여준 "아하 모멘트(aha moment)"입니다. 중간 버전의 모델에서 관찰된 이 현상은, 모델이 초기 접근 방식을 재평가하면서 문제에 더 많은 사고 시간을 할애하는 법을 학습했다는 것을 보여줍니다. 이는 단순히 모델의 추론 능력이 향상되었다는 것을 넘어서, 강화학습이 예상치 못한 정교한 결과를 이끌어낼 수 있다는 것을 보여주는 흥미로운 사례입니다.

그러나 DeepSeek-R1-Zero는 몇 가지 한계점도 가지고 있습니다. 가독성이 떨어지고 여러 언어가 혼합되는 등의 문제가 발생했습니다. 이러한 문제를 해결하고 추론 과정을 더 읽기 쉽게 만들어 오픈 커뮤니티와 공유하기 위해, 연구진은 인간 친화적인 콜드 스타트 데이터를 활용하는 DeepSeek-R1 방법론을 추가로 탐구하게 되었습니다.

### DeepSeek-R1: 콜드 스타트를 활용한 강화학습

DeepSeek-R1은 DeepSeek-R1-Zero의 성공적인 결과를 바탕으로, 두 가지 핵심 질문에 답하기 위해 개발되었습니다. 첫째, 소량의 고품질 데이터를 초기에 활용하는 콜드 스타트 방식으로 추론 성능을 더욱 향상시키거나 수렴을 가속화할 수 있는지, 둘째, 명확하고 일관된 사고의 연쇄(Chain of Thought, CoT)를 생성하면서도 강력한 일반 능력을 보여주는 사용자 친화적인 모델을 어떻게 학습할 수 있는지에 대한 질문입니다.

이러한 목표를 달성하기 위해 연구진은 네 단계로 구성된 학습 파이프라인을 설계했습니다. 첫 번째 단계인 콜드 스타트에서는 DeepSeek-R1-Zero와 달리, 강화학습의 초기 불안정한 단계를 방지하기 위해 소량의 긴 CoT 데이터를 수집하여 모델을 미세조정합니다. 이 데이터는 다양한 방법을 통해 수집되었습니다.

1. 긴 CoT를 예시로 사용한 퓨 샷 프롬프팅
2. 모델에게 직접 성찰과 검증이 포함된 상세한 답변 생성 요청
3. DeepSeek-R1-Zero의 출력을 가독성 있는 형식으로 수집
4. 인간 주석자를 통한 후처리 및 개선

이렇게 수집된 수천 개의 콜드 스타트 데이터는 DeepSeek-V3-Base 모델의 미세조정에 사용되어 강화학습의 시작점이 됩니다. 이러한 콜드 스타트 접근 방식은 DeepSeek-R1-Zero와 비교하여 두 가지 주요 장점을 제공합니다.

첫째, 가독성 측면에서 큰 개선이 이루어졌습니다. DeepSeek-R1-Zero의 주요 한계점 중 하나는 출력 내용이 읽기에 적합하지 않다는 것이었습니다. 여러 언어가 혼합되어 있거나 사용자를 위한 답변 강조를 위한 마크다운 서식이 부족했습니다. 반면 DeepSeek-R1의 콜드 스타트 데이터는 각 응답의 끝에 요약을 포함하고 읽기 어려운 응답을 필터링하는 등 가독성을 고려한 패턴으로 설계되었습니다. 구체적으로, 출력 형식은 '\|special_token\| 추론 과정 \|special_token\| 요약' 형태로 구조화되어 있어, 추론 과정은 질의에 대한 CoT를 제공하고 요약은 추론 결과를 정리합니다.

둘째, 인간의 직관을 반영한 콜드 스타트 데이터 패턴을 신중하게 설계함으로써 DeepSeek-R1-Zero보다 더 나은 성능을 달성할 수 있었습니다. 연구진은 이러한 반복적 학습 방식이 추론 모델에 더 적합하다고 판단했습니다.
두 번째 단계인 추론 중심 강화학습에서는 콜드 스타트 데이터로 미세조정된 DeepSeek-V3-Base 모델에 DeepSeek-R1-Zero와 동일한 대규모 강화학습 과정을 적용합니다. 이 단계는 코딩, 수학, 과학, 논리적 추론과 같이 명확한 해결책이 있는 추론 집약적 과제에서 모델의 능력을 향상시키는 데 중점을 둡니다.

학습 과정에서 연구진은 CoT가 여러 언어가 혼합되는 현상을 보이는 것을 발견했습니다. 특히 강화학습 프롬프트가 여러 언어를 포함할 때 이러한 현상이 두드러졌습니다. 이 문제를 해결하기 위해 강화학습 과정에 언어 일관성 보상을 도입했습니다. 이 보상은 CoT 내에서 목표 언어 단어의 비율로 계산됩니다. 실험 결과, 이러한 언어 일관성 정렬이 모델의 성능을 약간 저하시키기는 했지만, 사용자의 선호도에 더 잘 부합하고 가독성을 향상시키는 효과가 있었습니다. 최종적으로 추론 과제의 정확도와 언어 일관성 보상을 직접 합산하여 최종 보상을 구성하고, 추론 과제에서 수렴할 때까지 강화학습을 진행했습니다.

세 번째 단계에서는 거부 샘플링과 지도 학습 미세조정을 수행합니다. 추론 중심 강화학습이 수렴하면, 해당 체크포인트를 사용하여 다음 라운드를 위한 지도 학습 미세조정(SFT) 데이터를 수집합니다. 초기 콜드 스타트 데이터가 주로 추론에 초점을 맞춘 것과 달리, 이 단계에서는 글쓰기, 역할 수행, 기타 일반적인 과제를 위한 데이터도 포함하여 모델의 전반적인 능력을 향상시킵니다.

추론 데이터의 경우, 이전 강화학습 체크포인트에서 거부 샘플링을 수행하여 추론 궤적을 생성합니다. 이전 단계에서는 규칙 기반 보상으로 평가할 수 있는 데이터만 포함했지만, 이 단계에서는 데이터셋을 확장하여 정답과 모델 예측을 DeepSeek-V3에 입력하여 판단하는 생성적 보상 모델을 사용하는 데이터도 포함합니다. 또한 모델 출력이 때로는 혼란스럽고 읽기 어려울 수 있기 때문에, 언어가 혼합된 사고의 연쇄, 긴 단락, 코드 블록 등을 필터링했습니다. 각 프롬프트에 대해 여러 응답을 샘플링하고 정확한 것만 유지하여, 총 60만 개의 추론 관련 학습 샘플을 수집했습니다.
비추론 데이터의 경우, 글쓰기, 사실 기반 질의응답, 자기 인식, 번역과 같은 과제에 대해서는 DeepSeek-V3 파이프라인을 채택하고 DeepSeek-V3의 지도 학습 데이터셋의 일부를 재사용했습니다. 특정 비추론 과제에 대해서는 프롬프팅을 통해 DeepSeek-V3를 호출하여 질문에 답하기 전에 잠재적인 사고의 연쇄를 생성하도록 했습니다. 하지만 "안녕하세요"와 같은 간단한 질의에 대해서는 CoT 없이 직접 응답하도록 했습니다. 최종적으로 약 20만 개의 비추론 관련 학습 샘플을 수집했습니다.

이렇게 수집된 총 80만 개의 샘플로 구성된 데이터셋을 사용하여 DeepSeek-V3-Base 모델을 2회의 에포크 동안 미세조정했습니다.

마지막 네 번째 단계에서는 모든 시나리오를 고려한 강화학습을 수행합니다. 이 단계의 목적은 모델의 유용성과 무해성을 인간의 선호도에 맞게 정렬하면서 동시에 추론 능력을 더욱 개선하는 것입니다. 구체적으로, 다양한 보상 신호와 프롬프트 분포를 조합하여 모델을 학습시킵니다.

추론 데이터에 대해서는 DeepSeek-R1-Zero에서 설명한 방법론을 따르며, 수학, 코드, 논리적 추론 영역에서 규칙 기반 보상을 사용하여 학습을 진행합니다. 일반 데이터에 대해서는 복잡하고 미묘한 시나리오에서 인간의 선호도를 포착하기 위해 보상 모델을 활용합니다. DeepSeek-V3 파이프라인을 기반으로 하여 유사한 선호도 쌍과 학습 프롬프트 분포를 채택했습니다.

유용성 측면에서는 최종 요약에만 초점을 맞추어 평가를 진행합니다. 이는 사용자에 대한 응답의 유용성과 관련성을 강조하면서도 기저의 추론 과정에 대한 간섭을 최소화하기 위한 전략입니다. 무해성 측면에서는 추론 과정과 요약을 모두 포함한 모델의 전체 응답을 평가하여 생성 과정에서 발생할 수 있는 잠재적 위험, 편향, 유해한 내용을 식별하고 완화합니다.

이러한 보상 신호와 다양한 데이터 분포의 통합을 통해, 연구진은 추론 능력이 뛰어나면서도 유용성과 무해성을 우선시하는 모델을 학습시킬 수 있었습니다. 이는 DeepSeek-R1이 단순한 추론 능력을 넘어서 실제 사용 환경에서 더욱 신뢰할 수 있고 안전한 AI 시스템으로 발전했음을 보여줍니다.

### 증류: 작은 모델에 추론 능력 부여하기

DeepSeek-R1의 뛰어난 추론 능력을 더 효율적인 작은 모델로 전달하기 위해, 연구진은 Qwen과 Llama와 같은 오픈소스 모델들을 DeepSeek-R1을 통해 생성된 80만 개의 샘플로 직접 미세조정하는 증류 방법을 적용했습니다. 이러한 단순한 증류 방법만으로도 작은 모델들의 추론 능력이 크게 향상되었습니다.

증류 실험에 사용된 기본 모델들은 다음과 같습니다.
- Qwen 시리즈: Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B
- Llama 시리즈: Llama-3.1-8B, Llama-3.3-70B-Instruct

연구진은 Llama-3.3을 선택한 이유로 Llama-3.1보다 약간 더 나은 추론 능력을 보여주었기 때문이라고 설명합니다. 증류된 모델들에 대해서는 지도 학습 미세조정(SFT)만을 적용하고 강화학습(RL) 단계는 포함하지 않았습니다. 이는 강화학습을 적용하면 모델의 성능이 상당히 향상될 수 있음에도 불구하고, 연구진의 주요 목표가 증류 기법의 효과성을 입증하는 것이었기 때문입니다.

실험 결과를 살펴보면, 증류된 모델들은 여러 벤치마크에서 인상적인 성능을 보여주었습니다. 특히 AIME 2024에서 DeepSeek-R1-Distill-Qwen-7B는 55.5%의 pass@1 점수를 달성했고, 다수결 투표(cons@64)를 적용했을 때는 83.3%까지 향상되었습니다. 이는 GPT-4o-0513(pass@1: 9.3%, cons@64: 13.4%)와 Claude-3.5-Sonnet-1022(pass@1: 16.0%, cons@64: 26.7%)를 크게 앞서는 결과입니다.

MATH-500 벤치마크에서도 증류된 모델들은 뛰어난 성능을 보여주었습니다. DeepSeek-R1-Distill-Qwen-32B는 94.3%의 정확도를 달성했으며, DeepSeek-R1-Distill-Llama-70B는 94.5%를 기록했습니다. 이는 QwQ-32B-Preview의 90.6%를 상회하는 결과입니다.

코딩 관련 벤치마크인 LiveCodeBench와 Codeforces에서도 증류된 모델들은 강력한 성능을 보여주었습니다. DeepSeek-R1-Distill-Qwen-32B는 LiveCodeBench에서 62.1%의 pass@1 점수를 달성했고, Codeforces에서는 1,691의 레이팅을 기록했습니다. 이러한 결과들은 증류 방법을 통해 작은 모델들도 복잡한 추론 과제를 효과적으로 수행할 수 있다는 것을 보여줍니다.

특히 주목할 만한 점은 14B 규모의 증류 모델이 32B 규모의 QwQ-32B-Preview를 대부분의 평가 지표에서 능가했다는 것입니다. 이는 모델의 크기보다 학습 방법이 더 중요할 수 있다는 것을 시사합니다. 연구진은 이러한 증류 모델들에 강화학습을 추가로 적용하면 성능이 더욱 향상될 수 있다고 예상하며, 이에 대한 추가 연구를 커뮤니티에 제안하고 있습니다.

### 논의: 증류와 강화학습의 비교 분석

DeepSeek-R1 연구에서 가장 흥미로운 발견 중 하나는 증류 방식과 강화학습 방식의 성능 차이입니다. 연구진은 Qwen-32B-Base 모델을 사용하여 두 가지 접근 방식을 직접 비교했습니다. 수학, 코드, STEM 데이터를 사용하여 10,000단계 이상의 대규모 강화학습을 진행한 DeepSeek-R1-Zero-Qwen-32B와 DeepSeek-R1에서 증류된 DeepSeek-R1-Distill-Qwen-32B의 성능을 비교한 결과, 증류 모델이 모든 벤치마크에서 월등히 우수한 성능을 보여주었습니다.

구체적인 성능 차이를 살펴보면, MATH-500에서 DeepSeek-R1-Zero-Qwen-32B는 47.0%의 pass@1 점수를 기록한 반면, DeepSeek-R1-Distill-Qwen-32B는 72.6%를 달성했습니다. GPQA Diamond에서도 증류 모델이 83.3%로, 강화학습 모델의 60.0%를 크게 앞섰습니다. LiveCodeBench에서는 증류 모델이 94.3%의 정확도를 보여주어, 강화학습 모델의 91.6%보다 우수했습니다.

이러한 결과는 두 가지 중요한 시사점을 제공합니다. 첫째, 더 강력한 모델의 지식을 작은 모델로 증류하는 방식이 매우 효과적이라는 것입니다. 작은 모델이 대규모 강화학습을 통해 직접 학습하는 것은 엄청난 컴퓨팅 자원을 필요로 할 뿐만 아니라, 증류를 통해 얻을 수 있는 성능에도 미치지 못합니다. 둘째, 지능의 경계를 더욱 확장하기 위해서는 여전히 더 강력한 기본 모델과 대규모 강화학습이 필요하다는 점입니다.

연구진은 또한 초기 개발 과정에서 겪은 실패 사례도 공유했습니다. 프로세스 보상 모델(Process Reward Model, PRM)의 경우, 추론 과제에서 더 나은 접근 방식으로 모델을 유도하는 합리적인 방법으로 보였으나, 실제 적용에서 세 가지 주요 한계에 직면했습니다. 첫째, 일반적인 추론에서 세부 단계를 명시적으로 정의하기가 어렵습니다. 둘째, 현재 중간 단계의 정확성을 판단하는 것이 매우 까다롭습니다. 모델을 사용한 자동 주석은 만족스러운 결과를 내지 못했고, 수동 주석은 확장성이 떨어집니다. 셋째, 모델 기반 PRM을 도입하면 보상 해킹(reward hacking)이 발생하며, 보상 모델을 재학습하는 데 추가 자원이 필요하고 전체 학습 파이프라인이 복잡해집니다.
몬테카를로 트리 탐색(Monte Carlo Tree Search, MCTS)은 DeepSeek-R1 개발 과정에서 시도된 또 다른 접근 방식이었습니다. AlphaGo와 AlphaZero의 성공에서 영감을 받아, 연구진은 MCTS를 활용하여 테스트 시간 계산 확장성을 향상시키고자 했습니다. 이 방법은 답변을 더 작은 부분으로 나누어 모델이 체계적으로 해결 공간을 탐색할 수 있도록 하는 것이 핵심입니다.

MCTS 구현을 위해 연구진은 모델이 탐색에 필요한 특정 추론 단계에 해당하는 여러 태그를 생성하도록 프롬프트를 설계했습니다. 학습 과정은 두 단계로 구성됩니다. 먼저 수집된 프롬프트를 사용하여 사전 학습된 가치 모델의 안내를 받아 MCTS를 통해 답을 찾습니다. 그 다음, 도출된 질문-답변 쌍을 사용하여 행위자 모델과 가치 모델을 학습시키고, 이 과정을 반복적으로 개선합니다.

그러나 이 접근 방식은 학습을 확장할 때 여러 가지 어려움에 직면했습니다. 첫 번째 문제는 탐색 공간의 복잡성입니다. 체스와 달리 토큰 생성은 탐색 공간이 기하급수적으로 큽니다. 연구진은 이 문제를 해결하기 위해 각 노드의 최대 확장 제한을 설정했지만, 이로 인해 모델이 지역 최적해에 갇히는 문제가 발생했습니다.

두 번째 문제는 가치 모델의 품질입니다. 가치 모델은 탐색 과정의 각 단계를 안내하기 때문에 생성의 품질에 직접적인 영향을 미칩니다. 세밀한 가치 모델을 학습시키는 것은 본질적으로 어려운 과제이며, 이는 모델이 반복적으로 성능을 향상시키는 것을 어렵게 만듭니다. AlphaGo의 핵심적인 성공은 가치 모델을 통해 점진적으로 성능을 향상시키는 데 있었지만, 토큰 생성의 복잡성으로 인해 이러한 원리를 그대로 적용하기는 어려웠습니다.

결론적으로, MCTS는 사전 학습된 가치 모델과 함께 사용할 때 추론 시간에 성능을 향상시킬 수는 있지만, 자체 탐색을 통한 반복적인 모델 성능 향상은 여전히 큰 과제로 남아있습니다. 이러한 경험은 언어 모델의 추론 능력 향상을 위해서는 더 혁신적인 접근 방식이 필요하다는 것을 보여줍니다.

### 결론, 한계점 및 향후 연구 방향

본 연구는 강화학습을 통해 언어 모델의 추론 능력을 향상시키는 여정을 공유했습니다. 연구진은 두 가지 주요 모델인 DeepSeek-R1-Zero와 DeepSeek-R1을 개발했는데, DeepSeek-R1-Zero는 콜드 스타트 데이터 없이 순수한 강화학습 접근 방식을 통해 다양한 과제에서 강력한 성능을 달성했습니다. 한편 DeepSeek-R1은 콜드 스타트 데이터와 반복적인 강화학습 미세조정을 결합하여 더욱 강력한 성능을 보여주었으며, 최종적으로 OpenAI-o1-1217와 비견할 만한 성능을 달성했습니다.

연구진은 더 나아가 이러한 추론 능력을 작은 규모의 밀집 모델로 전달하는 증류 실험을 진행했습니다. DeepSeek-R1을 교사 모델로 사용하여 80만 개의 학습 샘플을 생성하고, 이를 통해 여러 소형 밀집 모델을 미세조정했습니다. 그 결과는 매우 고무적이었습니다. 특히 DeepSeek-R1-Distill-Qwen-1.5B는 수학 벤치마크에서 GPT-4o와 Claude-3.5-Sonnet을 능가하는 성능을 보여주었는데, AIME에서 28.9%, MATH에서 83.9%의 점수를 달성했습니다. 다른 밀집 모델들도 인상적인 결과를 보여주었으며, 동일한 기본 체크포인트를 사용한 다른 지도 학습 모델들을 크게 앞섰습니다.

향후 연구 방향으로는 다음과 같은 영역에 중점을 둘 계획입니다.

첫째, 일반적 능력의 향상입니다. 현재 DeepSeek-R1은 함수 호출, 다중 턴 대화, 복잡한 역할 수행, JSON 출력과 같은 과제에서 DeepSeek-V3에 비해 부족한 면을 보입니다. 연구진은 긴 사고의 연쇄(Chain of Thought)를 활용하여 이러한 영역에서의 성능을 향상시키는 방안을 모색할 예정입니다.

둘째, 언어 혼합 문제의 해결입니다. DeepSeek-R1은 현재 중국어와 영어에 최적화되어 있어, 다른 언어로 된 질의를 처리할 때 언어 혼합 문제가 발생할 수 있습니다. 예를 들어, 영어나 중국어가 아닌 언어로 질의가 들어와도 영어로 추론하고 응답하는 경향이 있습니다. 연구진은 향후 업데이트에서 이러한 한계를 해결하고자 합니다.

셋째, 프롬프트 엔지니어링의 개선입니다. DeepSeek-R1을 평가하는 과정에서 모델이 프롬프트에 민감하다는 것을 발견했습니다. 특히 퓨 샷 프롬프팅은 오히려 성능을 저하시키는 경향이 있었습니다. 따라서 사용자들에게는 제로 샷 설정에서 문제를 직접 설명하고 출력 형식을 지정하는 방식을 권장합니다.

마지막으로, 소프트웨어 엔지니어링 과제의 개선입니다. 긴 평가 시간으로 인해 강화학습 과정의 효율성이 저하되어, 소프트웨어 엔지니어링 과제에서는 대규모 강화학습을 충분히 적용하지 못했습니다. 그 결과 DeepSeek-R1은 소프트웨어 엔지니어링 벤치마크에서 DeepSeek-V3에 비해 큰 성능 향상을 보여주지 못했습니다. 향후 버전에서는 소프트웨어 엔지니어링 데이터에 대한 거부 샘플링을 구현하거나 강화학습 과정에서 비동기 평가를 도입하여 효율성을 개선할 계획입니다.

- - -
### References
* [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/pdf/2501.12948v1)