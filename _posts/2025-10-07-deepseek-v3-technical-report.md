---
layout: post
title: "DeepSeek-V3 Technical Report"
date: 2024-12-27 04:03:16
author: "DeepSeek AI Research"
categories: ["Paper Reviews", "Training & Inference Optimization"]
tags: ["Auxiliary-Loss-Free-Load-Balancing", "Multi-Head-Latent-Attention", "Multi-Token-Prediction", "DeepSeekMoE-Architecture", "FP8-Mixed-Precision-Training", "Efficient-Cross-Node-All-to-All-Communication", "Computation-Communication-Overlap", "Tile-Wise-Fine-Grained-Quantization", "Group-Relative-Policy-Optimization", "Speculative-Decoding"]
cover: /assets/images/default.jpg
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

대규모 언어 모델(LLMs)의 발전은 인공지능 분야에서 혁명적인 변화를 가져오고 있습니다. 최근 몇 년간 클로즈드소스 모델들의 급속한 성장과 함께 DeepSeek, LLaMA, Qwen, Mistral과 같은 오픈소스 모델들도 상당한 진전을 이루며 성능 격차를 좁혀가고 있습니다. 그러나 기존 대규모 언어 모델들은 여전히 계산 효율성, 학습 안정성, 추론 속도 등에서 중요한 기술적 한계에 직면해 있었습니다.

특히 Mixture-of-Experts(MoE) 아키텍처는 모델의 계산 효율성을 높일 수 있는 유망한 접근법으로 주목받았지만, 전문가 간 로드 밸런싱과 학습 안정성 문제로 인해 그 잠재력을 충분히 실현하지 못하고 있었습니다. 연구진은 이러한 기술적 도전 과제들을 해결하고, 오픈소스 대규모 언어 모델의 성능과 효율성을 획기적으로 향상시키기 위해 DeepSeek-V3 연구를 시작하게 되었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

DeepSeek-V3는 두 가지 핵심 혁신적인 접근법을 제시합니다. 첫째, **보조 손실 없는 로드 밸런싱 전략**을 도입하여 전문가 간 로드 밸런싱을 자연스럽게 유도합니다. 기존의 보조 손실 함수 대신 각 전문가의 최근 로드를 기반으로 동적 편향을 적용하여 전문가들이 균형 있게 활성화되도록 합니다. 이는 로드 밸런싱을 장려하면서도 모델 성능 저하를 최소화하는 혁신적인 접근법입니다.

둘째, **멀티 토큰 예측(Multi-Token Prediction, MTP) 학습 목표**를 도입하여 모델의 추론 능력을 향상시킵니다. 기존의 단일 토큰 예측 방식과 달리, 각 위치에서 여러 개의 미래 토큰을 동시에 예측하도록 학습시킵니다. 이 접근법은 모델이 더 먼 미래를 내다보며 더 나은 표현을 학습할 수 있게 하며, 코딩과 수학 작업에서 특히 효과적임을 실험을 통해 입증했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?

DeepSeek-V3의 구현은 여러 혁신적인 기술적 최적화를 포함합니다. 총 671B개의 파라미터를 가진 모델로, 각 토큰당 37B개의 파라미터가 활성화됩니다. 14.8조 개의 고품질 토큰으로 사전 학습되었으며, FP8 혼합 정밀도 학습 프레임워크를 도입하여 계산 효율성을 크게 향상시켰습니다.

학습 인프라 측면에서는 2048개의 NVIDIA H800 GPU로 구성된 클러스터를 활용하고, DualPipe 알고리즘을 통해 파이프라인 병렬성을 최적화했습니다. 특히 계산-통신 오버랩 전략을 구현하여 통신 오버헤드를 최소화하고, InfiniBand와 NVLink 대역폭을 최대한 활용하는 맞춤형 통신 커널을 개발했습니다. 후처리 과정에서는 DeepSeek-R1 모델로부터 지식 증류 기법을 적용하여 추론 능력을 더욱 강화했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?

DeepSeek-V3는 오픈소스 대규모 언어 모델 분야에서 획기적인 성과를 달성했습니다. 다양한 벤치마크에서 GPT-4o, Claude-3.5-Sonnet과 같은 최첨단 클로즈드소스 모델들과 경쟁할 만한 성능을 보여주었으며, 특히 코드 생성, 수학 문제 해결, 다국어 작업에서 뛰어난 성능을 입증했습니다.

더욱 주목할 만한 점은 이러한 뛰어난 성능을 매우 경제적인 학습 비용으로 달성했다는 것입니다. 전체 학습에 단 2.788M H800 GPU 시간만이 소요되었으며, 학습 과정에서 복구 불가능한 손실 급증이나 롤백을 경험하지 않았습니다. 이는 MoE 아키텍처의 잠재력을 입증하고, 오픈소스 AI 생태계의 발전에 중요한 기여를 했다는 점에서 큰 의의가 있습니다.
- - -
# DeepSeek-V3 기술 보고서

## 초록

DeepSeek-V3는 총 671B개의 파라미터를 가지며 각 토큰당 37B개의 파라미터가 활성화되는 강력한 Mixture-of-Experts (MoE) 언어 모델입니다. 효율적인 추론과 비용 효과적인 학습을 달성하기 위해 DeepSeek-V3는 DeepSeek-V2에서 철저히 검증된 Multi-head Latent Attention (MLA)와 DeepSeekMoE 아키텍처를 채택했습니다.

![DeepSeek-V3 벤치마크 성능](/assets/2025-10-07-deepseek-v3-technical-report/Chart Type: bar MMLU-Pro (EM)GPQA-Diamond (Pascals)MATH 500 (EM)AIME 2024 (Pass@1)Codeforces (Percentile)SWE-bench Verified (Resolved)item_0175.9%78.0%90.2%51.6%42.0%38.8%)

위 그래프에서 보듯이 DeepSeek-V3는 다양한 벤치마크에서 뛰어난 성능을 보여줍니다. MMLU-Pro에서 75.9%, MATH 500에서 90.2%, AIME 2024에서 51.6%의 성과를 달성했습니다.

DeepSeek-V3의 핵심 혁신은 두 가지 주요 전략에 있습니다. 첫째, 로드 밸런싱을 위한 보조 손실 없는 전략(auxiliary-loss-free strategy)을 선도적으로 도입했습니다. 이는 로드 밸런싱을 장려하면서 발생하는 모델 성능 저하를 최소화하는 것을 목표로 합니다. 둘째, 더 강력한 성능을 위해 멀티 토큰 예측 학습 목표(multi-token prediction training objective)를 설정했습니다.

모델은 14.8조 개의 다양하고 고품질 토큰으로 사전 학습되었으며, 이후 지도 학습 미세 조정(Supervised Fine-Tuning)과 강화 학습(Reinforcement Learning) 단계를 거쳐 모델의 능력을 완전히 활용했습니다. 종합적인 평가 결과 DeepSeek-V3는 다른 오픈소스 모델들을 능가하며 선도적인 클로즈드소스 모델들과 비교할 만한 성능을 달성했습니다.

뛰어난 성능에도 불구하고 DeepSeek-V3의 전체 학습에는 단 2.788M H800 GPU 시간만이 소요되었습니다. 또한 학습 과정이 매우 안정적이어서 전체 학습 과정에서 복구 불가능한 손실 급증이나 롤백을 경험하지 않았습니다.

## 서론

최근 몇 년간 대규모 언어 모델(LLMs)은 급속한 반복과 진화를 거듭하며 인공 일반 지능(AGI)을 향한 격차를 점진적으로 줄여나가고 있습니다. 클로즈드소스 모델들 외에도 DeepSeek 시리즈, LLaMA 시리즈, Qwen 시리즈, Mistral 시리즈 등의 오픈소스 모델들도 상당한 진전을 이루며 클로즈드소스 모델들과의 격차를 줄이기 위해 노력하고 있습니다.

오픈소스 모델 능력의 경계를 더욱 확장하기 위해, 연구진은 모델을 확장하여 671B개의 파라미터를 가진 대규모 Mixture-of-Experts (MoE) 모델인 DeepSeek-V3를 소개합니다. 각 토큰당 37B개의 파라미터가 활성화됩니다.

미래 지향적인 관점에서 연구진은 지속적으로 강력한 모델 성능과 경제적 비용을 추구합니다. 따라서 아키텍처 측면에서 DeepSeek-V3는 여전히 효율적인 추론을 위한 Multi-head Latent Attention (MLA)과 비용 효과적인 학습을 위한 DeepSeekMoE를 채택합니다. 이 두 아키텍처는 DeepSeek-V2에서 검증되었으며, 효율적인 학습과 추론을 달성하면서 견고한 모델 성능을 유지할 수 있는 능력을 입증했습니다.

기본 아키텍처 외에도 모델 능력을 더욱 향상시키기 위해 두 가지 추가 전략을 구현했습니다. 첫째, DeepSeek-V3는 로드 밸런싱을 위한 보조 손실 없는 전략을 선도적으로 도입했습니다. 이는 로드 밸런싱을 장려하려는 노력에서 발생하는 모델 성능에 대한 부정적 영향을 최소화하는 것을 목표로 합니다. 둘째, DeepSeek-V3는 멀티 토큰 예측 학습 목표를 사용하여 평가 벤치마크에서 전반적인 성능을 향상시키는 것으로 관찰되었습니다.

효율적인 학습을 달성하기 위해 FP8 혼합 정밀도 학습을 지원하고 학습 프레임워크에 대한 포괄적인 최적화를 구현했습니다. 저정밀도 학습은 효율적인 학습을 위한 유망한 솔루션으로 부상했으며, 그 발전은 하드웨어 능력의 발전과 밀접하게 연결되어 있습니다. 이 연구에서는 FP8 혼합 정밀도 학습 프레임워크를 도입하고 극도로 대규모인 모델에서 처음으로 그 효과를 검증했습니다. FP8 계산과 저장 지원을 통해 학습 가속화와 GPU 메모리 사용량 감소를 모두 달성했습니다.

학습 프레임워크 측면에서는 효율적인 파이프라인 병렬성을 위한 DualPipe 알고리즘을 설계했습니다. 이는 파이프라인 버블이 적고 계산-통신 오버랩을 통해 학습 중 대부분의 통신을 숨깁니다. 이러한 오버랩은 모델이 더욱 확장되더라도 일정한 계산 대 통신 비율을 유지하는 한, 노드 간 세분화된 전문가를 사용하면서도 거의 제로에 가까운 all-to-all 통신 오버헤드를 달성할 수 있도록 보장합니다.

또한 InfiniBand (IB)와 NVLink 대역폭을 완전히 활용하기 위한 효율적인 노드 간 all-to-all 통신 커널을 개발했습니다. 더 나아가 메모리 사용량을 세심하게 최적화하여 비용이 많이 드는 텐서 병렬성을 사용하지 않고도 DeepSeek-V3를 학습할 수 있게 했습니다. 이러한 노력들을 결합하여 높은 학습 효율성을 달성했습니다.

사전 학습 동안 DeepSeek-V3를 14.8T개의 고품질이고 다양한 토큰으로 학습시켰습니다. 사전 학습 과정은 매우 안정적이었습니다. 전체 학습 과정에서 복구 불가능한 손실 급증을 경험하거나 롤백을 수행할 필요가 없었습니다.

다음으로 DeepSeek-V3에 대해 2단계 컨텍스트 길이 확장을 수행했습니다. 첫 번째 단계에서는 최대 컨텍스트 길이를 32K로 확장하고, 두 번째 단계에서는 128K로 더욱 확장했습니다. 이후 DeepSeek-V3의 기본 모델에 대해 지도 학습 미세 조정(SFT)과 강화 학습(RL)을 포함한 후처리를 수행하여 인간의 선호도에 맞추고 잠재력을 더욱 발휘했습니다.

후처리 단계에서는 DeepSeek-R1 시리즈 모델로부터 추론 능력을 증류하는 동시에 모델 정확도와 생성 길이 간의 균형을 신중하게 유지했습니다.

| 단계 | H800 GPU 시간 | 비용 (USD) |
|------|---------------|------------|
| 사전 학습 | 2664K | $5.328M |
| 컨텍스트 확장 | 119K | $0.238M |
| 후처리 | 5K | $0.01M |
| **총계** | **2788K** | **$5.576M** |

위 표에서 보듯이 DeepSeek-V3의 학습 비용은 매우 경제적입니다. H800 GPU 시간당 $2의 임대 가격을 가정할 때 총 학습 비용은 단 $5.576M에 불과합니다.

DeepSeek-V3를 포괄적인 벤치마크 배열에서 평가했습니다. 경제적인 학습 비용에도 불구하고 종합적인 평가 결과 DeepSeek-V3-Base는 현재 이용 가능한 가장 강력한 오픈소스 기본 모델로 부상했으며, 특히 코드와 수학 분야에서 뛰어납니다. 채팅 버전 또한 다른 오픈소스 모델들을 능가하며 일련의 표준 및 개방형 벤치마크에서 GPT-4o와 Claude-3.5-Sonnet을 포함한 선도적인 클로즈드소스 모델들과 비교할 만한 성능을 달성했습니다.

사전 학습 단계에서 각 1조 토큰에 대해 DeepSeek-V3를 학습시키는 데 단 180K H800 GPU 시간만 필요합니다. 즉, 2048개의 H800 GPU를 가진 클러스터에서 3.7일이면 됩니다. 따라서 사전 학습 단계는 2개월 미만에 완료되며 2664K GPU 시간이 소요됩니다. 컨텍스트 길이 확장을 위한 119K GPU 시간과 후처리를 위한 5K GPU 시간을 합하면 DeepSeek-V3의 전체 학습에는 단 2.788M GPU 시간만 소요됩니다.

연구진의 주요 기여는 다음과 같습니다.

**아키텍처: 혁신적인 로드 밸런싱 전략과 학습 목표**
DeepSeek-V2의 효율적인 아키텍처를 기반으로 로드 밸런싱을 위한 보조 손실 없는 전략을 선도적으로 도입하여 로드 밸런싱 장려로 인한 성능 저하를 최소화했습니다. 또한 멀티 토큰 예측(MTP) 목표를 조사하여 모델 성능에 유익함을 입증했으며, 이는 추론 가속화를 위한 투기적 디코딩에도 사용될 수 있습니다.

**사전 학습: 궁극적인 학습 효율성을 향해**
FP8 혼합 정밀도 학습 프레임워크를 설계하고 극도로 대규모인 모델에서 FP8 학습의 실현 가능성과 효과를 처음으로 검증했습니다. 알고리즘, 프레임워크, 하드웨어의 공동 설계를 통해 노드 간 MoE 학습의 통신 병목을 극복하여 거의 완전한 계산-통신 오버랩을 달성했습니다. 이는 학습 효율성을 크게 향상시키고 학습 비용을 줄여 추가 오버헤드 없이 모델 크기를 더욱 확장할 수 있게 했습니다.

**후처리: DeepSeek-R1로부터의 지식 증류**
긴 체인 오브 소트(CoT) 모델, 특히 DeepSeek R1 시리즈 모델 중 하나로부터 표준 LLM, 특히 DeepSeek-V3로 추론 능력을 증류하는 혁신적인 방법론을 도입했습니다. 이 파이프라인은 R1의 검증 및 반성 패턴을 DeepSeek-V3에 우아하게 통합하여 추론 성능을 현저히 향상시키는 동시에 DeepSeek-V3의 출력 스타일과 길이에 대한 제어를 유지했습니다.

**핵심 평가 결과 요약**
지식 분야에서 MMLU, MMLU-Pro, GPQA와 같은 교육 벤치마크에서 DeepSeek-V3는 모든 다른 오픈소스 모델들을 능가하여 MMLU에서 88.5, MMLU-Pro에서 75.9, GPQA에서 59.1을 달성했습니다. 이는 GPT-4o와 Claude-Sonnet-3.5와 같은 선도적인 클로즈드소스 모델들과 비교할 만한 성능으로, 이 영역에서 오픈소스와 클로즈드소스 모델 간의 격차를 좁혔습니다.

코드, 수학, 추론 분야에서 DeepSeek-V3는 모든 비긴 체인 오브 소트 오픈소스 및 클로즈드소스 모델들 중에서 수학 관련 벤치마크에서 최첨단 성능을 달성했습니다. 특히 MATH-500과 같은 특정 벤치마크에서는 o1-preview를 능가하기도 하여 견고한 수학적 추론 능력을 보여줍니다.

코딩 관련 작업에서 DeepSeek-V3는 LiveCodeBench와 같은 코딩 경쟁 벤치마크에서 최고 성능 모델로 부상하여 이 영역에서의 선도적 위치를 확고히 했습니다. 엔지니어링 관련 작업에서는 Claude-Sonnet-3.5보다 약간 낮은 성능을 보이지만, 여전히 다른 모든 모델들을 상당한 차이로 앞서며 다양한 기술 벤치마크에서의 경쟁력을 보여줍니다.
# DeepSeek-V3 기술 보고서

## 초록

DeepSeek-V3는 총 671B개의 파라미터를 가지며 각 토큰당 37B개의 파라미터가 활성화되는 강력한 Mixture-of-Experts (MoE) 언어 모델입니다. 효율적인 추론과 비용 효과적인 학습을 달성하기 위해 DeepSeek-V3는 DeepSeek-V2에서 철저히 검증된 Multi-head Latent Attention (MLA)와 DeepSeekMoE 아키텍처를 채택했습니다.

DeepSeek-V3의 핵심 혁신은 두 가지 주요 전략에 있습니다. 첫째, 로드 밸런싱을 위한 보조 손실 없는 전략(auxiliary-loss-free strategy)을 선도적으로 도입했습니다. 이는 로드 밸런싱을 장려하면서 발생하는 모델 성능 저하를 최소화하는 것을 목표로 합니다. 둘째, 더 강력한 성능을 위해 멀티 토큰 예측 학습 목표(multi-token prediction training objective)를 설정했습니다.

모델은 14.8조 개의 다양하고 고품질 토큰으로 사전 학습되었으며, 이후 지도 학습 미세 조정(Supervised Fine-Tuning)과 강화 학습(Reinforcement Learning) 단계를 거쳐 모델의 능력을 완전히 활용했습니다. 종합적인 평가 결과 DeepSeek-V3는 다른 오픈소스 모델들을 능가하며 선도적인 클로즈드소스 모델들과 비교할 만한 성능을 달성했습니다.

뛰어난 성능에도 불구하고 DeepSeek-V3의 전체 학습에는 단 2.788M H800 GPU 시간만이 소요되었습니다. 또한 학습 과정이 매우 안정적이어서 전체 학습 과정에서 복구 불가능한 손실 급증이나 롤백을 경험하지 않았습니다.

## 서론

최근 몇 년간 대규모 언어 모델(LLMs)은 급속한 반복과 진화를 거듭하며 인공 일반 지능(AGI)을 향한 격차를 점진적으로 줄여나가고 있습니다. 클로즈드소스 모델들 외에도 DeepSeek 시리즈, LLaMA 시리즈, Qwen 시리즈, Mistral 시리즈 등의 오픈소스 모델들도 상당한 진전을 이루며 클로즈드소스 모델들과의 격차를 줄이기 위해 노력하고 있습니다.

오픈소스 모델 능력의 경계를 더욱 확장하기 위해, 연구진은 모델을 확장하여 671B개의 파라미터를 가진 대규모 Mixture-of-Experts (MoE) 모델인 DeepSeek-V3를 소개합니다. 각 토큰당 37B개의 파라미터가 활성화됩니다.

### 아키텍처 혁신과 설계 철학

미래 지향적인 관점에서 연구진은 지속적으로 강력한 모델 성능과 경제적 비용을 추구합니다. 이러한 철학은 DeepSeek-V3의 모든 설계 결정에 반영되어 있습니다. 아키텍처 측면에서 DeepSeek-V3는 여전히 효율적인 추론을 위한 Multi-head Latent Attention (MLA)과 비용 효과적인 학습을 위한 DeepSeekMoE를 채택합니다.

Multi-head Latent Attention은 [DeepSeek-V2에서 제안된 혁신적인 어텐션 메커니즘](https://arxiv.org/pdf/2405.04434)으로, 기존 멀티헤드 어텐션의 메모리 효율성 문제를 해결합니다. 이 메커니즘은 키-값 캐시의 크기를 대폭 줄여 추론 시 메모리 사용량을 현저히 감소시킵니다. 특히 긴 시퀀스를 처리할 때 그 효과가 두드러지며, 이는 대규모 언어 모델의 실용적 배포에 있어 핵심적인 요소입니다.

DeepSeekMoE 아키텍처는 [전문가 특화를 극대화하는 혁신적인 MoE 설계](https://arxiv.org/pdf/2401.06066)를 통해 비용 효과적인 학습을 가능하게 합니다. 이 아키텍처는 세분화된 전문가 분할과 공유 전문가 격리를 통해 전문가들이 특정 도메인에 더욱 특화되도록 유도합니다. 이러한 특화는 모델의 전체적인 성능 향상으로 이어지며, 동시에 학습 효율성도 크게 개선합니다.

### 핵심 기술적 혁신

기본 아키텍처 외에도 모델 능력을 더욱 향상시키기 위해 두 가지 혁신적인 전략을 구현했습니다.

첫째, DeepSeek-V3는 [보조 손실 없는 로드 밸런싱 전략](https://arxiv.org/pdf/2408.15664)을 선도적으로 도입했습니다. 전통적인 MoE 모델에서는 전문가 간 로드 밸런싱을 위해 보조 손실 함수를 사용하는데, 이는 종종 주요 학습 목표와 상충하여 모델 성능을 저하시킵니다. 새로운 접근법은 보조 손실 함수 없이도 효과적인 로드 밸런싱을 달성하여 이러한 성능 저하를 최소화합니다.

이 전략의 핵심은 각 전문가의 최근 로드를 기반으로 라우팅 점수에 동적 편향을 적용하는 것입니다. 과부하된 전문가의 점수는 감소시키고 저부하된 전문가의 점수는 증가시켜 자연스러운 균형을 유도합니다. 이는 기존의 보조 손실 기반 방법보다 더 안정적이고 효과적인 로드 밸런싱을 제공합니다.

둘째, DeepSeek-V3는 멀티 토큰 예측 학습 목표를 사용합니다. [이 접근법은 모델이 단일 토큰이 아닌 여러 토큰을 동시에 예측하도록 학습시키는 것](https://arxiv.org/pdf/2404.10830)으로, 평가 벤치마크에서 전반적인 성능 향상을 가져옵니다. 또한 이 기법은 추론 시 투기적 디코딩(speculative decoding)을 통한 속도 향상에도 활용될 수 있어 실용적 가치가 높습니다.

### 효율적 학습을 위한 인프라 혁신

효율적인 학습을 달성하기 위해 여러 차원에서 혁신적인 접근을 취했습니다. 가장 주목할 만한 것은 [FP8 혼합 정밀도 학습 프레임워크](https://arxiv.org/pdf/2209.05433)의 도입입니다. 저정밀도 학습은 효율적인 학습을 위한 유망한 솔루션으로 부상했으며, 그 발전은 하드웨어 능력의 발전과 밀접하게 연결되어 있습니다.

DeepSeek-V3에서는 FP8 혼합 정밀도 학습 프레임워크를 도입하고 극도로 대규모인 모델에서 처음으로 그 효과를 검증했습니다. FP8 계산과 저장 지원을 통해 학습 가속화와 GPU 메모리 사용량 감소를 모두 달성했습니다. 이는 단순히 정밀도를 낮추는 것이 아니라, 세밀한 양자화 전략과 개선된 누적 정밀도를 통해 수치적 안정성을 유지하면서도 효율성을 극대화하는 정교한 접근법입니다.

학습 프레임워크 측면에서는 [DualPipe 알고리즘](https://arxiv.org/pdf/2401.10241)을 설계했습니다. 이는 효율적인 파이프라인 병렬성을 위한 혁신적인 접근법으로, 파이프라인 버블을 최소화하고 계산-통신 오버랩을 통해 학습 중 대부분의 통신을 숨깁니다. 이러한 오버랩은 모델이 더욱 확장되더라도 일정한 계산 대 통신 비율을 유지하는 한, 노드 간 세분화된 전문가를 사용하면서도 거의 제로에 가까운 all-to-all 통신 오버헤드를 달성할 수 있도록 보장합니다.

### 학습 과정과 안정성

사전 학습 동안 DeepSeek-V3를 14.8T개의 고품질이고 다양한 토큰으로 학습시켰습니다. 이 과정에서 가장 주목할 만한 점은 학습의 안정성입니다. 전체 학습 과정에서 복구 불가능한 손실 급증을 경험하거나 롤백을 수행할 필요가 없었습니다. 이는 대규모 모델 학습에서 매우 드문 성과로, 신중한 하이퍼파라미터 설정과 안정적인 학습 프레임워크의 결과입니다.

다음으로 DeepSeek-V3에 대해 2단계 컨텍스트 길이 확장을 수행했습니다. 첫 번째 단계에서는 최대 컨텍스트 길이를 32K로 확장하고, 두 번째 단계에서는 128K로 더욱 확장했습니다. 이러한 점진적 확장 방식은 모델이 긴 컨텍스트를 안정적으로 처리할 수 있도록 보장합니다.

### 후처리와 지식 증류

후처리 단계에서는 DeepSeek-V3의 기본 모델에 대해 지도 학습 미세 조정(SFT)과 강화 학습(RL)을 포함한 포괄적인 후처리를 수행했습니다. 특히 주목할 만한 것은 DeepSeek-R1 시리즈 모델로부터의 추론 능력 증류입니다. 이는 긴 체인 오브 소트 모델의 고급 추론 능력을 표준 언어 모델로 전달하는 혁신적인 접근법입니다.

이 증류 과정에서는 모델 정확도와 생성 길이 간의 균형을 신중하게 유지했습니다. R1 모델의 강력한 추론 능력을 흡수하면서도 DeepSeek-V3의 출력 스타일과 길이를 적절히 제어하여 실용적 사용성을 보장했습니다.

### 경제적 효율성과 성능

| 단계 | H800 GPU 시간 | 비용 (USD) |
|------|---------------|------------|
| 사전 학습 | 2664K | $5.328M |
| 컨텍스트 확장 | 119K | $0.238M |
| 후처리 | 5K | $0.01M |
| **총계** | **2788K** | **$5.576M** |

DeepSeek-V3의 가장 인상적인 측면 중 하나는 경제적 효율성입니다. H800 GPU 시간당 $2의 임대 가격을 가정할 때 총 학습 비용은 단 $5.576M에 불과합니다. 이는 알고리즘, 프레임워크, 하드웨어의 최적화된 공동 설계를 통해 달성된 성과입니다.

사전 학습 단계에서 각 1조 토큰에 대해 DeepSeek-V3를 학습시키는 데 단 180K H800 GPU 시간만 필요합니다. 즉, 2048개의 H800 GPU를 가진 클러스터에서 3.7일이면 됩니다. 따라서 사전 학습 단계는 2개월 미만에 완료되며 2664K GPU 시간이 소요됩니다.

### 성능 평가와 벤치마크 결과

DeepSeek-V3를 포괄적인 벤치마크 배열에서 평가한 결과, 경제적인 학습 비용에도 불구하고 DeepSeek-V3-Base는 현재 이용 가능한 가장 강력한 오픈소스 기본 모델로 부상했습니다. 특히 코드와 수학 분야에서 뛰어난 성능을 보입니다.

지식 분야에서 MMLU, MMLU-Pro, GPQA와 같은 교육 벤치마크에서 DeepSeek-V3는 모든 다른 오픈소스 모델들을 능가하여 MMLU에서 88.5, MMLU-Pro에서 75.9, GPQA에서 59.1을 달성했습니다. 이는 GPT-4o와 Claude-Sonnet-3.5와 같은 선도적인 클로즈드소스 모델들과 비교할 만한 성능으로, 이 영역에서 오픈소스와 클로즈드소스 모델 간의 격차를 현저히 좁혔습니다.

코드, 수학, 추론 분야에서 DeepSeek-V3는 모든 비긴 체인 오브 소트 오픈소스 및 클로즈드소스 모델들 중에서 수학 관련 벤치마크에서 최첨단 성능을 달성했습니다. 특히 MATH-500과 같은 특정 벤치마크에서는 o1-preview를 능가하기도 하여 견고한 수학적 추론 능력을 보여줍니다.

채팅 버전 또한 다른 오픈소스 모델들을 능가하며 일련의 표준 및 개방형 벤치마크에서 GPT-4o와 Claude-3.5-Sonnet을 포함한 선도적인 클로즈드소스 모델들과 비교할 만한 성능을 달성했습니다.

### 주요 기여와 혁신

연구진의 주요 기여는 세 가지 핵심 영역으로 요약됩니다.

**아키텍처 혁신**에서는 DeepSeek-V2의 효율적인 아키텍처를 기반으로 로드 밸런싱을 위한 보조 손실 없는 전략을 선도적으로 도입하여 로드 밸런싱 장려로 인한 성능 저하를 최소화했습니다. 또한 멀티 토큰 예측(MTP) 목표를 조사하여 모델 성능에 유익함을 입증했으며, 이는 추론 가속화를 위한 투기적 디코딩에도 사용될 수 있습니다.

**궁극적인 학습 효율성**을 향한 노력에서는 FP8 혼합 정밀도 학습 프레임워크를 설계하고 극도로 대규모인 모델에서 FP8 학습의 실현 가능성과 효과를 처음으로 검증했습니다. 알고리즘, 프레임워크, 하드웨어의 공동 설계를 통해 노드 간 MoE 학습의 통신 병목을 극복하여 거의 완전한 계산-통신 오버랩을 달성했습니다.

**지식 증류** 측면에서는 긴 체인 오브 소트 모델인 DeepSeek R1 시리즈로부터 표준 LLM인 DeepSeek-V3로 추론 능력을 증류하는 혁신적인 방법론을 도입했습니다. 이 파이프라인은 R1의 검증 및 반성 패턴을 DeepSeek-V3에 우아하게 통합하여 추론 성능을 현저히 향상시키는 동시에 출력 스타일과 길이에 대한 제어를 유지했습니다.

이러한 혁신들은 DeepSeek-V3를 단순히 더 큰 모델이 아닌, 효율성과 성능을 동시에 추구하는 차세대 언어 모델의 새로운 패러다임으로 자리매김하게 했습니다.
## 아키텍처

DeepSeek-V3의 아키텍처는 효율적인 추론과 경제적인 학습을 위해 설계된 혁신적인 구조를 가지고 있습니다. 이 섹션에서는 Multi-head Latent Attention (MLA)과 DeepSeekMoE를 특징으로 하는 기본 아키텍처와 함께, 전체 성능을 향상시키는 것으로 관찰된 Multi-Token Prediction (MTP) 학습 목표를 소개합니다.

### 기본 아키텍처

DeepSeek-V3의 기본 아키텍처는 여전히 [Transformer 프레임워크](https://arxiv.org/pdf/1706.03762v7) 내에서 구현됩니다. 효율적인 추론과 경제적인 학습을 위해 DeepSeek-V3는 DeepSeek-V2에서 철저히 검증된 MLA와 DeepSeekMoE를 채택합니다. DeepSeek-V2와 비교했을 때 한 가지 예외는 로드 밸런스를 보장하려는 노력으로 인한 성능 저하를 완화하기 위해 DeepSeekMoE에 대한 보조 손실 없는 로드 밸런싱 전략을 추가로 도입했다는 점입니다.

![DeepSeek-V3의 기본 아키텍처](/assets/2025-10-07-deepseek-v3-technical-report/Figure 2)

위 그림은 DeepSeek-V3의 기본 아키텍처를 보여줍니다. DeepSeek-V2를 따라 효율적인 추론과 경제적인 학습을 위해 MLA와 DeepSeekMoE를 채택했습니다.

#### Multi-Head Latent Attention

어텐션 메커니즘에서 DeepSeek-V3는 MLA 아키텍처를 채택합니다. 이 혁신적인 접근법을 이해하기 위해 먼저 기본 개념부터 살펴보겠습니다.

전통적인 Multi-Head Attention에서는 각 토큰에 대해 Query, Key, Value 벡터를 생성하고 이들을 모두 메모리에 저장해야 합니다. 특히 Key-Value (KV) 캐시는 추론 시 메모리 사용량의 주요 병목이 됩니다. 긴 시퀀스를 처리할 때 이 문제는 더욱 심각해집니다.

MLA의 핵심 아이디어는 어텐션 키와 값에 대한 저랭크 공동 압축을 통해 추론 중 Key-Value 캐시를 줄이는 것입니다. 이를 위해 다음과 같은 수학적 정의를 사용합니다.

$d$를 임베딩 차원, $n_h$를 어텐션 헤드 수, $d_h$를 헤드당 차원, $h_t \in \mathbb{R}^d$를 주어진 어텐션 레이어에서 $t$번째 토큰의 어텐션 입력이라고 하겠습니다.

**키와 값의 압축 과정:**

MLA의 핵심은 키와 값을 공동으로 압축하는 것입니다.

$$[k_{t,1}^C; k_{t,2}^C; \ldots; k_{t,n_h}^C] = k_t^C = W^{DKV} h_t$$

$$[v_{t,1}^C; v_{t,2}^C; \ldots; v_{t,n_h}^C] = v_t^C = W^{UV} c^{kv}_t$$

여기서 $c^{kv} \in \mathbb{R}^{d_c}$는 키와 값을 위한 압축된 잠재 벡터이고, $d_c (\ll d_h n_h)$는 KV 압축 차원을 나타냅니다. $W^{DKV} \in \mathbb{R}^{d_c \times d}$는 다운 프로젝션 행렬이고, $W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$는 각각 키와 값을 위한 업 프로젝션 행렬입니다.

**RoPE를 위한 분리된 키 생성:**

위치 정보를 효과적으로 인코딩하기 위해 RoPE (Rotary Positional Embedding)를 적용하는 분리된 키를 생성합니다.

$$k_t^R = W^{KR} h_t$$

$$k_{t,i} = [k_{t,i}^C; \text{RoPE}(k_{t,i}^R)]$$

여기서 $W^{KR} \in \mathbb{R}^{d_R \times d}$는 RoPE를 적용할 분리된 키를 생성하는 행렬이고, $\text{RoPE}(\cdot)$는 RoPE 행렬을 적용하는 연산입니다.

이러한 설계의 핵심 장점은 MLA에서는 파란색 박스로 표시된 벡터들($c^{kv}$와 $k^R$)만 생성 중에 캐시하면 된다는 것입니다. 이는 표준 Multi-Head Attention과 비교할 만한 성능을 유지하면서 KV 캐시를 현저히 줄입니다.

**쿼리의 저랭크 압축:**

학습 중 활성화 메모리를 줄이기 위해 어텐션 쿼리에도 저랭크 압축을 수행합니다.

$$[q_{t,1}^C; q_{t,2}^C; \ldots; q_{t,n_h}^C] = q_t^C = W^{DQ} h_t$$

$$[q_{t,1}^R; q_{t,2}^R; \ldots; q_{t,n_h}^R] = q_t^R = W^{QR} q_t^C$$

$$q_{t,i} = [q_{t,i}^C; \text{RoPE}(q_{t,i}^R)]$$

여기서 $q_t^C \in \mathbb{R}^{d'_c}$는 쿼리를 위한 압축된 잠재 벡터이고, $d'_c (< d_h n_h)$는 쿼리 압축 차원을 나타냅니다. $W^{DQ} \in \mathbb{R}^{d'_c \times d}$, $W^{UQ} \in \mathbb{R}^{d_h n_h \times d'_c}$는 각각 쿼리를 위한 다운 프로젝션과 업 프로젝션 행렬입니다.

**최종 어텐션 계산:**

최종적으로 어텐션 쿼리($q_{t,i}$), 키($k_{j,i}$), 값($v_{j,i}^C$)이 결합되어 최종 어텐션 출력 $u_t$를 생성합니다.

$$\Omega_{t,i} = \sum_{j=1}^{t} \text{Softmax}_j\left(\frac{q_{t,i}^T k_{j,i}}{\sqrt{d_h + d_h^R}}\right) v_{j,i}^C$$

$$u_t = W^O [\Omega_{t,1}; \Omega_{t,2}; \ldots; \Omega_{t,n_h}]$$

여기서 $W^O \in \mathbb{R}^{d \times d_h n_h}$는 출력 프로젝션 행렬입니다.

이러한 MLA 설계는 메모리 효율성과 성능 사이의 최적 균형을 달성합니다. 압축된 표현을 통해 메모리 사용량을 대폭 줄이면서도 전체 어텐션 정보를 보존하여 모델 성능을 유지합니다.

#### 보조 손실 없는 로드 밸런싱을 갖춘 DeepSeekMoE

**DeepSeekMoE의 기본 아키텍처**

Feed-Forward Networks (FFNs)에서 DeepSeek-V3는 [DeepSeekMoE 아키텍처](https://arxiv.org/pdf/2401.06066)를 채택합니다. 이 아키텍처는 전통적인 MoE 아키텍처와 비교하여 두 가지 핵심 혁신을 제공합니다.

첫째, **세분화된 전문가 분할**을 통해 각 전문가를 더 작은 단위로 나누어 전문가들의 조합 유연성을 높입니다. 둘째, **공유 전문가 격리**를 통해 일부 전문가를 항상 활성화되는 공유 전문가로 지정하여 공통 지식을 효과적으로 캡처합니다.

$u_t$를 $t$번째 토큰의 FFN 입력이라고 할 때, FFN 출력 $h_t$는 다음과 같이 계산됩니다.

$$h_t' = u_t + \sum_{i=1}^{N_s} \text{FFN}_i^{(s)}(u_t) + \sum_{i=1}^{N_r} g_{i,t} \text{FFN}_i^{(r)}(u_t)$$

여기서 $N_s$와 $N_r$은 각각 공유 전문가와 라우팅 전문가의 수를 나타냅니다. $\text{FFN}_i^{(s)}(\cdot)$와 $\text{FFN}_i^{(r)}(\cdot)$는 각각 $i$번째 공유 전문가와 $i$번째 라우팅 전문가를 나타냅니다.

**게이팅 메커니즘:**

라우팅 전문가의 선택은 다음과 같은 게이팅 메커니즘을 통해 이루어집니다.

$$g_{i,t}' = \begin{cases}
s_{i,t}, & s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r) \\
0, & \text{otherwise}
\end{cases}$$

$$s_{i,t} = \text{Sigmoid}(u_t^T e_i)$$

여기서 $K_r$은 활성화된 라우팅 전문가의 수, $g_{i,t}$는 $i$번째 전문가의 게이팅 값, $s_{i,t}$는 토큰-전문가 친화도, $e_i$는 $i$번째 라우팅 전문가의 중심 벡터입니다.

DeepSeek-V2와 약간 다른 점은 DeepSeek-V3에서는 친화도 점수를 계산하기 위해 시그모이드 함수를 사용하고, 선택된 모든 친화도 점수 간에 정규화를 적용하여 게이팅 값을 생성한다는 것입니다.

**보조 손실 없는 로드 밸런싱**

MoE 모델에서 불균형한 전문가 로드는 라우팅 붕괴를 초래하고 전문가 병렬성이 있는 시나리오에서 계산 효율성을 저하시킵니다. 기존 솔루션들은 일반적으로 [보조 손실](https://arxiv.org/pdf/2408.15664)에 의존하여 불균형한 로드를 방지하지만, 너무 큰 보조 손실은 모델 성능을 저해합니다.

로드 밸런스와 모델 성능 간의 더 나은 균형을 달성하기 위해, DeepSeek-V3는 로드 밸런스를 보장하는 보조 손실 없는 로드 밸런싱 전략을 선도적으로 도입합니다.

구체적으로, 각 전문가에 대해 편향 항 $b_i$를 도입하고 이를 해당 친화도 점수 $s_{i,t}$에 추가하여 top-K 라우팅을 결정합니다.

$$g_{i,t}' = \begin{cases}
s_{i,t}, & s_{i,t} + b_i \in \text{Topk}(\{s_{j,t} + b_j | 1 \leq j \leq N_r\}, K_r) \\
0, & \text{otherwise}
\end{cases}$$

중요한 점은 편향 항이 라우팅에만 사용된다는 것입니다. FFN 출력과 곱해질 게이팅 값은 여전히 원래 친화도 점수 $s_{i,t}$에서 파생됩니다.

학습 중에는 각 학습 스텝의 전체 배치에서 전문가 로드를 지속적으로 모니터링합니다. 각 스텝이 끝날 때, 해당 전문가가 과부하 상태이면 편향 항을 $\gamma$만큼 감소시키고, 해당 전문가가 저부하 상태이면 $\gamma$만큼 증가시킵니다. 여기서 $\gamma$는 편향 업데이트 속도라는 하이퍼파라미터입니다.

이러한 동적 조정을 통해 DeepSeek-V3는 학습 중 균형 잡힌 전문가 로드를 유지하고, 순수한 보조 손실을 통해 로드 밸런스를 장려하는 모델보다 더 나은 성능을 달성합니다.

**보완적 시퀀스별 보조 손실**

DeepSeek-V3는 주로 로드 밸런스를 위한 보조 손실 없는 전략에 의존하지만, 단일 시퀀스 내에서 극단적인 불균형을 방지하기 위해 보완적 시퀀스별 밸런스 손실도 사용합니다.

$$\mathcal{L}_{\text{Bal}} = \alpha \sum_{i=1}^{N_r} f_i P_i$$

$$f_i = \frac{N_r}{K_r T} \sum_{t=1}^{T} \mathbb{I}(s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r))$$

$$P_i = \frac{1}{T} \sum_{t=1}^{T} \frac{s_{i,t}}{\sum_{j=1}^{N_r} s_{j,t}}$$

여기서 밸런스 팩터 $\alpha$는 하이퍼파라미터로, DeepSeek-V3에서는 극도로 작은 값이 할당됩니다. $\mathbb{I}(\cdot)$는 지시 함수이고, $T$는 시퀀스의 토큰 수를 나타냅니다. 시퀀스별 밸런스 손실은 각 시퀀스에서 전문가 로드가 균형을 이루도록 장려합니다.

**노드 제한 라우팅**

DeepSeek-V2에서 사용된 디바이스 제한 라우팅과 마찬가지로, DeepSeek-V3도 학습 중 통신 비용을 제한하기 위해 제한된 라우팅 메커니즘을 사용합니다. 간단히 말해, 각 토큰이 최대 $M$개의 노드로 전송되도록 보장하며, 이는 각 노드에 분산된 전문가들의 가장 높은 $K_r$개 친화도 점수의 합에 따라 선택됩니다. 이 $M$ 제약 하에서 MoE 학습 프레임워크는 거의 완전한 계산-통신 오버랩을 달성할 수 있습니다.

**토큰 드롭 없음**

효과적인 로드 밸런싱 전략으로 인해 DeepSeek-V3는 전체 학습 동안 좋은 로드 밸런스를 유지합니다. 따라서 DeepSeek-V3는 학습 중 어떤 토큰도 드롭하지 않습니다. 또한 추론 로드 밸런스를 보장하는 특정 배포 전략도 구현하여, DeepSeek-V3는 추론 중에도 토큰을 드롭하지 않습니다.

### Multi-Token Prediction

[Gloeckle et al. (2024)](https://arxiv.org/pdf/2404.19737v1)에서 영감을 받아, DeepSeek-V3에 대해 Multi-Token Prediction (MTP) 목표를 조사하고 설정했습니다. 이는 각 위치에서 예측 범위를 여러 미래 토큰으로 확장합니다.

MTP 접근법의 핵심 아이디어는 단순합니다. 전통적인 언어 모델이 다음 토큰만 예측하는 것과 달리, MTP는 모델이 현재 위치에서 여러 개의 미래 토큰을 동시에 예측하도록 학습시킵니다. 이는 마치 체스에서 한 수만 내다보는 것이 아니라 여러 수를 미리 계획하는 것과 같습니다.

한편으로는 MTP 목표가 학습 신호를 조밀화하고 데이터 효율성을 향상시킬 수 있습니다. 다른 한편으로는 MTP가 모델이 미래 토큰의 더 나은 예측을 위해 표현을 미리 계획할 수 있게 할 수 있습니다.

![Multi-Token Prediction 구현](/assets/2025-10-07-deepseek-v3-technical-report/Figure 3)

위 그림은 Multi-Token Prediction (MTP) 구현을 보여줍니다. 각 깊이에서 각 토큰의 예측을 위한 완전한 인과 체인을 유지합니다.

**MTP와 기존 접근법의 차이점**

Gloeckle et al. (2024)와 다르게, 독립적인 출력 헤드를 사용하여 $D$개의 추가 토큰을 병렬로 예측하는 대신, DeepSeek-V3는 추가 토큰을 순차적으로 예측하고 각 예측 깊이에서 완전한 인과 체인을 유지합니다.

이러한 설계 선택의 핵심은 예측의 일관성과 품질을 보장하는 것입니다. 병렬 예측에서는 각 미래 토큰이 독립적으로 예측되어 서로 일관되지 않을 수 있습니다. 반면 순차적 예측에서는 각 토큰이 이전에 예측된 토큰들을 고려하여 예측되므로 더 일관된 시퀀스를 생성할 수 있습니다.

**MTP 모듈**

구체적으로, MTP 구현은 $D$개의 순차적 모듈을 사용하여 $D$개의 추가 토큰을 예측합니다. $k$번째 MTP 모듈은 공유 임베딩 레이어 $\text{Emb}(\cdot)$, 공유 출력 헤드 $\text{OutHead}(\cdot)$, 트랜스포머 블록 $\text{TRM}_k(\cdot)$, 그리고 프로젝션 행렬 $M_k \in \mathbb{R}^{d \times 2d}$로 구성됩니다.

$i$번째 입력 토큰 $t_i$에 대해, $k$번째 예측 깊이에서, 먼저 $(k-1)$번째 깊이에서 $i$번째 토큰의 표현 $h_i^{k-1} \in \mathbb{R}^d$와 $(i+k)$번째 토큰의 임베딩 $\text{Emb}(t_{i+k}) \in \mathbb{R}^d$를 선형 프로젝션으로 결합합니다.

$$h_i'^k = M_k [\text{RMSNorm}(h_i^{k-1}); \text{RMSNorm}(\text{Emb}(t_{i+k}))]$$

여기서 $[\cdot]$는 연결을 나타냅니다. 특히 $k=1$일 때, $h_i^{k-1}$은 메인 모델에서 주어진 표현을 참조합니다. 각 MTP 모듈에 대해 임베딩 레이어는 메인 모델과 공유됩니다.

결합된 $h_i'^k$는 $k$번째 깊이에서 트랜스포머 블록의 입력으로 사용되어 현재 깊이에서 출력 표현 $h_i^k$를 생성합니다.

$$h_{1:T-k}^k = \text{TRM}_k(h_{1:T-k}'^k)$$

여기서 $T$는 입력 시퀀스 길이를 나타내고, $i:j$는 슬라이싱 연산(왼쪽과 오른쪽 경계 모두 포함)을 나타냅니다.

마지막으로, $h_i^k$를 입력으로 하여 공유 출력 헤드가 $k$번째 추가 예측 토큰에 대한 확률 분포 $P_{i+k+1}^k \in \mathbb{R}^V$를 계산합니다. 여기서 $V$는 어휘 크기입니다.

$$P_{i+k+1}^k = \text{OutHead}(h_i^k)$$

출력 헤드 $\text{OutHead}(\cdot)$는 표현을 로짓에 선형 매핑하고 이후 $\text{Softmax}(\cdot)$ 함수를 적용하여 $k$번째 추가 토큰의 예측 확률을 계산합니다. 또한 각 MTP 모듈에 대해 출력 헤드는 메인 모델과 공유됩니다.

예측의 인과 체인을 유지하는 원칙은 [EAGLE](https://arxiv.org/pdf/2401.10241)과 유사하지만, EAGLE의 주요 목적은 투기적 디코딩인 반면, DeepSeek-V3에서는 MTP를 학습 개선을 위해 활용합니다.

**MTP 학습 목표**

각 예측 깊이에 대해 교차 엔트로피 손실 $\mathcal{L}_{\text{MTP}}^k$를 계산합니다.

$$\mathcal{L}_{\text{MTP}}^k = \text{CrossEntropy}(P_{2+k:T+1}^k, t_{2+k:T+1}) = -\frac{1}{T} \sum_{i=2+k}^{T+1} \log P_i^k[t_i]$$

여기서 $T$는 입력 시퀀스 길이, $t_i$는 $i$번째 위치의 정답 토큰, $P_i^k[t_i]$는 $k$번째 MTP 모듈에서 주어진 $t_i$의 해당 예측 확률을 나타냅니다.

마지막으로, 모든 깊이에 걸쳐 MTP 손실의 평균을 계산하고 가중 팩터 $\lambda$를 곱하여 전체 MTP 손실 $\mathcal{L}_{\text{MTP}}$를 얻습니다. 이는 DeepSeek-V3의 추가 학습 목표로 사용됩니다.

$$\mathcal{L}_{\text{MTP}} = \frac{\lambda}{D} \sum_{k=1}^{D} \mathcal{L}_{\text{MTP}}^k$$

**추론에서의 MTP**

MTP 전략은 주로 메인 모델의 성능을 향상시키는 것을 목표로 하므로, 추론 중에는 MTP 모듈을 직접 폐기할 수 있고 메인 모델은 독립적으로 정상적으로 기능할 수 있습니다. 또한 이러한 MTP 모듈을 투기적 디코딩을 위해 재활용하여 생성 지연 시간을 더욱 개선할 수도 있습니다.

이러한 설계는 MTP의 이중 이점을 제공합니다. 학습 중에는 더 풍부한 학습 신호를 통해 모델 성능을 향상시키고, 추론 중에는 투기적 디코딩을 통해 속도를 향상시킬 수 있는 잠재력을 제공합니다.
## 인프라스트럭처

DeepSeek-V3의 성공적인 학습과 배포를 위해서는 고도로 최적화된 인프라스트럭처가 필요합니다. 이 섹션에서는 컴퓨팅 클러스터 구성, 혁신적인 학습 프레임워크, FP8 저정밀도 학습 기법, 그리고 효율적인 추론 배포 전략에 대해 상세히 살펴보겠습니다.

### 컴퓨팅 클러스터

DeepSeek-V3는 2048개의 NVIDIA H800 GPU로 구성된 클러스터에서 학습됩니다. 각 노드는 8개의 GPU를 포함하며, 노드 내에서는 NVLink와 NVSwitch를 통해 GPU들이 연결됩니다. 서로 다른 노드 간에는 InfiniBand (IB) 인터커넥트를 활용하여 통신을 수행합니다.

이러한 하드웨어 구성은 대규모 MoE 모델의 학습에서 발생하는 통신 병목을 해결하기 위해 신중하게 설계되었습니다. NVLink는 160 GB/s의 대역폭을 제공하여 노드 내 GPU 간 고속 통신을 가능하게 하며, InfiniBand는 50 GB/s의 대역폭으로 노드 간 통신을 담당합니다. 이러한 계층적 통신 구조는 후에 설명할 all-to-all 통신 최적화의 기반이 됩니다.

### 학습 프레임워크

DeepSeek-V3의 학습은 HAI-LLM 프레임워크에 의해 지원됩니다. 이는 연구진이 처음부터 개발한 효율적이고 경량화된 학습 프레임워크입니다. 전체적으로 DeepSeek-V3는 16-way Pipeline Parallelism (PP), 8개 노드에 걸친 64-way Expert Parallelism (EP), 그리고 ZeRO-1 Data Parallelism (DP)을 적용합니다.

효율적인 DeepSeek-V3 학습을 위해 세심한 엔지니어링 최적화를 구현했습니다. 첫째, 효율적인 파이프라인 병렬성을 위한 DualPipe 알고리즘을 설계했습니다. 기존 PP 방법들과 비교하여 DualPipe는 파이프라인 버블이 적습니다. 더 중요한 것은 순방향과 역방향 프로세스에서 계산과 통신 단계를 오버랩하여 노드 간 전문가 병렬성으로 인한 무거운 통신 오버헤드 문제를 해결한다는 점입니다.

둘째, IB와 NVLink 대역폭을 완전히 활용하고 통신 전용 Streaming Multiprocessors (SMs)를 절약하기 위한 효율적인 노드 간 all-to-all 통신 커널을 개발했습니다. 마지막으로, 학습 중 메모리 사용량을 세심하게 최적화하여 비용이 많이 드는 Tensor Parallelism (TP) 없이도 DeepSeek-V3를 학습할 수 있게 했습니다.

#### DualPipe와 계산-통신 오버랩

DeepSeek-V3에서 노드 간 전문가 병렬성으로 인한 통신 오버헤드는 약 1:1의 비효율적인 계산 대 통신 비율을 초래합니다. 이 문제를 해결하기 위해 DualPipe라는 혁신적인 파이프라인 병렬성 알고리즘을 설계했습니다. 이 알고리즘은 순방향과 역방향 계산-통신 단계를 효과적으로 오버랩하여 모델 학습을 가속화할 뿐만 아니라 파이프라인 버블도 줄입니다.

![DualPipe 오버랩 전략](/assets/2025-10-07-deepseek-v3-technical-report/Figure 4)

위 그림은 개별 순방향 및 역방향 청크 쌍에 대한 오버랩 전략을 보여줍니다. 트랜스포머 블록의 경계가 정렬되지 않은 상황에서 주황색은 순방향, 녹색은 "입력에 대한 역방향", 파란색은 "가중치에 대한 역방향", 보라색은 PP 통신, 빨간색은 배리어를 나타냅니다. all-to-all과 PP 통신 모두 완전히 숨겨질 수 있습니다.

DualPipe의 핵심 아이디어는 개별 순방향 및 역방향 청크 쌍 내에서 계산과 통신을 오버랩하는 것입니다. 구체적으로, 각 청크를 네 가지 구성 요소로 나눕니다. 어텐션, all-to-all dispatch, MLP, 그리고 all-to-all combine입니다. 특히 역방향 청크의 경우, 어텐션과 MLP 모두 [ZeroBubble](https://arxiv.org/pdf/2401.10241)에서와 같이 입력에 대한 역방향과 가중치에 대한 역방향의 두 부분으로 더 분할됩니다.

이러한 오버랩 전략에서는 통신 대 계산에 전용되는 GPU SM의 비율을 수동으로 조정하고 이러한 구성 요소들을 재배열합니다. 이 오버랩 전략을 통해 all-to-all과 PP 통신 모두가 실행 중에 완전히 숨겨질 수 있도록 보장할 수 있습니다.

![DualPipe 스케줄링](/assets/2025-10-07-deepseek-v3-technical-report/Figure 5)

위 그림은 8개의 PP 랭크와 20개의 마이크로 배치를 양방향으로 처리하는 DualPipe 스케줄링 예시를 보여줍니다. 역방향의 마이크로 배치들은 순방향과 대칭이므로 설명의 단순화를 위해 배치 ID를 생략했습니다. 공유된 검은색 테두리로 둘러싸인 두 셀은 상호 오버랩된 계산과 통신을 가집니다.

효율적인 오버랩 전략이 주어지면, 전체 DualPipe 스케줄링은 양방향 파이프라인 스케줄링을 사용합니다. 이는 파이프라인의 양쪽 끝에서 동시에 마이크로 배치를 공급하며, 통신의 상당 부분이 완전히 오버랩될 수 있습니다. 이러한 오버랩은 모델이 더욱 확장되더라도 일정한 계산 대 통신 비율을 유지하는 한, 노드 간 세분화된 전문가를 사용하면서도 거의 제로에 가까운 all-to-all 통신 오버헤드를 달성할 수 있도록 보장합니다.

| 방법 | 버블 | 파라미터 | 활성화 |
|------|------|----------|--------|
| 1F1B | $(PP - 1) \times (F + B)$ | $1 \times PP$ | - |
| ZB1P | $(PP - 1) \times (F + B - 2W)$ | $1 \times PP$ | - |
| DualPipe (제안) | $(\frac{PP}{2} - 1) \times (F\&B + B - 3W)$ | $2 \times PP + 1$ | - |

위 표는 서로 다른 파이프라인 병렬 방법들 간의 파이프라인 버블과 메모리 사용량을 비교합니다. $F$는 순방향 청크의 실행 시간, $B$는 전체 역방향 청크의 실행 시간, $W$는 "가중치에 대한 역방향" 청크의 실행 시간, $F\&B$는 상호 오버랩된 두 순방향 및 역방향 청크의 실행 시간을 나타냅니다.

무거운 통신 부담이 없는 더 일반적인 시나리오에서도 DualPipe는 여전히 효율성 장점을 보입니다. 표에서 보듯이 [ZB1P](https://arxiv.org/pdf/2401.10241)와 [1F1B](https://arxiv.org/pdf/1806.03377)와 비교하여 DualPipe는 파이프라인 버블을 현저히 줄이면서 최대 활성화 메모리를 $1/PP$배만 증가시킵니다.

DualPipe가 모델 파라미터의 두 복사본을 유지해야 하지만, 학습 중 큰 EP 크기를 사용하므로 이것이 메모리 소비를 크게 증가시키지는 않습니다. [Chimera](https://arxiv.org/pdf/2401.10241)와 비교하여 DualPipe는 파이프라인 스테이지와 마이크로 배치가 2로 나누어떨어지기만 하면 되며, 마이크로 배치가 파이프라인 스테이지로 나누어떨어질 필요가 없습니다. 또한 DualPipe의 경우 마이크로 배치 수가 증가해도 버블이나 활성화 메모리가 증가하지 않습니다.

#### 효율적인 노드 간 All-to-All 통신 구현

DualPipe에 충분한 계산 성능을 보장하기 위해 통신 전용 SM 수를 절약하는 효율적인 노드 간 all-to-all 통신 커널(dispatching과 combining 포함)을 맞춤 제작했습니다. 커널의 구현은 MoE 게이팅 알고리즘과 클러스터의 네트워크 토폴로지와 공동 설계되었습니다.

구체적으로, 클러스터에서 노드 간 GPU들은 IB로 완전히 상호 연결되어 있으며, 노드 내 통신은 NVLink를 통해 처리됩니다. NVLink는 160 GB/s의 대역폭을 제공하여 IB(50 GB/s)의 약 3.2배입니다. IB와 NVLink의 서로 다른 대역폭을 효과적으로 활용하기 위해 각 토큰이 최대 4개의 노드로만 dispatch되도록 제한하여 IB 트래픽을 줄입니다.

각 토큰에 대해 라우팅 결정이 내려지면, 먼저 IB를 통해 대상 노드들의 동일한 노드 내 인덱스를 가진 GPU로 전송됩니다. 대상 노드에 도달하면, 후속 도착 토큰들에 의해 차단되지 않고 대상 전문가를 호스팅하는 특정 GPU로 NVLink를 통해 즉시 전달되도록 노력합니다. 이러한 방식으로 IB와 NVLink를 통한 통신이 완전히 오버랩되며, 각 토큰은 NVLink로부터 추가 오버헤드 없이 노드당 평균 3.2개의 전문가를 효율적으로 선택할 수 있습니다.

이는 DeepSeek-V3가 실제로는 8개의 라우팅된 전문가만 선택하지만, 동일한 통신 비용을 유지하면서 이 수를 최대 13개의 전문가(4개 노드 × 노드당 3.2개 전문가)까지 확장할 수 있음을 의미합니다.

이러한 통신 전략 하에서 IB와 NVLink의 대역폭을 완전히 활용하기 위해서는 단 20개의 SM만으로 충분합니다. 구체적으로, [warp specialization 기법](https://arxiv.org/pdf/2209.05433)을 사용하여 20개의 SM을 10개의 통신 채널로 분할합니다. dispatching 과정에서 (1) IB 전송, (2) IB-to-NVLink 전달, (3) NVLink 수신이 각각의 warp에 의해 처리됩니다. 각 통신 작업에 할당되는 warp의 수는 모든 SM에 걸친 실제 워크로드에 따라 동적으로 조정됩니다.

마찬가지로 combining 과정에서도 (1) NVLink 전송, (2) NVLink-to-IB 전달 및 누적, (3) IB 수신 및 누적이 동적으로 조정되는 warp에 의해 처리됩니다. 또한 dispatching과 combining 커널 모두 계산 스트림과 오버랩되므로 다른 SM 계산 커널에 미치는 영향도 고려합니다. 구체적으로, 맞춤형 PTX (Parallel Thread Execution) 명령어를 사용하고 통신 청크 크기를 자동 조정하여 L2 캐시 사용량과 다른 SM에 대한 간섭을 현저히 줄입니다.

#### 최소 오버헤드로 극도의 메모리 절약

학습 중 메모리 사용량을 줄이기 위해 다음과 같은 기법들을 사용합니다.

**RMSNorm과 MLA Up-Projection의 재계산**: 역전파 중에 모든 RMSNorm 연산과 MLA up-projection을 재계산하여 출력 활성화를 지속적으로 저장할 필요를 없앱니다. 약간의 오버헤드로 이 전략은 활성화 저장을 위한 메모리 요구사항을 현저히 줄입니다.

**CPU에서의 지수 이동 평균**: 학습 중에 학습률 감소 후 모델 성능의 조기 추정을 위해 모델 파라미터의 지수 이동 평균(EMA)을 보존합니다. EMA 파라미터는 CPU 메모리에 저장되며 각 학습 스텝 후 비동기적으로 업데이트됩니다. 이 방법을 통해 추가적인 메모리나 시간 오버헤드 없이 EMA 파라미터를 유지할 수 있습니다.

**멀티 토큰 예측을 위한 공유 임베딩과 출력 헤드**: DualPipe 전략을 통해 모델의 가장 얕은 레이어들(임베딩 레이어 포함)과 가장 깊은 레이어들(출력 헤드 포함)을 동일한 PP 랭크에 배치합니다. 이러한 배치는 MTP 모듈과 메인 모델 간에 공유 임베딩과 출력 헤드의 파라미터와 그래디언트를 물리적으로 공유할 수 있게 합니다. 이러한 물리적 공유 메커니즘은 메모리 효율성을 더욱 향상시킵니다.

### FP8 학습

[최근 저정밀도 학습의 발전](https://arxiv.org/pdf/2209.05433)에서 영감을 받아, DeepSeek-V3 학습을 위해 FP8 데이터 형식을 활용하는 세분화된 혼합 정밀도 프레임워크를 제안합니다. 저정밀도 학습은 큰 가능성을 가지고 있지만, 종종 활성화, 가중치, 그래디언트의 이상치 존재로 인해 제한됩니다.

[추론 양자화에서 상당한 진전](https://arxiv.org/pdf/2310.18313)이 있었지만, 대규모 언어 모델 사전 학습에서 저정밀도 기법의 성공적인 적용을 보여주는 연구는 상대적으로 적습니다. 이 문제를 해결하고 FP8 형식의 동적 범위를 효과적으로 확장하기 위해 세분화된 양자화 전략을 도입합니다. $1 \times N_c$ 요소의 타일별 그룹화 또는 $N_c \times N_c$ 요소의 블록별 그룹화입니다.

![FP8 혼합 정밀도 프레임워크](/assets/2025-10-07-deepseek-v3-technical-report/Figure 6)

위 그림은 FP8 데이터 형식을 사용한 전체 혼합 정밀도 프레임워크를 보여줍니다. 명확성을 위해 Linear 연산자만 설명되어 있습니다.

관련된 역양자화 오버헤드는 정확한 FP8 General Matrix Multiplication (GEMM)을 달성하기 위한 중요한 측면인 증가된 정밀도 누적 과정에서 크게 완화됩니다. 더 나아가 MoE 학습에서 메모리와 통신 오버헤드를 더욱 줄이기 위해 FP8로 활성화를 캐시하고 dispatch하며, 저정밀도 옵티마이저 상태를 BF16으로 저장합니다.

DeepSeek-V2-Lite와 DeepSeek-V2와 유사한 두 모델 규모에서 제안된 FP8 혼합 정밀도 프레임워크를 검증하여 약 1조 토큰에 대해 학습했습니다. 주목할 점은 BF16 기준선과 비교하여 FP8 학습 모델의 상대적 손실 오차가 지속적으로 0.25% 미만을 유지한다는 것으로, 이는 학습 무작위성의 허용 범위 내에 있는 수준입니다.
#### 혼합 정밀도 프레임워크

[광범위하게 채택된 저정밀도 학습 기법](https://arxiv.org/pdf/2209.05433)을 기반으로 FP8 학습을 위한 혼합 정밀도 프레임워크를 제안합니다. 이 프레임워크에서는 대부분의 계산 집약적 연산을 FP8로 수행하는 반면, 몇 가지 핵심 연산은 학습 효율성과 수치적 안정성의 균형을 맞추기 위해 전략적으로 원래 데이터 형식으로 유지합니다.

먼저 모델 학습을 가속화하기 위해 대부분의 핵심 계산 커널, 즉 GEMM 연산을 FP8 정밀도로 구현합니다. 이러한 GEMM 연산은 FP8 텐서를 입력으로 받아 BF16 또는 FP32로 출력을 생성합니다. 앞서 보여준 그림에서 보듯이 Linear 연산자와 관련된 세 가지 GEMM, 즉 Fprop(순방향 패스), Dgrad(활성화 역방향 패스), Wgrad(가중치 역방향 패스)가 모두 FP8로 실행됩니다. 이러한 설계는 이론적으로 원래 BF16 방법과 비교하여 계산 속도를 두 배로 향상시킵니다.

또한 FP8 Wgrad GEMM을 통해 활성화를 FP8로 저장하여 역방향 패스에서 사용할 수 있습니다. 이는 메모리 소비를 현저히 줄입니다. FP8 형식의 효율성 장점에도 불구하고 특정 연산자들은 저정밀도 계산에 대한 민감성으로 인해 여전히 더 높은 정밀도가 필요합니다. 또한 일부 저비용 연산자들도 전체 학습 비용에 무시할 만한 오버헤드로 더 높은 정밀도를 활용할 수 있습니다.

이러한 이유로 신중한 조사를 거쳐 다음 구성 요소들에 대해서는 원래 정밀도(예: BF16 또는 FP32)를 유지합니다. 임베딩 모듈, 출력 헤드, MoE 게이팅 모듈, 정규화 연산자, 그리고 어텐션 연산자입니다. 이러한 타겟 고정밀도 유지는 DeepSeek-V3의 안정적인 학습 동역학을 보장합니다.

수치적 안정성을 더욱 보장하기 위해 마스터 가중치, 가중치 그래디언트, 옵티마이저 상태를 더 높은 정밀도로 저장합니다. 이러한 고정밀도 구성 요소들이 일부 메모리 오버헤드를 발생시키지만, 분산 학습 시스템에서 여러 DP 랭크에 걸친 효율적인 샤딩을 통해 그 영향을 최소화할 수 있습니다.

#### 양자화와 곱셈을 통한 정밀도 향상

혼합 정밀도 FP8 프레임워크를 기반으로 양자화 방법과 곱셈 과정 모두에 초점을 맞춘 저정밀도 학습 정확도를 향상시키는 여러 전략을 도입합니다.

**세분화된 양자화**

저정밀도 학습 프레임워크에서 오버플로우와 언더플로우는 지수 비트 감소로 인해 제한된 FP8 형식의 동적 범위로 인한 일반적인 문제입니다. 표준 관행으로, 입력 분포는 입력 텐서의 최대 절댓값을 FP8의 최대 표현 가능한 값으로 스케일링하여 FP8 형식의 표현 가능한 범위에 정렬됩니다. 이 방법은 저정밀도 학습을 활성화 이상치에 매우 민감하게 만들어 양자화 정확도를 심각하게 저하시킬 수 있습니다.

![세분화된 양자화와 누적 정밀도 향상](/assets/2025-10-07-deepseek-v3-technical-report/Figure 7)

위 그림은 (a) 특성 이상치로 인한 양자화 오류를 완화하기 위한 세분화된 양자화 방법을 제안하고, (b) 양자화 전략과 함께 $N_c = 128$ 요소 간격으로 CUDA 코어로의 승격을 통해 FP8 GEMM 정밀도를 향상시키는 과정을 보여줍니다.

이를 해결하기 위해 더 세분화된 수준에서 스케일링을 적용하는 세분화된 양자화 방법을 제안합니다. 그림 (a)에서 보듯이, (1) 활성화의 경우 1x128 타일 기준(즉, 토큰당 128 채널당)으로 요소를 그룹화하고 스케일링하며, (2) 가중치의 경우 128x128 블록 기준(즉, 128 입력 채널당 128 출력 채널당)으로 요소를 그룹화하고 스케일링합니다. 이 접근법은 양자화 과정이 더 작은 요소 그룹에 따라 스케일을 조정하여 이상치를 더 잘 수용할 수 있도록 보장합니다.

이 방법의 핵심 수정 사항은 GEMM 연산의 내부 차원을 따라 그룹별 스케일링 팩터를 도입하는 것입니다. 이 기능은 표준 FP8 GEMM에서 직접 지원되지 않습니다. 그러나 정밀한 FP32 누적 전략과 결합하면 효율적으로 구현할 수 있습니다.

주목할 점은 세분화된 양자화 전략이 [microscaling 형식](https://arxiv.org/pdf/2209.05433)의 아이디어와 매우 일치한다는 것입니다. NVIDIA 차세대 GPU(Blackwell 시리즈)의 Tensor Core는 더 작은 양자화 세분성을 가진 microscaling 형식에 대한 지원을 발표했습니다. 이 설계가 최신 GPU 아키텍처와 보조를 맞추는 향후 작업의 참고 자료가 되기를 희망합니다.

**누적 정밀도 증가**

저정밀도 GEMM 연산은 종종 언더플로우 문제로 어려움을 겪으며, 그 정확도는 일반적으로 FP32 정밀도로 수행되는 고정밀도 누적에 크게 의존합니다. 그러나 NVIDIA H800 GPU에서 FP8 GEMM의 누적 정밀도가 약 14비트 유지에 제한되어 있어 FP32 누적 정밀도보다 현저히 낮다는 것을 관찰했습니다.

이 문제는 내부 차원 K가 클 때 더욱 두드러지는데, 이는 배치 크기와 모델 폭이 증가하는 대규모 모델 학습의 전형적인 시나리오입니다. K = 4096인 두 무작위 행렬의 GEMM 연산을 예로 들면, 예비 테스트에서 Tensor Core의 제한된 누적 정밀도는 거의 2%의 최대 상대 오차를 초래합니다. 이러한 문제에도 불구하고 제한된 누적 정밀도는 여전히 몇몇 FP8 프레임워크에서 기본 옵션으로 남아 있어 학습 정확도를 심각하게 제약합니다.

이 문제를 해결하기 위해 더 높은 정밀도를 위한 CUDA 코어로의 승격 전략을 채택합니다. 과정은 그림 (b)에 설명되어 있습니다. 구체적으로, Tensor Core에서 MMA(Matrix Multiply-Accumulate) 실행 중에 중간 결과는 제한된 비트 폭을 사용하여 누적됩니다. $N_c$ 간격에 도달하면 이러한 부분 결과가 CUDA 코어의 FP32 레지스터로 복사되어 완전 정밀도 FP32 누적이 수행됩니다.

앞서 언급했듯이 세분화된 양자화는 내부 차원 K를 따라 그룹별 스케일링 팩터를 적용합니다. 이러한 스케일링 팩터는 최소한의 추가 계산 비용으로 역양자화 과정으로서 CUDA 코어에서 효율적으로 곱해질 수 있습니다.

이 수정이 단일 warpgroup에 대한 WGMMA(Warpgroup-level Matrix Multiply-Accumulate) 명령어 발행률을 감소시킨다는 점은 주목할 가치가 있습니다. 그러나 H800 아키텍처에서는 두 개의 WGMMA가 동시에 지속되는 것이 일반적입니다. 한 warpgroup이 승격 연산을 수행하는 동안 다른 하나는 MMA 연산을 실행할 수 있습니다. 이 설계는 두 연산의 오버랩을 가능하게 하여 Tensor Core의 높은 활용률을 유지합니다.

실험을 바탕으로 $N_c = 128$ 요소, 즉 4개의 WGMMA에 해당하는 설정이 상당한 오버헤드를 도입하지 않으면서 정밀도를 현저히 향상시킬 수 있는 최소 누적 간격을 나타냅니다.

**지수보다 가수**

E4M3(4비트 지수와 3비트 가수)를 Fprop에서 사용하고 E5M2(5비트 지수와 2비트 가수)를 Dgrad와 Wgrad에서 사용하는 이전 작업에서 채택된 하이브리드 FP8 형식과 대조적으로, 더 높은 정밀도를 위해 모든 텐서에서 E4M3 형식을 채택합니다. 이 접근법의 실현 가능성을 세분화된 양자화 전략, 즉 타일 및 블록별 스케일링에 기인합니다. 더 작은 요소 그룹에서 작동함으로써 방법론은 이러한 그룹화된 요소들 간에 지수 비트를 효과적으로 공유하여 제한된 동적 범위의 영향을 완화합니다.

**온라인 양자화**

지연된 양자화는 현재 값을 추론하기 위해 이전 반복의 최대 절댓값 이력을 유지하는 텐서별 양자화 프레임워크에서 사용됩니다. 정확한 스케일을 보장하고 프레임워크를 단순화하기 위해 각 1x128 활성화 타일 또는 128x128 가중치 블록에 대해 온라인으로 최대 절댓값을 계산합니다. 이를 기반으로 스케일링 팩터를 도출한 다음 활성화 또는 가중치를 온라인으로 FP8 형식으로 양자화합니다.

#### 저정밀도 저장 및 통신

FP8 학습 프레임워크와 함께 캐시된 활성화와 옵티마이저 상태를 저정밀도 형식으로 압축하여 메모리 소비와 통신 오버헤드를 더욱 줄입니다.

**저정밀도 옵티마이저 상태**: 관찰 가능한 성능 저하 없이 AdamW 옵티마이저에서 1차 및 2차 모멘트를 추적하기 위해 FP32 대신 BF16 데이터 형식을 채택합니다. 그러나 마스터 가중치(옵티마이저에 의해 저장됨)와 그래디언트(배치 크기 누적에 사용됨)는 학습 전반에 걸쳐 수치적 안정성을 보장하기 위해 여전히 FP32로 유지됩니다.

**저정밀도 활성화**: 앞서 보여준 그림에서 보듯이 Wgrad 연산은 FP8로 수행됩니다. 메모리 소비를 줄이기 위해 Linear 연산자의 역방향 패스를 위해 활성화를 FP8 형식으로 캐시하는 것이 자연스러운 선택입니다. 그러나 저비용 고정밀도 학습을 위해 여러 연산자에 대해 특별한 고려사항이 적용됩니다.

(1) 어텐션 연산자 후 Linear의 입력: 이러한 활성화는 어텐션 연산자의 역방향 패스에서도 사용되어 정밀도에 민감합니다. 이러한 활성화에 대해서만 맞춤형 E5M6 데이터 형식을 채택합니다. 또한 이러한 활성화는 역방향 패스에서 1x128 양자화 타일에서 128x1 타일로 변환됩니다. 추가 양자화 오류 도입을 피하기 위해 모든 스케일링 팩터는 라운드 스케일링, 즉 2의 정수 거듭제곱입니다.

(2) MoE에서 SwiGLU 연산자의 입력: 메모리 비용을 더욱 줄이기 위해 SwiGLU 연산자의 입력을 캐시하고 역방향 패스에서 출력을 재계산합니다. 이러한 활성화도 세분화된 양자화 방법으로 FP8에 저장되어 메모리 효율성과 계산 정확도 간의 균형을 맞춥니다.

**저정밀도 통신**: 통신 대역폭은 MoE 모델 학습에서 중요한 병목입니다. 이 문제를 완화하기 위해 MoE up-projection 전의 활성화를 FP8로 양자화한 다음 dispatch 구성 요소를 적용하는데, 이는 MoE up-projection에서 FP8 Fprop과 호환됩니다. 어텐션 연산자 후 Linear의 입력과 마찬가지로 이 활성화의 스케일링 팩터는 2의 정수 거듭제곱입니다. 유사한 전략이 MoE down-projection 전의 활성화 그래디언트에 적용됩니다. 순방향 및 역방향 combine 구성 요소 모두에 대해서는 학습 파이프라인의 중요한 부분에서 학습 정밀도를 보존하기 위해 BF16으로 유지합니다.

### 추론 및 배포

DeepSeek-V3를 H800 클러스터에 배포하는데, 각 노드 내의 GPU들은 NVLink를 사용하여 상호 연결되고 클러스터 전체의 모든 GPU는 IB를 통해 완전히 상호 연결됩니다. 온라인 서비스의 Service-Level Objective (SLO)와 높은 처리량을 동시에 보장하기 위해 prefilling과 decoding 단계를 분리하는 다음과 같은 배포 전략을 사용합니다.

#### Prefilling

prefilling 단계의 최소 배포 단위는 32개의 GPU를 가진 4개의 노드로 구성됩니다. 어텐션 부분은 Sequence Parallelism (SP)과 결합된 4-way Tensor Parallelism (TP4)과 8-way Data Parallelism (DP8)을 사용합니다. 작은 TP 크기인 4는 TP 통신의 오버헤드를 제한합니다. MoE 부분의 경우 32-way Expert Parallelism (EP32)을 사용하여 각 전문가가 충분히 큰 배치 크기를 처리하도록 보장하여 계산 효율성을 향상시킵니다.

MoE all-to-all 통신의 경우 학습에서와 동일한 방법을 사용합니다. 먼저 IB를 통해 노드 간 토큰을 전송한 다음 NVLink를 통해 노드 내 GPU 간에 전달합니다. 특히 얕은 레이어의 dense MLP에 대해서는 TP 통신을 절약하기 위해 1-way Tensor Parallelism을 사용합니다.

MoE 부분에서 서로 다른 전문가 간의 로드 밸런싱을 달성하기 위해 각 GPU가 대략 동일한 수의 토큰을 처리하도록 보장해야 합니다. 이를 위해 고부하 전문가를 복제하여 중복 배포하는 중복 전문가의 배포 전략을 도입합니다. 고부하 전문가는 온라인 배포 중 수집된 통계를 기반으로 감지되며 주기적으로(예: 10분마다) 조정됩니다.

중복 전문가 세트를 결정한 후, 노드 간 all-to-all 통신 오버헤드를 증가시키지 않으면서 GPU 간 로드를 최대한 균형 있게 맞추기 위해 관찰된 로드를 기반으로 노드 내 GPU 간에 전문가를 신중하게 재배열합니다. DeepSeek-V3의 배포를 위해 prefilling 단계에서 32개의 중복 전문가를 설정합니다. 각 GPU는 호스팅하는 원래 8개의 전문가 외에 하나의 추가 중복 전문가도 호스팅합니다.

더 나아가 prefilling 단계에서 처리량을 향상시키고 all-to-all 및 TP 통신의 오버헤드를 숨기기 위해 유사한 계산 워크로드를 가진 두 개의 마이크로 배치를 동시에 처리하여 한 마이크로 배치의 어텐션과 MoE를 다른 마이크로 배치의 dispatch 및 combine과 오버랩합니다.

마지막으로 전문가에 대한 동적 중복성 전략을 탐색하고 있는데, 각 GPU가 더 많은 전문가(예: 16개 전문가)를 호스팅하지만 각 추론 스텝에서는 9개만 활성화됩니다. 각 레이어에서 all-to-all 연산이 시작되기 전에 전역적으로 최적의 라우팅 스킴을 즉석에서 계산합니다. prefilling 단계의 상당한 계산을 고려할 때 이 라우팅 스킴을 계산하는 오버헤드는 거의 무시할 수 있습니다.

#### Decoding

decoding 중에는 공유 전문가를 라우팅된 전문가로 취급합니다. 이 관점에서 각 토큰은 라우팅 중에 9개의 전문가를 선택하며, 공유 전문가는 항상 선택되는 고부하 전문가로 간주됩니다. decoding 단계의 최소 배포 단위는 320개의 GPU를 가진 40개의 노드로 구성됩니다. 어텐션 부분은 SP와 결합된 TP4와 DP80을 사용하는 반면, MoE 부분은 EP320을 사용합니다.

MoE 부분의 경우 각 GPU는 하나의 전문가만 호스팅하며, 64개의 GPU가 중복 전문가와 공유 전문가 호스팅을 담당합니다. dispatch 및 combine 부분의 all-to-all 통신은 낮은 지연 시간을 달성하기 위해 IB를 통한 직접 점대점 전송으로 수행됩니다. 또한 지연 시간을 더욱 최소화하고 통신 효율성을 향상시키기 위해 IBGDA 기술을 활용합니다.

prefilling과 마찬가지로 온라인 서비스의 통계적 전문가 로드를 기반으로 특정 간격에서 중복 전문가 세트를 주기적으로 결정합니다. 그러나 각 GPU가 하나의 전문가만 호스팅하므로 전문가를 재배열할 필요가 없습니다. decoding을 위한 동적 중복성 전략도 탐색하고 있습니다. 그러나 이는 전역적으로 최적의 라우팅 스킴을 계산하는 알고리즘과 오버헤드를 줄이기 위한 dispatch 커널과의 융합에 대한 더 신중한 최적화가 필요합니다.

또한 처리량을 향상시키고 all-to-all 통신의 오버헤드를 숨기기 위해 decoding 단계에서도 유사한 계산 워크로드를 가진 두 개의 마이크로 배치를 동시에 처리하는 것을 탐색하고 있습니다. prefilling과 달리 decoding 단계에서는 어텐션이 더 큰 시간 비중을 차지합니다. 따라서 한 마이크로 배치의 어텐션을 다른 마이크로 배치의 dispatch+MoE+combine과 오버랩합니다.

decoding 단계에서 전문가당 배치 크기는 상대적으로 작으며(보통 256 토큰 이내) 병목은 계산보다는 메모리 액세스입니다. MoE 부분은 하나의 전문가의 파라미터만 로드하면 되므로 메모리 액세스 오버헤드가 최소화되어 더 적은 SM을 사용해도 전체 성능에 크게 영향을 주지 않습니다. 따라서 어텐션 부분의 계산 속도에 영향을 주지 않기 위해 dispatch+MoE+combine에 SM의 작은 부분만 할당할 수 있습니다.

### 하드웨어 설계에 대한 제안

all-to-all 통신과 FP8 학습 스킴의 구현을 바탕으로 AI 하드웨어 벤더들에게 칩 설계에 대한 다음과 같은 제안을 합니다.

#### 통신 하드웨어

DeepSeek-V3에서는 계산 중 통신 지연 시간을 숨기기 위해 계산과 통신 간의 오버랩을 구현합니다. 이는 직렬 계산 및 통신과 비교하여 통신 대역폭에 대한 의존성을 현저히 줄입니다. 그러나 현재 통신 구현은 비싼 SM에 의존하며(예: H800 GPU에서 사용 가능한 132개의 SM 중 20개를 이 목적으로 할당), 이는 계산 처리량을 제한합니다. 더욱이 통신을 위해 SM을 사용하면 텐서 코어가 완전히 활용되지 않아 상당한 비효율성을 초래합니다.

현재 SM은 all-to-all 통신을 위해 주로 다음 작업을 수행합니다. IB(InfiniBand)와 NVLink 도메인 간 데이터 전달과 동시에 동일한 노드 내 여러 GPU로 향하는 IB 트래픽을 단일 GPU에서 집계, RDMA 버퍼(등록된 GPU 메모리 영역)와 입력/출력 버퍼 간 데이터 전송, all-to-all combine을 위한 reduce 연산 실행, IB와 NVLink 도메인에 걸쳐 여러 전문가로의 청크 데이터 전송 중 세분화된 메모리 레이아웃 관리입니다.

향후 벤더들이 이러한 통신 작업을 귀중한 계산 단위 SM에서 오프로드하는 하드웨어를 개발하여 GPU 코프로세서 또는 NVIDIA SHARP와 같은 네트워크 코프로세서 역할을 하기를 희망합니다. 더 나아가 애플리케이션 프로그래밍 복잡성을 줄이기 위해 이 하드웨어가 계산 단위의 관점에서 IB(scale-out)와 NVLink(scale-up) 네트워크를 통합하기를 목표로 합니다. 이러한 통합 인터페이스를 통해 계산 단위는 간단한 프리미티브를 기반으로 한 통신 요청 제출을 통해 전체 IB-NVLink-통합 도메인에 걸쳐 읽기, 쓰기, 멀티캐스트, reduce와 같은 연산을 쉽게 수행할 수 있습니다.

#### 컴퓨팅 하드웨어

**Tensor Core에서 더 높은 FP8 GEMM 누적 정밀도**: 현재 NVIDIA Hopper 아키텍처의 Tensor Core 구현에서 FP8 GEMM은 제한된 누적 정밀도로 어려움을 겪습니다. 최대 지수를 기반으로 한 우측 시프트를 통해 32개의 가수 곱을 정렬한 후, Tensor Core는 각 가수 곱의 최고 14비트만 덧셈에 사용하고 이 범위를 초과하는 비트는 잘라냅니다. 덧셈 결과의 레지스터로의 누적도 14비트 정밀도를 사용합니다.

구현에서는 128개의 FP8xFP8 곱셈의 덧셈 결과를 CUDA 코어의 FP32 정밀도 레지스터로 누적하여 이 제한을 부분적으로 완화합니다. 성공적인 FP8 학습 달성에 도움이 되지만, 이는 Hopper 아키텍처의 FP8 GEMM 누적 정밀도 하드웨어 결함으로 인한 타협책일 뿐입니다. 향후 칩은 더 높은 정밀도를 채택해야 합니다.

**타일 및 블록별 양자화 지원**: 현재 GPU는 타일 및 블록별 양자화와 같은 세분화된 양자화에 대한 네이티브 지원이 부족하고 텐서별 양자화만 지원합니다. 현재 구현에서 $N_c$ 간격에 도달하면 부분 결과가 Tensor Core에서 CUDA 코어로 복사되어 스케일링 팩터와 곱해지고 CUDA 코어의 FP32 레지스터에 추가됩니다. 정밀한 FP32 누적 전략과 결합하여 역양자화 오버헤드가 현저히 완화되지만, Tensor Core와 CUDA 코어 간의 빈번한 데이터 이동은 여전히 계산 효율성을 제한합니다.

따라서 향후 칩이 Tensor Core가 스케일링 팩터를 받고 그룹 스케일링으로 MMA를 구현할 수 있게 하여 세분화된 양자화를 지원하기를 권장합니다. 이러한 방식으로 전체 부분합 누적과 역양자화가 최종 결과가 생성될 때까지 Tensor Core 내부에서 직접 완료되어 빈번한 데이터 이동을 피할 수 있습니다.

**온라인 양자화 지원**: 현재 구현은 연구에서 입증된 효과에도 불구하고 온라인 양자화를 효과적으로 지원하는 데 어려움을 겪습니다. 기존 과정에서는 양자화를 위해 HBM(High Bandwidth Memory)에서 128개의 BF16 활성화 값(이전 계산의 출력)을 읽어야 하고, 양자화된 FP8 값이 HBM에 다시 쓰여진 후 MMA를 위해 다시 읽혀집니다.

이 비효율성을 해결하기 위해 향후 칩이 FP8 캐스트와 TMA(Tensor Memory Accelerator) 액세스를 단일 융합 연산으로 통합하여 글로벌 메모리에서 공유 메모리로 활성화를 전송하는 동안 양자화가 완료되어 빈번한 메모리 읽기와 쓰기를 피할 수 있기를 권장합니다. 또한 더 나은 레이어 정규화와 FP8 캐스트의 융합을 촉진하는 warp 수준 캐스트 명령어 지원을 권장합니다.

대안으로 컴퓨팅 로직이 HBM 근처에 배치되는 near-memory 컴퓨팅 접근법을 채택할 수 있습니다. 이 경우 BF16 요소가 HBM에서 GPU로 읽혀질 때 직접 FP8로 캐스트되어 오프칩 메모리 액세스를 약 50% 줄일 수 있습니다.

**전치된 GEMM 연산 지원**: 현재 아키텍처는 행렬 전치를 GEMM 연산과 융합하는 것을 번거롭게 만듭니다. 워크플로우에서 순방향 패스 중 활성화는 1x128 FP8 타일로 양자화되어 저장됩니다. 역방향 패스 중에는 행렬을 읽어내고, 역양자화하고, 전치하고, 128x1 타일로 재양자화하여 HBM에 저장해야 합니다.

메모리 연산을 줄이기 위해 향후 칩이 학습과 추론 모두에서 필요한 정밀도에 대해 MMA 연산 전에 공유 메모리에서 행렬의 직접 전치 읽기를 가능하게 하기를 권장합니다. FP8 형식 변환과 TMA 액세스의 융합과 결합하여 이 향상은 양자화 워크플로우를 현저히 간소화할 것입니다.
## 사전 학습

DeepSeek-V3의 사전 학습 과정은 14.8조 개의 고품질 토큰으로 수행되었으며, 수학 및 프로그래밍 샘플의 비율을 향상시키고 다국어 지원을 확장하는 등 데이터 구성을 최적화했습니다. 이 섹션에서는 데이터 구성, 하이퍼파라미터 설정, 긴 컨텍스트 확장, 그리고 포괄적인 평가 결과에 대해 상세히 살펴보겠습니다.

### 데이터 구성

DeepSeek-V2와 비교하여 DeepSeek-V3는 수학 및 프로그래밍 샘플의 비율을 향상시키고, 영어와 중국어를 넘어 21개 언어로 다국어 커버리지를 확장하여 사전 학습 코퍼스를 최적화했습니다. 또한 데이터 처리 파이프라인을 개선하여 코퍼스 다양성을 유지하면서 중복성을 최소화했습니다.

[Ding et al. (2024)](https://arxiv.org/pdf/2404.10830)에서 영감을 받아 데이터 무결성을 위한 문서 패킹 방법을 구현했지만, 학습 중에는 교차 샘플 어텐션 마스킹을 적용하지 않았습니다. 이 접근법은 불필요한 문서 절단을 제거하면서도 학습 효율성을 유지하는 것을 목표로 합니다. 문서 패킹은 본질적으로 빈 패킹 최적화 문제로 공식화되며, 최적화된 Best-Fit-Decreasing (BFD) 알고리즘을 사용하여 해결됩니다.

구체적으로, 문서 청크 집합을 $C = \{c_1, \dots, c_N\}$이라 하고, 청크 $c$의 토큰 길이를 $l(c)$라 할 때, $l(c_i) \leq L$인 조건 하에서 $C$의 분할 $S = \{s_1, \dots, s_M\}$을 찾는 것이 목표입니다. 여기서 $\sum_{c\in s_i} l(c) \leq L$이고 분할 수 $M$이 최소화되어야 합니다. 이러한 패킹 전략을 통해 DeepSeek-V3의 최종 학습 코퍼스는 토크나이저 기준으로 14.8T개의 고품질이고 다양한 토큰으로 구성되었습니다.

**Fill-in-Middle 전략 도입**

[DeepSeekCoder-V2](https://arxiv.org/pdf/2108.07732)의 학습 과정에서 Fill-in-Middle (FIM) 전략이 다음 토큰 예측 능력을 손상시키지 않으면서 모델이 컨텍스트 단서를 기반으로 중간 텍스트를 정확하게 예측할 수 있게 한다는 것을 관찰했습니다. DeepSeekCoder-V2와 일치하게, DeepSeek-V3의 사전 학습에도 FIM 전략을 통합했습니다.

구체적으로, Prefix-Suffix-Middle (PSM) 프레임워크를 사용하여 데이터를 다음과 같이 구조화합니다.

```
<|fim_begin|>fpre<|fim_hole|>fsuf<|fim_middle|>fmiddle<|eos_token|>
```

이 구조는 사전 패킹 과정의 일부로 문서 수준에서 적용됩니다. FIM 전략은 PSM 프레임워크와 일치하게 0.1의 비율로 적용됩니다. 이는 전체 학습 데이터의 10%에서 중간 부분 예측 작업이 수행됨을 의미합니다.

**토크나이저 개선**

DeepSeek-V3의 토크나이저는 확장된 128K 토큰 어휘를 가진 [Byte-level BPE](https://arxiv.org/pdf/1808.10615)를 사용합니다. 토크나이저의 사전 토크나이저와 학습 데이터는 다국어 압축 효율성을 최적화하도록 수정되었습니다. DeepSeek-V2와 비교하여, 새로운 사전 토크나이저는 구두점과 줄바꿈을 결합하는 토큰을 도입했습니다.

그러나 이러한 기법은 모델이 터미널 줄바꿈 없이 여러 줄 프롬프트를 처리할 때, 특히 퓨 샷 평가 프롬프트에서 토큰 경계 편향을 도입할 수 있습니다. 이 문제를 해결하기 위해 학습 중에 이러한 결합된 토큰의 일정 비율을 무작위로 분할하여 모델이 더 넓은 범위의 특수 사례에 노출되도록 하고 이러한 편향을 완화했습니다.

### 하이퍼파라미터

**모델 하이퍼파라미터**

DeepSeek-V3는 61개의 트랜스포머 레이어와 7168의 은닉 차원을 설정했습니다. 모든 학습 가능한 파라미터는 0.006의 표준편차로 무작위 초기화됩니다. MLA에서는 어텐션 헤드 수 $n_h$를 128로, 헤드당 차원 $d_h$를 128로 설정했습니다. KV 압축 차원 $d_c$는 512로, 쿼리 압축 차원 $d'_c$는 1536으로 설정했습니다. 분리된 쿼리와 키의 경우, 헤드당 차원 $d_R^h$를 64로 설정했습니다.

처음 세 개 레이어를 제외한 모든 FFN을 MoE 레이어로 대체했습니다. 각 MoE 레이어는 1개의 공유 전문가와 256개의 라우팅 전문가로 구성되며, 각 전문가의 중간 은닉 차원은 2048입니다. 라우팅 전문가 중에서 각 토큰당 8개의 전문가가 활성화되며, 각 토큰은 최대 4개의 노드로 전송되도록 보장됩니다.

멀티 토큰 예측 깊이 $D$는 1로 설정되어, 정확한 다음 토큰 외에 각 토큰이 하나의 추가 토큰을 예측합니다. DeepSeek-V2와 마찬가지로, DeepSeek-V3도 압축된 잠재 벡터 이후에 추가 RMSNorm 레이어를 사용하고, 폭 병목에서 추가 스케일링 팩터를 곱합니다. 이러한 구성 하에서 DeepSeek-V3는 총 671B개의 파라미터를 포함하며, 각 토큰당 37B개가 활성화됩니다.

**학습 하이퍼파라미터**

[AdamW 옵티마이저](https://arxiv.org/pdf/1711.05101)를 사용하며 하이퍼파라미터를 $\beta_1 = 0.9$, $\beta_2 = 0.95$, weight_decay = 0.1로 설정했습니다. 사전 학습 중 최대 시퀀스 길이를 4K로 설정하고, DeepSeek-V3를 14.8T 토큰에 대해 사전 학습했습니다.

학습률 스케줄링의 경우, 처음 2K 스텝 동안 0에서 $2.2 \times 10^{-4}$까지 선형적으로 증가시킵니다. 그 다음 모델이 10T 학습 토큰을 소비할 때까지 $2.2 \times 10^{-4}$의 일정한 학습률을 유지합니다. 이후 코사인 감소 곡선을 따라 4.3T 토큰에서 학습률을 $2.2 \times 10^{-5}$로 점진적으로 감소시킵니다. 마지막 500B 토큰의 학습 동안, 처음 333B 토큰에서는 $2.2 \times 10^{-5}$의 일정한 학습률을 유지하고, 나머지 167B 토큰에서는 $7.3 \times 10^{-6}$의 다른 일정한 학습률로 전환합니다.

그래디언트 클리핑 노름은 1.0으로 설정했습니다. 배치 크기 스케줄링 전략을 사용하여, 처음 469B 토큰의 학습에서 배치 크기를 3072에서 15360으로 점진적으로 증가시키고, 나머지 학습에서는 15360을 유지합니다.

**병렬화 전략**

파이프라인 병렬성을 활용하여 모델의 서로 다른 레이어를 서로 다른 GPU에 배포하고, 각 레이어에 대해 라우팅 전문가들을 8개 노드에 속한 64개 GPU에 균등하게 배포합니다. 노드 제한 라우팅의 경우, 각 토큰은 최대 4개의 노드(즉, $M = 4$)로 전송됩니다.

보조 손실 없는 로드 밸런싱의 경우, 처음 14.3T 토큰에 대해 편향 업데이트 속도 $\gamma$를 0.001로 설정하고, 나머지 500B 토큰에 대해서는 0.0으로 설정합니다. 밸런스 손실의 경우, 단일 시퀀스 내에서 극단적인 불균형을 방지하기 위해 $\alpha$를 0.0001로 설정합니다.

MTP 손실 가중치 $\lambda$는 처음 10T 토큰에 대해 0.3으로 설정하고, 나머지 4.8T 토큰에 대해서는 0.1로 설정합니다. 이러한 가중치 조정은 학습 초기에는 멀티 토큰 예측의 영향을 크게 하고, 후기에는 안정성을 위해 그 영향을 줄이는 전략입니다.

### 긴 컨텍스트 확장

DeepSeek-V3에서 긴 컨텍스트 능력을 활성화하기 위해 [DeepSeek-V2](https://arxiv.org/pdf/2405.04434)와 유사한 접근법을 채택했습니다. 사전 학습 단계 이후, 컨텍스트 확장을 위해 [YaRN](https://arxiv.org/pdf/2309.00071)을 적용하고 두 개의 추가 학습 단계를 수행하여 컨텍스트 윈도우를 4K에서 32K로, 그 다음 128K로 점진적으로 확장했습니다. 각 단계는 1000 스텝으로 구성됩니다.

YaRN 구성은 DeepSeek-V2에서 사용된 것과 일치하며, 분리된 공유 키 $k^R$에만 적용됩니다. 하이퍼파라미터는 두 단계 모두에서 동일하게 유지되며, 스케일 $s = 40$, $\alpha = 1$, $\beta = 32$, 그리고 스케일링 팩터 $\psi_t = 0.1 \ln s + 1$로 설정됩니다.

**2단계 확장 과정**

첫 번째 단계에서는 시퀀스 길이를 32K로 설정하고 배치 크기를 1920으로 설정합니다. 두 번째 단계에서는 시퀀스 길이를 128K로 증가시키고 배치 크기를 480으로 줄입니다. 두 단계 모두의 학습률은 사전 학습 단계의 최종 학습률과 일치하는 $7.3 \times 10^{-6}$으로 설정됩니다.

YaRN의 핵심 기술적 특징은 "NTK-by-parts" 보간 방법을 사용하여 파장에 따라 RoPE 임베딩을 선택적으로 보간한다는 것입니다. 이는 기존의 "맹목적" 보간 방법들보다 우수한 성능을 보입니다. 또한 어텐션 계산에서 온도 스케일링 팩터를 도입하여 확장된 컨텍스트 윈도우 전반에 걸쳐 perplexity에 균등하게 영향을 미칩니다.

이러한 2단계 확장 학습을 통해 DeepSeek-V3는 강력한 성능을 유지하면서 최대 128K 길이의 입력을 처리할 수 있게 됩니다.

![DeepSeek-V3 128K 컨텍스트 "Needle In A Haystack" 압력 테스트](/assets/2025-10-07-deepseek-v3-technical-report/Pressure Testing DeepSeek-V3 128K Context via "Needle In A HayStack"  Chart Type: line 2K11K20K29K38K47K56K65K74K83K92K101K110K119K128Kitem_0110%10%10%10%10%10%10%10%10%10%10%10%10%10%10% Figure 8 I Evaluation results on the "Needle In A Haystack" (NIAH) tests. DeepSeek-V3 performs well across all context window lengths up to 128K.)

위 그래프는 "Needle In A Haystack" (NIAH) 테스트에서의 평가 결과를 보여줍니다. DeepSeek-V3는 지도 학습 미세 조정 이후 NIAH 테스트에서 주목할 만한 성능을 달성하며, 128K까지의 컨텍스트 윈도우 길이 전반에 걸쳐 일관된 견고성을 보여줍니다. 그래프에서 볼 수 있듯이 모든 컨텍스트 길이에서 거의 완벽한 10% 성능을 유지하고 있어, 긴 컨텍스트 처리 능력이 매우 안정적임을 확인할 수 있습니다.

### 평가

**평가 벤치마크**

DeepSeek-V3의 기본 모델은 영어와 중국어가 대부분을 차지하는 다국어 코퍼스에서 사전 학습되었으므로, 주로 영어와 중국어 벤치마크와 다국어 벤치마크에서 성능을 평가했습니다. 평가는 HAI-LLM 프레임워크에 통합된 내부 평가 프레임워크를 기반으로 수행되었습니다.

고려된 벤치마크들은 다음과 같이 분류됩니다 (밑줄 친 벤치마크는 중국어, 이중 밑줄 친 벤치마크는 다국어):

**다과목 객관식 데이터셋**: MMLU, MMLU-Redux, MMLU-Pro, MMMLU, C-Eval, CMMLU를 포함합니다. 이들은 다양한 학문 분야의 지식을 평가하는 표준화된 테스트입니다.

**언어 이해 및 추론 데이터셋**: HellaSwag, PIQA, ARC, BigBench Hard (BBH)를 포함합니다. 이들은 상식 추론과 복잡한 언어 이해 능력을 측정합니다.

**폐쇄형 질의응답 데이터셋**: TriviaQA와 NaturalQuestions를 포함합니다. 이들은 외부 지식 없이 모델의 내재된 지식을 평가합니다.

**독해 데이터셋**: RACE, DROP, C3, CMRC를 포함합니다. 이들은 주어진 텍스트를 이해하고 관련 질문에 답하는 능력을 평가합니다.

**참조 명확화 데이터셋**: CLUEWSC와 WinoGrande를 포함합니다. 이들은 대명사 해결과 같은 언어적 추론 능력을 측정합니다.

**수학 데이터셋**: [GSM8K](https://arxiv.org/pdf/2110.14168), [MATH](https://arxiv.org/pdf/2103.03874), MGSM, CMath를 포함합니다. 이들은 수학적 문제 해결 능력을 평가합니다.

**코드 데이터셋**: [HumanEval](https://arxiv.org/pdf/2107.03374), LiveCodeBench-Base, [MBPP](https://arxiv.org/pdf/2108.07732), CRUXEval을 포함합니다. 이들은 프로그래밍 능력과 코드 생성 능력을 측정합니다.

이전 연구를 따라 HellaSwag, PIQA, WinoGrande, RACE-Middle, RACE-High, MMLU, MMLU-Redux, MMLU-Pro, MMMLU, ARC-Easy, ARC-Challenge, C-Eval, CMMLU, C3, CCPM에 대해서는 perplexity 기반 평가를 채택하고, TriviaQA, NaturalQuestions, DROP, MATH, GSM8K, MGSM, HumanEval, MBPP, LiveCodeBench-Base, CRUXEval, BBH, AGIEval, CLUEWSC, CMRC, CMath에 대해서는 생성 기반 평가를 채택했습니다.

**평가 결과**

| 벤치마크 (메트릭) | # Shots | DeepSeek-V2 Base | Qwen2.5 72B Base | LLaMA-3.1 405B Base | DeepSeek-V3 Base |
|------------------|---------|------------------|-------------------|---------------------|------------------|
| **아키텍처** | - | MoE | Dense | Dense | MoE |
| **활성화 파라미터** | - | 21B | 72B | 405B | 37B |
| **총 파라미터** | - | 236B | 72B | 405B | 671B |
| **영어** | | | | | |
| Pile-test (BPB) | - | 0.606 | 0.638 | 0.542 | 0.548 |
| BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | 87.5 |
| MMLU (EM) | 5-shot | 78.4 | 85.0 | 84.4 | 87.1 |
| MMLU-Redux (EM) | 5-shot | 75.6 | 83.2 | 81.3 | 86.2 |
| MMLU-Pro (EM) | 5-shot | 51.4 | 58.3 | 52.8 | 64.4 |
| DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | 89.0 |
| ARC-Easy (EM) | 25-shot | 97.6 | 98.4 | 98.4 | 98.9 |
| ARC-Challenge (EM) | 25-shot | 92.2 | 94.5 | 95.3 | 95.3 |
| HellaSwag (EM) | 10-shot | 87.1 | 84.8 | 89.2 | 88.9 |
| PIQA (EM) | 0-shot | 83.9 | 82.6 | 85.9 | 84.7 |
| WinoGrande (EM) | 5-shot | 86.3 | 82.3 | 85.2 | 84.9 |
| TriviaQA (EM) | 5-shot | 80.0 | 71.9 | 82.7 | 82.9 |
| NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | 41.5 | 40.0 |
| AGIEval (EM) | 0-shot | 57.5 | 75.8 | 60.6 | 79.6 |
| **코드** | | | | | |
| HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | 65.2 |
| MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | 75.4 |
| LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | 19.4 |
| CRUXEval-I (EM) | 2-shot | 52.5 | 59.1 | 58.5 | 67.3 |
| CRUXEval-O (EM) | 2-shot | 49.8 | 59.9 | 59.9 | 69.8 |
| **수학** | | | | | |
| GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | 89.3 |
| MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | 61.6 |
| MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | 79.8 |
| CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | 90.7 |
| **중국어** | | | | | |
| CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | 83.0 | 82.7 |
| C-Eval (EM) | 5-shot | 81.4 | 89.2 | 72.5 | 90.1 |
| CMMLU (EM) | 5-shot | 84.0 | 89.5 | 73.7 | 88.8 |
| CMRC (EM) | 1-shot | 77.4 | 75.8 | 76.0 | 76.3 |
| C3 (EM) | 0-shot | 77.4 | 76.7 | 79.7 | 78.6 |
| CCPM (EM) | 0-shot | 93.0 | 88.5 | 78.6 | 92.0 |
| **다국어** | | | | | |
| MMMLU-non-English (EM) | 5-shot | 64.0 | 74.8 | 73.8 | 79.4 |

위 표는 DeepSeek-V3-Base와 다른 대표적인 오픈소스 기본 모델들 간의 비교를 보여줍니다. 모든 모델은 동일한 평가 설정을 공유하는 내부 평가 프레임워크에서 평가되었습니다. 0.3을 초과하지 않는 점수 차이는 동일한 수준으로 간주됩니다.

전반적으로 DeepSeek-V3-Base는 DeepSeek-V2-Base와 Qwen2.5 72B Base를 포괄적으로 능가하며, 대부분의 벤치마크에서 LLaMA-3.1 405B Base를 뛰어넘어 본질적으로 가장 강력한 오픈소스 모델이 되었습니다. 특히 수학과 코드 작업에서 최고의 성능을 달성했습니다.

더 자세한 관점에서 DeepSeek-V3-Base를 다른 오픈소스 기본 모델들과 개별적으로 비교해보면:

**(1) DeepSeek-V2-Base와의 비교**: 모델 아키텍처의 개선, 모델 크기와 학습 토큰의 확장, 데이터 품질의 향상으로 인해 DeepSeek-V3-Base는 예상대로 현저히 더 나은 성능을 달성했습니다.

**(2) Qwen2.5 72B Base와의 비교**: 최첨단 중국어 오픈소스 모델인 Qwen2.5 72B Base와 비교하여, 활성화 파라미터가 절반에 불과함에도 불구하고 DeepSeek-V3-Base는 특히 영어, 다국어, 코드, 수학 벤치마크에서 놀라운 우위를 보여줍니다. 중국어 벤치마크의 경우, 중국어 다과목 객관식 작업인 CMMLU를 제외하고는 DeepSeek-V3-Base가 Qwen2.5 72B보다 더 나은 성능을 보입니다.

**(3) LLaMA-3.1 405B Base와의 비교**: 활성화 파라미터가 11배인 가장 큰 오픈소스 모델과 비교하여, DeepSeek-V3-Base는 다국어, 코드, 수학 벤치마크에서 훨씬 더 나은 성능을 보입니다. 영어와 중국어 언어 벤치마크의 경우, DeepSeek-V3-Base는 경쟁력 있거나 더 나은 성능을 보이며, 특히 BBH, MMLU 시리즈, DROP, C-Eval, CMMLU, CCPM에서 뛰어납니다.

효율적인 아키텍처와 포괄적인 엔지니어링 최적화로 인해 DeepSeek-V3는 극도로 높은 학습 효율성을 달성합니다. 학습 프레임워크와 인프라 하에서 각 1조 토큰에 대해 DeepSeek-V3를 학습시키는 데 단 180K H800 GPU 시간만 필요하며, 이는 72B 또는 405B 밀집 모델을 학습하는 것보다 훨씬 저렴합니다.

### 논의

**멀티 토큰 예측에 대한 절제 연구**

| 벤치마크 (메트릭) | # Shots | Small MoE Baseline | Small MoE w/ MTP | Large MoE Baseline | Large MoE w/ MTP |
|------------------|---------|-------------------|------------------|-------------------|------------------|
| **활성화 파라미터 (추론)** | - | 2.4B | 2.4B | 20.9B | 20.9B |
| **총 파라미터 (추론)** | - | 15.7B | 15.7B | 228.7B | 228.7B |
| **학습 토큰** | - | 1.33T | 1.33T | 540B | 540B |
| Pile-test (BPB) | - | 0.729 | 0.729 | 0.658 | 0.657 |
| BBH (EM) | 3-shot | 39.0 | 41.4 | 70.0 | 70.7 |
| MMLU (EM) | 5-shot | 50.0 | 53.3 | 67.5 | 66.6 |
| DROP (F1) | 1-shot | 39.2 | 41.3 | 68.5 | 70.6 |
| TriviaQA (EM) | 5-shot | 56.9 | 57.7 | 67.0 | 67.3 |
| NaturalQuestions (EM) | 5-shot | 22.7 | 22.3 | 27.2 | 28.5 |
| HumanEval (Pass@1) | 0-shot | 20.7 | 26.8 | 44.5 | 53.7 |
| MBPP (Pass@1) | 3-shot | 35.8 | 36.8 | 61.6 | 62.2 |
| GSM8K (EM) | 8-shot | 25.4 | 31.4 | 72.3 | 74.0 |
| MATH (EM) | 4-shot | 10.7 | 12.6 | 38.6 | 39.8 |

위 표는 MTP 전략에 대한 절제 결과를 보여줍니다. 구체적으로, 서로 다른 규모에서 두 개의 기준 모델 위에 MTP 전략을 검증했습니다. 소규모에서는 1.33T 토큰에 대해 15.7B 총 파라미터를 포함하는 기준 MoE 모델을 학습했습니다. 대규모에서는 540B 토큰에 대해 228.7B 총 파라미터를 포함하는 기준 MoE 모델을 학습했습니다.

이들 위에 학습 데이터와 다른 아키텍처를 동일하게 유지하면서 1-깊이 MTP 모듈을 추가하고 비교를 위해 MTP 전략을 사용하는 두 모델을 학습했습니다. 추론 중에는 MTP 모듈을 직접 폐기하므로 비교된 모델들의 추론 비용은 정확히 동일합니다.

표에서 볼 수 있듯이 MTP 전략은 대부분의 평가 벤치마크에서 모델 성능을 일관되게 향상시킵니다. 특히 코딩 작업(HumanEval에서 20.7%에서 26.8%로, 대규모에서는 44.5%에서 53.7%로 향상)과 수학 작업(GSM8K에서 25.4%에서 31.4%로, 대규모에서는 72.3%에서 74.0%로 향상)에서 상당한 개선을 보입니다.

**보조 손실 없는 밸런싱 전략에 대한 절제 연구**

| 벤치마크 (메트릭) | # Shots | Small MoE Aux-Loss-Based | Small MoE Aux-Loss-Free | Large MoE Aux-Loss-Based | Large MoE Aux-Loss-Free |
|------------------|---------|--------------------------|-------------------------|--------------------------|-------------------------|
| **활성화 파라미터** | - | 2.4B | 2.4B | 20.9B | 20.9B |
| **총 파라미터** | - | 15.7B | 15.7B | 228.7B | 228.7B |
| **학습 토큰** | - | 1.33T | 1.33T | 578B | 578B |
| Pile-test (BPB) | - | 0.727 | 0.724 | 0.656 | 0.652 |
| BBH (EM) | 3-shot | 37.3 | 39.3 | 66.7 | 67.9 |
| MMLU (EM) | 5-shot | 51.0 | 51.8 | 68.3 | 67.2 |
| DROP (F1) | 1-shot | 38.1 | 39.0 | 67.1 | 67.1 |
| TriviaQA (EM) | 5-shot | 58.3 | 58.5 | 66.7 | 67.7 |
| NaturalQuestions (EM) | 5-shot | 23.2 | 23.4 | 27.1 | 28.1 |
| HumanEval (Pass@1) | 0-shot | 22.0 | 22.6 | 40.2 | 46.3 |
| MBPP (Pass@1) | 3-shot | 36.6 | 35.8 | 59.2 | 61.2 |
| GSM8K (EM) | 8-shot | 27.1 | 29.6 | 70.7 | 74.5 |
| MATH (EM) | 4-shot | 10.9 | 11.1 | 37.2 | 39.6 |

위 표는 보조 손실 없는 밸런싱 전략에 대한 절제 결과를 보여줍니다. 서로 다른 규모에서 두 개의 기준 모델 위에 이 전략을 검증했습니다. 소규모에서는 1.33T 토큰에 대해 15.7B 총 파라미터를 포함하는 기준 MoE 모델을 학습했습니다. 대규모에서는 578B 토큰에 대해 228.7B 총 파라미터를 포함하는 기준 MoE 모델을 학습했습니다.

두 기준 모델 모두 순수하게 보조 손실을 사용하여 로드 밸런스를 장려하고, top-K 친화도 정규화와 함께 시그모이드 게이팅 함수를 사용합니다. 보조 손실의 강도를 제어하는 하이퍼파라미터는 각각 DeepSeek-V2-Lite와 DeepSeek-V2와 동일합니다.

이 두 기준 모델 위에 학습 데이터와 다른 아키텍처를 동일하게 유지하면서 모든 보조 손실을 제거하고 비교를 위해 보조 손실 없는 밸런싱 전략을 도입했습니다. 표에서 볼 수 있듯이 보조 손실 없는 전략은 대부분의 평가 벤치마크에서 일관되게 더 나은 모델 성능을 달성합니다.

특히 주목할 만한 것은 HumanEval에서 대규모 모델의 경우 40.2%에서 46.3%로의 상당한 향상과 GSM8K에서 70.7%에서 74.5%로의 개선입니다. 이는 보조 손실 없는 전략이 단순히 로드 밸런싱을 개선하는 것뿐만 아니라 실제 모델 성능도 향상시킨다는 것을 보여줍니다.

**배치별 로드 밸런스 대 시퀀스별 로드 밸런스**

보조 손실 없는 밸런싱과 시퀀스별 보조 손실 간의 주요 차이점은 밸런싱 범위에 있습니다. 배치별 대 시퀀스별입니다. 시퀀스별 보조 손실과 비교하여 배치별 밸런싱은 각 시퀀스에서 도메인 내 밸런스를 강제하지 않으므로 더 유연한 제약을 부과합니다. 이러한 유연성은 전문가들이 서로 다른 도메인에서 더 잘 특화될 수 있게 합니다.

이를 검증하기 위해 Pile 테스트 세트의 서로 다른 도메인에서 16B 보조 손실 기반 기준선과 16B 보조 손실 없는 모델의 전문가 로드를 기록하고 분석했습니다.

![보조 손실 기반 및 보조 손실 없는 모델의 전문가 특화 패턴](/assets/2025-10-07-deepseek-v3-technical-report/Aux-Loss-Based Layer 9 X-Axis: Autox-Loss-Free Layer 18 Y-Axis: Relative Expert Load Chart Type: bar Wikipedia (en) - GithubDM MathematicsWikipedia (en) - DM Mathematicsitem_011Relative Expert Load1Relative Expert Load1Relative Expert Load Figure 9 I Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. Due to space constraints, we only present the results of two layers as an example, with the results of all layers provided in Appendix C.)

위 그래프에서 보듯이 보조 손실 없는 모델이 예상대로 더 큰 전문가 특화 패턴을 보여줍니다. 상대적 전문가 로드는 실제 전문가 로드와 이론적으로 균형 잡힌 전문가 로드 간의 비율을 나타냅니다. 공간 제약으로 인해 두 레이어의 결과만 예시로 제시했으며, 모든 레이어의 결과는 부록 C에서 제공됩니다.

이러한 유연성과 모델 성능에서의 우위 간의 상관관계를 더 조사하기 위해 각 시퀀스 대신 각 학습 배치에서 로드 밸런스를 장려하는 배치별 보조 손실을 추가로 설계하고 검증했습니다. 실험 결과는 유사한 수준의 배치별 로드 밸런스를 달성할 때 배치별 보조 손실도 보조 손실 없는 방법과 유사한 모델 성능을 달성할 수 있음을 보여줍니다.

구체적으로, 1B MoE 모델을 사용한 실험에서 검증 손실은 다음과 같습니다. 2.258 (시퀀스별 보조 손실 사용), 2.253 (보조 손실 없는 방법 사용), 2.253 (배치별 보조 손실 사용). 3B MoE 모델에서도 유사한 결과를 관찰했습니다. 시퀀스별 보조 손실을 사용하는 모델은 2.085의 검증 손실을 달성했고, 보조 손실 없는 방법이나 배치별 보조 손실을 사용하는 모델들은 모두 2.080의 동일한 검증 손실을 달성했습니다.

배치별 로드 밸런싱 방법들이 일관된 성능 우위를 보이지만, 효율성 측면에서 두 가지 잠재적 과제에도 직면합니다. (1) 특정 시퀀스나 작은 배치 내에서의 로드 불균형, (2) 추론 중 도메인 이동으로 인한 로드 불균형입니다. 첫 번째 과제는 대규모 전문가 병렬성과 데이터 병렬성을 사용하는 학습 프레임워크를 통해 자연스럽게 해결되며, 이는 각 마이크로 배치의 큰 크기를 보장합니다. 두 번째 과제의 경우, 이를 극복하기 위해 중복 전문가 배포를 포함한 효율적인 추론 프레임워크도 설계하고 구현했습니다.
## 사전 학습

DeepSeek-V3의 사전 학습 과정은 14.8조 개의 고품질 토큰으로 수행되었으며, 수학 및 프로그래밍 샘플의 비율을 향상시키고 다국어 지원을 확장하는 등 데이터 구성을 최적화했습니다. 이 섹션에서는 데이터 구성, 하이퍼파라미터 설정, 긴 컨텍스트 확장, 그리고 포괄적인 평가 결과에 대해 상세히 살펴보겠습니다.

### 데이터 구성

DeepSeek-V2와 비교하여 DeepSeek-V3는 수학 및 프로그래밍 샘플의 비율을 향상시키고, 영어와 중국어를 넘어 21개 언어로 다국어 커버리지를 확장하여 사전 학습 코퍼스를 최적화했습니다. 또한 데이터 처리 파이프라인을 개선하여 코퍼스 다양성을 유지하면서 중복성을 최소화했습니다.

[Ding et al. (2024)](https://arxiv.org/pdf/2404.10830)에서 영감을 받아 데이터 무결성을 위한 문서 패킹 방법을 구현했지만, 학습 중에는 교차 샘플 어텐션 마스킹을 적용하지 않았습니다. 이 접근법은 불필요한 문서 절단을 제거하면서도 학습 효율성을 유지하는 것을 목표로 합니다. 

문서 패킹의 핵심 아이디어는 매우 직관적입니다. 마치 여행용 가방에 옷을 효율적으로 넣는 것처럼, 서로 다른 길이의 문서들을 최대 시퀀스 길이라는 "가방"에 최적으로 배치하는 것입니다. 기존의 단순한 연결 방식에서는 긴 문서가 잘리는 문제가 발생했지만, 문서 패킹은 이를 해결합니다.

문서 패킹은 본질적으로 빈 패킹 최적화 문제로 공식화됩니다. 문서 청크 집합을 $C = \{c_1, \dots, c_N\}$이라 하고, 청크 $c$의 토큰 길이를 $l(c)$라 할 때, $l(c_i) \leq L$인 조건 하에서 $C$의 분할 $S = \{s_1, \dots, s_M\}$을 찾는 것이 목표입니다. 여기서 $\sum_{c\in s_i} l(c) \leq L$이고 분할 수 $M$이 최소화되어야 합니다. 

이 문제는 NP-hard이지만, Best-Fit-Decreasing (BFD) 휴리스틱을 사용하여 효율적으로 해결할 수 있습니다. 연구진은 수십억 개의 문서를 처리하기 위해 BFD 알고리즘을 최적화하여 런타임 복잡도를 $O(N \log N)$에서 $O(N \log L)$로 줄였습니다. 여기서 $L$은 최대 시퀀스 길이입니다.

이러한 패킹 전략을 통해 DeepSeek-V3의 최종 학습 코퍼스는 토크나이저 기준으로 14.8T개의 고품질이고 다양한 토큰으로 구성되었습니다. 중요한 점은 이 방법이 학습 효율성을 거의 손상시키지 않으면서도 문서 무결성을 크게 향상시킨다는 것입니다.

**Fill-in-Middle 전략 도입**

[DeepSeekCoder-V2](https://arxiv.org/pdf/2107.03374)의 학습 과정에서 Fill-in-Middle (FIM) 전략이 다음 토큰 예측 능력을 손상시키지 않으면서 모델이 컨텍스트 단서를 기반으로 중간 텍스트를 정확하게 예측할 수 있게 한다는 것을 관찰했습니다. 이는 마치 문장의 앞뒤 맥락을 보고 빈칸을 채우는 능력과 같습니다.

FIM 전략의 핵심 아이디어는 모델이 단순히 왼쪽에서 오른쪽으로만 텍스트를 생성하는 것이 아니라, 주어진 맥락에서 중간 부분을 추론할 수 있도록 학습시키는 것입니다. 이는 특히 코드 생성에서 유용한데, 프로그래머가 함수의 시작과 끝을 작성한 후 중간 로직을 채우는 경우가 많기 때문입니다.

구체적으로, Prefix-Suffix-Middle (PSM) 프레임워크를 사용하여 데이터를 다음과 같이 구조화합니다.

```
<|fim_begin|>fpre<|fim_hole|>fsuf<|fim_middle|>fmiddle<|eos_token|>
```

여기서 `fpre`는 앞부분 텍스트, `fsuf`는 뒷부분 텍스트, `fmiddle`은 중간에 채워야 할 텍스트를 나타냅니다. 이 구조는 사전 패킹 과정의 일부로 문서 수준에서 적용됩니다. FIM 전략은 PSM 프레임워크와 일치하게 0.1의 비율로 적용됩니다. 이는 전체 학습 데이터의 10%에서 중간 부분 예측 작업이 수행됨을 의미합니다.

**토크나이저 개선**

DeepSeek-V3의 토크나이저는 확장된 128K 토큰 어휘를 가진 Byte-level BPE를 사용합니다. Byte-level BPE는 모든 유니코드 문자를 바이트 수준에서 처리할 수 있어 다국어 지원에 매우 효과적입니다. 이는 마치 모든 언어를 공통된 바이트 표현으로 변환하여 처리하는 것과 같습니다.

토크나이저의 사전 토크나이저와 학습 데이터는 다국어 압축 효율성을 최적화하도록 수정되었습니다. DeepSeek-V2와 비교하여, 새로운 사전 토크나이저는 구두점과 줄바꿈을 결합하는 토큰을 도입했습니다. 예를 들어, ".\n"이나 "?\n"과 같은 패턴을 단일 토큰으로 처리하여 압축 효율성을 높입니다.

그러나 이러한 기법은 토큰 경계 편향(token boundary bias)이라는 문제를 야기할 수 있습니다. 모델이 터미널 줄바꿈 없이 여러 줄 프롬프트를 처리할 때, 특히 퓨 샷 평가 프롬프트에서 예상과 다른 토큰화가 발생할 수 있습니다. 이는 마치 문장 끝에 마침표가 없을 때 문장의 경계를 잘못 인식하는 것과 같습니다.

이 문제를 해결하기 위해 학습 중에 이러한 결합된 토큰의 일정 비율을 무작위로 분할하여 모델이 더 넓은 범위의 특수 사례에 노출되도록 했습니다. 이는 모델의 견고성을 높이고 다양한 입력 형태에 대한 적응력을 향상시킵니다.

### 하이퍼파라미터

**모델 하이퍼파라미터**

DeepSeek-V3는 61개의 트랜스포머 레이어와 7168의 은닉 차원을 설정했습니다. 이는 모델의 표현 능력과 계산 효율성 간의 균형을 고려한 설계입니다. 모든 학습 가능한 파라미터는 0.006의 표준편차로 무작위 초기화됩니다. 이는 Xavier 초기화의 변형으로, 그래디언트 소실과 폭발을 방지하면서 안정적인 학습을 보장합니다.

MLA에서는 어텐션 헤드 수 $n_h$를 128로, 헤드당 차원 $d_h$를 128로 설정했습니다. 이는 총 어텐션 차원이 $128 \times 128 = 16384$가 됨을 의미합니다. KV 압축 차원 $d_c$는 512로, 쿼리 압축 차원 $d'_c$는 1536으로 설정했습니다. 이러한 압축 차원 설정은 메모리 효율성과 성능 간의 최적 균형을 달성합니다.

분리된 쿼리와 키의 경우, 헤드당 차원 $d_R^h$를 64로 설정했습니다. 이는 RoPE(Rotary Positional Embedding)를 적용하기 위한 별도의 차원으로, 위치 정보를 효과적으로 인코딩하는 데 사용됩니다.

처음 세 개 레이어를 제외한 모든 FFN을 MoE 레이어로 대체했습니다. 이는 모델의 초기 레이어에서는 공통된 저수준 특징을 학습하고, 상위 레이어에서는 전문화된 처리를 수행한다는 직관에 기반합니다. 각 MoE 레이어는 1개의 공유 전문가와 256개의 라우팅 전문가로 구성되며, 각 전문가의 중간 은닉 차원은 2048입니다.

라우팅 전문가 중에서 각 토큰당 8개의 전문가가 활성화되며, 각 토큰은 최대 4개의 노드로 전송되도록 보장됩니다. 이는 통신 비용을 제한하면서도 충분한 전문가 다양성을 확보하는 전략입니다.

멀티 토큰 예측 깊이 $D$는 1로 설정되어, 정확한 다음 토큰 외에 각 토큰이 하나의 추가 토큰을 예측합니다. 이는 모델이 더 먼 미래를 내다보며 학습할 수 있게 하여 전반적인 성능을 향상시킵니다.

이러한 구성 하에서 DeepSeek-V3는 총 671B개의 파라미터를 포함하며, 각 토큰당 37B개가 활성화됩니다. 이는 매우 효율적인 파라미터 활용률을 보여주며, 대규모 모델의 장점을 유지하면서도 추론 비용을 크게 줄입니다.

**학습 하이퍼파라미터**

[AdamW 옵티마이저](https://arxiv.org/pdf/1711.05101)를 사용하며 하이퍼파라미터를 $\beta_1 = 0.9$, $\beta_2 = 0.95$, weight_decay = 0.1로 설정했습니다. AdamW는 기존 Adam 옵티마이저의 개선된 버전으로, 가중치 감쇠를 L2 정규화와 분리하여 더 나은 일반화 성능을 달성합니다.

AdamW의 핵심 아이디어는 적응적 학습률의 장점을 유지하면서도 SGD의 일반화 능력을 확보하는 것입니다. 기존 Adam에서는 L2 정규화와 가중치 감쇠가 동일하지 않지만, AdamW에서는 이를 명시적으로 분리하여 처리합니다. 업데이트 규칙은 다음과 같습니다.

$$\theta_{t+1} = (1-\lambda)\theta_t - \alpha \frac{m_t}{\sqrt{v_t} + \epsilon}$$

여기서 $\lambda$는 가중치 감쇠 계수, $m_t$와 $v_t$는 각각 1차 및 2차 모멘트 추정치입니다.

사전 학습 중 최대 시퀀스 길이를 4K로 설정하고, DeepSeek-V3를 14.8T 토큰에 대해 사전 학습했습니다. 이는 현재까지 공개된 오픈소스 모델 중 가장 큰 규모의 학습 데이터입니다.

학습률 스케줄링은 매우 신중하게 설계되었습니다. 처음 2K 스텝 동안 0에서 $2.2 \times 10^{-4}$까지 선형적으로 증가시키는 워밍업 단계를 거칩니다. 이는 대규모 모델에서 초기 학습 불안정성을 방지하는 중요한 기법입니다.

그 다음 모델이 10T 학습 토큰을 소비할 때까지 $2.2 \times 10^{-4}$의 일정한 학습률을 유지합니다. 이후 코사인 감소 곡선을 따라 4.3T 토큰에서 학습률을 $2.2 \times 10^{-5}$로 점진적으로 감소시킵니다. 코사인 감소는 다음 공식을 따릅니다.

$$\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})(1 + \cos(\frac{t\pi}{T}))$$

여기서 $\eta_{\max}$는 최대 학습률, $\eta_{\min}$은 최소 학습률, $T$는 총 스텝 수입니다.

마지막 500B 토큰의 학습 동안에는 더욱 세밀한 조정을 수행합니다. 처음 333B 토큰에서는 $2.2 \times 10^{-5}$의 일정한 학습률을 유지하고, 나머지 167B 토큰에서는 $7.3 \times 10^{-6}$의 더 낮은 학습률로 전환합니다. 이는 학습 후반부에서 더욱 정밀한 최적화를 가능하게 합니다.

그래디언트 클리핑 노름은 1.0으로 설정했습니다. 이는 그래디언트 폭발을 방지하고 학습 안정성을 보장하는 중요한 기법입니다. 그래디언트 클리핑은 다음과 같이 적용됩니다.

$$g \leftarrow \frac{g}{\max(1, \frac{\|g\|}{c})}$$

여기서 $g$는 그래디언트, $c$는 클리핑 임계값(1.0)입니다.

배치 크기 스케줄링 전략도 매우 흥미로운 접근법입니다. 처음 469B 토큰의 학습에서 배치 크기를 3072에서 15360으로 점진적으로 증가시키고, 나머지 학습에서는 15360을 유지합니다. 이는 학습 초기에는 작은 배치로 빠른 수렴을 도모하고, 후기에는 큰 배치로 안정적인 학습을 수행하는 전략입니다.

### 긴 컨텍스트 확장

DeepSeek-V3에서 긴 컨텍스트 능력을 활성화하기 위해 [YaRN](https://arxiv.org/pdf/2309.00071)과 유사한 접근법을 채택했습니다. 사전 학습 단계 이후, 컨텍스트 확장을 위해 YaRN을 적용하고 두 개의 추가 학습 단계를 수행하여 컨텍스트 윈도우를 4K에서 32K로, 그 다음 128K로 점진적으로 확장했습니다.

YaRN의 핵심 혁신은 RoPE 임베딩의 선택적 보간입니다. 기존의 Position Interpolation (PI) 방법이 모든 차원에 대해 동일한 보간을 적용하는 "맹목적" 접근법이었다면, YaRN은 각 차원의 파장에 따라 서로 다른 보간 전략을 사용합니다.

구체적으로, YaRN은 "NTK-by-parts" 보간 방법을 사용합니다. 각 차원 $d$에 대해 비율 $r(d) = L/\lambda_d$를 계산하며, 여기서 $L$은 확장하려는 컨텍스트 길이, $\lambda_d = 2\pi/\theta_d$는 해당 차원의 파장입니다.

- $r(d) < \alpha$인 차원들은 선형 보간을 적용
- $r(d) > \beta$인 차원들은 보간을 적용하지 않음  
- 그 사이의 차원들은 보간 램프 함수 $\gamma(r)$을 사용하여 부드럽게 전환

이러한 선택적 보간은 각 주파수 성분의 특성을 고려하여 최적의 확장을 달성합니다. 낮은 주파수 성분은 긴 범위의 의존성을 담당하므로 보간이 필요하지만, 높은 주파수 성분은 지역적 패턴을 담당하므로 보간이 불필요합니다.

또한 YaRN은 어텐션 스케일링을 도입합니다. 어텐션 계산에서 온도 스케일링 팩터 $t$를 사용하여 다음과 같이 수정합니다.

$$\text{softmax}\left(\frac{\mathbf{q}_m^T\mathbf{k}_n}{t\sqrt{|D|}}\right)$$

스케일링 팩터는 $\sqrt{1/t} = 0.1\ln(s) + 1$로 설정되며, 여기서 $s$는 확장 비율입니다.

YaRN 구성은 DeepSeek-V2에서 사용된 것과 일치하며, 분리된 공유 키 $k^R$에만 적용됩니다. 하이퍼파라미터는 두 단계 모두에서 동일하게 유지되며, 스케일 $s = 40$, $\alpha = 1$, $\beta = 32$, 그리고 스케일링 팩터 $\psi_t = 0.1 \ln s + 1$로 설정됩니다.

**2단계 확장 과정**

첫 번째 단계에서는 시퀀스 길이를 32K로 설정하고 배치 크기를 1920으로 설정합니다. 두 번째 단계에서는 시퀀스 길이를 128K로 증가시키고 배치 크기를 480으로 줄입니다. 배치 크기 감소는 메모리 제약을 고려한 것으로, 긴 시퀀스를 처리하기 위해 필요한 조치입니다.

두 단계 모두의 학습률은 사전 학습 단계의 최종 학습률과 일치하는 $7.3 \times 10^{-6}$으로 설정됩니다. 각 단계는 1000 스텝으로 구성되어 비교적 짧은 시간에 효과적인 확장을 달성합니다.

이러한 2단계 확장 학습을 통해 DeepSeek-V3는 강력한 성능을 유지하면서 최대 128K 길이의 입력을 처리할 수 있게 됩니다. 점진적 확장 방식은 모델이 각 단계에서 안정적으로 적응할 수 있도록 보장하며, 급격한 컨텍스트 길이 변화로 인한 성능 저하를 방지합니다.

![Pressure Testing DeepSeek-V3 128K Context via "Needle In A HayStack"](/assets/2025-10-07-deepseek-v3-technical-report/Chart Type: line 2K11K20K29K38K47K56K65K74K83K92K101K110K119K128Kitem_0110%10%10%10%10%10%10%10%10%10%10%10%10%10%10% Figure 8 I Evaluation results on the "Needle In A Haystack" (NIAH) tests. DeepSeek-V3 performs well across all context window lengths up to 128K.)

위 그래프는 "Needle In A Haystack" (NIAH) 테스트에서의 평가 결과를 보여줍니다. NIAH 테스트는 긴 컨텍스트에서 특정 정보를 찾는 능력을 평가하는 벤치마크로, 마치 건초더미에서 바늘을 찾는 것과 같은 작업입니다. DeepSeek-V3는 지도 학습 미세 조정 이후 NIAH 테스트에서 주목할 만한 성능을 달성하며, 128K까지의 컨텍스트 윈도우 길이 전반에 걸쳐 일관된 견고성을 보여줍니다.

그래프에서 볼 수 있듯이 모든 컨텍스트 길이에서 거의 완벽한 10% 성능을 유지하고 있어, 긴 컨텍스트 처리 능력이 매우 안정적임을 확인할 수 있습니다. 이는 YaRN 기법의 효과성과 점진적 확장 전략의 성공을 보여주는 강력한 증거입니다.
### 평가

**평가 벤치마크**

DeepSeek-V3의 기본 모델은 영어와 중국어가 대부분을 차지하는 다국어 코퍼스에서 사전 학습되었으므로, 주로 영어와 중국어 벤치마크와 다국어 벤치마크에서 성능을 평가했습니다. 평가는 HAI-LLM 프레임워크에 통합된 내부 평가 프레임워크를 기반으로 수행되었습니다.

고려된 벤치마크들은 다음과 같이 분류됩니다 (밑줄 친 벤치마크는 중국어, 이중 밑줄 친 벤치마크는 다국어):

**다과목 객관식 데이터셋**에는 MMLU, MMLU-Redux, MMLU-Pro, MMMLU, C-Eval, CMMLU가 포함됩니다. 이들은 다양한 학문 분야의 지식을 평가하는 표준화된 테스트로, 마치 대학 입시나 자격증 시험과 같은 형태입니다. 각 벤치마크는 수학, 과학, 역사, 문학 등 다양한 영역의 문제를 포함하여 모델의 종합적인 지식 수준을 측정합니다.

**언어 이해 및 추론 데이터셋**에는 HellaSwag, PIQA, ARC, BigBench Hard (BBH)가 포함됩니다. 이들은 상식 추론과 복잡한 언어 이해 능력을 측정합니다. 예를 들어, HellaSwag는 일상적인 상황에서 가장 적절한 다음 행동을 선택하는 능력을 평가하며, PIQA는 물리적 상식을 요구하는 문제들을 다룹니다.

**폐쇄형 질의응답 데이터셋**에는 TriviaQA와 NaturalQuestions가 포함됩니다. 이들은 외부 지식 없이 모델의 내재된 지식을 평가합니다. 마치 참고서 없이 시험을 보는 것과 같아서, 모델이 사전 학습 중에 얼마나 많은 사실적 지식을 습득했는지를 측정합니다.

**독해 데이터셋**에는 RACE, DROP, C3, CMRC가 포함됩니다. 이들은 주어진 텍스트를 이해하고 관련 질문에 답하는 능력을 평가합니다. DROP은 특히 수치적 추론이 필요한 독해 문제들을 포함하여, 단순한 정보 추출을 넘어선 복합적 사고 능력을 요구합니다.

**참조 명확화 데이터셋**에는 CLUEWSC와 WinoGrande가 포함됩니다. 이들은 대명사 해결과 같은 언어적 추론 능력을 측정합니다. 예를 들어, "존이 마크에게 책을 주었다. 그는 고마워했다"에서 "그"가 누구를 가리키는지 맥락을 통해 추론하는 능력을 평가합니다.

**수학 데이터셋**에는 [GSM8K](https://arxiv.org/pdf/2110.14168), [MATH](https://arxiv.org/pdf/2103.03874), MGSM, CMath가 포함됩니다. GSM8K는 초등학교 수준의 수학 문제를 다루며, 단계별 추론 과정을 통해 해결해야 합니다. MATH 데이터셋은 더욱 도전적인 고등학교 및 대학 수준의 수학 경시대회 문제들을 포함하여, 고급 수학적 추론 능력을 평가합니다.

**코드 데이터셋**에는 [HumanEval](https://arxiv.org/pdf/2107.03374), LiveCodeBench-Base, [MBPP](https://arxiv.org/pdf/2108.07732), CRUXEval이 포함됩니다. HumanEval은 함수 구현 문제를 통해 프로그래밍 능력을 측정하며, 생성된 코드가 주어진 테스트 케이스를 통과하는지 확인합니다. LiveCodeBench-Base는 실시간으로 업데이트되는 코딩 문제들을 포함하여 최신 프로그래밍 도전 과제에 대한 적응력을 평가합니다.

이전 연구를 따라 HellaSwag, PIQA, WinoGrande, RACE-Middle, RACE-High, MMLU, MMLU-Redux, MMLU-Pro, MMMLU, ARC-Easy, ARC-Challenge, C-Eval, CMMLU, C3, CCPM에 대해서는 perplexity 기반 평가를 채택했습니다. 이는 모델이 정답 선택지에 할당하는 확률을 기반으로 성능을 측정하는 방식입니다.

반면 TriviaQA, NaturalQuestions, DROP, MATH, GSM8K, MGSM, HumanEval, MBPP, LiveCodeBench-Base, CRUXEval, BBH, AGIEval, CLUEWSC, CMRC, CMath에 대해서는 생성 기반 평가를 채택했습니다. 이는 모델이 실제로 답을 생성하고, 그 답이 정답과 일치하는지 확인하는 방식입니다.

또한 Pile-test에 대해서는 언어 모델링 기반 평가를 수행하고 Bits-Per-Byte (BPB)를 메트릭으로 사용하여 서로 다른 토크나이저를 사용하는 모델들 간의 공정한 비교를 보장했습니다.

**평가 결과**

앞서 제시된 표에서 DeepSeek-V3-Base와 다른 대표적인 오픈소스 기본 모델들 간의 비교 결과를 확인할 수 있습니다. 모든 모델은 동일한 평가 설정을 공유하는 내부 평가 프레임워크에서 평가되었습니다. 0.3을 초과하지 않는 점수 차이는 동일한 수준으로 간주됩니다.

전반적으로 DeepSeek-V3-Base는 DeepSeek-V2-Base와 Qwen2.5 72B Base를 포괄적으로 능가하며, 대부분의 벤치마크에서 LLaMA-3.1 405B Base를 뛰어넘어 본질적으로 가장 강력한 오픈소스 모델이 되었습니다. 특히 수학과 코드 작업에서 최고의 성능을 달성했습니다.

더 자세한 관점에서 DeepSeek-V3-Base를 다른 오픈소스 기본 모델들과 개별적으로 비교해보면 다음과 같습니다.

**DeepSeek-V2-Base와의 비교**에서는 모델 아키텍처의 개선, 모델 크기와 학습 토큰의 확장, 데이터 품질의 향상으로 인해 DeepSeek-V3-Base가 예상대로 현저히 더 나은 성능을 달성했습니다. 특히 BBH에서 78.8%에서 87.5%로, MMLU에서 78.4%에서 87.1%로 상당한 향상을 보였습니다. 이는 아키텍처 혁신과 학습 데이터 최적화의 효과를 명확히 보여줍니다.

**Qwen2.5 72B Base와의 비교**에서는 최첨단 중국어 오픈소스 모델인 Qwen2.5 72B Base와 비교하여, 활성화 파라미터가 절반에 불과함(37B vs 72B)에도 불구하고 DeepSeek-V3-Base가 특히 영어, 다국어, 코드, 수학 벤치마크에서 놀라운 우위를 보여줍니다. 예를 들어, HumanEval에서 53.0% 대 65.2%, MATH에서 54.4% 대 61.6%의 성능 차이를 보입니다. 중국어 벤치마크의 경우, 중국어 다과목 객관식 작업인 CMMLU를 제외하고는 DeepSeek-V3-Base가 Qwen2.5 72B보다 더 나은 성능을 보입니다.

**LLaMA-3.1 405B Base와의 비교**에서는 활성화 파라미터가 11배인 가장 큰 오픈소스 모델과 비교하여, DeepSeek-V3-Base가 다국어, 코드, 수학 벤치마크에서 훨씬 더 나은 성능을 보입니다. 특히 주목할 만한 것은 MMLU-Pro에서 52.8% 대 64.4%, HumanEval에서 54.9% 대 65.2%의 성능 차이입니다. 영어와 중국어 언어 벤치마크의 경우, DeepSeek-V3-Base는 경쟁력 있거나 더 나은 성능을 보이며, 특히 BBH, MMLU 시리즈, DROP, C-Eval, CMMLU, CCPM에서 뛰어납니다.

효율적인 아키텍처와 포괄적인 엔지니어링 최적화로 인해 DeepSeek-V3는 극도로 높은 학습 효율성을 달성합니다. 학습 프레임워크와 인프라 하에서 각 1조 토큰에 대해 DeepSeek-V3를 학습시키는 데 단 180K H800 GPU 시간만 필요하며, 이는 72B 또는 405B 밀집 모델을 학습하는 것보다 훨씬 저렴합니다.

이러한 결과는 MoE 아키텍처의 효율성과 혁신적인 학습 전략의 효과를 명확히 보여줍니다. 특히 보조 손실 없는 로드 밸런싱과 멀티 토큰 예측 전략이 모델 성능 향상에 크게 기여했음을 알 수 있습니다.

### 논의

**멀티 토큰 예측에 대한 절제 연구**

앞서 제시된 MTP 전략에 대한 절제 연구 표에서 볼 수 있듯이, MTP 전략은 대부분의 평가 벤치마크에서 모델 성능을 일관되게 향상시킵니다. 이는 매우 흥미로운 결과인데, 추론 시에는 MTP 모듈을 완전히 제거함에도 불구하고 학습 중의 멀티 토큰 예측이 기본 모델의 성능을 향상시킨다는 것을 보여줍니다.

MTP의 효과를 이해하기 위해서는 그 작동 원리를 살펴볼 필요가 있습니다. 전통적인 언어 모델은 각 위치에서 다음 토큰만 예측하도록 학습됩니다. 이는 마치 체스에서 한 수만 내다보는 것과 같습니다. 반면 MTP는 모델이 현재 위치에서 여러 개의 미래 토큰을 동시에 예측하도록 학습시킵니다. 이는 모델이 더 먼 미래를 내다보며 더 나은 표현을 학습하도록 유도합니다.

특히 주목할 만한 것은 코딩 작업에서의 개선입니다. 소규모 모델에서 HumanEval 성능이 20.7%에서 26.8%로, 대규모 모델에서는 44.5%에서 53.7%로 향상되었습니다. 이는 코드 생성에서 MTP가 특히 효과적임을 시사합니다. 코드는 구조적이고 논리적인 연속성을 가지므로, 미래 토큰을 예측하는 학습이 더 나은 코드 이해와 생성 능력으로 이어질 수 있습니다.

수학 작업에서도 상당한 개선을 보였습니다. GSM8K에서 소규모 모델은 25.4%에서 31.4%로, 대규모 모델은 72.3%에서 74.0%로 향상되었습니다. 수학 문제 해결은 단계별 추론이 중요한데, MTP가 이러한 순차적 추론 능력을 향상시키는 것으로 보입니다.

MTP의 또 다른 장점은 투기적 디코딩(speculative decoding)에 활용될 수 있다는 점입니다. 학습된 MTP 모듈을 추론 시에 재활용하여 여러 토큰을 동시에 생성하고, 이를 통해 생성 속도를 향상시킬 수 있습니다.

**보조 손실 없는 밸런싱 전략에 대한 절제 연구**

앞서 제시된 보조 손실 없는 밸런싱 전략에 대한 절제 연구 표에서 볼 수 있듯이, 보조 손실 없는 전략은 대부분의 평가 벤치마크에서 일관되게 더 나은 모델 성능을 달성합니다.

전통적인 MoE 모델에서는 전문가 간 로드 밸런싱을 위해 보조 손실 함수를 사용합니다. 이는 다음과 같은 형태를 가집니다.

$$\mathcal{L}_{\text{aux}} = \alpha \sum_{i=1}^{N} f_i P_i$$

여기서 $f_i$는 $i$번째 전문가의 선택 빈도, $P_i$는 해당 전문가의 평균 게이팅 확률입니다. 이 보조 손실은 전문가들이 균등하게 사용되도록 장려하지만, 동시에 주요 학습 목표와 상충할 수 있습니다.

보조 손실 없는 전략의 핵심 아이디어는 매우 직관적입니다. 각 전문가에 대해 편향 항 $b_i$를 도입하고, 이를 라우팅 점수에 추가하여 top-K 선택을 수행합니다.

$$s'_{i,t} = s_{i,t} + b_i$$

편향 항은 각 전문가의 최근 로드를 기반으로 동적으로 조정됩니다. 과부하된 전문가의 편향은 감소시키고, 저부하된 전문가의 편향은 증가시켜 자연스러운 균형을 유도합니다. 이는 마치 교통 신호등이 교통량에 따라 신호 시간을 조정하는 것과 같습니다.

특히 주목할 만한 것은 HumanEval에서 대규모 모델의 경우 40.2%에서 46.3%로의 상당한 향상과 GSM8K에서 70.7%에서 74.5%로의 개선입니다. 이는 보조 손실 없는 전략이 단순히 로드 밸런싱을 개선하는 것뿐만 아니라 실제 모델 성능도 향상시킨다는 것을 보여줍니다.

이러한 성능 향상의 이유는 보조 손실이 제거됨으로써 모델이 주요 학습 목표에 더 집중할 수 있기 때문입니다. 보조 손실은 종종 주요 손실과 상충하는 그래디언트를 생성하여 최적화를 방해할 수 있습니다.

**배치별 로드 밸런스 대 시퀀스별 로드 밸런스**

보조 손실 없는 밸런싱과 시퀀스별 보조 손실 간의 주요 차이점은 밸런싱 범위에 있습니다. 시퀀스별 보조 손실은 각 개별 시퀀스에서 전문가 로드가 균형을 이루도록 강제합니다. 이는 매우 엄격한 제약으로, 전문가들이 특정 도메인에 특화되는 것을 방해할 수 있습니다.

반면 배치별 밸런싱은 더 유연한 제약을 부과합니다. 개별 시퀀스에서는 불균형이 허용되지만, 전체 배치 수준에서는 균형을 유지합니다. 이러한 유연성은 전문가들이 서로 다른 도메인에서 더 잘 특화될 수 있게 합니다.

앞서 제시된 전문가 특화 패턴 그래프에서 보듯이, 보조 손실 없는 모델이 예상대로 더 큰 전문가 특화 패턴을 보여줍니다. 상대적 전문가 로드는 실제 전문가 로드와 이론적으로 균형 잡힌 전문가 로드 간의 비율을 나타냅니다.

예를 들어, Wikipedia 도메인에서는 특정 전문가들이 더 높은 로드를 보이는 반면, DM Mathematics 도메인에서는 다른 전문가들이 더 활발하게 사용됩니다. 이는 전문가들이 각자의 전문 분야를 가지고 있음을 의미합니다.

이러한 전문가 특화의 이점을 더 조사하기 위해 배치별 보조 손실을 추가로 설계하고 검증했습니다. 실험 결과는 유사한 수준의 배치별 로드 밸런스를 달성할 때 배치별 보조 손실도 보조 손실 없는 방법과 유사한 모델 성능을 달성할 수 있음을 보여줍니다.

구체적으로, 1B MoE 모델을 사용한 실험에서 검증 손실은 다음과 같습니다. 2.258 (시퀀스별 보조 손실 사용), 2.253 (보조 손실 없는 방법 사용), 2.253 (배치별 보조 손실 사용). 이는 배치별 밸런싱의 핵심이 유연성에 있음을 시사합니다.

배치별 로드 밸런싱 방법들이 일관된 성능 우위를 보이지만, 효율성 측면에서 두 가지 잠재적 과제에도 직면합니다. 첫째, 특정 시퀀스나 작은 배치 내에서의 로드 불균형이 발생할 수 있습니다. 둘째, 추론 중 도메인 이동으로 인한 로드 불균형이 발생할 수 있습니다.

첫 번째 과제는 대규모 전문가 병렬성과 데이터 병렬성을 사용하는 학습 프레임워크를 통해 자연스럽게 해결됩니다. 이는 각 마이크로 배치의 큰 크기를 보장하여 통계적 안정성을 제공합니다. 두 번째 과제의 경우, 중복 전문가 배포를 포함한 효율적인 추론 프레임워크를 설계하고 구현하여 이를 극복했습니다.

이러한 연구 결과들은 DeepSeek-V3의 혁신적인 접근법들이 단순히 이론적으로 우수한 것이 아니라 실제로 측정 가능한 성능 향상을 가져온다는 것을 보여줍니다. 특히 보조 손실 없는 로드 밸런싱과 멀티 토큰 예측은 향후 대규모 언어 모델 개발에 중요한 기여를 할 것으로 예상됩니다.
## 후처리

DeepSeek-V3의 후처리 과정은 사전 학습된 기본 모델을 실제 사용자의 요구사항에 맞게 조정하는 핵심 단계입니다. 이 과정은 지도 학습 미세 조정(Supervised Fine-Tuning)과 강화 학습(Reinforcement Learning)의 두 단계로 구성되며, 특히 DeepSeek-R1 모델로부터의 지식 증류를 통해 추론 능력을 현저히 향상시켰습니다.

### 지도 학습 미세 조정

DeepSeek-V3의 지도 학습 미세 조정은 1.5M개의 인스턴스로 구성된 다양한 도메인의 데이터셋을 활용합니다. 각 도메인은 고유한 특성에 맞춘 차별화된 데이터 생성 방법을 채택하여 최적의 성능을 달성하도록 설계되었습니다.

**추론 데이터 생성 전략**

수학, 코드 경쟁 문제, 논리 퍼즐을 포함한 추론 관련 데이터셋의 경우, 내부 DeepSeek-R1 모델을 활용하여 데이터를 생성합니다. 이 과정에서 핵심적인 도전 과제는 R1이 생성한 데이터의 높은 정확도와 일반적인 형식의 추론 데이터가 가진 명확성과 간결성 사이의 균형을 맞추는 것입니다.

R1 생성 데이터는 강력한 정확도를 보여주지만, 과도한 사고(overthinking), 형식 문제, 과도한 길이 등의 문제점을 가지고 있습니다. 이러한 문제를 해결하기 위해 연구진은 혁신적인 방법론을 개발했습니다.

먼저 코드, 수학, 일반 추론과 같은 특정 도메인에 맞춤화된 전문가 모델을 개발합니다. 이 전문가 모델은 지도 학습 미세 조정과 강화 학습을 결합한 학습 파이프라인을 통해 학습되며, 최종 모델을 위한 데이터 생성기 역할을 수행합니다.

학습 과정에서는 각 인스턴스에 대해 두 가지 서로 다른 유형의 SFT 샘플을 생성합니다. 첫 번째는 문제와 원래 응답을 `<문제, 응답>` 형식으로 결합하는 것이고, 두 번째는 시스템 프롬프트와 함께 문제와 R1 응답을 `<시스템 프롬프트, 문제, R1 응답>` 형식으로 구성하는 것입니다.

시스템 프롬프트는 모델이 반성과 검증 메커니즘이 풍부한 응답을 생성하도록 안내하는 지침을 포함하도록 세심하게 설계됩니다. 이는 마치 학생에게 문제를 풀 때 단계별로 검토하고 답을 확인하라고 지시하는 것과 같습니다.

강화 학습 단계에서는 모델이 높은 온도 샘플링을 활용하여 명시적인 시스템 프롬프트가 없어도 R1 생성 데이터와 원래 데이터 모두의 패턴을 통합하는 응답을 생성합니다. 수백 번의 RL 스텝을 거친 후, 중간 RL 모델은 R1 패턴을 통합하는 방법을 학습하여 전체적인 성능을 전략적으로 향상시킵니다.

RL 학습 단계가 완료되면, 최종 모델을 위한 고품질 SFT 데이터를 선별하기 위해 거부 샘플링(rejection sampling)을 구현합니다. 이 과정에서 전문가 모델들이 데이터 생성 소스로 사용됩니다. 이 방법은 최종 학습 데이터가 DeepSeek-R1의 강점을 유지하면서도 간결하고 효과적인 응답을 생성하도록 보장합니다.

**비추론 데이터 처리**

창작 글쓰기, 역할 연기, 간단한 질의응답과 같은 비추론 데이터의 경우, DeepSeek-V2.5를 활용하여 응답을 생성하고 인간 주석자들이 데이터의 정확성과 올바름을 검증합니다. 이는 추론이 필요하지 않은 작업에서는 기존의 검증된 방법을 사용하는 것이 더 효율적이기 때문입니다.

**SFT 설정**

DeepSeek-V3-Base를 SFT 데이터셋으로 2 에포크 동안 미세 조정합니다. 학습률은 $5 \times 10^{-6}$에서 시작하여 $1 \times 10^{-6}$까지 점진적으로 감소하는 코사인 감쇠 스케줄링을 사용합니다.

학습 중에는 각 단일 시퀀스가 여러 샘플로부터 패킹되지만, 샘플 마스킹 전략을 채택하여 이러한 예시들이 격리되고 상호 간에 보이지 않도록 보장합니다. 이는 서로 다른 작업의 샘플들이 서로 간섭하지 않도록 하는 중요한 기법입니다.

### 강화 학습

DeepSeek-V3의 강화 학습 과정에서는 규칙 기반 보상 모델과 모델 기반 보상 모델을 모두 활용합니다. 이러한 이중 접근법은 다양한 유형의 작업에 대해 최적의 피드백을 제공하기 위해 설계되었습니다.

**보상 모델**

**규칙 기반 보상 모델**은 특정 규칙을 사용하여 검증할 수 있는 질문들에 대해 규칙 기반 보상 시스템을 채택하여 피드백을 결정합니다. 예를 들어, 특정 수학 문제들은 결정론적 결과를 가지며, 모델이 지정된 형식(예: 박스 안에) 내에서 최종 답을 제공하도록 요구하여 규칙을 적용해 정확성을 검증할 수 있습니다.

마찬가지로 LeetCode 문제의 경우, 컴파일러를 활용하여 테스트 케이스를 기반으로 피드백을 생성할 수 있습니다. 가능한 곳에서 규칙 기반 검증을 활용함으로써 더 높은 수준의 신뢰성을 보장할 수 있으며, 이 접근법은 조작이나 악용에 저항력이 있습니다.

**모델 기반 보상 모델**은 자유 형식의 정답이 있는 질문들에 대해 보상 모델을 사용하여 응답이 예상 정답과 일치하는지 판단합니다. 반대로, 창작 글쓰기와 같이 명확한 정답이 없는 질문들의 경우, 보상 모델은 질문과 해당 답변을 입력으로 받아 피드백을 제공하는 역할을 담당합니다.

보상 모델은 DeepSeek-V3 SFT 체크포인트로부터 학습됩니다. 신뢰성을 향상시키기 위해 최종 보상뿐만 아니라 보상에 이르는 체인 오브 소트도 포함하는 선호도 데이터를 구성합니다. 이 접근법은 특정 작업에서 보상 해킹의 위험을 완화하는 데 도움이 됩니다.

**Group Relative Policy Optimization**

DeepSeek-V2와 유사하게, 일반적으로 정책 모델과 동일한 크기의 비평 모델을 사용하는 대신 그룹 점수로부터 기준선을 추정하는 Group Relative Policy Optimization (GRPO)를 채택합니다.

구체적으로, 각 질문 $q$에 대해 GRPO는 이전 정책 모델로부터 출력 그룹 $\{o_1, o_2, \ldots, o_G\}$를 샘플링하고 다음 목표를 최대화하여 정책 모델 $\pi_\theta$를 최적화합니다.

$$J_{\text{GRPO}}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|q)} \left[ \frac{1}{G}\sum_{i=1}^{G}\left(\min\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)}A_i, \text{Clip}\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)}, 1-\varepsilon, 1+\varepsilon\right)A_i\right) - \beta D_{\text{KL}}(\pi_\theta || \pi_{\text{ref}})\right) \right]$$

여기서 $\varepsilon$과 $\beta$는 하이퍼파라미터이고, $\pi_{\text{ref}}$는 참조 모델입니다. $A_i$는 각 그룹 내 출력에 해당하는 보상 $\{r_1, r_2, \ldots, r_G\}$로부터 도출된 이점(advantage)입니다.

$$A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \ldots, r_G\})}{\text{Std}(\{r_1, r_2, \ldots, r_G\})}$$

이 공식에서 $A_i$는 개별 출력의 보상이 그룹 평균으로부터 얼마나 벗어나는지를 표준화된 형태로 나타냅니다. 이는 각 그룹 내에서 상대적으로 더 나은 출력에 더 높은 가중치를 부여하는 효과를 가집니다.

RL 과정 중에는 코딩, 수학, 글쓰기, 역할 연기, 질의응답 등 다양한 도메인의 프롬프트를 통합합니다. 이 접근법은 모델을 인간의 선호도에 더 밀접하게 정렬시킬 뿐만 아니라 벤치마크에서의 성능을 향상시키며, 특히 사용 가능한 SFT 데이터가 제한적인 시나리오에서 효과적입니다.

### 평가

**평가 설정**

기본 모델 테스트에 사용된 벤치마크 외에도, 지시 조정된 모델들을 IFEval, FRAMES, LongBench v2, GPQA, SimpleQA, C-SimpleQA, SWE-Bench Verified, Aider, LiveCodeBench(2024년 8월부터 11월까지의 문제), Codeforces, 중국 전국 고등학교 수학 올림피아드(CNMO 2024), 미국 수학 초청 시험 2024(AIME 2024)에서 추가로 평가했습니다.

비교 기준선으로는 DeepSeek-V2-0506, DeepSeek-V2.5-0905, Qwen2.5 72B Instruct, LLaMA-3.1 405B Instruct, Claude-Sonnet-3.5-1022, GPT-4o-0513을 포함한 여러 강력한 기준선들에 대해 포괄적인 평가를 수행했습니다. DeepSeek-V2 모델 시리즈의 경우, 비교를 위해 가장 대표적인 변형들을 선택했습니다. 클로즈드소스 모델들의 경우, 각각의 API를 통해 평가를 수행했습니다.

표준 벤치마크인 MMLU, DROP, GPQA, SimpleQA에 대해서는 simple-evals 프레임워크의 평가 프롬프트를 채택했습니다. MMLU-Redux의 경우 Zero-Eval 프롬프트 형식을 제로샷 설정에서 활용했습니다. 다른 데이터셋들의 경우, 데이터셋 제작자들이 제공한 기본 프롬프트와 함께 원래의 평가 프로토콜을 따랐습니다.

코드와 수학 벤치마크의 경우, HumanEval-Mul 데이터셋은 총 8개의 주류 프로그래밍 언어(Python, Java, Cpp, C#, JavaScript, TypeScript, PHP, Bash)를 포함합니다. LiveCodeBench에서는 CoT와 비CoT 방법을 모두 사용하여 모델 성능을 평가했으며, 데이터는 2024년 8월부터 11월까지 수집되었습니다. Codeforces 데이터셋은 경쟁자 백분위수를 사용하여 측정됩니다. SWE-Bench verified는 agentless 프레임워크를 사용하여 평가됩니다. Aider 관련 벤치마크는 "diff" 형식을 사용하여 평가합니다.

수학 평가의 경우, AIME과 CNMO 2024는 온도 0.7로 평가되며 결과는 16회 실행의 평균값이고, MATH-500은 탐욕적 디코딩을 사용합니다. 모든 모델이 각 벤치마크에 대해 최대 8192개의 토큰을 출력할 수 있도록 허용했습니다.

**표준 평가 결과**

앞서 제시된 비교 표에서 보듯이, DeepSeek-V3는 최고 성능의 오픈소스 모델로 자리매김했습니다. 또한 GPT-4o와 Claude-3.5-Sonnet과 같은 최첨단 클로즈드소스 모델들과 경쟁력 있는 성능을 보여줍니다.

**영어 벤치마크 성능**: MMLU는 다양한 지식 도메인과 작업에 걸쳐 대규모 언어 모델의 성능을 평가하기 위해 널리 인정받는 벤치마크입니다. DeepSeek-V3는 LLaMA-3.1-405B, GPT-4o, Claude-Sonnet 3.5와 같은 최고 수준의 모델들과 동등한 경쟁력 있는 성능을 보여주며, Qwen2.5 72B를 현저히 능가합니다.

더욱이 DeepSeek-V3는 더 도전적인 교육 지식 벤치마크인 MMLU-Pro에서 뛰어난 성능을 보이며, Claude-Sonnet 3.5에 근접한 성과를 달성합니다. 수정된 라벨을 가진 MMLU의 개선된 버전인 MMLU-Redux에서는 DeepSeek-V3가 동료들을 능가합니다.

또한 박사 수준의 평가 테스트베드인 GPQA-Diamond에서 DeepSeek-V3는 주목할 만한 결과를 달성하여 Claude 3.5 Sonnet에 이어 2위를 차지하고 다른 모든 경쟁자들을 상당한 차이로 앞섭니다.

DROP, LongBench v2, FRAMES와 같은 긴 컨텍스트 이해 벤치마크에서 DeepSeek-V3는 계속해서 최고 수준의 모델로서의 위치를 보여줍니다. DROP의 3샷 설정에서 인상적인 91.6 F1 점수를 달성하여 이 카테고리의 다른 모든 모델들을 능가합니다. 100k 토큰 컨텍스트에 대한 질의응답을 요구하는 벤치마크인 FRAMES에서는 DeepSeek-V3가 GPT-4o에 근접하면서 다른 모든 모델들을 상당한 차이로 앞섭니다. 이는 DeepSeek-V3의 극도로 긴 컨텍스트 작업 처리에서의 강력한 능력을 보여줍니다.

DeepSeek-V3의 긴 컨텍스트 능력은 DeepSeek V3 출시 불과 몇 주 전에 공개된 데이터셋인 LongBench v2에서의 최고 수준의 성능으로 더욱 검증됩니다.

사실적 지식 벤치마크인 SimpleQA에서 DeepSeek-V3는 GPT-4o와 Claude-Sonnet에 뒤처지는데, 이는 주로 설계 초점과 자원 할당 때문입니다. DeepSeek-V3는 중국어 지식 학습에 더 많은 학습 토큰을 할당하여 C-SimpleQA에서 뛰어난 성능을 달성했습니다. 지시 따르기 벤치마크에서 DeepSeek-V3는 이전 버전인 DeepSeek-V2 시리즈를 현저히 능가하여 사용자 정의 형식 제약을 이해하고 준수하는 향상된 능력을 강조합니다.

**코드와 수학 벤치마크 성능**: 코딩은 SWE-Bench-Verified와 Aider와 같은 엔지니어링 중심 작업과 HumanEval 및 LiveCodeBench와 같은 알고리즘 작업을 포괄하는 LLM에게 도전적이고 실용적인 작업입니다. 엔지니어링 작업에서 DeepSeek-V3는 Claude-Sonnet-3.5-1022에 뒤처지지만 오픈소스 모델들을 현저히 능가합니다. 오픈소스 DeepSeek-V3는 코딩 관련 엔지니어링 작업의 발전을 촉진할 것으로 예상됩니다.

알고리즘 작업에서 DeepSeek-V3는 HumanEval-Mul과 LiveCodeBench와 같은 벤치마크에서 모든 기준선을 능가하는 뛰어난 성능을 보여줍니다. 이러한 성공은 알고리즘 중심 작업에서 코드 생성과 문제 해결 능력을 효과적으로 향상시키는 고급 지식 증류 기법에 기인할 수 있습니다.

수학 벤치마크에서 DeepSeek-V3는 뛰어난 성능을 보여주며, 기준선들을 현저히 능가하고 비o1형 모델들에 대한 새로운 최첨단 성능을 설정합니다. 구체적으로, AIME, MATH-500, CNMO 2024에서 DeepSeek-V3는 두 번째로 좋은 모델인 Qwen2.5 72B를 절대 점수로 약 10% 앞서며, 이는 이러한 도전적인 벤치마크에 대한 상당한 차이입니다. 이 주목할 만한 능력은 비o1형 모델들에게 매우 유익한 것으로 입증된 DeepSeek-R1로부터의 증류 기법의 효과를 강조합니다.

**중국어 벤치마크 성능**: Qwen과 DeepSeek은 중국어와 영어 모두에 대한 강력한 지원을 가진 두 개의 대표적인 모델 시리즈입니다. 사실적 벤치마크인 Chinese SimpleQA에서 DeepSeek-V3는 Qwen2.5-72B를 16.4점 앞서며, Qwen2.5가 DeepSeek-V3의 14.8T 토큰보다 20% 많은 18T 토큰으로 구성된 더 큰 코퍼스에서 학습되었음에도 불구하고 이러한 성과를 달성했습니다.

중국어 교육 지식 평가를 위한 대표적인 벤치마크인 C-Eval과 CLUEWSC(Chinese Winograd Schema Challenge)에서 DeepSeek-V3와 Qwen2.5-72B는 유사한 성능 수준을 보여주며, 두 모델 모두 도전적인 중국어 추론과 교육 작업에 대해 잘 최적화되어 있음을 나타냅니다.

**개방형 평가**

표준 벤치마크 외에도, LLM을 판사로 사용하는 개방형 생성 작업에서도 모델들을 평가했습니다. AlpacaEval 2.0과 Arena-Hard의 원래 구성을 준수하여 GPT-4-Turbo-1106을 쌍별 비교를 위한 판사로 활용했습니다.

Arena-Hard에서 DeepSeek-V3는 기준선 GPT-4-0314에 대해 86%를 넘는 인상적인 승률을 달성하여 Claude-Sonnet-3.5-1022와 같은 최고 수준의 모델들과 동등한 성능을 보입니다. 이는 특히 코딩과 디버깅 작업을 포함한 복잡한 프롬프트를 다루는 DeepSeek-V3의 강력한 능력을 강조합니다.

더욱이 DeepSeek-V3는 Arena-Hard 벤치마크에서 85%를 넘는 첫 번째 오픈소스 모델이 되는 획기적인 이정표를 달성했습니다. 이 성취는 오픈소스와 클로즈드소스 모델 간의 성능 격차를 현저히 줄이며, 도전적인 도메인에서 오픈소스 모델이 달성할 수 있는 새로운 기준을 설정합니다.

마찬가지로 DeepSeek-V3는 AlpacaEval 2.0에서 뛰어난 성능을 보여주며, 클로즈드소스와 오픈소스 모델 모두를 능가합니다. 이는 글쓰기 작업과 간단한 질의응답 시나리오를 처리하는 뛰어난 숙련도를 보여줍니다. 특히, DeepSeek-V2.5-0905를 20%의 상당한 차이로 능가하여 간단한 작업을 다루는 데 있어서의 상당한 개선과 발전의 효과를 보여줍니다.

**생성형 보상 모델로서의 DeepSeek-V3**

DeepSeek-V3의 판단 능력을 GPT-4o와 Claude-3.5와 같은 최첨단 모델들과 비교했습니다. RewardBench에서 이러한 모델들의 성능을 보여주는 표에서 DeepSeek-V3는 최고 버전인 GPT-4o-0806과 Claude-3.5-Sonnet-1022와 동등한 성능을 달성하면서 다른 버전들을 능가합니다. 또한 DeepSeek-V3의 판단 능력은 투표 기법으로 향상될 수 있습니다. 따라서 개방형 질문에 대한 자기 피드백을 제공하기 위해 투표와 함께 DeepSeek-V3를 사용하여 정렬 과정의 효과와 견고성을 향상시킵니다.

### 논의

**DeepSeek-R1로부터의 증류**

DeepSeek-V2.5를 기반으로 DeepSeek-R1로부터의 증류 기여도를 절제 연구했습니다. 기준선은 짧은 CoT 데이터로 학습되었으며, 경쟁자는 위에서 설명한 전문가 체크포인트에 의해 생성된 데이터를 사용했습니다.

앞서 제시된 표는 증류 데이터의 효과를 보여주며, LiveCodeBench와 MATH-500 벤치마크 모두에서 상당한 개선을 보여줍니다. 실험에서 흥미로운 트레이드오프를 발견했습니다. 증류는 더 나은 성능을 가져오지만 평균 응답 길이도 상당히 증가시킵니다. 모델 정확도와 계산 효율성 간의 균형을 유지하기 위해 DeepSeek-V3에서 증류를 위한 최적 설정을 신중하게 선택했습니다.

연구 결과는 추론 모델로부터의 지식 증류가 후처리 최적화를 위한 유망한 방향을 제시한다고 제안합니다. 현재 작업이 수학과 코딩 도메인에서 데이터를 증류하는 데 초점을 맞추고 있지만, 이 접근법은 다양한 작업 도메인에 걸친 더 넓은 응용에 대한 잠재력을 보여줍니다. 이러한 특정 영역에서 보여준 효과는 긴 CoT 증류가 복잡한 추론을 요구하는 다른 인지 작업에서 모델 성능을 향상시키는 데 가치가 있을 수 있음을 나타냅니다. 다양한 도메인에 걸친 이 접근법의 추가 탐구는 향후 연구를 위한 중요한 방향으로 남아 있습니다.

**자기 보상**

보상은 RL에서 최적화 과정을 조정하는 핵심적인 역할을 합니다. 외부 도구를 통한 검증이 간단한 도메인, 예를 들어 일부 코딩이나 수학 시나리오에서 RL은 뛰어난 효과를 보여줍니다. 그러나 더 일반적인 시나리오에서는 하드 코딩을 통한 피드백 메커니즘 구축이 비실용적입니다.

DeepSeek-V3 개발 중에는 이러한 더 넓은 맥락에서 [Constitutional AI 접근법](https://arxiv.org/pdf/2212.08073)을 사용하여 DeepSeek-V3 자체의 투표 평가 결과를 피드백 소스로 활용했습니다. 이 방법은 주목할 만한 정렬 효과를 가져왔으며, 주관적 평가에서 DeepSeek-V3의 성능을 현저히 향상시켰습니다.

추가적인 헌법적 입력을 통합함으로써 DeepSeek-V3는 헌법적 방향으로 최적화할 수 있습니다. 이 패러다임은 보조 정보와 LLM을 피드백 소스로 결합하는 것이 매우 중요하다고 믿습니다. LLM은 다양한 시나리오의 비구조화된 정보를 보상으로 변환할 수 있는 다재다능한 프로세서 역할을 하여 궁극적으로 LLM의 자기 개선을 촉진합니다.

자기 보상을 넘어서, 일반적인 시나리오에서 모델 능력을 지속적으로 발전시키기 위한 다른 일반적이고 확장 가능한 보상 방법들을 발견하는 데도 전념하고 있습니다.

**멀티 토큰 예측 평가**

단일 다음 토큰만 예측하는 대신, DeepSeek-V3는 MTP 기법을 통해 다음 2개의 토큰을 예측합니다. [투기적 디코딩](https://arxiv.org/pdf/2302.01318) 프레임워크와 결합하면 모델의 디코딩 속도를 현저히 가속화할 수 있습니다. 추가로 예측된 토큰의 수용률에 대한 자연스러운 질문이 제기됩니다.

평가를 바탕으로, 두 번째 토큰 예측의 수용률은 다양한 생성 주제에 걸쳐 85%와 90% 사이에서 일관된 신뢰성을 보여줍니다. 이 높은 수용률은 DeepSeek-V3가 현저히 향상된 디코딩 속도를 달성할 수 있게 하여 1.8배의 TPS(초당 토큰 수)를 제공합니다.

이러한 결과는 MTP 기법이 단순히 학습 중 성능 향상을 위한 도구가 아니라, 실제 추론 시에도 실질적인 속도 향상을 제공할 수 있는 실용적인 기술임을 보여줍니다. 85-90%의 높은 수용률은 모델이 미래 토큰을 매우 정확하게 예측할 수 있음을 의미하며, 이는 투기적 디코딩의 효과를 극대화합니다.
## 결론, 한계, 그리고 향후 연구 방향

이 논문에서는 총 671B개의 파라미터를 가지며 각 토큰당 37B개의 파라미터가 활성화되는 대규모 MoE 언어 모델인 DeepSeek-V3를 소개했습니다. 14.8T개의 토큰으로 학습된 이 모델은 앞서 설명한 MLA와 DeepSeekMoE 아키텍처를 기반으로 하며, 로드 밸런싱을 위한 혁신적인 보조 손실 없는 전략과 더 강력한 성능을 위한 멀티 토큰 예측 학습 목표를 선도적으로 도입했습니다.

### 주요 성과와 기술적 혁신

DeepSeek-V3의 학습은 FP8 학습 지원과 세심한 엔지니어링 최적화 덕분에 매우 비용 효과적으로 이루어졌습니다. 후처리 과정에서는 DeepSeek-R1 시리즈 모델로부터 추론 능력을 성공적으로 증류하는 데 성공했습니다. 종합적인 평가 결과 DeepSeek-V3는 현재 이용 가능한 가장 강력한 오픈소스 모델로 부상했으며, GPT-4o와 Claude-3.5-Sonnet과 같은 선도적인 클로즈드소스 모델들과 비교할 만한 성능을 달성했습니다.

특히 주목할 만한 점은 뛰어난 성능에도 불구하고 경제적인 학습 비용을 유지했다는 것입니다. 사전 학습, 컨텍스트 길이 확장, 후처리를 포함한 전체 학습에 단 2.788M H800 GPU 시간만이 소요되었습니다. 또한 학습 과정이 매우 안정적이어서 전체 학습 과정에서 복구 불가능한 손실 급증이나 롤백을 경험하지 않았습니다.

### 현재의 한계점

뛰어난 성능과 비용 효과성을 인정하면서도, DeepSeek-V3가 특히 배포 측면에서 몇 가지 한계점을 가지고 있음을 인식하고 있습니다. 

첫째, 효율적인 추론을 보장하기 위해 DeepSeek-V3의 권장 배포 단위가 상대적으로 크다는 점입니다. 이는 소규모 팀에게는 부담이 될 수 있습니다. 앞서 설명한 바와 같이 prefilling 단계에서는 최소 32개의 GPU를 가진 4개 노드가 필요하고, decoding 단계에서는 320개의 GPU를 가진 40개 노드가 필요합니다. 이러한 대규모 배포 요구사항은 제한된 자원을 가진 연구팀이나 스타트업에게는 접근 장벽이 될 수 있습니다.

둘째, DeepSeek-V3의 배포 전략이 DeepSeek-V2 대비 2배 이상의 종단 간 생성 속도를 달성했음에도 불구하고, 여전히 추가적인 향상 가능성이 남아있습니다. 현재의 투기적 디코딩과 계산-통신 오버랩 기법들이 상당한 성능 향상을 가져왔지만, 더욱 정교한 최적화 기법들을 통해 추가적인 속도 향상이 가능할 것으로 예상됩니다.

다행히 이러한 한계점들은 더욱 발전된 하드웨어의 개발과 함께 자연스럽게 해결될 것으로 예상됩니다. 특히 앞서 제안한 통신 하드웨어와 컴퓨팅 하드웨어 개선사항들이 실현되면, 배포 요구사항을 줄이고 추론 성능을 더욱 향상시킬 수 있을 것입니다.

### 장기적 비전과 연구 철학

DeepSeek은 장기주의적 관점에서 오픈소스 모델의 길을 일관되게 고수하며, AGI(Artificial General Intelligence)라는 궁극적 목표에 꾸준히 접근하는 것을 목표로 하고 있습니다. 이러한 철학은 단순히 단기적인 성과를 추구하는 것이 아니라, 인공지능 기술의 민주화와 지속 가능한 발전을 통해 인류 전체에게 도움이 되는 기술을 개발하고자 하는 의지를 반영합니다.

### 향후 연구 방향

미래에는 다음과 같은 방향으로 전략적 투자를 계획하고 있습니다.

**모델 아키텍처의 지속적 연구와 개선**에서는 학습과 추론 효율성을 모두 더욱 향상시키는 것을 목표로 하며, 무한 컨텍스트 길이에 대한 효율적 지원에 접근하고자 합니다. 현재의 128K 컨텍스트 길이도 상당한 성과이지만, 실제 응용에서는 더욱 긴 컨텍스트를 효율적으로 처리할 수 있는 능력이 필요합니다. 또한 트랜스포머의 아키텍처적 한계를 돌파하여 모델링 능력의 경계를 확장하고자 합니다. 이는 현재의 어텐션 메커니즘을 넘어서는 새로운 아키텍처 패러다임의 탐구를 의미할 수 있습니다.

**학습 데이터의 양과 질에 대한 지속적 반복 개선**에서는 추가적인 학습 신호 소스의 통합을 탐구하여 더욱 포괄적인 차원에서 데이터 스케일링을 추진하고자 합니다. 현재의 14.8T 토큰도 상당한 규모이지만, 데이터의 다양성과 품질을 더욱 향상시키고, 멀티모달 데이터나 전문 도메인 데이터 등 새로운 형태의 학습 신호를 통합하는 연구가 필요합니다.

**모델의 깊은 사고 능력에 대한 지속적 탐구와 반복**에서는 추론 길이와 깊이를 확장하여 지능과 문제 해결 능력을 향상시키는 것을 목표로 합니다. 앞서 소개한 DeepSeek-R1으로부터의 지식 증류가 좋은 시작점이지만, 더욱 복잡하고 다단계적인 추론 능력을 개발하는 것이 필요합니다. 이는 단순한 패턴 매칭을 넘어서 진정한 이해와 창의적 문제 해결 능력을 갖춘 모델을 개발하는 것을 의미합니다.

**더욱 포괄적이고 다차원적인 모델 평가 방법의 탐구**에서는 연구 과정에서 고정된 벤치마크 세트에 대한 최적화 경향을 방지하고자 합니다. 현재의 벤치마크들이 유용하지만, 이들에만 의존할 경우 모델 능력에 대한 오해를 불러일으키고 기본적인 평가에 영향을 미칠 수 있습니다. 따라서 실제 사용 시나리오를 더 잘 반영하고, 모델의 다양한 능력을 종합적으로 평가할 수 있는 새로운 평가 프레임워크의 개발이 필요합니다.

이러한 연구 방향들은 모두 궁극적으로 인공 일반 지능이라는 목표를 향한 단계적 접근을 나타냅니다. DeepSeek-V3의 성과는 이러한 장기적 비전을 향한 중요한 이정표이며, 오픈소스 AI 생태계의 발전과 AI 기술의 민주화에 기여하는 의미 있는 진전을 보여줍니다.
## 저정밀도 학습을 위한 절제 연구

DeepSeek-V3의 FP8 혼합 정밀도 학습 프레임워크의 효과를 검증하기 위해 포괄적인 절제 연구를 수행했습니다. 이 연구는 FP8와 BF16 학습 간의 성능 비교, 블록별 양자화의 한계점 분석, 그리고 보조 손실 기반 모델과 보조 손실 없는 모델 간의 전문가 특화 패턴 차이를 다룹니다.

### FP8 대 BF16 학습 비교

FP8 혼합 정밀도 프레임워크의 실효성을 검증하기 위해 서로 다른 규모의 두 기준 모델에서 BF16 학습과의 비교를 수행했습니다. 소규모에서는 약 16B개의 총 파라미터를 포함하는 기준 MoE 모델을 1.33T 토큰에 대해 학습시켰습니다. 대규모에서는 약 230B개의 총 파라미터를 포함하는 기준 MoE 모델을 약 0.9T 토큰에 대해 학습시켰습니다.

![FP8와 BF16 학습 간의 손실 곡선 비교](/assets/2025-10-07-deepseek-v3-technical-report/Figure 10 I Loss curves comparison between BF16 and FP8 training. Results are smoothed by Exponential Moving Average (EMA) with a coefficient of 0.9.)

위 그래프는 BF16과 FP8 학습 간의 손실 곡선 비교를 보여줍니다. 결과는 0.9 계수를 가진 지수 이동 평균(EMA)으로 평활화되었습니다. 실험 결과는 고정밀도 누적과 세분화된 양자화 전략을 통해 상대적 오차가 0.25% 미만으로 유지됨을 보여줍니다.

이러한 결과는 매우 고무적입니다. FP8 학습이 BF16 학습과 거의 동일한 수준의 성능을 달성하면서도 메모리 사용량과 계산 비용을 현저히 줄일 수 있음을 의미합니다. 0.25%라는 상대적 오차는 학습 무작위성의 허용 범위 내에 있는 수준으로, 실질적으로 성능 저하 없이 저정밀도 학습의 이점을 누릴 수 있음을 보여줍니다.

### 블록별 양자화에 대한 논의

타일별 세분화된 양자화가 특성 이상치로 인한 오류를 효과적으로 완화하지만, 활성화 양자화에 대해 서로 다른 그룹화가 필요합니다. 순방향 패스에서는 1x128, 역방향 패스에서는 128x1 그룹화가 필요합니다. 활성화 그래디언트에 대해서도 유사한 과정이 필요합니다.

간단한 전략은 모델 가중치를 양자화하는 방식과 같이 128x128 요소당 블록별 양자화를 적용하는 것입니다. 이 방식에서는 역방향을 위해 전치만 필요합니다. 따라서 Dgrad와 관련된 모든 텐서를 블록별 기준으로 양자화하는 실험을 수행했습니다.

결과는 활성화 그래디언트를 계산하고 연쇄적 방식으로 얕은 레이어로 역전파하는 Dgrad 연산이 정밀도에 매우 민감함을 보여줍니다. 구체적으로, 활성화 그래디언트의 블록별 양자화는 약 16B개의 총 파라미터를 포함하는 MoE 모델에서 약 300B 토큰에 대해 학습할 때 모델 발산을 초래했습니다.

이러한 민감성은 활성화 그래디언트가 토큰 간에 매우 불균형하여 토큰 상관 이상치를 발생시키기 때문으로 가정됩니다. 이러한 이상치는 블록별 양자화 접근법으로는 효과적으로 관리될 수 없습니다. 이는 왜 DeepSeek-V3에서 타일별 세분화된 양자화 전략을 채택했는지를 명확히 보여주는 중요한 발견입니다.

블록별 양자화의 실패는 저정밀도 학습에서 양자화 전략의 선택이 얼마나 중요한지를 보여줍니다. 단순히 더 큰 블록으로 양자화하는 것이 항상 더 나은 것은 아니며, 데이터의 분포 특성과 그래디언트 흐름의 특성을 고려한 세심한 설계가 필요합니다.

### 16B 보조 손실 기반 및 보조 손실 없는 모델의 전문가 특화 패턴

16B 보조 손실 기반 기준선과 보조 손실 없는 모델의 전문가 로드를 Pile 테스트 세트의 서로 다른 도메인에서 기록하고 분석했습니다. 보조 손실 없는 모델이 보조 손실 기반 모델보다 더 큰 전문가 특화 패턴을 보여줍니다.

상대적 전문가 로드는 실제 전문가 로드와 이론적으로 균형 잡힌 전문가 로드 간의 비율을 나타냅니다. 공간 제약으로 인해 두 레이어의 결과만 예시로 제시되었지만, 모든 레이어에서 일관된 패턴이 관찰됩니다.

분석 결과는 매우 흥미로운 통찰을 제공합니다. 보조 손실 없는 전략을 사용한 모델에서는 각 전문가가 특정 도메인에 더욱 특화되는 경향을 보입니다. 예를 들어, Wikipedia 도메인에서는 특정 전문가들이 더 높은 활성화율을 보이는 반면, DM Mathematics 도메인에서는 다른 전문가들이 더 활발하게 사용됩니다.

이러한 전문가 특화는 모델 성능 향상에 직접적으로 기여합니다. 각 전문가가 특정 도메인의 패턴과 특성을 더 깊이 학습할 수 있게 되어, 해당 도메인의 작업에서 더 나은 성능을 발휘할 수 있습니다. 반면 보조 손실 기반 모델에서는 로드 밸런싱을 강제하는 과정에서 이러한 자연스러운 특화가 억제되어 전반적인 성능이 저하될 수 있습니다.

이는 보조 손실 없는 로드 밸런싱 전략의 핵심 장점을 보여줍니다. 단순히 계산 효율성을 위한 로드 밸런싱을 달성하는 것뿐만 아니라, 모델의 학습 능력 자체를 향상시키는 효과를 가져옵니다. 각 전문가가 자신의 "전문 분야"를 가질 수 있게 함으로써, 전체 모델의 표현 능력과 성능이 향상됩니다.

이러한 발견은 MoE 모델 설계에서 중요한 시사점을 제공합니다. 효율성과 성능 사이의 트레이드오프를 고려할 때, 단순한 균등 분배보다는 자연스러운 특화를 허용하는 것이 더 나은 결과를 가져올 수 있음을 보여줍니다.
- - -
### References
* [DeepSeek-V3 Technical Report](http://arxiv.org/pdf/2412.19437v2)