---
layout: post
title: "DeepSeek-V3 Technical Report"
date: 2024-12-27 04:03:16
author: "DeepSeek AI Research"
categories: ["Paper Reviews", "Training & Inference Optimization"]
tags: ["Auxiliary-Loss-Free-Load-Balancing", "Multi-Token-Prediction", "Multi-Head-Latent-Attention", "DeepSeekMoE-Architecture", "FP8-Mixed-Precision-Training", "Efficient-Cross-Node-All-to-All-Communication", "Computation-Communication-Overlap", "Tile-Wise-Fine-Grained-Quantization", "Node-Limited-Routing", "Speculative-Decoding"]
cover: /assets/images/default.jpg
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

대규모 언어 모델(LLM) 분야는 최근 몇 년간 급속한 발전을 거듭하며 인공 일반 지능(AGI)을 향한 중요한 진전을 이루고 있습니다. 클로즈드소스 모델들의 독점적 발전에 대응하여, 오픈소스 커뮤니티는 DeepSeek, LLaMA, Qwen, Mistral 등의 모델 시리즈를 통해 지속적으로 기술적 격차를 줄이고자 노력해왔습니다. 특히 모델의 규모와 성능을 확장하면서도 계산 효율성을 유지하는 것이 핵심 과제로 대두되었습니다.

기존 오픈소스 모델들은 대부분 계산 비용과 훈련의 복잡성으로 인해 성능 향상에 제약을 받아왔습니다. 이에 DeepSeek 연구팀은 Mixture-of-Experts(MoE) 아키텍처를 기반으로 하여, 모델의 크기를 대폭 확장하면서도 추론 효율성을 극대화할 수 있는 혁신적인 접근법을 모색하게 되었습니다. 특히 각 토큰당 활성화되는 파라미터의 수를 최적화하고, 훈련 프레임워크의 근본적인 혁신을 통해 경제적이면서도 강력한 성능의 모델을 개발하고자 했습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

DeepSeek-V3는 총 671B개의 파라미터를 가진 대규모 Mixture-of-Experts(MoE) 언어 모델로, 각 토큰당 37B개의 파라미터가 활성화되는 혁신적인 아키텍처를 제안합니다. 핵심 혁신은 Multi-head Latent Attention(MLA)과 DeepSeekMoE 아키텍처를 결합하여 효율적인 추론과 비용 효과적인 훈련을 동시에 달성한다는 점입니다. 특히 로드 밸런싱을 위한 보조 손실 없는 전략과 멀티 토큰 예측 훈련 목표를 통해 모델의 성능을 획기적으로 향상시켰습니다.

연구팀은 FP8 혼합 정밀도 훈련 프레임워크와 세분화된 양자화 전략을 도입하여 훈련의 계산 효율성을 크게 개선했습니다. 특히 DualPipe 알고리즘을 통해 파이프라인 병렬성을 최적화하고, 크로스 노드 all-to-all 통신의 오버헤드를 최소화하는 혁신적인 접근법을 개발했습니다. 이를 통해 14.8조 개의 다양하고 고품질의 토큰으로 사전 훈련을 수행하면서도 전체 훈련 비용을 경제적으로 유지할 수 있었습니다.

#### 제안된 방법은 어떻게 구현되었습니까?

DeepSeek-V3의 구현은 크게 세 단계로 나뉩니다. 첫째, 사전 훈련 단계에서는 14.8조 개의 토큰으로 구성된 고품질 다국어 코퍼스를 사용했으며, Fill-in-Middle(FIM) 전략과 128K 토큰 어휘를 가진 Byte-level BPE 토크나이저를 도입했습니다. 둘째, 컨텍스트 길이 확장을 위해 YaRN 기법을 적용하여 4K에서 128K 토큰까지 단계적으로 모델의 컨텍스트 윈도우를 확장했습니다.

후처리 단계에서는 지도 학습 미세 조정(SFT)과 강화 학습(RL)을 통해 모델의 성능을 최적화했습니다. 특히 DeepSeek-R1 모델로부터 추론 능력을 증류하는 혁신적인 방법론을 도입하여 모델의 수학, 코딩, 추론 능력을 크게 향상시켰습니다. Group Relative Policy Optimization(GRPO) 기법을 활용하여 다양한 도메인의 프롬프트에 대해 모델을 인간의 선호도에 정렬시켰습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?

DeepSeek-V3는 현재 가장 강력한 오픈소스 대규모 언어 모델로 자리 잡았으며, GPT-4o와 Claude-Sonnet-3.5와 같은 클로즈드소스 모델들과 비교할 만한 성능을 달성했습니다. MMLU에서 88.5점, MMLU-Pro에서 75.9점, GPQA에서 59.1점을 기록하며 지식 영역에서 탁월한 성능을 보였습니다. 특히 코드, 수학, 추론 분야에서 최첨단 성능을 달성하여 오픈소스 모델의 새로운 가능성을 제시했습니다.

더욱 주목할 만한 점은 이러한 뛰어난 성능을 단 2.788M H800 GPU 시간, 총 $5.576M의 경제적인 훈련 비용으로 달성했다는 것입니다. 이는 알고리즘, 프레임워크, 하드웨어의 혁신적인 공동 설계를 통해 가능했으며, 대규모 언어 모델 훈련의 새로운 패러다임을 제시합니다. 연구팀은 오픈소스 AI 생태계의 발전에 기여하고, AGI를 향한 중요한 이정표를 마련했다는 점에서 이 연구의 의의를 찾을 수 있습니다.
- - -
# DeepSeek-V3 기술 보고서

## 초록

DeepSeek-V3는 총 671B개의 파라미터를 가지며 각 토큰당 37B개의 파라미터가 활성화되는 강력한 Mixture-of-Experts (MoE) 언어 모델입니다. 효율적인 추론과 비용 효과적인 훈련을 달성하기 위해 DeepSeek-V3는 DeepSeek-V2에서 철저히 검증된 Multi-head Latent Attention (MLA)과 DeepSeekMoE 아키텍처를 채택합니다.

![벤치마크 성능 비교](https://arxiv.org/html/2412.19437/x1.png)

더 나아가 DeepSeek-V3는 로드 밸런싱을 위한 보조 손실 없는 전략을 개척하고 더 강력한 성능을 위한 멀티 토큰 예측 훈련 목표를 설정합니다. 14.8조 개의 다양하고 고품질 토큰으로 사전 훈련을 수행한 후, 지도 학습 미세 조정과 강화 학습 단계를 통해 모델의 능력을 완전히 활용합니다.

포괄적인 평가 결과 DeepSeek-V3는 다른 오픈소스 모델들을 능가하며 선도적인 클로즈드소스 모델들과 비교할 만한 성능을 달성합니다. 뛰어난 성능에도 불구하고 DeepSeek-V3는 전체 훈련에 단 2.788M H800 GPU 시간만을 필요로 합니다. 또한 훈련 과정이 매우 안정적이어서 전체 훈련 과정에서 복구 불가능한 손실 급증이나 롤백을 경험하지 않았습니다.

## 서론

최근 몇 년간 대규모 언어 모델(LLM)은 급속한 반복과 진화를 겪으며 인공 일반 지능(AGI)을 향한 격차를 점진적으로 줄여나가고 있습니다. 클로즈드소스 모델들 외에도 DeepSeek 시리즈, LLaMA 시리즈, Qwen 시리즈, Mistral 시리즈를 포함한 오픈소스 모델들도 상당한 진전을 이루며 클로즈드소스 모델들과의 격차를 줄이기 위해 노력하고 있습니다.

오픈소스 모델 능력의 경계를 더욱 확장하기 위해 모델을 확장하여 671B 파라미터를 가진 대규모 Mixture-of-Experts (MoE) 모델인 DeepSeek-V3를 소개합니다. 각 토큰당 37B개의 파라미터가 활성화됩니다. 미래 지향적인 관점에서 강력한 모델 성능과 경제적 비용을 지속적으로 추구합니다.

### 아키텍처 혁신

아키텍처 측면에서 DeepSeek-V3는 여전히 효율적인 추론을 위한 Multi-head Latent Attention (MLA)과 비용 효과적인 훈련을 위한 DeepSeekMoE를 채택합니다. 이 두 아키텍처는 DeepSeek-V2에서 검증되어 효율적인 훈련과 추론을 달성하면서도 견고한 모델 성능을 유지할 수 있는 능력을 입증했습니다.

기본 아키텍처를 넘어서 모델 능력을 더욱 향상시키기 위해 두 가지 추가 전략을 구현합니다. 첫째, DeepSeek-V3는 로드 밸런싱을 장려하려는 노력에서 발생하는 모델 성능에 대한 부정적 영향을 최소화하는 것을 목표로 하는 보조 손실 없는 전략을 개척합니다. 둘째, DeepSeek-V3는 평가 벤치마크에서 전반적인 성능을 향상시키는 것으로 관찰된 멀티 토큰 예측 훈련 목표를 사용합니다.

### 효율적인 훈련 프레임워크

효율적인 훈련을 달성하기 위해 FP8 혼합 정밀도 훈련을 지원하고 훈련 프레임워크에 대한 포괄적인 최적화를 구현합니다. 저정밀도 훈련은 효율적인 훈련을 위한 유망한 솔루션으로 부상했으며, 그 발전은 하드웨어 능력의 발전과 밀접하게 연결되어 있습니다. 이 연구에서는 FP8 혼합 정밀도 훈련 프레임워크를 도입하고 극도로 대규모 모델에서 처음으로 그 효과를 검증합니다.

FP8 계산과 저장 지원을 통해 훈련 가속화와 GPU 메모리 사용량 감소를 모두 달성합니다. 훈련 프레임워크의 경우, 파이프라인 버블이 적고 훈련 중 계산-통신 오버랩을 통해 대부분의 통신을 숨기는 효율적인 파이프라인 병렬성을 위한 DualPipe 알고리즘을 설계합니다.

이러한 오버랩은 모델이 더욱 확장되더라도 일정한 계산 대 통신 비율을 유지하는 한, 거의 제로에 가까운 all-to-all 통신 오버헤드를 달성하면서 노드 간 세분화된 전문가를 여전히 사용할 수 있도록 보장합니다. 또한 InfiniBand (IB)와 NVLink 대역폭을 완전히 활용하기 위한 효율적인 크로스 노드 all-to-all 통신 커널도 개발합니다.

더 나아가 메모리 사용량을 세심하게 최적화하여 비용이 많이 드는 텐서 병렬성을 사용하지 않고도 DeepSeek-V3를 훈련할 수 있게 만듭니다. 이러한 노력들을 결합하여 높은 훈련 효율성을 달성합니다.

### 훈련 과정과 비용

사전 훈련 동안 14.8T개의 고품질이고 다양한 토큰으로 DeepSeek-V3를 훈련합니다. 사전 훈련 과정은 매우 안정적입니다. 전체 훈련 과정에서 복구 불가능한 손실 급증을 경험하거나 롤백을 수행할 필요가 없었습니다. 이후 DeepSeek-V3에 대해 2단계 컨텍스트 길이 확장을 수행합니다. 첫 번째 단계에서는 최대 컨텍스트 길이가 32K로 확장되고, 두 번째 단계에서는 128K로 더욱 확장됩니다.

이어서 DeepSeek-V3의 기본 모델에 대해 지도 학습 미세 조정(SFT)과 강화 학습(RL)을 포함한 후처리를 수행하여 인간의 선호도에 맞추고 잠재력을 더욱 발휘합니다. 후처리 단계에서는 DeepSeek-R1 시리즈 모델로부터 추론 능력을 증류하고, 동시에 모델 정확도와 생성 길이 간의 균형을 신중하게 유지합니다.

| 훈련 단계 | 사전 훈련 | 컨텍스트 확장 | 후처리 | 총합 |
|----------|----------|-------------|--------|------|
| H800 GPU 시간 | 2664K | 119K | 5K | 2788K |
| USD 비용 | $5.328M | $0.238M | $0.01M | $5.576M |

DeepSeek-V3의 경제적인 훈련 비용을 다시 한 번 강조하며, 이는 알고리즘, 프레임워크, 하드웨어의 최적화된 공동 설계를 통해 달성되었습니다. 사전 훈련 단계에서 각 조 토큰에 대해 DeepSeek-V3를 훈련하는 데 단 180K H800 GPU 시간만 필요하며, 이는 2048개의 H800 GPU를 가진 클러스터에서 3.7일에 해당합니다.

결과적으로 사전 훈련 단계는 2개월 미만에 완료되며 2664K GPU 시간이 소요됩니다. 컨텍스트 길이 확장을 위한 119K GPU 시간과 후처리를 위한 5K GPU 시간을 합하면, DeepSeek-V3는 전체 훈련에 단 2.788M GPU 시간만 소요됩니다. H800 GPU의 임대 가격을 시간당 $2로 가정하면, 총 훈련 비용은 단 $5.576M에 불과합니다.

### 주요 기여사항

**아키텍처: 혁신적인 로드 밸런싱 전략과 훈련 목표**
- DeepSeek-V2의 효율적인 아키텍처를 기반으로 로드 밸런싱을 장려하는 데서 발생하는 성능 저하를 최소화하는 보조 손실 없는 전략을 개척합니다.
- 멀티 토큰 예측(MTP) 목표를 조사하고 모델 성능에 유익함을 증명합니다. 이는 추론 가속을 위한 투기적 디코딩에도 사용될 수 있습니다.

**사전 훈련: 궁극적인 훈련 효율성을 향해**
- FP8 혼합 정밀도 훈련 프레임워크를 설계하고 극도로 대규모 모델에서 처음으로 FP8 훈련의 실현 가능성과 효과를 검증합니다.
- 알고리즘, 프레임워크, 하드웨어의 공동 설계를 통해 크로스 노드 MoE 훈련에서 통신 병목 현상을 극복하여 거의 완전한 계산-통신 오버랩을 달성합니다.
- 단 2.664M H800 GPU 시간의 경제적 비용으로 14.8T 토큰에 대한 DeepSeek-V3의 사전 훈련을 완료하여 현재 가장 강력한 오픈소스 기본 모델을 생산합니다.

**후처리: DeepSeek-R1로부터의 지식 증류**
- 긴 체인 오브 소트(CoT) 모델, 특히 DeepSeek R1 시리즈 모델 중 하나로부터 표준 LLM, 특히 DeepSeek-V3로 추론 능력을 증류하는 혁신적인 방법론을 도입합니다.

### 핵심 평가 결과 요약

**지식 영역**에서 MMLU, MMLU-Pro, GPQA와 같은 교육 벤치마크에서 DeepSeek-V3는 모든 다른 오픈소스 모델들을 능가하며, MMLU에서 88.5, MMLU-Pro에서 75.9, GPQA에서 59.1을 달성합니다. 그 성능은 GPT-4o와 Claude-Sonnet-3.5와 같은 선도적인 클로즈드소스 모델들과 비교할 만하여 이 영역에서 오픈소스와 클로즈드소스 모델 간의 격차를 좁힙니다.

**코드, 수학, 추론** 영역에서 DeepSeek-V3는 모든 비긴 CoT 오픈소스 및 클로즈드소스 모델들 중에서 수학 관련 벤치마크에서 최첨단 성능을 달성합니다. 특히 MATH-500과 같은 특정 벤치마크에서 o1-preview를 능가하여 견고한 수학적 추론 능력을 보여줍니다.

코딩 관련 작업에서 DeepSeek-V3는 LiveCodeBench와 같은 코딩 경쟁 벤치마크에서 최고 성능 모델로 부상하여 이 영역에서의 선도적 위치를 확고히 합니다. 엔지니어링 관련 작업에서는 Claude-Sonnet-3.5보다 약간 낮은 성능을 보이지만, 여전히 다른 모든 모델들을 상당한 차이로 앞서며 다양한 기술 벤치마크에서의 경쟁력을 보여줍니다.
## 아키텍처

DeepSeek-V3는 효율적인 추론을 위한 Multi-head Latent Attention (MLA)과 경제적인 훈련을 위한 DeepSeekMoE를 특징으로 하는 기본 아키텍처를 먼저 소개합니다. 이후 평가 벤치마크에서 전반적인 성능을 향상시키는 것으로 관찰된 Multi-Token Prediction (MTP) 훈련 목표를 제시합니다. 명시적으로 언급되지 않은 다른 세부 사항들에 대해서는 DeepSeek-V3가 DeepSeek-V2의 설정을 따릅니다.

![DeepSeek-V3 기본 아키텍처](https://arxiv.org/html/2412.19437/x2.png)

위 그림은 DeepSeek-V3의 기본 아키텍처를 보여줍니다. DeepSeek-V2를 따라 효율적인 추론과 경제적인 훈련을 위해 MLA와 DeepSeekMoE를 채택합니다. 이 아키텍처는 Feed-Forward Network, RMSNorm, Attention, 그리고 RMSNorm 구성 요소들로 이루어져 있으며, Multi-Head Latent Attention (MLA) 모듈과 Routed Expert 및 Shared Expert 구성 요소들을 포함합니다.

### 기본 아키텍처

DeepSeek-V3의 기본 아키텍처는 여전히 Transformer 프레임워크 내에 있습니다. 효율적인 추론과 경제적인 훈련을 위해 DeepSeek-V3는 DeepSeek-V2에서 철저히 검증된 MLA와 DeepSeekMoE를 채택합니다. DeepSeek-V2와 비교했을 때 예외적인 점은 로드 밸런스를 보장하려는 노력으로 인한 성능 저하를 완화하기 위해 DeepSeekMoE에 대한 보조 손실 없는 로드 밸런싱 전략을 추가로 도입한다는 것입니다.

#### Multi-Head Latent Attention

어텐션의 경우 DeepSeek-V3는 MLA 아키텍처를 채택합니다. 이 메커니즘을 이해하기 위해 먼저 기본 개념부터 살펴보겠습니다. 전통적인 Multi-Head Attention에서는 각 토큰에 대해 Query, Key, Value 벡터들을 생성하고 이들을 모두 메모리에 저장해야 합니다. 하지만 이는 추론 시 상당한 메모리 사용량을 초래합니다.

MLA의 핵심 아이디어는 Key와 Value를 저차원 공간으로 압축하여 저장하는 것입니다. 마치 고해상도 이미지를 압축하여 저장한 후 필요할 때 복원하는 것과 유사한 개념입니다. 이를 통해 메모리 사용량을 크게 줄이면서도 성능은 유지할 수 있습니다.

$d$를 임베딩 차원, $n_h$를 어텐션 헤드 수, $d_h$를 헤드당 차원, $\mathbf{h}_t \in \mathbb{R}^d$를 주어진 어텐션 레이어에서 $t$번째 토큰의 어텐션 입력이라고 하겠습니다. MLA의 핵심은 추론 중 Key-Value (KV) 캐시를 줄이기 위한 어텐션 키와 값의 저차원 결합 압축입니다.

$$\boxed{\color[rgb]{0,0,1}\mathbf{c}_t^{KV}} = W^{DKV}\mathbf{h}_t,$$

$$[\mathbf{k}_{t,1}^C;\mathbf{k}_{t,2}^C;...;\mathbf{k}_{t,n_h}^C] = \mathbf{k}_t^C = W^{UK}\mathbf{c}_t^{KV},$$

$$\boxed{\color[rgb]{0,0,1}\mathbf{k}_t^R} = \operatorname{RoPE}(W^{KR}\mathbf{h}_t),$$

$$\mathbf{k}_{t,i} = [\mathbf{k}_{t,i}^C;\mathbf{k}_t^R],$$

$$[\mathbf{v}_{t,1}^C;\mathbf{v}_{t,2}^C;...;\mathbf{v}_{t,n_h}^C] = \mathbf{v}_t^C = W^{UV}\mathbf{c}_t^{KV}$$

여기서 $\mathbf{c}_t^{KV} \in \mathbb{R}^{d_c}$는 키와 값을 위한 압축된 잠재 벡터이고, $d_c(\ll d_h n_h)$는 KV 압축 차원을 나타냅니다. $W^{DKV} \in \mathbb{R}^{d_c \times d}$는 다운 프로젝션 행렬이며, $W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$는 각각 키와 값을 위한 업 프로젝션 행렬입니다. $W^{KR} \in \mathbb{R}^{d_h^R \times d}$는 Rotary Positional Embedding (RoPE)을 담는 분리된 키를 생성하는 데 사용되는 행렬입니다.

이 과정을 단계별로 이해해보겠습니다. 먼저 입력 $\mathbf{h}_t$를 저차원 공간 $\mathbf{c}_t^{KV}$로 압축합니다. 이는 정보의 핵심만을 추출하는 과정입니다. 그 다음 이 압축된 표현을 사용하여 각 헤드별 키와 값을 복원합니다. 동시에 위치 정보를 담는 별도의 키 성분 $\mathbf{k}_t^R$을 생성하여 최종 키에 결합합니다.

MLA에서는 파란색 박스로 표시된 벡터들($\mathbf{c}_t^{KV}$와 $\mathbf{k}_t^R$)만 생성 중에 캐시되어야 하므로, 표준 Multi-Head Attention (MHA)과 비교할 만한 성능을 유지하면서 KV 캐시가 크게 줄어듭니다.

어텐션 쿼리의 경우에도 훈련 중 활성화 메모리를 줄일 수 있는 저차원 압축을 수행합니다.

$$\mathbf{c}_t^Q = W^{DQ}\mathbf{h}_t,$$

$$[\mathbf{q}_{t,1}^C;\mathbf{q}_{t,2}^C;...;\mathbf{q}_{t,n_h}^C] = \mathbf{q}_t^C = W^{UQ}\mathbf{c}_t^Q,$$

$$[\mathbf{q}_{t,1}^R;\mathbf{q}_{t,2}^R;...;\mathbf{q}_{t,n_h}^R] = \mathbf{q}_t^R = \operatorname{RoPE}(W^{QR}\mathbf{c}_t^Q),$$

$$\mathbf{q}_{t,i} = [\mathbf{q}_{t,i}^C;\mathbf{q}_{t,i}^R]$$

여기서 $\mathbf{c}_t^Q \in \mathbb{R}^{d_c'}$는 쿼리를 위한 압축된 잠재 벡터이고, $d_c'(\ll d_h n_h)$는 쿼리 압축 차원을 나타냅니다. $W^{DQ} \in \mathbb{R}^{d_c' \times d}, W^{UQ} \in \mathbb{R}^{d_h n_h \times d_c'}$는 각각 쿼리를 위한 다운 프로젝션과 업 프로젝션 행렬이며, $W^{QR} \in \mathbb{R}^{d_h^R n_h \times d_c'}$는 RoPE를 담는 분리된 쿼리를 생성하는 행렬입니다.

최종적으로 어텐션 쿼리($\mathbf{q}_{t,i}$), 키($\mathbf{k}_{j,i}$), 값($\mathbf{v}_{j,i}^C$)이 결합되어 최종 어텐션 출력 $\mathbf{u}_t$를 생성합니다.

$$\mathbf{o}_{t,i} = \sum_{j=1}^{t}\operatorname{Softmax}_j\left(\frac{\mathbf{q}_{t,i}^T\mathbf{k}_{j,i}}{\sqrt{d_h+d_h^R}}\right)\mathbf{v}_{j,i}^C,$$

$$\mathbf{u}_t = W^O[\mathbf{o}_{t,1};\mathbf{o}_{t,2};...;\mathbf{o}_{t,n_h}]$$

여기서 $W^O \in \mathbb{R}^{d \times d_h n_h}$는 출력 프로젝션 행렬을 나타냅니다.

#### 보조 손실 없는 로드 밸런싱을 갖춘 DeepSeekMoE

**DeepSeekMoE의 기본 아키텍처**

Feed-Forward Networks (FFNs)의 경우 DeepSeek-V3는 DeepSeekMoE 아키텍처를 사용합니다. 전통적인 MoE 아키텍처와 비교하여 DeepSeekMoE는 더 세분화된 전문가를 사용하고 일부 전문가를 공유 전문가로 분리합니다.

MoE의 기본 개념을 이해해보겠습니다. 하나의 큰 네트워크 대신 여러 개의 작은 "전문가" 네트워크를 두고, 각 입력에 대해 가장 적합한 전문가들을 선택하여 처리하는 방식입니다. 이는 마치 다양한 분야의 전문가들이 있는 상담센터에서 문의 내용에 따라 적절한 전문가에게 연결해주는 것과 유사합니다.

$\mathbf{u}_t$를 $t$번째 토큰의 FFN 입력이라고 하면, FFN 출력 $\mathbf{h}_t'$를 다음과 같이 계산합니다.

$$\mathbf{h}_t' = \mathbf{u}_t + \sum_{i=1}^{N_s}\operatorname{FFN}^{(s)}_i(\mathbf{u}_t) + \sum_{i=1}^{N_r}g_{i,t}\operatorname{FFN}^{(r)}_i(\mathbf{u}_t),$$

$$g_{i,t} = \frac{g'_{i,t}}{\sum_{j=1}^{N_r}g'_{j,t}},$$

$$g'_{i,t} = \begin{cases}
s_{i,t}, & s_{i,t} \in \operatorname{Topk}(\{s_{j,t}|1 \leqslant j \leqslant N_r\}, K_r), \\
0, & \text{otherwise},
\end{cases}$$

$$s_{i,t} = \operatorname{Sigmoid}(\mathbf{u}_t^T\mathbf{e}_i)$$

여기서 $N_s$와 $N_r$은 각각 공유 전문가와 라우팅 전문가의 수를 나타냅니다. $\operatorname{FFN}^{(s)}_i(\cdot)$와 $\operatorname{FFN}^{(r)}_i(\cdot)$는 각각 $i$번째 공유 전문가와 $i$번째 라우팅 전문가를 나타냅니다. $K_r$은 활성화된 라우팅 전문가의 수이고, $g_{i,t}$는 $i$번째 전문가에 대한 게이팅 값입니다. $s_{i,t}$는 토큰-전문가 친화도이며, $\mathbf{e}_i$는 $i$번째 라우팅 전문가의 중심 벡터입니다.

DeepSeek-V2와 약간 다른 점은 DeepSeek-V3가 친화도 점수를 계산하기 위해 시그모이드 함수를 사용하고, 선택된 모든 친화도 점수들 간의 정규화를 적용하여 게이팅 값을 생성한다는 것입니다.

**보조 손실 없는 로드 밸런싱**

MoE 모델에서 불균형한 전문가 로드는 라우팅 붕괴를 초래하고 전문가 병렬성이 있는 시나리오에서 계산 효율성을 저하시킵니다. 기존 솔루션들은 일반적으로 불균형한 로드를 피하기 위해 보조 손실에 의존합니다. 하지만 너무 큰 보조 손실은 모델 성능을 저해합니다.

로드 밸런스와 모델 성능 간의 더 나은 균형을 달성하기 위해 로드 밸런스를 보장하는 보조 손실 없는 로드 밸런싱 전략을 개척합니다. 구체적으로 각 전문가에 대해 편향 항 $b_i$를 도입하고 이를 해당 친화도 점수 $s_{i,t}$에 추가하여 top-K 라우팅을 결정합니다.

$$g'_{i,t} = \begin{cases}
s_{i,t}, & s_{i,t}+b_i \in \operatorname{Topk}(\{s_{j,t}+b_j|1 \leqslant j \leqslant N_r\}, K_r), \\
0, & \text{otherwise}.
\end{cases}$$

편향 항은 라우팅에만 사용됩니다. FFN 출력과 곱해질 게이팅 값은 여전히 원래 친화도 점수 $s_{i,t}$에서 파생됩니다. 훈련 중에는 각 훈련 단계의 전체 배치에서 전문가 로드를 계속 모니터링합니다. 각 단계가 끝날 때 해당 전문가가 과부하 상태이면 편향 항을 $\gamma$만큼 감소시키고, 해당 전문가가 부족 상태이면 $\gamma$만큼 증가시킵니다. 여기서 $\gamma$는 편향 업데이트 속도라고 하는 하이퍼파라미터입니다.

이러한 동적 조정을 통해 DeepSeek-V3는 훈련 중 균형 잡힌 전문가 로드를 유지하고, 순수한 보조 손실을 통해 로드 밸런스를 장려하는 모델들보다 더 나은 성능을 달성합니다.

**보완적 시퀀스별 보조 손실**

DeepSeek-V3는 주로 로드 밸런스를 위한 보조 손실 없는 전략에 의존하지만, 단일 시퀀스 내에서 극단적인 불균형을 방지하기 위해 보완적 시퀀스별 밸런스 손실도 사용합니다.

$$\mathcal{L}_{\mathrm{Bal}} = \alpha\sum_{i=1}^{N_r}f_i P_i,$$

$$f_i = \frac{N_r}{K_r T}\sum_{t=1}^{T}\mathds{1}(s_{i,t} \in \operatorname{Topk}(\{s_{j,t}|1 \leqslant j \leqslant N_r\}, K_r)),$$

$$s'_{i,t} = \frac{s_{i,t}}{\sum_{j=1}^{N_r}s_{j,t}},$$

$$P_i = \frac{1}{T}\sum_{t=1}^{T}s'_{i,t}$$

여기서 밸런스 인수 $\alpha$는 DeepSeek-V3에 대해 극도로 작은 값이 할당될 하이퍼파라미터입니다. $\mathds{1}(\cdot)$는 지시 함수이고, $T$는 시퀀스의 토큰 수를 나타냅니다. 시퀀스별 밸런스 손실은 각 시퀀스에서 전문가 로드가 균형을 이루도록 장려합니다.

**노드 제한 라우팅**

DeepSeek-V2에서 사용된 디바이스 제한 라우팅과 마찬가지로 DeepSeek-V3도 훈련 중 통신 비용을 제한하기 위해 제한된 라우팅 메커니즘을 사용합니다. 간단히 말해서 각 토큰이 최대 $M$개의 노드로 전송되도록 보장하며, 이는 각 노드에 분산된 전문가들의 최고 $\frac{K_r}{M}$개 친화도 점수의 합에 따라 선택됩니다. 이러한 제약 하에서 MoE 훈련 프레임워크는 거의 완전한 계산-통신 오버랩을 달성할 수 있습니다.

**토큰 드롭 없음**

효과적인 로드 밸런싱 전략으로 인해 DeepSeek-V3는 전체 훈련 동안 좋은 로드 밸런스를 유지합니다. 따라서 DeepSeek-V3는 훈련 중 어떤 토큰도 드롭하지 않습니다. 또한 추론 로드 밸런스를 보장하기 위한 특정 배포 전략도 구현하므로 DeepSeek-V3는 추론 중에도 토큰을 드롭하지 않습니다.

### Multi-Token Prediction

![Multi-Token Prediction 구현](https://arxiv.org/html/2412.19437/x3.png)

위 그림은 Multi-Token Prediction (MTP) 구현을 보여줍니다. 각 깊이에서 각 토큰의 예측을 위한 완전한 인과 체인을 유지합니다. 이 구조는 Main Model이 다음 토큰 예측을 담당하고, MTP Module 1이 두 번째 다음 토큰 예측을, MTP Module 2가 세 번째 다음 토큰 예측을 담당하는 방식으로 구성됩니다.

Gloeckle et al.에서 영감을 받아 DeepSeek-V3에 대한 Multi-Token Prediction (MTP) 목표를 조사하고 설정합니다. 이는 각 위치에서 예측 범위를 여러 미래 토큰으로 확장합니다. 한편으로는 MTP 목표가 훈련 신호를 조밀화하고 데이터 효율성을 향상시킬 수 있습니다. 다른 한편으로는 MTP가 모델이 미래 토큰의 더 나은 예측을 위해 표현을 미리 계획할 수 있게 할 수 있습니다.

독립적인 출력 헤드를 사용하여 $D$개의 추가 토큰을 병렬로 예측하는 Gloeckle et al.과 달리, 우리는 추가 토큰을 순차적으로 예측하고 각 예측 깊이에서 완전한 인과 체인을 유지합니다.

**MTP 모듈**

구체적으로 MTP 구현은 $D$개의 추가 토큰을 예측하기 위해 $D$개의 순차적 모듈을 사용합니다. $k$번째 MTP 모듈은 공유 임베딩 레이어 $\operatorname{Emb}(\cdot)$, 공유 출력 헤드 $\operatorname{OutHead}(\cdot)$, 트랜스포머 블록 $\operatorname{TRM}_k(\cdot)$, 그리고 프로젝션 행렬 $M_k \in \mathbb{R}^{d \times 2d}$로 구성됩니다.

$i$번째 입력 토큰 $t_i$에 대해 $k$번째 예측 깊이에서 먼저 $(k-1)$번째 깊이에서 $i$번째 토큰의 표현 $\mathbf{h}_i^{k-1} \in \mathbb{R}^d$와 $(i+k)$번째 토큰의 임베딩 $Emb(t_{i+k}) \in \mathbb{R}^d$를 선형 프로젝션으로 결합합니다.

$$\mathbf{h}_i^{\prime k} = M_k[\operatorname{RMSNorm}(\mathbf{h}_i^{k-1});\operatorname{RMSNorm}(\operatorname{Emb}(t_{i+k}))]$$

여기서 $[\cdot;\cdot]$는 연결을 나타냅니다. 특히 $k=1$일 때 $\mathbf{h}_i^{k-1}$은 메인 모델에서 주어진 표현을 참조합니다. 각 MTP 모듈에 대해 임베딩 레이어는 메인 모델과 공유됩니다.

결합된 $\mathbf{h}_i^{\prime k}$는 현재 깊이에서 출력 표현 $\mathbf{h}_i^k$를 생성하기 위해 $k$번째 깊이의 트랜스포머 블록의 입력으로 사용됩니다.

$$\mathbf{h}_{1:T-k}^k = \operatorname{TRM}_k(\mathbf{h}_{1:T-k}^{\prime k})$$

여기서 $T$는 입력 시퀀스 길이를 나타내고 i:j는 슬라이싱 연산(왼쪽과 오른쪽 경계 모두 포함)을 나타냅니다.

마지막으로 $\mathbf{h}_i^k$를 입력으로 하여 공유 출력 헤드가 $k$번째 추가 예측 토큰 $P_{i+1+k}^k \in \mathbb{R}^V$에 대한 확률 분포를 계산합니다. 여기서 $V$는 어휘 크기입니다.

$$P_{i+k+1}^k = \operatorname{OutHead}(\mathbf{h}_i^k)$$

출력 헤드 $\operatorname{OutHead}(\cdot)$는 표현을 로짓에 선형 매핑하고 이후 $\operatorname{Softmax}(\cdot)$ 함수를 적용하여 $k$번째 추가 토큰의 예측 확률을 계산합니다. 또한 각 MTP 모듈에 대해 출력 헤드는 메인 모델과 공유됩니다.

예측의 인과 체인을 유지하는 원칙은 EAGLE과 유사하지만, EAGLE의 주요 목적은 투기적 디코딩인 반면 우리는 MTP를 훈련 개선에 활용합니다.

**MTP 훈련 목표**

각 예측 깊이에 대해 교차 엔트로피 손실 $\mathcal{L}_{\text{MTP}}^k$를 계산합니다.

$$\mathcal{L}_{\text{MTP}}^k = \operatorname{CrossEntropy}(P_{2+k:T+1}^k, t_{2+k:T+1}) = -\frac{1}{T}\sum_{i=2+k}^{T+1}\log P_i^k[t_i]$$

여기서 $T$는 입력 시퀀스 길이를 나타내고, $t_i$는 $i$번째 위치의 정답 토큰을 나타내며, $P_i^k[t_i]$는 $k$번째 MTP 모듈에서 주어진 $t_i$의 해당 예측 확률을 나타냅니다.

마지막으로 모든 깊이에 걸쳐 MTP 손실의 평균을 계산하고 가중 인수 $\lambda$를 곱하여 DeepSeek-V3의 추가 훈련 목표로 사용되는 전체 MTP 손실 $\mathcal{L}_{\text{MTP}}$를 얻습니다.

$$\mathcal{L}_{\text{MTP}} = \frac{\lambda}{D}\sum_{k=1}^{D}\mathcal{L}_{\text{MTP}}^k$$

**추론에서의 MTP**

MTP 전략은 주로 메인 모델의 성능을 향상시키는 것을 목표로 하므로 추론 중에는 MTP 모듈을 직접 폐기할 수 있고 메인 모델이 독립적으로 정상적으로 기능할 수 있습니다. 또한 이러한 MTP 모듈을 투기적 디코딩에 재사용하여 생성 지연 시간을 더욱 개선할 수도 있습니다.
## 인프라스트럭처

DeepSeek-V3의 훈련을 지원하는 포괄적인 인프라스트럭처는 컴퓨팅 클러스터, 훈련 프레임워크 최적화, FP8 훈련 구현, 그리고 배포 전략을 포함합니다.

### 컴퓨팅 클러스터

DeepSeek-V3는 2048개의 NVIDIA H800 GPU로 구성된 클러스터에서 훈련됩니다. H800 클러스터의 각 노드는 NVLink와 NVSwitch를 통해 노드 내에서 연결된 8개의 GPU를 포함합니다. 서로 다른 노드 간에는 InfiniBand (IB) 인터커넥트를 활용하여 통신을 촉진합니다.

이러한 하드웨어 구성은 대규모 MoE 모델 훈련에 필수적인 높은 대역폭과 낮은 지연 시간을 제공합니다. NVLink는 노드 내 GPU 간 고속 통신을 담당하며, InfiniBand는 노드 간 통신을 처리하여 전체 클러스터가 하나의 통합된 컴퓨팅 환경으로 작동할 수 있게 합니다.

### 훈련 프레임워크

DeepSeek-V3의 훈련은 엔지니어들이 처음부터 제작한 효율적이고 경량화된 훈련 프레임워크인 HAI-LLM 프레임워크의 지원을 받습니다. 전체적으로 DeepSeek-V3는 16-way Pipeline Parallelism (PP), 8개 노드에 걸친 64-way Expert Parallelism (EP), 그리고 ZeRO-1 Data Parallelism (DP)을 적용합니다.

이러한 다차원 병렬성 전략은 각각 고유한 역할을 수행합니다. Pipeline Parallelism은 모델의 레이어들을 여러 GPU에 분산시켜 메모리 사용량을 줄이고, Expert Parallelism은 MoE의 전문가들을 다른 GPU에 배치하여 계산을 병렬화하며, Data Parallelism은 배치 데이터를 여러 GPU에 분산시켜 처리 속도를 향상시킵니다.

DeepSeek-V3의 효율적인 훈련을 촉진하기 위해 세심한 엔지니어링 최적화를 구현합니다. 첫째, 효율적인 파이프라인 병렬성을 위한 DualPipe 알고리즘을 설계합니다. 기존 PP 방법들과 비교하여 DualPipe는 파이프라인 버블이 더 적습니다. 더 중요한 것은 순방향과 역방향 프로세스에서 계산과 통신 단계를 오버랩시켜 크로스 노드 전문가 병렬성으로 인한 무거운 통신 오버헤드의 문제를 해결한다는 점입니다.

둘째, IB와 NVLink 대역폭을 완전히 활용하고 통신에 전용되는 Streaming Multiprocessors (SMs)를 보존하기 위한 효율적인 크로스 노드 all-to-all 통신 커널을 개발합니다. 마지막으로, 훈련 중 메모리 사용량을 세심하게 최적화하여 비용이 많이 드는 Tensor Parallelism (TP)을 사용하지 않고도 DeepSeek-V3를 훈련할 수 있게 합니다.

#### DualPipe와 계산-통신 오버랩

![DualPipe 오버랩 전략](https://arxiv.org/html/2412.19437/x4.png)

위 그림은 트랜스포머 모델에서 개별 순방향 및 역방향 청크 쌍에 대한 오버랩 전략을 보여주는 아키텍처 다이어그램입니다. 이 다이어그램은 MLP, DISPATCH, COMBINE, ATTN, PP 연산을 포함한 순방향 및 역방향 패스에 관련된 다양한 구성 요소와 통신 패턴, 그리고 트랜스포머 블록의 경계를 보여줍니다. 주황색은 순방향을, 녹색은 "입력에 대한 역방향"을, 파란색은 "가중치에 대한 역방향"을, 보라색은 PP 통신을, 빨간색은 장벽을 나타냅니다. all-to-all과 PP 통신 모두 완전히 숨겨질 수 있습니다.

DeepSeek-V3에서 크로스 노드 전문가 병렬성으로 인한 통신 오버헤드는 약 1:1의 비효율적인 계산 대 통신 비율을 초래합니다. 이 문제를 해결하기 위해 순방향과 역방향 계산-통신 단계를 효과적으로 오버랩시켜 모델 훈련을 가속화할 뿐만 아니라 파이프라인 버블도 줄이는 혁신적인 파이프라인 병렬성 알고리즘인 DualPipe를 설계합니다.

DualPipe의 핵심 아이디어는 개별 순방향 및 역방향 청크 쌍 내에서 계산과 통신을 오버랩시키는 것입니다. 구체적으로, 각 청크를 네 가지 구성 요소로 나눕니다. 어텐션, all-to-all dispatch, MLP, 그리고 all-to-all combine입니다. 특히 역방향 청크의 경우, 어텐션과 MLP 모두 ZeroBubble에서와 같이 입력에 대한 역방향과 가중치에 대한 역방향의 두 부분으로 더 분할됩니다. 또한 PP 통신 구성 요소도 있습니다.

이 오버랩 전략에서는 통신 대 계산에 전용되는 GPU SM의 비율을 수동으로 조정하여 이러한 구성 요소들을 재배열합니다. 이 오버랩 전략을 통해 all-to-all과 PP 통신 모두가 실행 중에 완전히 숨겨질 수 있도록 보장할 수 있습니다.

![DualPipe 스케줄링](https://arxiv.org/html/2412.19437/x5.png)

위 그림은 8개의 PP 랭크와 두 방향으로 20개의 마이크로 배치에 대한 DualPipe 스케줄링 예시를 보여주는 스케줄링 다이어그램입니다. 역방향의 마이크로 배치들은 순방향의 것들과 대칭이므로 설명의 단순화를 위해 배치 ID를 생략했습니다. 공유된 검은색 테두리로 둘러싸인 두 셀은 상호 오버랩된 계산과 통신을 가집니다.

효율적인 오버랩 전략이 주어지면, 전체 DualPipe 스케줄링이 그림에 나타나 있습니다. 이는 파이프라인의 양 끝에서 동시에 마이크로 배치를 공급하는 양방향 파이프라인 스케줄링을 사용하며, 통신의 상당 부분이 완전히 오버랩될 수 있습니다. 이 오버랩은 또한 모델이 더욱 확장되더라도 일정한 계산 대 통신 비율을 유지하는 한, 거의 제로에 가까운 all-to-all 통신 오버헤드를 달성하면서 노드 간 세분화된 전문가를 여전히 사용할 수 있도록 보장합니다.

무거운 통신 부담이 없는 더 일반적인 시나리오에서도 DualPipe는 여전히 효율성 장점을 보입니다. 다음 표에서는 다양한 PP 방법들 간의 파이프라인 버블과 메모리 사용량을 요약합니다.

| 방법 | 버블 | 파라미터 | 활성화 |
|------|------|----------|--------|
| 1F1B | $(PP-1)(F+B)$ | $1 \times$ | $PP$ |
| ZB1P | $(PP-1)(F+B-2W)$ | $1 \times$ | $PP$ |
| DualPipe (제안) | $(\frac{PP}{2}-1)(F\&B+B-3W)$ | $2 \times$ | $PP+1$ |

표에서 보듯이, ZB1P와 1F1B와 비교하여 DualPipe는 최대 활성화 메모리를 $\frac{1}{PP}$배만 증가시키면서 파이프라인 버블을 크게 줄입니다. DualPipe가 모델 파라미터의 두 복사본을 유지해야 하지만, 훈련 중 큰 EP 크기를 사용하기 때문에 이것이 메모리 소비를 크게 증가시키지는 않습니다.

Chimera와 비교하여 DualPipe는 파이프라인 단계와 마이크로 배치가 2로 나누어떨어지기만 하면 되며, 마이크로 배치가 파이프라인 단계로 나누어떨어질 필요가 없습니다. 또한 DualPipe의 경우 마이크로 배치 수가 증가해도 버블이나 활성화 메모리가 증가하지 않습니다.

#### 효율적인 크로스 노드 All-to-All 통신 구현

DualPipe에 충분한 계산 성능을 보장하기 위해 통신에 전용되는 SM 수를 보존하는 효율적인 크로스 노드 all-to-all 통신 커널(dispatching과 combining 포함)을 맞춤 제작합니다. 커널의 구현은 MoE 게이팅 알고리즘과 클러스터의 네트워크 토폴로지와 공동 설계됩니다.

구체적으로, 클러스터에서 크로스 노드 GPU들은 IB로 완전히 상호 연결되고, 노드 내 통신은 NVLink를 통해 처리됩니다. NVLink는 160 GB/s의 대역폭을 제공하며, 이는 IB(50 GB/s)의 약 3.2배입니다. IB와 NVLink의 서로 다른 대역폭을 효과적으로 활용하기 위해 각 토큰이 최대 4개의 노드로만 전송되도록 제한하여 IB 트래픽을 줄입니다.

각 토큰에 대해 라우팅 결정이 내려지면, 먼저 IB를 통해 대상 노드에서 동일한 노드 내 인덱스를 가진 GPU로 전송됩니다. 대상 노드에 도달하면, 후속 도착 토큰에 의해 차단되지 않고 대상 전문가를 호스팅하는 특정 GPU로 NVLink를 통해 즉시 전달되도록 노력합니다. 이러한 방식으로 IB와 NVLink를 통한 통신이 완전히 오버랩되며, 각 토큰은 NVLink로부터 추가 오버헤드를 발생시키지 않고 노드당 평균 3.2개의 전문가를 효율적으로 선택할 수 있습니다.

이는 DeepSeek-V3가 실제로는 8개의 라우팅된 전문가만 선택하지만, 동일한 통신 비용을 유지하면서 이 수를 최대 13개의 전문가(4개 노드 × 노드당 3.2개 전문가)까지 확장할 수 있음을 의미합니다.

전반적으로, 이러한 통신 전략 하에서 IB와 NVLink의 대역폭을 완전히 활용하기 위해서는 단 20개의 SM만으로 충분합니다. 세부적으로, warp specialization 기법을 사용하여 20개의 SM을 10개의 통신 채널로 분할합니다. dispatching 과정에서 (1) IB 전송, (2) IB-to-NVLink 전달, (3) NVLink 수신이 각각의 warp에 의해 처리됩니다. 각 통신 작업에 할당되는 warp의 수는 모든 SM에 걸친 실제 워크로드에 따라 동적으로 조정됩니다.

마찬가지로 combining 과정에서도 (1) NVLink 전송, (2) NVLink-to-IB 전달 및 누적, (3) IB 수신 및 누적이 동적으로 조정되는 warp에 의해 처리됩니다. 또한 dispatching과 combining 커널 모두 계산 스트림과 오버랩되므로 다른 SM 계산 커널에 미치는 영향도 고려합니다. 구체적으로, 맞춤형 PTX (Parallel Thread Execution) 명령어를 사용하고 통신 청크 크기를 자동 조정하여 L2 캐시 사용량과 다른 SM에 대한 간섭을 크게 줄입니다.

#### 최소 오버헤드로 극도의 메모리 절약

훈련 중 메모리 사용량을 줄이기 위해 다음 기법들을 사용합니다.

**RMSNorm과 MLA Up-Projection의 재계산**: 역전파 중에 모든 RMSNorm 연산과 MLA up-projection을 재계산하여 출력 활성화를 지속적으로 저장할 필요를 없앱니다. 약간의 오버헤드로 이 전략은 활성화 저장을 위한 메모리 요구사항을 크게 줄입니다.

**CPU에서의 지수 이동 평균**: 훈련 중에 학습률 감소 후 모델 성능의 조기 추정을 위해 모델 파라미터의 지수 이동 평균(EMA)을 보존합니다. EMA 파라미터는 CPU 메모리에 저장되고 각 훈련 단계 후 비동기적으로 업데이트됩니다. 이 방법을 통해 추가적인 메모리나 시간 오버헤드를 발생시키지 않고 EMA 파라미터를 유지할 수 있습니다.

**멀티 토큰 예측을 위한 공유 임베딩과 출력 헤드**: DualPipe 전략을 통해 모델의 가장 얕은 레이어들(임베딩 레이어 포함)과 가장 깊은 레이어들(출력 헤드 포함)을 동일한 PP 랭크에 배치합니다. 이러한 배치는 MTP 모듈과 메인 모델 간에 공유 임베딩과 출력 헤드의 파라미터와 그래디언트의 물리적 공유를 가능하게 합니다. 이 물리적 공유 메커니즘은 메모리 효율성을 더욱 향상시킵니다.

### FP8 훈련

![FP8 혼합 정밀도 프레임워크](https://arxiv.org/html/2412.19437/x6.png)

위 그림은 FP8 데이터 형식을 사용한 전체 혼합 정밀도 프레임워크를 보여주는 아키텍처 다이어그램입니다. 명확성을 위해 Linear 연산자만 설명되어 있습니다. 이 다이어그램은 Input, Fprop, Σ (합계), Wgrad, Output, Weight, Input Gradient, Master Weight, Optimizer States와 그들의 연결을 포함한 혼합 정밀도 프레임워크의 핵심 구성 요소와 데이터 흐름을 보여줍니다.

저정밀도 훈련의 최근 발전에서 영감을 받아 DeepSeek-V3 훈련을 위한 FP8 데이터 형식을 활용하는 세분화된 혼합 정밀도 프레임워크를 제안합니다. 저정밀도 훈련은 큰 가능성을 가지고 있지만, 종종 활성화, 가중치, 그래디언트의 이상치 존재로 인해 제한됩니다. 추론 양자화에서 상당한 진전이 있었지만, 대규모 언어 모델 사전 훈련에서 저정밀도 기법의 성공적인 적용을 보여주는 연구는 상대적으로 적습니다.

이 문제를 해결하고 FP8 형식의 동적 범위를 효과적으로 확장하기 위해 세분화된 양자화 전략을 도입합니다. $1 \times N_c$ 요소를 가진 타일별 그룹화 또는 $N_c \times N_c$ 요소를 가진 블록별 그룹화입니다. 관련된 역양자화 오버헤드는 정확한 FP8 General Matrix Multiplication (GEMM)을 달성하기 위한 중요한 측면인 증가된 정밀도 누적 과정에서 크게 완화됩니다.

더 나아가 MoE 훈련에서 메모리와 통신 오버헤드를 더욱 줄이기 위해 활성화를 FP8로 캐시하고 전송하면서 저정밀도 옵티마이저 상태를 BF16으로 저장합니다. DeepSeek-V2-Lite와 DeepSeek-V2와 유사한 두 모델 규모에서 약 1조 개의 토큰으로 훈련하여 제안된 FP8 혼합 정밀도 프레임워크를 검증합니다. 주목할 점은 BF16 기준선과 비교하여 FP8 훈련 모델의 상대적 손실 오차가 지속적으로 0.25% 미만을 유지한다는 것으로, 이는 훈련 무작위성의 허용 범위 내에 있는 수준입니다.
#### 혼합 정밀도 프레임워크

널리 채택된 저정밀도 훈련 기법들을 기반으로 FP8 훈련을 위한 혼합 정밀도 프레임워크를 제안합니다. 이 프레임워크에서는 대부분의 계산 집약적 연산이 FP8로 수행되는 반면, 몇 가지 핵심 연산들은 훈련 효율성과 수치적 안정성의 균형을 맞추기 위해 전략적으로 원래 데이터 형식으로 유지됩니다.

먼저 모델 훈련을 가속화하기 위해 핵심 계산 커널의 대부분, 즉 GEMM 연산들이 FP8 정밀도로 구현됩니다. 이러한 GEMM 연산들은 FP8 텐서를 입력으로 받아 BF16 또는 FP32로 출력을 생성합니다. 앞서 제시한 그림에서 보듯이, Linear 연산자와 관련된 세 가지 GEMM, 즉 Fprop(순방향 패스), Dgrad(활성화 역방향 패스), Wgrad(가중치 역방향 패스) 모두 FP8로 실행됩니다. 이러한 설계는 이론적으로 원래 BF16 방법과 비교하여 계산 속도를 두 배로 향상시킵니다.

또한 FP8 Wgrad GEMM은 활성화를 역방향 패스에서 사용하기 위해 FP8로 저장할 수 있게 합니다. 이는 메모리 소비를 크게 줄입니다. FP8 형식의 효율성 장점에도 불구하고, 특정 연산자들은 저정밀도 계산에 대한 민감성으로 인해 여전히 더 높은 정밀도를 필요로 합니다. 또한 일부 저비용 연산자들도 전체 훈련 비용에 무시할 만한 오버헤드로 더 높은 정밀도를 활용할 수 있습니다.

이러한 이유로 신중한 조사를 거쳐 다음 구성 요소들에 대해서는 원래 정밀도(예: BF16 또는 FP32)를 유지합니다. 임베딩 모듈, 출력 헤드, MoE 게이팅 모듈, 정규화 연산자, 그리고 어텐션 연산자입니다. 이러한 고정밀도의 목표적 유지는 DeepSeek-V3의 안정적인 훈련 역학을 보장합니다.

수치적 안정성을 더욱 보장하기 위해 마스터 가중치, 가중치 그래디언트, 옵티마이저 상태를 더 높은 정밀도로 저장합니다. 이러한 고정밀도 구성 요소들이 일부 메모리 오버헤드를 발생시키지만, 분산 훈련 시스템에서 여러 DP 랭크에 걸친 효율적인 샤딩을 통해 그 영향을 최소화할 수 있습니다.

#### 양자화와 곱셈을 통한 정밀도 향상

![FP8 정밀도 향상 기법](https://arxiv.org/html/2412.19437/x7.png)

위 그림은 두 가지 핵심 구성 요소를 보여주는 아키텍처 다이어그램입니다. (a) 세분화된 양자화 방법과 (b) FP8 GEMM 정밀도를 향상시키는 접근법입니다. 이 다이어그램은 제안된 양자화 및 정밀도 향상 기법에 관련된 입력 스케일링 인수, 가중치 스케일링 인수, 텐서 코어 연산, CUDA 코어 구성 요소들을 보여줍니다.

FP8 혼합 정밀도 프레임워크를 기반으로 양자화 방법과 곱셈 과정 모두에 초점을 맞춘 저정밀도 훈련 정확도를 향상시키는 여러 전략을 도입합니다.

**세분화된 양자화**

저정밀도 훈련 프레임워크에서 오버플로우와 언더플로우는 지수 비트 감소로 인해 제한된 FP8 형식의 동적 범위 때문에 흔한 문제입니다. 표준 관행으로, 입력 분포는 입력 텐서의 최대 절댓값을 FP8의 최대 표현 가능한 값으로 스케일링하여 FP8 형식의 표현 가능한 범위에 정렬됩니다. 이 방법은 저정밀도 훈련을 활성화 이상치에 매우 민감하게 만들어 양자화 정확도를 크게 저하시킬 수 있습니다.

이를 해결하기 위해 더 세분화된 수준에서 스케일링을 적용하는 세분화된 양자화 방법을 제안합니다. 그림 (a)에서 보듯이, (1) 활성화의 경우 1x128 타일 기준(즉, 토큰당 128 채널당)으로 요소들을 그룹화하고 스케일링하며, (2) 가중치의 경우 128x128 블록 기준(즉, 128 입력 채널당 128 출력 채널당)으로 요소들을 그룹화하고 스케일링합니다. 이 접근법은 양자화 과정이 더 작은 요소 그룹에 따라 스케일을 조정함으로써 이상치를 더 잘 수용할 수 있도록 보장합니다.

우리 방법의 한 가지 핵심 수정사항은 GEMM 연산의 내부 차원을 따라 그룹별 스케일링 인수를 도입하는 것입니다. 이 기능은 표준 FP8 GEMM에서 직접 지원되지 않습니다. 하지만 정밀한 FP32 누적 전략과 결합하면 효율적으로 구현할 수 있습니다. 주목할 점은 우리의 세분화된 양자화 전략이 마이크로스케일링 형식의 아이디어와 매우 일치한다는 것이며, NVIDIA 차세대 GPU(Blackwell 시리즈)의 텐서 코어는 더 작은 양자화 세분성을 가진 마이크로스케일링 형식에 대한 지원을 발표했습니다. 우리의 설계가 최신 GPU 아키텍처와 보조를 맞추기 위한 향후 작업의 참고 자료가 되기를 희망합니다.

**누적 정밀도 증가**

저정밀도 GEMM 연산은 종종 언더플로우 문제로 어려움을 겪으며, 그 정확도는 일반적으로 FP32 정밀도로 수행되는 고정밀도 누적에 크게 의존합니다. 하지만 NVIDIA H800 GPU에서 FP8 GEMM의 누적 정밀도가 약 14비트 유지에 제한된다는 것을 관찰했으며, 이는 FP32 누적 정밀도보다 현저히 낮습니다. 이 문제는 배치 크기와 모델 폭이 증가하는 대규모 모델 훈련의 일반적인 시나리오인 내부 차원 K가 클 때 더욱 두드러집니다.

K = 4096인 두 무작위 행렬의 GEMM 연산을 예로 들면, 예비 테스트에서 텐서 코어의 제한된 누적 정밀도는 거의 2%의 최대 상대 오차를 초래합니다. 이러한 문제에도 불구하고 제한된 누적 정밀도는 여전히 몇몇 FP8 프레임워크에서 기본 옵션으로 남아 있어 훈련 정확도를 심각하게 제약합니다.

이 문제를 해결하기 위해 더 높은 정밀도를 위한 CUDA 코어로의 승격 전략을 채택합니다. 과정은 그림 (b)에 나타나 있습니다. 구체적으로, 텐서 코어에서 MMA(Matrix Multiply-Accumulate) 실행 중에 중간 결과들이 제한된 비트 폭을 사용하여 누적됩니다. $N_C$의 간격에 도달하면, 이러한 부분 결과들이 CUDA 코어의 FP32 레지스터로 복사되어 완전한 정밀도 FP32 누적이 수행됩니다.

앞서 언급했듯이, 우리의 세분화된 양자화는 내부 차원 K를 따라 그룹별 스케일링 인수를 적용합니다. 이러한 스케일링 인수들은 최소한의 추가 계산 비용으로 역양자화 과정으로서 CUDA 코어에서 효율적으로 곱해질 수 있습니다. 이 수정이 단일 워프그룹에 대한 WGMMA(Warpgroup-level Matrix Multiply-Accumulate) 명령어 발행률을 감소시키지만, H800 아키텍처에서는 두 개의 WGMMA가 동시에 지속되는 것이 일반적입니다. 한 워프그룹이 승격 연산을 수행하는 동안 다른 워프그룹은 MMA 연산을 실행할 수 있습니다. 이 설계는 두 연산의 오버랩을 가능하게 하여 텐서 코어의 높은 활용률을 유지합니다.

실험을 바탕으로 $N_C = 128$ 요소, 즉 4개의 WGMMA에 해당하는 설정이 상당한 오버헤드를 도입하지 않으면서 정밀도를 크게 향상시킬 수 있는 최소 누적 간격을 나타냅니다.

**지수보다 가수**

E4M3(4비트 지수와 3비트 가수)를 Fprop에서 사용하고 E5M2(5비트 지수와 2비트 가수)를 Dgrad와 Wgrad에서 사용하는 이전 연구에서 채택된 하이브리드 FP8 형식과 달리, 더 높은 정밀도를 위해 모든 텐서에 E4M3 형식을 채택합니다. 이 접근법의 실현 가능성을 우리의 세분화된 양자화 전략, 즉 타일 및 블록별 스케일링에 기인한다고 봅니다. 더 작은 요소 그룹에서 작동함으로써 우리의 방법론은 이러한 그룹화된 요소들 간에 지수 비트를 효과적으로 공유하여 제한된 동적 범위의 영향을 완화합니다.

**온라인 양자화**

지연된 양자화는 현재 값을 추론하기 위해 이전 반복에서의 최대 절댓값의 이력을 유지하는 텐서별 양자화 프레임워크에서 사용됩니다. 정확한 스케일을 보장하고 프레임워크를 단순화하기 위해 각 1x128 활성화 타일 또는 128x128 가중치 블록에 대해 온라인으로 최대 절댓값을 계산합니다. 이를 기반으로 스케일링 인수를 도출한 다음 활성화 또는 가중치를 온라인으로 FP8 형식으로 양자화합니다.

#### 저정밀도 저장 및 통신

FP8 훈련 프레임워크와 함께 캐시된 활성화와 옵티마이저 상태를 저정밀도 형식으로 압축하여 메모리 소비와 통신 오버헤드를 더욱 줄입니다.

**저정밀도 옵티마이저 상태**: 관찰 가능한 성능 저하를 발생시키지 않고 AdamW 옵티마이저에서 첫 번째와 두 번째 모멘트를 추적하기 위해 FP32 대신 BF16 데이터 형식을 채택합니다. 하지만 마스터 가중치(옵티마이저에 의해 저장됨)와 그래디언트(배치 크기 누적에 사용됨)는 훈련 전반에 걸쳐 수치적 안정성을 보장하기 위해 여전히 FP32로 유지됩니다.

**저정밀도 활성화**: 앞서 제시한 그림에서 보듯이, Wgrad 연산이 FP8로 수행됩니다. 메모리 소비를 줄이기 위해 Linear 연산자의 역방향 패스를 위해 활성화를 FP8 형식으로 캐시하는 것이 자연스러운 선택입니다. 하지만 저비용 고정밀도 훈련을 위해 여러 연산자에 대해 특별한 고려사항이 적용됩니다.

(1) 어텐션 연산자 후 Linear의 입력들: 이러한 활성화들은 정밀도에 민감한 어텐션 연산자의 역방향 패스에서도 사용됩니다. 이러한 활성화들을 위해 독점적으로 맞춤형 E5M6 데이터 형식을 채택합니다. 또한 이러한 활성화들은 역방향 패스에서 1x128 양자화 타일에서 128x1 타일로 변환됩니다. 추가 양자화 오차 도입을 피하기 위해 모든 스케일링 인수는 라운드 스케일링, 즉 2의 정수 거듭제곱입니다.

(2) MoE에서 SwiGLU 연산자의 입력들: 메모리 비용을 더욱 줄이기 위해 SwiGLU 연산자의 입력을 캐시하고 역방향 패스에서 출력을 재계산합니다. 이러한 활성화들도 메모리 효율성과 계산 정확도 간의 균형을 맞추면서 세분화된 양자화 방법으로 FP8에 저장됩니다.

**저정밀도 통신**: 통신 대역폭은 MoE 모델 훈련에서 중요한 병목 현상입니다. 이 문제를 완화하기 위해 MoE up-projection에서 FP8 Fprop과 호환되는 MoE up-projection 전 활성화를 FP8로 양자화한 다음 dispatch 구성 요소를 적용합니다. 어텐션 연산자 후 Linear의 입력과 마찬가지로, 이 활성화에 대한 스케일링 인수는 2의 정수 거듭제곱입니다. 유사한 전략이 MoE down-projection 전 활성화 그래디언트에 적용됩니다. 순방향과 역방향 combine 구성 요소 모두에 대해서는 훈련 파이프라인의 중요한 부분에서 훈련 정밀도를 보존하기 위해 BF16으로 유지합니다.

### 추론 및 배포

H800 클러스터에 DeepSeek-V3를 배포하며, 각 노드 내의 GPU들은 NVLink를 사용하여 상호 연결되고 클러스터 전체의 모든 GPU들은 IB를 통해 완전히 상호 연결됩니다. 온라인 서비스의 Service-Level Objective (SLO)와 높은 처리량을 동시에 보장하기 위해 prefilling과 decoding 단계를 분리하는 다음 배포 전략을 사용합니다.

#### Prefilling

prefilling 단계의 최소 배포 단위는 32개의 GPU를 가진 4개의 노드로 구성됩니다. 어텐션 부분은 Sequence Parallelism (SP)과 결합된 4-way Tensor Parallelism (TP4)을 사용하고, 8-way Data Parallelism (DP8)과 결합됩니다. 4라는 작은 TP 크기는 TP 통신의 오버헤드를 제한합니다. MoE 부분의 경우 32-way Expert Parallelism (EP32)을 사용하여 각 전문가가 충분히 큰 배치 크기를 처리하도록 보장함으로써 계산 효율성을 향상시킵니다.

MoE all-to-all 통신의 경우 훈련에서와 동일한 방법을 사용합니다. 먼저 IB를 통해 노드 간에 토큰을 전송한 다음 NVLink를 통해 노드 내 GPU들 간에 전달합니다. 특히 얕은 레이어의 밀집 MLP에 대해서는 TP 통신을 절약하기 위해 1-way Tensor Parallelism을 사용합니다.

MoE 부분에서 서로 다른 전문가들 간의 로드 밸런싱을 달성하기 위해 각 GPU가 대략 동일한 수의 토큰을 처리하도록 보장해야 합니다. 이를 위해 고부하 전문가를 복제하여 중복 배포하는 중복 전문가의 배포 전략을 도입합니다. 고부하 전문가들은 온라인 배포 중에 수집된 통계를 기반으로 감지되며 주기적으로(예: 10분마다) 조정됩니다.

중복 전문가 세트를 결정한 후, 크로스 노드 all-to-all 통신 오버헤드를 증가시키지 않으면서 GPU 간 로드를 최대한 균형 있게 맞추기 위해 관찰된 로드를 기반으로 노드 내 GPU들 간에 전문가를 신중하게 재배열합니다. DeepSeek-V3의 배포를 위해 prefilling 단계에 32개의 중복 전문가를 설정합니다. 각 GPU는 원래 호스팅하는 8개의 전문가 외에 하나의 추가 중복 전문가도 호스팅합니다.

더 나아가 prefilling 단계에서 처리량을 향상시키고 all-to-all 및 TP 통신의 오버헤드를 숨기기 위해 유사한 계산 워크로드를 가진 두 개의 마이크로 배치를 동시에 처리하여 한 마이크로 배치의 어텐션과 MoE를 다른 마이크로 배치의 dispatch와 combine과 오버랩시킵니다.

마지막으로, 각 GPU가 더 많은 전문가(예: 16개 전문가)를 호스팅하지만 각 추론 단계에서 9개만 활성화되는 전문가의 동적 중복 전략을 탐색하고 있습니다. 각 레이어에서 all-to-all 연산이 시작되기 전에 전역적으로 최적의 라우팅 방식을 즉석에서 계산합니다. prefilling 단계에서 상당한 계산이 관련되어 있기 때문에 이 라우팅 방식을 계산하는 오버헤드는 거의 무시할 수 있습니다.

#### Decoding

디코딩 중에는 공유 전문가를 라우팅된 전문가로 취급합니다. 이 관점에서 각 토큰은 라우팅 중에 9개의 전문가를 선택하며, 공유 전문가는 항상 선택되는 고부하 전문가로 간주됩니다. 디코딩 단계의 최소 배포 단위는 320개의 GPU를 가진 40개의 노드로 구성됩니다. 어텐션 부분은 SP와 결합된 TP4를 사용하고 DP80과 결합되는 반면, MoE 부분은 EP320을 사용합니다.

MoE 부분의 경우 각 GPU는 하나의 전문가만 호스팅하며, 64개의 GPU가 중복 전문가와 공유 전문가 호스팅을 담당합니다. dispatch와 combine 부분의 all-to-all 통신은 낮은 지연 시간을 달성하기 위해 IB를 통한 직접 점대점 전송으로 수행됩니다. 또한 지연 시간을 더욱 최소화하고 통신 효율성을 향상시키기 위해 IBGDA 기술을 활용합니다.

prefilling과 마찬가지로 온라인 서비스의 통계적 전문가 로드를 기반으로 특정 간격에서 중복 전문가 세트를 주기적으로 결정합니다. 하지만 각 GPU가 하나의 전문가만 호스팅하므로 전문가를 재배열할 필요가 없습니다. 디코딩을 위한 동적 중복 전략도 탐색하고 있습니다. 하지만 이는 전역적으로 최적의 라우팅 방식을 계산하는 알고리즘과 오버헤드를 줄이기 위한 dispatch 커널과의 융합에 대한 더 신중한 최적화가 필요합니다.

또한 처리량을 향상시키고 all-to-all 통신의 오버헤드를 숨기기 위해 디코딩 단계에서도 유사한 계산 워크로드를 가진 두 개의 마이크로 배치를 동시에 처리하는 것을 탐색하고 있습니다. prefilling과 달리 디코딩 단계에서는 어텐션이 더 큰 시간 비중을 차지합니다. 따라서 한 마이크로 배치의 어텐션을 다른 마이크로 배치의 dispatch+MoE+combine과 오버랩시킵니다.

디코딩 단계에서 전문가당 배치 크기는 상대적으로 작으며(보통 256 토큰 이내), 병목 현상은 계산보다는 메모리 액세스입니다. MoE 부분은 하나의 전문가의 파라미터만 로드하면 되므로 메모리 액세스 오버헤드가 최소화되어 더 적은 SM을 사용해도 전체 성능에 크게 영향을 주지 않습니다. 따라서 어텐션 부분의 계산 속도에 영향을 주지 않기 위해 dispatch+MoE+combine에 SM의 작은 부분만 할당할 수 있습니다.

### 하드웨어 설계에 대한 제안

all-to-all 통신과 FP8 훈련 방식의 구현을 바탕으로 AI 하드웨어 벤더들에게 칩 설계에 대한 다음 제안을 합니다.

#### 통신 하드웨어

DeepSeek-V3에서는 계산 중 통신 지연 시간을 숨기기 위해 계산과 통신 간의 오버랩을 구현합니다. 이는 직렬 계산 및 통신과 비교하여 통신 대역폭에 대한 의존성을 크게 줄입니다. 하지만 현재 통신 구현은 비싼 SM에 의존하며(예: 이 목적을 위해 H800 GPU에서 사용 가능한 132개의 SM 중 20개를 할당), 이는 계산 처리량을 제한합니다. 더욱이 통신을 위해 SM을 사용하면 텐서 코어가 완전히 활용되지 않아 상당한 비효율성을 초래합니다.

현재 SM은 all-to-all 통신을 위해 주로 다음 작업들을 수행합니다.
• 동일한 노드 내의 여러 GPU로 향하는 IB 트래픽을 단일 GPU에서 집계하면서 IB(InfiniBand)와 NVLink 도메인 간에 데이터를 전달
• RDMA 버퍼(등록된 GPU 메모리 영역)와 입력/출력 버퍼 간에 데이터 전송
• all-to-all combine을 위한 reduce 연산 실행
• IB와 NVLink 도메인에 걸쳐 여러 전문가로의 청크된 데이터 전송 중 세분화된 메모리 레이아웃 관리

우리는 향후 벤더들이 이러한 통신 작업을 귀중한 계산 단위 SM에서 오프로드하여 GPU 코프로세서나 NVIDIA SHARP와 같은 네트워크 코프로세서 역할을 하는 하드웨어를 개발하기를 희망합니다. 더 나아가 애플리케이션 프로그래밍 복잡성을 줄이기 위해 이 하드웨어가 계산 단위의 관점에서 IB(스케일 아웃)와 NVLink(스케일 업) 네트워크를 통합하기를 목표합니다. 이러한 통합 인터페이스를 통해 계산 단위는 간단한 프리미티브를 기반으로 통신 요청을 제출하여 전체 IB-NVLink-통합 도메인에 걸쳐 read, write, multicast, reduce와 같은 연산을 쉽게 수행할 수 있습니다.

#### 컴퓨팅 하드웨어

**텐서 코어에서 더 높은 FP8 GEMM 누적 정밀도**: NVIDIA Hopper 아키텍처의 현재 텐서 코어 구현에서 FP8 GEMM은 제한된 누적 정밀도로 어려움을 겪습니다. 최대 지수를 기반으로 오른쪽 시프트를 통해 32개의 가수 곱을 정렬한 후, 텐서 코어는 덧셈을 위해 각 가수 곱의 최고 14비트만 사용하고 이 범위를 초과하는 비트는 잘라냅니다. 덧셈 결과의 레지스터로의 누적도 14비트 정밀도를 사용합니다.

우리의 구현은 128개의 FP8 × FP8 곱셈의 덧셈 결과를 CUDA 코어에서 FP32 정밀도를 가진 레지스터로 누적함으로써 이 제한을 부분적으로 완화합니다. 성공적인 FP8 훈련을 달성하는 데 도움이 되지만, 이는 Hopper 아키텍처의 FP8 GEMM 누적 정밀도에서의 하드웨어 결함으로 인한 타협책일 뿐입니다. 향후 칩들은 더 높은 정밀도를 채택해야 합니다.

**타일 및 블록별 양자화 지원**: 현재 GPU들은 텐서별 양자화만 지원하며, 우리의 타일 및 블록별 양자화와 같은 세분화된 양자화에 대한 네이티브 지원이 부족합니다. 현재 구현에서 $N_C$ 간격에 도달하면 부분 결과들이 텐서 코어에서 CUDA 코어로 복사되고, 스케일링 인수와 곱해진 다음 CUDA 코어의 FP32 레지스터에 추가됩니다. 정밀한 FP32 누적 전략과 결합하여 역양자화 오버헤드가 크게 완화되지만, 텐서 코어와 CUDA 코어 간의 빈번한 데이터 이동은 여전히 계산 효율성을 제한합니다.

따라서 텐서 코어가 스케일링 인수를 받고 그룹 스케일링으로 MMA를 구현할 수 있도록 하여 세분화된 양자화를 지원하는 향후 칩을 권장합니다. 이러한 방식으로 전체 부분합 누적과 역양자화가 최종 결과가 생성될 때까지 텐서 코어 내부에서 직접 완료되어 빈번한 데이터 이동을 피할 수 있습니다.

**온라인 양자화 지원**: 현재 구현들은 우리 연구에서 입증된 효과에도 불구하고 온라인 양자화를 효과적으로 지원하는 데 어려움을 겪습니다. 기존 과정에서는 양자화를 위해 HBM(High Bandwidth Memory)에서 128개의 BF16 활성화 값(이전 계산의 출력)을 읽어야 하고, 양자화된 FP8 값들이 HBM에 다시 쓰여진 후 MMA를 위해 다시 읽혀집니다.

이 비효율성을 해결하기 위해 향후 칩들이 FP8 캐스트와 TMA(Tensor Memory Accelerator) 액세스를 단일 융합 연산으로 통합하여 전역 메모리에서 공유 메모리로 활성화를 전송하는 동안 양자화가 완료되어 빈번한 메모리 읽기와 쓰기를 피할 수 있기를 권장합니다. 또한 레이어 정규화와 FP8 캐스트의 더 나은 융합을 촉진하는 가속을 위한 warp 수준 캐스트 명령어 지원도 권장합니다.

대안으로, 컴퓨팅 로직이 HBM 근처에 배치되는 근메모리 컴퓨팅 접근법을 채택할 수 있습니다. 이 경우 BF16 요소들이 HBM에서 GPU로 읽혀질 때 직접 FP8로 캐스트되어 오프칩 메모리 액세스를 대략 50% 줄일 수 있습니다.

**전치된 GEMM 연산 지원**: 현재 아키텍처는 행렬 전치를 GEMM 연산과 융합하는 것을 번거롭게 만듭니다. 우리의 워크플로우에서 순방향 패스 중의 활성화는 1x128 FP8 타일로 양자화되어 저장됩니다. 역방향 패스 중에는 행렬을 읽어내고, 역양자화하고, 전치하고, 128x1 타일로 재양자화한 다음 HBM에 저장해야 합니다.

메모리 연산을 줄이기 위해 향후 칩들이 훈련과 추론 모두에서 필요한 정밀도에 대해 MMA 연산 전에 공유 메모리에서 행렬의 직접 전치 읽기를 가능하게 하기를 권장합니다. FP8 형식 변환과 TMA 액세스의 융합과 결합하여 이 향상은 양자화 워크플로우를 크게 간소화할 것입니다.
## 사전 훈련

DeepSeek-V3의 사전 훈련 과정은 데이터 구성, 하이퍼파라미터 설정, 긴 컨텍스트 확장, 그리고 포괄적인 평가를 통해 현재 가장 강력한 오픈소스 기본 모델을 구축하는 과정을 다룹니다.

### 데이터 구성

DeepSeek-V3의 사전 훈련을 위한 데이터 구성은 DeepSeek-V2와 비교하여 수학 및 프로그래밍 샘플의 비율을 향상시키고, 영어와 중국어를 넘어 다국어 커버리지를 확장하는 방향으로 최적화되었습니다. 이러한 개선은 모델의 수학적 추론 능력과 코딩 성능을 크게 향상시키는 데 기여합니다.

데이터 처리 파이프라인은 코퍼스 다양성을 유지하면서 중복성을 최소화하도록 정제되었습니다. 이는 훈련 데이터의 품질을 높이고 모델이 더 다양한 지식을 학습할 수 있도록 돕습니다. [Ding et al.](https://arxiv.org/html/2412.19437v2#bib.bib18)에서 영감을 받아 데이터 무결성을 위한 문서 패킹 방법을 구현하되, 훈련 중에는 크로스 샘플 어텐션 마스킹을 포함하지 않았습니다.

최종적으로 DeepSeek-V3의 훈련 코퍼스는 토크나이저 기준으로 14.8조 개의 고품질이고 다양한 토큰으로 구성됩니다. 이는 이전 모델들과 비교하여 상당히 큰 규모로, 모델의 성능 향상에 중요한 역할을 합니다.

**Fill-in-Middle 전략 구현**

[DeepSeekCoder-V2](https://arxiv.org/pdf/2406.11931)의 훈련 과정에서 관찰된 바에 따르면, Fill-in-Middle (FIM) 전략은 다음 토큰 예측 능력을 손상시키지 않으면서도 모델이 컨텍스트 단서를 기반으로 중간 텍스트를 정확하게 예측할 수 있게 합니다. 이러한 관찰을 바탕으로 DeepSeek-V3의 사전 훈련에도 FIM 전략을 통합했습니다.

구체적으로 Prefix-Suffix-Middle (PSM) 프레임워크를 사용하여 데이터를 다음과 같이 구조화합니다.

$$\texttt{<|fim\_begin|>}f_{\text{pre}}\texttt{<|fim\_hole|>}f_{\text{suf}}\texttt{<|fim\_end|>}f_{\text{middle}}\texttt{<|eos\_token|>}$$

이 구조는 문서 수준에서 사전 패킹 과정의 일부로 적용됩니다. FIM 전략은 PSM 프레임워크와 일치하게 0.1의 비율로 적용됩니다. 이는 전체 훈련 데이터의 10%에서 FIM 형식을 사용한다는 의미입니다.

FIM 전략의 핵심 아이디어는 모델이 텍스트의 앞부분(prefix)과 뒷부분(suffix)을 보고 중간 부분(middle)을 예측하도록 훈련하는 것입니다. 이는 특히 코드 생성 작업에서 유용한데, 개발자가 함수의 시작과 끝을 작성한 후 중간 구현 부분을 자동 완성하는 시나리오에 직접 적용될 수 있습니다.

**토크나이저 개선**

DeepSeek-V3의 토크나이저는 확장된 128K 토큰 어휘를 가진 Byte-level BPE를 사용합니다. 이는 이전 모델들보다 더 큰 어휘 크기로, 다양한 언어와 도메인의 텍스트를 더 효율적으로 처리할 수 있게 합니다.

토크나이저의 사전 토크나이저와 훈련 데이터는 다국어 압축 효율성을 최적화하도록 수정되었습니다. 이는 영어와 중국어뿐만 아니라 다른 언어들도 효율적으로 토큰화할 수 있도록 돕습니다.

DeepSeek-V2와 비교하여 새로운 사전 토크나이저는 구두점과 줄바꿈을 결합하는 토큰을 도입합니다. 하지만 이러한 방식은 모델이 터미널 줄바꿈 없이 다중 줄 프롬프트를 처리할 때, 특히 퓨 샷 평가 프롬프트에서 토큰 경계 편향을 도입할 수 있습니다.

이 문제를 해결하기 위해 훈련 중에 이러한 결합된 토큰의 일정 비율을 무작위로 분할하여 모델이 더 넓은 범위의 특수 사례에 노출되도록 하고 이러한 편향을 완화합니다. 이는 모델의 견고성을 향상시키고 다양한 입력 형식에 대한 적응성을 높입니다.

### 하이퍼파라미터

**모델 하이퍼파라미터**

DeepSeek-V3는 61개의 트랜스포머 레이어와 7168의 은닉 차원을 설정합니다. 모든 학습 가능한 파라미터는 0.006의 표준편차로 무작위 초기화됩니다. 이러한 초기화 전략은 훈련 초기의 안정성을 보장하면서도 모델이 효과적으로 학습할 수 있도록 돕습니다.

MLA에서 어텐션 헤드 수 $n_h$는 128로, 헤드당 차원 $d_h$는 128로 설정됩니다. KV 압축 차원 $d_c$는 512로, 쿼리 압축 차원 $d_c'$는 1536으로 설정됩니다. 분리된 쿼리와 키에 대한 헤드당 차원 $d_h^R$은 64로 설정됩니다.

처음 세 레이어를 제외한 모든 FFN을 MoE 레이어로 대체합니다. 각 MoE 레이어는 1개의 공유 전문가와 256개의 라우팅 전문가로 구성되며, 각 전문가의 중간 은닉 차원은 2048입니다. 라우팅 전문가 중에서 각 토큰당 8개의 전문가가 활성화되며, 각 토큰은 최대 4개의 노드로 전송되도록 보장됩니다.

멀티 토큰 예측 깊이 $D$는 1로 설정됩니다. 즉, 정확한 다음 토큰 외에 각 토큰은 하나의 추가 토큰을 예측합니다. 이는 훈련 신호를 조밀화하고 모델의 예측 능력을 향상시키는 데 도움이 됩니다.

이러한 구성 하에서 DeepSeek-V3는 총 671B개의 파라미터를 포함하며, 각 토큰당 37B개가 활성화됩니다. 이는 효율적인 추론을 가능하게 하면서도 강력한 성능을 제공하는 균형잡힌 설계입니다.

**훈련 하이퍼파라미터**

AdamW 옵티마이저를 사용하며 하이퍼파라미터는 $\beta_1 = 0.9$, $\beta_2 = 0.95$, $\text{weight\_decay} = 0.1$로 설정됩니다. [AdamW](https://arxiv.org/pdf/1711.05101)는 가중치 감쇠를 그래디언트 기반 업데이트에서 분리하여 더 나은 일반화 성능을 제공합니다.

사전 훈련 중 최대 시퀀스 길이는 4K로 설정되며, DeepSeek-V3를 14.8조 토큰에 대해 사전 훈련합니다. 이는 현재까지 오픈소스 모델 중 가장 큰 규모의 훈련 데이터입니다.

학습률 스케줄링의 경우, 처음 2K 단계 동안 0에서 $2.2 \times 10^{-4}$까지 선형적으로 증가시킵니다. 그 다음 모델이 10조 훈련 토큰을 소비할 때까지 $2.2 \times 10^{-4}$의 일정한 학습률을 유지합니다. 이후 코사인 감쇠 곡선을 따라 4.3조 토큰에 걸쳐 학습률을 $2.2 \times 10^{-5}$로 점진적으로 감소시킵니다.

마지막 500B 토큰의 훈련 동안에는 처음 333B 토큰에서 $2.2 \times 10^{-5}$의 일정한 학습률을 유지하고, 나머지 167B 토큰에서는 $7.3 \times 10^{-6}$의 다른 일정한 학습률로 전환합니다. 그래디언트 클리핑 노름은 1.0으로 설정됩니다.

배치 크기 스케줄링 전략을 사용하여 처음 469B 토큰의 훈련에서 배치 크기를 3072에서 15360으로 점진적으로 증가시키고, 나머지 훈련에서는 15360을 유지합니다. 이러한 점진적 배치 크기 증가는 훈련 초기의 안정성을 보장하면서도 후반부에서는 더 효율적인 훈련을 가능하게 합니다.

파이프라인 병렬성을 활용하여 모델의 서로 다른 레이어를 다른 GPU에 배포하고, 각 레이어에 대해 라우팅 전문가들을 8개 노드에 속한 64개 GPU에 균등하게 배포합니다. 노드 제한 라우팅의 경우 각 토큰은 최대 4개의 노드로 전송됩니다($M = 4$).

보조 손실 없는 로드 밸런싱을 위해 편향 업데이트 속도 $\gamma$를 처음 14.3조 토큰에 대해 0.001로, 나머지 500B 토큰에 대해 0.0으로 설정합니다. 밸런스 손실의 경우 극단적인 불균형을 피하기 위해 $\alpha$를 0.0001로 설정합니다.

MTP 손실 가중치 $\lambda$는 처음 10조 토큰에 대해 0.3으로, 나머지 4.8조 토큰에 대해 0.1로 설정됩니다. 이러한 점진적 감소는 훈련 초기에는 MTP의 영향을 크게 하고, 후반부에서는 주요 다음 토큰 예측 목표에 더 집중하도록 합니다.

### 긴 컨텍스트 확장

DeepSeek-V3에서 긴 컨텍스트 기능을 활성화하기 위해 DeepSeek-V2와 유사한 접근법을 채택합니다. 사전 훈련 단계 후에 컨텍스트 확장을 위해 [YaRN](https://arxiv.org/pdf/2309.00071)을 적용하고, 컨텍스트 윈도우를 4K에서 32K로, 그 다음 128K로 점진적으로 확장하기 위해 각각 1000단계로 구성된 두 개의 추가 훈련 단계를 수행합니다.

YaRN 구성은 DeepSeek-V2에서 사용된 것과 일치하며, 분리된 공유 키 $\mathbf{k}_t^R$에만 적용됩니다. 이는 위치 임베딩의 고주파 정보를 보존하면서도 효과적인 컨텍스트 확장을 가능하게 합니다.

두 단계 모두에서 하이퍼파라미터는 동일하게 유지되며, 스케일 $s = 40$, $\alpha = 1$, $\beta = 32$, 그리고 스케일링 인수 $\sqrt{t} = 0.1\ln s + 1$로 설정됩니다. 이러한 설정은 YaRN의 "NTK-by-parts" 보간 방법을 활용하여 고주파 차원은 보간하지 않고 저주파 차원만 보간하여 지역적 상대 위치 정보를 보존합니다.

첫 번째 단계에서는 시퀀스 길이를 32K로 설정하고 배치 크기는 1920입니다. 두 번째 단계에서는 시퀀스 길이를 128K로 증가시키고 배치 크기를 480으로 줄입니다. 두 단계 모두의 학습률은 사전 훈련 단계의 최종 학습률과 일치하는 $7.3 \times 10^{-6}$으로 설정됩니다.

이러한 2단계 확장 훈련을 통해 DeepSeek-V3는 강력한 성능을 유지하면서 최대 128K 길이의 입력을 처리할 수 있게 됩니다.

![Needle In A Haystack 테스트 결과](https://arxiv.org/html/2412.19437/x8.png)

위 그림은 DeepSeek-V3 모델의 "Needle In A Haystack" (NIAH) 테스트 평가 결과를 보여주는 선 그래프입니다. 이 그래프는 128K 토큰까지의 다양한 컨텍스트 윈도우 길이(x축)에 걸쳐 DeepSeek-V3 모델이 달성한 문서 깊이 백분율(y축)을 시각화합니다. 이 그림의 연구적 의미는 DeepSeek-V3 모델이 광범위한 컨텍스트 윈도우 길이에서 강력한 성능을 보여준다는 것으로, 정보 검색 시스템을 평가하는 중요한 벤치마크인 "Needle In A Haystack" 작업에서의 견고성과 효과성을 나타냅니다.

지도 학습 미세 조정을 거친 후 DeepSeek-V3는 NIAH 테스트에서 주목할 만한 성능을 달성하여 128K까지의 컨텍스트 윈도우 길이에서 일관된 견고성을 보여줍니다. 이는 모델이 긴 문서에서 특정 정보를 정확하게 찾고 활용할 수 있는 능력을 입증합니다.

### 평가

**평가 벤치마크**

DeepSeek-V3의 기본 모델은 영어와 중국어가 대부분을 차지하는 다국어 코퍼스에서 사전 훈련되었으므로, 주로 영어와 중국어 벤치마크와 다국어 벤치마크에서 성능을 평가합니다. 평가는 HAI-LLM 프레임워크에 통합된 내부 평가 프레임워크를 기반으로 합니다.

고려된 벤치마크들은 다음과 같이 분류되며, 밑줄 친 벤치마크는 중국어, 이중 밑줄 친 벤치마크는 다국어입니다.

다중 주제 객관식 데이터셋에는 MMLU, MMLU-Redux, MMLU-Pro, MMMLU, C-Eval, CMMLU가 포함됩니다. 언어 이해 및 추론 데이터셋에는 HellaSwag, PIQA, ARC, BigBench Hard (BBH)가 포함됩니다. 폐쇄형 질문 답변 데이터셋에는 TriviaQA와 NaturalQuestions가 포함됩니다.

독해 데이터셋에는 RACE, DROP, C3, CMRC가 포함됩니다. 참조 명확화 데이터셋에는 CLUEWSC와 WinoGrande가 포함됩니다. 언어 모델링 데이터셋에는 Pile이 포함됩니다. 중국어 이해 및 문화 데이터셋에는 CCPM이 포함됩니다.

수학 데이터셋에는 GSM8K, MATH, MGSM, CMath가 포함됩니다. 코드 데이터셋에는 HumanEval, LiveCodeBench-Base (0801-1101), MBPP, CRUXEval이 포함됩니다. 표준화된 시험에는 AGIEval이 포함되며, 이는 영어와 중국어 하위 집합을 모두 포함합니다.

이전 연구를 따라 HellaSwag, PIQA, WinoGrande, RACE-Middle, RACE-High, MMLU, MMLU-Redux, MMLU-Pro, MMMLU, ARC-Easy, ARC-Challenge, C-Eval, CMMLU, C3, CCPM을 포함한 데이터셋에 대해서는 퍼플렉시티 기반 평가를 채택하고, TriviaQA, NaturalQuestions, DROP, MATH, GSM8K, MGSM, HumanEval, MBPP, LiveCodeBench-Base, CRUXEval, BBH, AGIEval, CLUEWSC, CMRC, CMath에 대해서는 생성 기반 평가를 채택합니다.

또한 Pile-test에 대해서는 언어 모델링 기반 평가를 수행하고 서로 다른 토크나이저를 사용하는 모델들 간의 공정한 비교를 보장하기 위해 Bits-Per-Byte (BPB)를 메트릭으로 사용합니다.

**평가 결과**

| 벤치마크 (메트릭) | # 샷 | DeepSeek-V2 | Qwen2.5 | LLaMA-3.1 | DeepSeek-V3 |
|------------------|------|-------------|---------|-----------|-------------|
|                  |      | Base 72B    | Base    | 405B Base | Base        |
| **아키텍처**      | -    | MoE         | Dense   | Dense     | MoE         |
| **활성화 파라미터** | -    | 21B         | 72B     | 405B      | 37B         |
| **총 파라미터**    | -    | 236B        | 72B     | 405B      | 671B        |
| **영어**          |      |             |         |           |             |
| Pile-test(BPB)   | -    | 0.606       | 0.638   | 0.542     | 0.548       |
| BBH(EM)          | 3-shot| 78.8        | 79.8    | 82.9      | 87.5        |
| MMLU(EM)         | 5-shot| 78.4        | 85.0    | 84.4      | 87.1        |
| MMLU-Redux(EM)   | 5-shot| 75.6        | 83.2    | 81.3      | 86.2        |
| MMLU-Pro(EM)     | 5-shot| 51.4        | 58.3    | 52.8      | 64.4        |
| DROP(F1)         | 3-shot| 80.4        | 80.6    | 86.0      | 89.0        |
| ARC-Easy(EM)     | 25-shot| 97.6       | 98.4    | 98.4      | 98.9        |
| ARC-Challenge(EM)| 25-shot| 92.2       | 94.5    | 95.3      | 95.3        |
| HellaSwag(EM)    | 10-shot| 87.1       | 84.8    | 89.2      | 88.9        |
| PIQA(EM)         | 0-shot| 83.9        | 82.6    | 85.9      | 84.7        |
| WinoGrande(EM)   | 5-shot| 86.3        | 82.3    | 85.2      | 84.9        |
| RACE-Middle(EM)  | 5-shot| 73.1        | 68.1    | 74.2      | 67.1        |
| RACE-High(EM)    | 5-shot| 52.6        | 50.3    | 56.8      | 51.3        |
| TriviaQA(EM)     | 5-shot| 80.0        | 71.9    | 82.7      | 82.9        |
| NaturalQuestions(EM)| 5-shot| 38.6     | 33.2    | 41.5      | 40.0        |
| AGIEval(EM)      | 0-shot| 57.5        | 75.8    | 60.6      | 79.6        |
| **코드**          |      |             |         |           |             |
| HumanEval(Pass@1)| 0-shot| 43.3        | 53.0    | 54.9      | 65.2        |
| MBPP(Pass@1)     | 3-shot| 65.0        | 72.6    | 68.4      | 75.4        |
| LiveCodeBench-Base(Pass@1)| 3-shot| 11.6| 12.9    | 15.5      | 19.4        |
| CRUXEval-I(EM)   | 2-shot| 52.5        | 59.1    | 58.5      | 67.3        |
| CRUXEval-O(EM)   | 2-shot| 49.8        | 59.9    | 59.9      | 69.8        |
| **수학**          |      |             |         |           |             |
| GSM8K(EM)        | 8-shot| 81.6        | 88.3    | 83.5      | 89.3        |
| MATH(EM)         | 4-shot| 43.4        | 54.4    | 49.0      | 61.6        |
| MGSM(EM)         | 8-shot| 63.6        | 76.2    | 69.9      | 79.8        |
| CMath(EM)        | 3-shot| 78.7        | 84.5    | 77.3      | 90.7        |
| **중국어**        |      |             |         |           |             |
| CLUEWSC(EM)      | 5-shot| 82.0        | 82.5    | 83.0      | 82.7        |
| C-Eval(EM)       | 5-shot| 81.4        | 89.2    | 72.5      | 90.1        |
| CMMLU(EM)        | 5-shot| 84.0        | 89.5    | 73.7      | 88.8        |
| CMRC(EM)         | 1-shot| 77.4        | 75.8    | 76.0      | 76.3        |
| C3(EM)           | 0-shot| 77.4        | 76.7    | 79.7      | 78.6        |
| CCPM(EM)         | 0-shot| 93.0        | 88.5    | 78.6      | 92.0        |
| **다국어**        |      |             |         |           |             |
| MMMLU-non-English(EM)| 5-shot| 64.0    | 74.8    | 73.8      | 79.4        |

위 표는 DeepSeek-V3-Base와 다른 대표적인 오픈소스 기본 모델들 간의 비교를 보여줍니다. 모든 모델은 내부 프레임워크에서 평가되었으며 동일한 평가 설정을 공유합니다. 0.3을 초과하지 않는 점수 차이는 동일한 수준으로 간주됩니다. DeepSeek-V3-Base는 대부분의 벤치마크에서 최고 성능을 달성하며, 특히 수학 및 코드 작업에서 뛰어난 성과를 보입니다.

전반적으로 DeepSeek-V3-Base는 DeepSeek-V2-Base와 Qwen2.5 72B Base를 포괄적으로 능가하며, 대부분의 벤치마크에서 LLaMA-3.1 405B Base를 뛰어넘어 본질적으로 가장 강력한 오픈소스 모델이 되었습니다.

더 자세한 관점에서 DeepSeek-V3-Base를 다른 오픈소스 기본 모델들과 개별적으로 비교해보면, DeepSeek-V2-Base와 비교하여 모델 아키텍처의 개선, 모델 크기와 훈련 토큰의 확장, 데이터 품질의 향상으로 인해 DeepSeek-V3-Base는 예상대로 상당히 더 나은 성능을 달성합니다.

최첨단 중국어 오픈소스 모델인 Qwen2.5 72B Base와 비교하여, 활성화 파라미터가 절반에 불과함에도 불구하고 DeepSeek-V3-Base는 특히 영어, 다국어, 코드, 수학 벤치마크에서 현저한 장점을 보여줍니다. 중국어 벤치마크의 경우, 중국어 다중 주제 객관식 작업인 CMMLU를 제외하고는 DeepSeek-V3-Base가 Qwen2.5 72B보다 더 나은 성능을 보입니다.

활성화 파라미터가 11배인 가장 큰 오픈소스 모델인 LLaMA-3.1 405B Base와 비교하여, DeepSeek-V3-Base는 다국어, 코드, 수학 벤치마크에서 훨씬 더 나은 성능을 보입니다. 영어와 중국어 언어 벤치마크의 경우, DeepSeek-V3-Base는 경쟁력 있거나 더 나은 성능을 보이며, 특히 BBH, MMLU 시리즈, DROP, C-Eval, CMMLU, CCPM에서 뛰어납니다.

효율적인 아키텍처와 포괄적인 엔지니어링 최적화로 인해 DeepSeek-V3는 극도로 높은 훈련 효율성을 달성합니다. 훈련 프레임워크와 인프라스트럭처 하에서 각 조 토큰에 대해 DeepSeek-V3를 훈련하는 데 단 180K H800 GPU 시간만 필요하며, 이는 72B 또는 405B 밀집 모델을 훈련하는 것보다 훨씬 저렴합니다.

### 논의

**멀티 토큰 예측에 대한 절제 연구**

| 벤치마크 (메트릭) | # 샷 | Small MoE | Small MoE | Large MoE | Large MoE |
|------------------|------|-----------|-----------|-----------|-----------|
|                  |      | Baseline  | w/ MTP    | Baseline  | w/ MTP    |
| **활성화 파라미터(추론)** | - | 2.4B      | 2.4B      | 20.9B     | 20.9B     |
| **총 파라미터(추론)**     | - | 15.7B     | 15.7B     | 228.7B    | 228.7B    |
| **훈련 토큰**            | - | 1.33T     | 1.33T     | 540B      | 540B      |
| Pile-test(BPB)   | -    | 0.729     | 0.729     | 0.658     | 0.657     |
| BBH(EM)          | 3-shot| 39.0      | 41.4      | 70.0      | 70.7      |
| MMLU(EM)         | 5-shot| 50.0      | 53.3      | 67.5      | 66.6      |
| DROP(F1)         | 1-shot| 39.2      | 41.3      | 68.5      | 70.6      |
| TriviaQA(EM)     | 5-shot| 56.9      | 57.7      | 67.0      | 67.3      |
| NaturalQuestions(EM)| 5-shot| 22.7   | 22.3      | 27.2      | 28.5      |
| HumanEval(Pass@1)| 0-shot| 20.7      | 26.8      | 44.5      | 53.7      |
| MBPP(Pass@1)     | 3-shot| 35.8      | 36.8      | 61.6      | 62.2      |
| GSM8K(EM)        | 8-shot| 25.4      | 31.4      | 72.3      | 74.0      |
| MATH(EM)         | 4-shot| 10.7      | 12.6      | 38.6      | 39.8      |

위 표는 MTP 전략에 대한 절제 결과를 보여줍니다. MTP 전략은 대부분의 평가 벤치마크에서 모델 성능을 일관되게 향상시킵니다.

구체적으로 서로 다른 규모의 두 기준 모델에서 MTP 전략을 검증합니다. 소규모에서는 1.33조 토큰에 대해 15.7B 총 파라미터를 포함하는 기준 MoE 모델을 훈련합니다. 대규모에서는 540B 토큰에 대해 228.7B 총 파라미터를 포함하는 기준 MoE 모델을 훈련합니다.

이들 위에 훈련 데이터와 다른 아키텍처를 동일하게 유지하면서 1-깊이 MTP 모듈을 추가하고 비교를 위해 MTP 전략을 사용하는 두 모델을 훈련합니다. 추론 중에는 MTP 모듈을 직접 폐기하므로 비교되는 모델들의 추론 비용은 정확히 동일합니다.

표에서 볼 수 있듯이 MTP 전략은 대부분의 평가 벤치마크에서 모델 성능을 일관되게 향상시킵니다. 특히 코딩 작업(HumanEval에서 20.7%에서 26.8%로, 대규모에서 44.5%에서 53.7%로)과 수학 작업(GSM8K에서 25.4%에서 31.4%로, 대규모에서 72.3%에서 74.0%로)에서 상당한 개선을 보입니다.

**보조 손실 없는 밸런싱 전략에 대한 절제 연구**

| 벤치마크 (메트릭) | # 샷 | Small MoE | Small MoE | Large MoE | Large MoE |
|------------------|------|-----------|-----------|-----------|-----------|
|                  |      | Aux-Loss-Based | Aux-Loss-Free | Aux-Loss-Based | Aux-Loss-Free |
| **활성화 파라미터** | - | 2.4B      | 2.4B      | 20.9B     | 20.9B     |
| **총 파라미터**    | - | 15.7B     | 15.7B     | 228.7B    | 228.7B    |
| **훈련 토큰**      | - | 1.33T     | 1.33T     | 578B      | 578B      |
| Pile-test(BPB)   | -    | 0.727     | 0.724     | 0.656     | 0.652     |
| BBH(EM)          | 3-shot| 37.3      | 39.3      | 66.7      | 67.9      |
| MMLU(EM)         | 5-shot| 51.0      | 51.8      | 68.3      | 67.2      |
| DROP(F1)         | 1-shot| 38.1      | 39.0      | 67.1      | 67.1      |
| TriviaQA(EM)     | 5-shot| 58.3      | 58.5      | 66.7      | 67.7      |
| NaturalQuestions(EM)| 5-shot| 23.2   | 23.4      | 27.1      | 28.1      |
| HumanEval(Pass@1)| 0-shot| 22.0      | 22.6      | 40.2      | 46.3      |
| MBPP(Pass@1)     | 3-shot| 36.6      | 35.8      | 59.2      | 61.2      |
| GSM8K(EM)        | 8-shot| 27.1      | 29.6      | 70.7      | 74.5      |
| MATH(EM)         | 4-shot| 10.9      | 11.1      | 37.2      | 39.6      |

위 표는 보조 손실 없는 밸런싱 전략에 대한 절제 결과를 보여줍니다. 순수한 보조 손실 기반 방법과 비교하여 보조 손실 없는 전략은 대부분의 평가 벤치마크에서 일관되게 더 나은 모델 성능을 달성합니다.

서로 다른 규모의 두 기준 모델에서 이 전략을 검증합니다. 소규모에서는 1.33조 토큰에 대해 15.7B 총 파라미터를 포함하는 기준 MoE 모델을 훈련합니다. 대규모에서는 578B 토큰에 대해 228.7B 총 파라미터를 포함하는 기준 MoE 모델을 훈련합니다.

두 기준 모델 모두 로드 밸런스를 장려하기 위해 순수하게 보조 손실을 사용하고, top-K 친화도 정규화와 함께 시그모이드 게이팅 함수를 사용합니다. 보조 손실의 강도를 제어하는 하이퍼파라미터는 각각 DeepSeek-V2-Lite와 DeepSeek-V2와 동일합니다.

이 두 기준 모델 위에 훈련 데이터와 다른 아키텍처를 동일하게 유지하면서 모든 보조 손실을 제거하고 비교를 위해 보조 손실 없는 밸런싱 전략을 도입합니다.

표에서 볼 수 있듯이 보조 손실 없는 전략은 대부분의 평가 벤치마크에서 일관되게 더 나은 모델 성능을 달성합니다. 특히 대규모 모델에서 HumanEval(40.2%에서 46.3%로), GSM8K(70.7%에서 74.5%로), MATH(37.2%에서 39.6%로)에서 상당한 개선을 보입니다.

**배치별 로드 밸런스 대 시퀀스별 로드 밸런스**

보조 손실 없는 밸런싱과 시퀀스별 보조 손실 간의 주요 차이점은 밸런싱 범위에 있습니다. 배치별 대 시퀀스별입니다. 시퀀스별 보조 손실과 비교하여 배치별 밸런싱은 각 시퀀스에서 도메인 내 밸런스를 강제하지 않으므로 더 유연한 제약을 부과합니다. 이러한 유연성은 전문가들이 서로 다른 도메인에서 더 잘 전문화할 수 있게 합니다.

이를 검증하기 위해 Pile 테스트 세트의 서로 다른 도메인에서 16B 보조 손실 기반 기준선과 16B 보조 손실 없는 모델의 전문가 로드를 기록하고 분석합니다.

![전문가 로드 분석](https://arxiv.org/html/2412.19437/x9.png)

위 그림은 Pile 테스트 세트의 세 도메인에서 보조 손실 없는 모델과 보조 손실 기반 모델의 전문가 로드를 보여주는 히트맵 세트입니다. 이 히트맵들은 2D 데이터 그리드의 상대적 값을 색상 코딩으로 나타내는 시각화 유형입니다. 주요 연구 의미는 보조 손실 없는 모델이 보조 손실 기반 모델과 비교하여 더 뚜렷한 전문가 전문화 패턴을 보인다는 것으로, 보조 손실 없는 모델의 히트맵에서 더 뚜렷한 색상 변화로 나타납니다. 상대적 전문가 로드는 실제 전문가 로드와 이론적으로 균형 잡힌 전문가 로드 간의 비율을 나타냅니다.

예상대로 보조 손실 없는 모델이 더 큰 전문가 전문화 패턴을 보여줍니다. 이는 배치별 밸런싱이 전문가들이 특정 도메인이나 작업 유형에 더 특화될 수 있도록 허용한다는 것을 의미합니다.

이러한 유연성과 모델 성능의 장점 간의 상관관계를 더 조사하기 위해 각 시퀀스 대신 각 훈련 배치에서 로드 밸런스를 장려하는 배치별 보조 손실을 추가로 설계하고 검증합니다. 실험 결과는 유사한 수준의 배치별 로드 밸런스를 달성할 때 배치별 보조 손실도 보조 손실 없는 방법과 유사한 모델 성능을 달성할 수 있음을 보여줍니다.

구체적으로 1B MoE 모델을 사용한 실험에서 검증 손실은 2.258(시퀀스별 보조 손실 사용), 2.253(보조 손실 없는 방법 사용), 2.253(배치별 보조 손실 사용)입니다. 3B MoE 모델에서도 유사한 결과를 관찰합니다. 시퀀스별 보조 손실을 사용하는 모델은 2.085의 검증 손실을 달성하고, 보조 손실 없는 방법이나 배치별 보조 손실을 사용하는 모델들은 2.080의 동일한 검증 손실을 달성합니다.

또한 배치별 로드 밸런싱 방법들이 일관된 성능 장점을 보이지만, 효율성 측면에서 두 가지 잠재적 과제에 직면합니다. (1) 특정 시퀀스나 작은 배치 내에서의 로드 불균형, (2) 추론 중 도메인 이동으로 인한 로드 불균형입니다.

첫 번째 과제는 대규모 전문가 병렬성과 데이터 병렬성을 사용하는 훈련 프레임워크에 의해 자연스럽게 해결되며, 이는 각 마이크로 배치의 큰 크기를 보장합니다. 두 번째 과제의 경우, 이를 극복하기 위해 중복 전문가 배포를 통한 효율적인 추론 프레임워크도 설계하고 구현합니다.
## 후처리

DeepSeek-V3의 후처리 과정은 사전 훈련된 기본 모델을 인간의 선호도에 맞추고 실용적인 응용을 위해 최적화하는 중요한 단계입니다. 이 과정은 지도 학습 미세 조정(Supervised Fine-Tuning, SFT)과 강화 학습(Reinforcement Learning, RL)의 두 단계로 구성되며, 특히 DeepSeek-R1 모델로부터의 추론 능력 증류를 통해 모델의 성능을 크게 향상시킵니다.

### 지도 학습 미세 조정

DeepSeek-V3의 지도 학습 미세 조정을 위해 다양한 도메인에 걸쳐 150만 개의 인스턴스로 구성된 지시 조정 데이터셋을 구축했습니다. 각 도메인은 특정 요구사항에 맞춘 고유한 데이터 생성 방법을 사용합니다.

**추론 데이터 생성**

수학, 코드 경쟁 문제, 논리 퍼즐을 포함한 추론 관련 데이터셋의 경우, 내부 DeepSeek-R1 모델을 활용하여 데이터를 생성합니다. R1이 생성한 데이터는 높은 정확도를 보이지만 과도한 사고(overthinking), 형식 문제, 과도한 길이 등의 문제점을 가지고 있습니다. 이러한 문제를 해결하기 위해 R1 생성 추론 데이터의 높은 정확도와 일반적으로 형식화된 추론 데이터의 명확성과 간결성 사이의 균형을 맞추는 것이 목표입니다.

이를 위한 방법론은 다음과 같습니다. 먼저 코드, 수학, 일반 추론과 같은 특정 도메인에 맞춘 전문가 모델을 지도 학습 미세 조정과 강화 학습 훈련 파이프라인을 결합하여 개발합니다. 이 전문가 모델은 최종 모델을 위한 데이터 생성기 역할을 합니다.

훈련 과정에서는 각 인스턴스에 대해 두 가지 서로 다른 유형의 SFT 샘플을 생성합니다. 첫 번째는 문제와 원래 응답을 <문제, 원래 응답> 형식으로 결합하고, 두 번째는 시스템 프롬프트와 문제, R1 응답을 <시스템 프롬프트, 문제, R1 응답> 형식으로 통합합니다. 시스템 프롬프트는 모델이 반성과 검증 메커니즘이 풍부한 응답을 생성하도록 안내하는 지시사항을 포함하도록 세심하게 설계됩니다.

강화 학습 단계에서는 모델이 높은 온도 샘플링을 활용하여 명시적인 시스템 프롬프트가 없어도 R1 생성 데이터와 원래 데이터 모두의 패턴을 통합하는 응답을 생성합니다. 수백 번의 RL 단계를 거친 후, 중간 RL 모델은 R1 패턴을 통합하는 방법을 학습하여 전반적인 성능을 전략적으로 향상시킵니다.

RL 훈련 단계를 완료한 후, 최종 모델을 위한 고품질 SFT 데이터를 선별하기 위해 거부 샘플링(rejection sampling)을 구현합니다. 여기서 전문가 모델들이 데이터 생성 소스로 사용됩니다. 이 방법은 최종 훈련 데이터가 DeepSeek-R1의 강점을 유지하면서도 간결하고 효과적인 응답을 생성하도록 보장합니다.

**비추론 데이터 생성**

창작 글쓰기, 역할 놀이, 간단한 질문 답변과 같은 비추론 데이터의 경우, DeepSeek-V2.5를 사용하여 응답을 생성하고 인간 주석자들이 데이터의 정확성과 올바름을 검증합니다.

**SFT 설정**

DeepSeek-V3-Base를 SFT 데이터셋으로 2 에포크 동안 미세 조정하며, $5 \times 10^{-6}$에서 시작하여 점진적으로 $1 \times 10^{-6}$까지 감소하는 코사인 감쇠 학습률 스케줄링을 사용합니다. 훈련 중에는 각 단일 시퀀스가 여러 샘플로부터 패킹되지만, 이러한 예시들이 격리되고 상호 보이지 않도록 보장하는 샘플 마스킹 전략을 채택합니다.

### 강화 학습

DeepSeek-V3의 강화 학습 과정에서는 규칙 기반 보상 모델과 모델 기반 보상 모델을 모두 사용합니다.

#### 보상 모델

**규칙 기반 보상 모델**

특정 규칙을 사용하여 검증할 수 있는 질문들의 경우, 규칙 기반 보상 시스템을 채택하여 피드백을 결정합니다. 예를 들어, 특정 수학 문제들은 결정론적 결과를 가지며, 모델이 지정된 형식(예: 박스 안에) 내에서 최종 답을 제공하도록 요구하여 규칙을 적용해 정확성을 검증할 수 있습니다. 마찬가지로 LeetCode 문제의 경우, 컴파일러를 활용하여 테스트 케이스를 기반으로 피드백을 생성할 수 있습니다. 가능한 곳에서 규칙 기반 검증을 활용함으로써 조작이나 악용에 저항력이 있는 더 높은 수준의 신뢰성을 보장합니다.

**모델 기반 보상 모델**

자유 형식의 정답을 가진 질문들의 경우, 보상 모델에 의존하여 응답이 예상 정답과 일치하는지 결정합니다. 반대로 창작 글쓰기와 같이 명확한 정답이 없는 질문들의 경우, 보상 모델은 질문과 해당 답변을 입력으로 하여 피드백을 제공하는 역할을 합니다.

보상 모델은 DeepSeek-V3 SFT 체크포인트로부터 훈련됩니다. 신뢰성을 향상시키기 위해 최종 보상뿐만 아니라 보상으로 이어지는 체인 오브 소트도 포함하는 선호 데이터를 구축합니다. 이 접근법은 특정 작업에서 보상 해킹의 위험을 완화하는 데 도움이 됩니다.

#### Group Relative Policy Optimization

DeepSeek-V2와 유사하게, 일반적으로 정책 모델과 동일한 크기의 비평 모델을 포기하고 대신 그룹 점수로부터 기준선을 추정하는 Group Relative Policy Optimization (GRPO)을 채택합니다.

구체적으로, 각 질문 $q$에 대해 GRPO는 이전 정책 모델 $\pi_{\theta_{old}}$로부터 출력 그룹 $\{o_1, o_2, \cdots, o_G\}$를 샘플링하고, 다음 목적 함수를 최대화하여 정책 모델 $\pi_{\theta}$를 최적화합니다.

$$\mathcal{J}_{GRPO}(\theta) = \mathbb{E}[q\sim P(Q),\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{old}}(O|q)] \frac{1}{G}\sum_{i=1}^{G}\left(\min\left(\frac{\pi_{\theta}(o_{i}|q)}{\pi_{\theta_{old}}(o_{i}|q)}A_{i},\text{clip}\left(\frac{\pi_{\theta}(o_{i}|q)}{\pi_{\theta_{old}}(o_{i}|q)},1-\varepsilon,1+\varepsilon\right)A_{i}\right)-\beta \mathbb{D}_{KL}\left(\pi_{\theta}||\pi_{ref}\right)\right)$$

여기서 KL 발산은 다음과 같이 정의됩니다.

$$\mathbb{D}_{KL}\left(\pi_{\theta}||\pi_{ref}\right)=\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}-\log\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}-1$$

$\varepsilon$과 $\beta$는 하이퍼파라미터이고, $\pi_{ref}$는 참조 모델이며, $A_i$는 각 그룹 내 출력에 해당하는 보상 $\{r_1, r_2, \ldots, r_G\}$로부터 도출되는 어드밴티지입니다.

$$A_{i}=\frac{r_{i}-{\operatorname{mean}(\{r_{1},r_{2},\cdots,r_{G}\})}}{{\operatorname{std}(\{r_{1},r_{2},\cdots,r_{G}\})}}$$

이 방법은 각 그룹 내에서 보상의 평균과 표준편차를 사용하여 어드밴티지를 정규화함으로써, 서로 다른 그룹 간의 보상 스케일 차이를 완화하고 더 안정적인 학습을 가능하게 합니다.

RL 과정에서는 코딩, 수학, 글쓰기, 역할 놀이, 질문 답변과 같은 다양한 도메인의 프롬프트를 통합합니다. 이 접근법은 모델을 인간의 선호도에 더 가깝게 정렬할 뿐만 아니라 특히 사용 가능한 SFT 데이터가 제한된 시나리오에서 벤치마크 성능을 향상시킵니다.
## 평가

DeepSeek-V3의 평가는 표준 벤치마크와 개방형 대화 평가를 통해 포괄적으로 수행되었으며, 현재 가장 강력한 오픈소스 모델임을 입증하는 결과를 보여줍니다.

### 평가 설정

**평가 벤치마크**

기본 모델 테스트에 사용된 벤치마크 외에도, 지시 조정된 모델들은 다양한 추가 벤치마크에서 평가되었습니다. [IFEval](https://arxiv.org/pdf/2311.07911)은 지시 따르기 능력을 객관적으로 평가하는 벤치마크로, "400단어 이상으로 작성하세요" 또는 "AI라는 키워드를 최소 3번 언급하세요"와 같은 검증 가능한 지시사항을 통해 모델의 지시 준수 능력을 측정합니다. 이는 주관적인 인간 판단에 의존하지 않고 객관적으로 평가할 수 있는 장점을 제공합니다.

[FRAMES](https://arxiv.org/pdf/2409.12941)는 검색 증강 생성(RAG) 시스템의 사실성, 검색, 추론 능력을 통합적으로 평가하는 프레임워크입니다. 이 벤치마크는 여러 Wikipedia 기사의 정보를 결합해야 하는 다중 홉 추론 문제를 포함하며, 100K 토큰 컨텍스트에서 질문 답변을 요구하여 모델의 긴 컨텍스트 이해 능력을 평가합니다.

[LongBench v2](https://arxiv.org/pdf/2412.15204)는 현실적인 긴 컨텍스트 다중 작업에 대한 깊은 이해와 추론을 평가하는 벤치마크입니다. GPQA는 박사 수준의 과학 질문을 다루는 평가 데이터셋이며, SimpleQA와 C-SimpleQA는 각각 영어와 중국어 사실 지식을 평가합니다.

코딩 관련 평가에서는 SWE-Bench Verified, Aider, LiveCodeBench(2024년 8월-11월 문제), Codeforces 등이 사용되었습니다. 수학 평가에는 중국 전국 고등학교 수학 올림피아드(CNMO 2024)와 미국 수학 초청 시험(AIME 2024)이 포함되었습니다.

**비교 기준선**

DeepSeek-V3는 여러 강력한 기준선과 비교되었습니다. 오픈소스 모델로는 DeepSeek-V2-0506, DeepSeek-V2.5-0905, Qwen2.5 72B Instruct, LLaMA-3.1 405B Instruct가 포함되었고, 클로즈드소스 모델로는 Claude-Sonnet-3.5-1022와 GPT-4o-0513이 사용되었습니다.

**세부 평가 구성**

MMLU, DROP, GPQA, SimpleQA와 같은 표준 벤치마크에서는 [simple-evals 프레임워크](https://github.com/openai/simple-evals)의 평가 프롬프트를 채택했습니다. MMLU-Redux의 경우 Zero-Eval 프롬프트 형식을 제로샷 설정에서 사용했습니다.

코드 및 수학 벤치마크에서는 특별한 설정이 적용되었습니다. HumanEval-Mul 데이터셋은 Python, Java, C++, C#, JavaScript, TypeScript, PHP, Bash 등 8개 주요 프로그래밍 언어를 포함합니다. LiveCodeBench에서는 CoT와 non-CoT 방법을 모두 사용하여 성능을 평가했습니다. SWE-Bench verified는 [agentless 프레임워크](https://arxiv.org/pdf/2403.07974)를 사용하여 평가되었으며, Aider 관련 벤치마크는 "diff" 형식으로 평가되었습니다.

수학 평가에서는 AIME과 CNMO 2024를 온도 0.7로 설정하여 16회 실행의 평균을 구했으며, MATH-500은 그리디 디코딩을 사용했습니다. 모든 모델은 각 벤치마크에서 최대 8192개의 토큰을 출력할 수 있도록 설정되었습니다.

### 표준 평가

| 벤치마크 (메트릭) | DeepSeek V2-0506 | DeepSeek V2.5-0905 | Qwen2.5 72B-Inst. | LLaMA-3.1 405B-Inst. | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 |
|------------------|------------------|--------------------|--------------------|----------------------|-------------------------|-------------|-------------|
| **아키텍처** | MoE | MoE | Dense | Dense | -- | -- | MoE |
| **활성화 파라미터** | 21B | 21B | 72B | 405B | -- | -- | 37B |
| **총 파라미터** | 236B | 236B | 72B | 405B | -- | -- | 671B |
| **영어** | | | | | | | |
| MMLU(EM) | 78.2 | 80.6 | 85.3 | 88.6 | 88.3 | 87.2 | 88.5 |
| MMLU-Redux(EM) | 77.9 | 80.3 | 85.6 | 86.2 | 88.9 | 88.0 | 89.1 |
| MMLU-Pro(EM) | 58.5 | 66.2 | 71.6 | 73.3 | 78.0 | 72.6 | 75.9 |
| DROP(3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | 91.6 |
| IF-Eval(Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | 86.5 | 84.3 | 86.1 |
| GPQA-Diamond(Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | 65.0 | 49.9 | 59.1 |
| SimpleQA(Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | 38.2 | 24.9 |
| FRAMES(Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | 80.5 | 73.3 |
| LongBench v2(Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | 48.7 |
| **코드** | | | | | | | |
| HumanEval-Mul(Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | 82.6 |
| LiveCodeBench(Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | 40.5 |
| LiveCodeBench(Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | 37.6 |
| Codeforces(Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | 51.6 |
| SWE Verified(Resolved) | - | 22.6 | 23.8 | 24.5 | 50.8 | 38.8 | 42.0 |
| Aider-Edit(Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | 84.2 | 72.9 | 79.7 |
| Aider-Polyglot(Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | 49.6 |
| **수학** | | | | | | | |
| AIME 2024(Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | 39.2 |
| MATH-500(EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | 90.2 |
| CNMO 2024(Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | 43.2 |
| **중국어** | | | | | | | |
| CLUEWSC(EM) | 89.9 | 90.4 | 91.4 | 84.7 | 85.4 | 87.9 | 90.9 |
| C-Eval(EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | 86.5 |
| C-SimpleQA(Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | 64.8 |

위 표는 DeepSeek-V3와 다른 대표적인 채팅 모델들 간의 비교를 보여줍니다. 모든 모델은 출력 길이를 8K로 제한하는 구성에서 평가되었으며, 1000개 미만의 샘플을 포함하는 벤치마크는 다양한 온도 설정을 사용하여 여러 번 테스트하여 견고한 최종 결과를 도출했습니다.

**영어 벤치마크 성능**

MMLU는 다양한 지식 도메인과 작업에서 대규모 언어 모델의 성능을 평가하는 널리 인정받는 벤치마크입니다. DeepSeek-V3는 88.5점으로 경쟁력 있는 성능을 보여주며, LLaMA-3.1-405B, GPT-4o, Claude-Sonnet 3.5와 같은 최고 수준의 모델들과 동등한 수준을 달성하면서 Qwen2.5 72B를 크게 앞섰습니다.

더 도전적인 교육 지식 벤치마크인 MMLU-Pro에서 DeepSeek-V3는 75.9점으로 Claude-Sonnet 3.5에 근접한 성능을 보였습니다. 수정된 라벨을 가진 MMLU의 개선된 버전인 MMLU-Redux에서는 89.1점으로 다른 모델들을 능가했습니다.

박사 수준의 평가 테스트베드인 GPQA-Diamond에서 DeepSeek-V3는 59.1점이라는 주목할 만한 결과를 달성하여 Claude 3.5 Sonnet에 이어 두 번째를 기록하고 다른 모든 경쟁자들을 상당한 차이로 앞섰습니다.

긴 컨텍스트 이해 벤치마크인 DROP, LongBench v2, FRAMES에서 DeepSeek-V3는 최고 수준 모델로서의 위치를 지속적으로 입증했습니다. DROP에서는 3-shot 설정에서 인상적인 91.6 F1 점수를 달성하여 이 카테고리의 모든 다른 모델들을 능가했습니다. 100K 토큰 컨텍스트에서 질문 답변을 요구하는 FRAMES에서는 73.3점으로 GPT-4o에 근접하면서 다른 모든 모델들을 상당한 차이로 앞섰습니다.

DeepSeek V3 출시 몇 주 전에 발표된 데이터셋인 LongBench v2에서 DeepSeek-V3는 48.7점으로 최고 성능을 달성하여 극도로 긴 컨텍스트 작업에서의 강력한 능력을 검증했습니다.

사실 지식 벤치마크인 SimpleQA에서 DeepSeek-V3는 24.9점으로 GPT-4o와 Claude-Sonnet보다 낮은 성능을 보였는데, 이는 설계 초점과 자원 할당 때문입니다. DeepSeek-V3는 중국어 지식 학습에 더 많은 훈련 토큰을 할당하여 C-SimpleQA에서 64.8점이라는 뛰어난 성능을 달성했습니다.

지시 따르기 벤치마크에서 DeepSeek-V3는 86.1점으로 이전 모델인 DeepSeek-V2 시리즈를 크게 능가하여 사용자 정의 형식 제약을 이해하고 준수하는 능력이 향상되었음을 보여줍니다.

**코드 및 수학 벤치마크 성능**

코딩은 LLM에게 도전적이고 실용적인 작업으로, SWE-Bench-Verified와 Aider 같은 엔지니어링 중심 작업과 HumanEval, LiveCodeBench 같은 알고리즘 작업을 포함합니다.

엔지니어링 작업에서 DeepSeek-V3는 Claude-Sonnet-3.5-1022보다는 낮지만 오픈소스 모델들을 크게 능가하는 성능을 보였습니다. 오픈소스 DeepSeek-V3는 코딩 관련 엔지니어링 작업의 발전을 촉진할 것으로 기대됩니다. 견고한 능력에 대한 접근을 제공함으로써 DeepSeek-V3는 소프트웨어 엔지니어링과 알고리즘 개발 같은 분야에서 혁신과 개선을 주도할 수 있습니다.

알고리즘 작업에서 DeepSeek-V3는 뛰어난 성능을 보여 HumanEval-Mul과 LiveCodeBench 같은 벤치마크에서 모든 기준선을 능가했습니다. 이러한 성공은 코드 생성과 알고리즘 중심 작업에서 문제 해결 능력을 효과적으로 향상시키는 고급 지식 증류 기법에 기인할 수 있습니다.

수학 벤치마크에서 DeepSeek-V3는 뛰어난 성능을 보여 기준선들을 크게 능가하고 비o1형 모델들에 대한 새로운 최첨단 성능을 설정했습니다. 구체적으로 AIME, MATH-500, CNMO 2024에서 DeepSeek-V3는 두 번째로 좋은 모델인 Qwen2.5 72B를 절대 점수로 약 10% 앞섰는데, 이는 이러한 도전적인 벤치마크에서 상당한 차이입니다.

이러한 주목할 만한 능력은 비o1형 모델들에게 매우 유익한 것으로 입증된 DeepSeek-R1으로부터의 증류 기법의 효과를 강조합니다.

**중국어 벤치마크 성능**

Qwen과 DeepSeek은 중국어와 영어를 모두 견고하게 지원하는 두 대표적인 모델 시리즈입니다. 중국어 사실 벤치마크인 Chinese SimpleQA에서 DeepSeek-V3는 Qwen2.5-72B를 16.4점 앞섰습니다. 이는 Qwen2.5가 DeepSeek-V3의 14.8T 토큰보다 20% 많은 18T 토큰으로 구성된 더 큰 코퍼스에서 훈련되었음에도 불구하고 달성한 결과입니다.

중국어 교육 지식 평가의 대표적인 벤치마크인 C-Eval과 CLUEWSC(Chinese Winograd Schema Challenge)에서 DeepSeek-V3와 Qwen2.5-72B는 유사한 성능 수준을 보여 두 모델 모두 도전적인 중국어 추론과 교육 작업에 잘 최적화되어 있음을 나타냅니다.

### 개방형 평가

표준 벤치마크 외에도 LLM을 판사로 사용하는 개방형 생성 작업에서도 모델을 평가했습니다.

| 모델 | Arena-Hard | AlpacaEval 2.0 |
|------|------------|----------------|
| DeepSeek-V2.5-0905 | 76.2 | 50.5 |
| Qwen2.5-72B-Instruct | 81.2 | 49.1 |
| LLaMA-3.1 405B | 69.3 | 40.5 |
| GPT-4o-0513 | 80.4 | 51.1 |
| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |
| DeepSeek-V3 | 85.5 | 70.0 |

위 표는 영어 개방형 대화 평가 결과를 보여줍니다. AlpacaEval 2.0에서는 길이 제어 승률을 메트릭으로 사용했습니다.

[AlpacaEval 2.0](https://arxiv.org/pdf/2404.04475)과 [Arena-Hard](https://arxiv.org/pdf/2406.11939)의 원래 구성을 준수하여 GPT-4-Turbo-1106을 쌍별 비교의 판사로 활용했습니다.

Arena-Hard에서 DeepSeek-V3는 기준선 GPT-4-0314에 대해 86% 이상의 인상적인 승률을 달성하여 Claude-Sonnet-3.5-1022와 같은 최고 수준 모델들과 동등한 성능을 보였습니다. 이는 특히 코딩과 디버깅 작업을 포함한 복잡한 프롬프트를 다루는 DeepSeek-V3의 견고한 능력을 강조합니다.

더 나아가 DeepSeek-V3는 Arena-Hard 벤치마크에서 85%를 넘어선 첫 번째 오픈소스 모델이라는 획기적인 이정표를 달성했습니다. 이 성취는 오픈소스와 클로즈드소스 모델 간의 성능 격차를 크게 줄여 오픈소스 모델이 도전적인 도메인에서 달성할 수 있는 새로운 기준을 설정했습니다.

마찬가지로 DeepSeek-V3는 AlpacaEval 2.0에서 70.0점으로 클로즈드소스와 오픈소스 모델 모두를 능가하는 뛰어난 성능을 보였습니다. 이는 글쓰기 작업과 간단한 질문 답변 시나리오를 처리하는 뛰어난 숙련도를 보여줍니다. 특히 DeepSeek-V2.5-0905를 20%라는 상당한 차이로 능가하여 간단한 작업을 다루는 데 있어서의 상당한 개선과 발전의 효과를 보여줍니다.

### 생성형 보상 모델로서의 DeepSeek-V3

DeepSeek-V3의 판단 능력을 GPT-4o와 Claude-3.5와 같은 최첨단 모델들과 비교했습니다.

| 모델 | Chat | Chat-Hard | Safety | Reasoning | Average |
|------|------|-----------|--------|-----------|---------|
| GPT-4o-0513 | 96.6 | 70.4 | 86.7 | 84.9 | 84.7 |
| GPT-4o-0806 | 96.1 | 76.1 | 88.1 | 86.6 | 86.7 |
| GPT-4o-1120 | 95.8 | 71.3 | 86.2 | 85.2 | 84.6 |
| Claude-3.5-sonnet-0620 | 96.4 | 74.0 | 81.6 | 84.7 | 84.2 |
| Claude-3.5-sonnet-1022 | 96.4 | 79.7 | 91.1 | 87.6 | 88.7 |
| DeepSeek-V3 | 96.9 | 79.8 | 87.0 | 84.3 | 87.0 |
| DeepSeek-V3 (maj@6) | 96.9 | 82.6 | 89.5 | 89.2 | 89.6 |

위 표는 [RewardBench](https://arxiv.org/pdf/2403.13787)에서 GPT-4o, Claude-3.5-sonnet, DeepSeek-V3의 성능을 보여줍니다.

DeepSeek-V3는 GPT-4o-0806과 Claude-3.5-Sonnet-1022의 최고 버전들과 동등한 성능을 달성하면서 다른 버전들을 능가했습니다. 또한 DeepSeek-V3의 판단 능력은 투표 기법을 통해 향상될 수 있습니다. 따라서 투표와 함께 DeepSeek-V3를 사용하여 개방형 질문에 대한 자기 피드백을 제공함으로써 정렬 과정의 효과성과 견고성을 향상시킵니다.

### 논의

**DeepSeek-R1으로부터의 증류**

DeepSeek-V2.5를 기반으로 DeepSeek-R1으로부터의 증류 기여도를 절제 연구했습니다. 기준선은 짧은 CoT 데이터로 훈련된 반면, 경쟁자는 위에서 설명한 전문가 체크포인트에 의해 생성된 데이터를 사용합니다.

| 모델 | LiveCodeBench-CoT | MATH-500 |
|------|-------------------|----------|
|      | Pass@1 | Length | Pass@1 | Length |
| DeepSeek-V2.5 Baseline | 31.1 | 787 | 74.6 | 769 |
| DeepSeek-V2.5 +R1 Distill | 37.4 | 783 | 83.2 | 1510 |

위 표는 DeepSeek-R1으로부터의 증류 기여도를 보여줍니다. LiveCodeBench와 MATH-500의 평가 설정은 앞서 제시한 표와 동일합니다.

증류 데이터의 효과를 보여주며 LiveCodeBench와 MATH-500 벤치마크 모두에서 상당한 개선을 나타냅니다. 실험에서 흥미로운 트레이드오프를 발견했습니다. 증류는 더 나은 성능으로 이어지지만 평균 응답 길이도 상당히 증가시킵니다. 모델 정확도와 계산 효율성 간의 균형을 유지하기 위해 DeepSeek-V3의 증류에서 최적 설정을 신중하게 선택했습니다.

연구 결과는 추론 모델로부터의 지식 증류가 후처리 최적화를 위한 유망한 방향을 제시한다고 제안합니다. 현재 작업은 수학과 코딩 도메인에서 데이터를 증류하는 데 초점을 맞추고 있지만, 이 접근법은 다양한 작업 도메인에 걸친 더 넓은 응용에 대한 잠재력을 보여줍니다. 이러한 특정 영역에서 입증된 효과는 복잡한 추론이 필요한 다른 인지 작업에서 긴 CoT 증류가 가치 있을 수 있음을 나타냅니다.

**자기 보상**

보상은 RL에서 최적화 과정을 조정하는 중추적인 역할을 합니다. 일부 코딩이나 수학 시나리오와 같이 외부 도구를 통한 검증이 간단한 도메인에서 RL은 뛰어난 효과를 보여줍니다. 하지만 더 일반적인 시나리오에서는 하드 코딩을 통한 피드백 메커니즘 구축이 비실용적입니다.

DeepSeek-V3 개발 중에 이러한 더 넓은 맥락에서 [constitutional AI 접근법](https://arxiv.org/pdf/2212.08073)을 사용하여 DeepSeek-V3 자체의 투표 평가 결과를 피드백 소스로 활용했습니다. 이 방법은 주목할 만한 정렬 효과를 가져와 주관적 평가에서 DeepSeek-V3의 성능을 크게 향상시켰습니다.

추가적인 헌법적 입력을 통합함으로써 DeepSeek-V3는 헌법적 방향으로 최적화할 수 있습니다. 보완 정보와 LLM을 피드백 소스로 결합하는 이 패러다임이 매우 중요하다고 믿습니다. LLM은 다양한 시나리오의 비구조화된 정보를 보상으로 변환할 수 있는 다재다능한 프로세서 역할을 하여 궁극적으로 LLM의 자기 개선을 촉진합니다.

자기 보상을 넘어서 일반적인 시나리오에서 모델 능력을 지속적으로 발전시키기 위한 다른 일반적이고 확장 가능한 보상 방법을 발견하는 데도 전념하고 있습니다.

**멀티 토큰 예측 평가**

단일 다음 토큰만 예측하는 대신 DeepSeek-V3는 MTP 기법을 통해 다음 2개 토큰을 예측합니다. [투기적 디코딩](https://arxiv.org/pdf/2403.07974) 프레임워크와 결합하여 모델의 디코딩 속도를 크게 가속화할 수 있습니다.

추가로 예측된 토큰의 수용률에 대한 자연스러운 질문이 제기됩니다. 평가에 따르면 두 번째 토큰 예측의 수용률은 다양한 생성 주제에 걸쳐 85%와 90% 사이에서 일관된 신뢰성을 보여줍니다. 이러한 높은 수용률은 DeepSeek-V3가 크게 향상된 디코딩 속도를 달성하여 1.8배의 TPS(초당 토큰 수)를 제공할 수 있게 합니다.
## 결론, 한계점, 그리고 향후 연구 방향

DeepSeek-V3는 총 671B개의 파라미터를 가지며 토큰당 37B개의 파라미터가 활성화되는 대규모 MoE 언어 모델로서, 14.8조 개의 토큰으로 훈련되었습니다. 앞서 설명한 MLA와 DeepSeekMoE 아키텍처 외에도, 로드 밸런싱을 위한 보조 손실 없는 전략을 개척하고 더 강력한 성능을 위한 멀티 토큰 예측 훈련 목표를 설정했습니다.

### 주요 성과와 기술적 혁신

DeepSeek-V3의 훈련은 FP8 훈련 지원과 세심한 엔지니어링 최적화로 인해 비용 효과적으로 이루어졌습니다. 후처리 과정에서는 DeepSeek-R1 시리즈 모델로부터 추론 능력을 성공적으로 증류했습니다. 포괄적인 평가 결과 DeepSeek-V3는 현재 사용 가능한 가장 강력한 오픈소스 모델로 부상했으며, GPT-4o와 Claude-3.5-Sonnet과 같은 선도적인 클로즈드소스 모델들과 비교할 만한 성능을 달성했습니다.

뛰어난 성능에도 불구하고 DeepSeek-V3는 경제적인 훈련 비용을 유지합니다. 사전 훈련, 컨텍스트 길이 확장, 후처리를 포함한 전체 훈련에 단 2.788M H800 GPU 시간만을 필요로 합니다. 특히 주목할 점은 전체 훈련 과정에서 복구 불가능한 손실 급증이나 롤백을 경험하지 않았다는 것으로, 이는 훈련 프레임워크의 안정성을 입증합니다.

### 배포 관련 한계점

강력한 성능과 비용 효과성을 인정하면서도, DeepSeek-V3가 특히 배포 측면에서 몇 가지 한계점을 가지고 있음을 인식해야 합니다. 

첫째, 효율적인 추론을 보장하기 위해 DeepSeek-V3의 권장 배포 단위가 상대적으로 크며, 이는 소규모 팀에게 부담이 될 수 있습니다. 앞서 설명한 배포 전략에서 보듯이 prefilling 단계에서는 최소 32개의 GPU(4개 노드), decoding 단계에서는 320개의 GPU(40개 노드)가 필요합니다. 이러한 대규모 하드웨어 요구사항은 제한된 자원을 가진 연구팀이나 스타트업에게는 접근 장벽이 될 수 있습니다.

둘째, DeepSeek-V3의 배포 전략이 DeepSeek-V2 대비 2배 이상의 종단간 생성 속도를 달성했음에도 불구하고, 여전히 추가적인 향상 가능성이 남아있습니다. 현재의 배포 최적화는 상당한 성과를 거두었지만, 더 진보된 하드웨어와 알고리즘 개선을 통해 추가적인 성능 향상이 가능할 것으로 예상됩니다.

다행히 이러한 한계점들은 더 진보된 하드웨어의 발전과 함께 자연스럽게 해결될 것으로 기대됩니다. 차세대 GPU 아키텍처와 네트워킹 기술의 발전은 배포 요구사항을 줄이고 추론 효율성을 더욱 향상시킬 것입니다.

### 장기적 비전과 연구 방향

DeepSeek은 장기주의적 관점에서 오픈소스 모델의 경로를 일관되게 고수하며, AGI(Artificial General Intelligence)라는 궁극적 목표에 꾸준히 접근하는 것을 목표로 합니다. 미래에는 다음과 같은 방향으로 전략적 투자를 계획하고 있습니다.

**모델 아키텍처의 지속적 연구와 개선**

훈련과 추론 효율성을 모두 향상시키기 위해 모델 아키텍처를 지속적으로 연구하고 개선할 예정입니다. 특히 무한 컨텍스트 길이에 대한 효율적 지원에 근접하는 것을 목표로 합니다. 앞서 소개한 MLA 아키텍처와 DeepSeekMoE의 성공을 바탕으로, 더욱 효율적인 어텐션 메커니즘과 전문가 활용 전략을 개발할 것입니다.

또한 트랜스포머의 아키텍처적 한계를 돌파하여 모델링 능력의 경계를 확장하려고 시도할 것입니다. 현재의 트랜스포머 기반 아키텍처가 가진 근본적인 제약사항들을 극복하고, 새로운 패러다임의 아키텍처를 탐구하여 더욱 강력한 모델링 능력을 달성하고자 합니다.

**훈련 데이터의 양과 질 지속적 개선**

훈련 데이터의 양과 질을 지속적으로 반복 개선하고, 추가적인 훈련 신호 소스의 통합을 탐구하여 더 포괄적인 차원에서 데이터 스케일링을 추진할 예정입니다. 현재 14.8조 토큰의 고품질 데이터셋을 구축한 경험을 바탕으로, 더욱 다양하고 풍부한 데이터 소스를 발굴하고 통합하는 방법을 연구할 것입니다.

특히 다국어 데이터의 균형적 확장, 전문 도메인 지식의 체계적 통합, 그리고 새로운 형태의 학습 신호(예: 멀티모달 데이터, 상호작용 데이터 등)를 활용하는 방안을 모색할 것입니다.

**깊은 사고 능력의 지속적 탐구**

모델의 추론 길이와 깊이를 확장하여 지능과 문제 해결 능력을 향상시키는 것을 목표로 모델의 깊은 사고 능력을 지속적으로 탐구하고 반복 개선할 예정입니다. 앞서 설명한 DeepSeek-R1으로부터의 증류 기법의 성공을 바탕으로, 더욱 정교하고 체계적인 추론 능력을 개발할 것입니다.

이는 단순히 더 긴 추론 체인을 생성하는 것을 넘어서, 복잡한 문제를 체계적으로 분해하고 해결하는 능력, 다단계 추론을 통한 논리적 일관성 유지, 그리고 창의적 문제 해결 능력의 향상을 포함합니다.

**포괄적이고 다차원적인 모델 평가 방법 탐구**

연구 과정에서 고정된 벤치마크 세트에 대한 최적화 경향을 방지하기 위해 더 포괄적이고 다차원적인 모델 평가 방법을 탐구할 예정입니다. 이는 모델 능력에 대한 오해를 불러일으키고 기본적인 평가에 영향을 줄 수 있는 문제를 해결하기 위함입니다.

현재의 표준 벤치마크들이 가진 한계를 인식하고, 실제 사용 환경에서의 모델 성능을 더 정확하게 반영할 수 있는 새로운 평가 프레임워크를 개발할 것입니다. 이는 정적인 벤치마크를 넘어서 동적이고 적응적인 평가 방법, 인간과의 상호작용을 통한 평가, 그리고 실제 응용 시나리오에서의 성능 측정을 포함합니다.

### 오픈소스 생태계에 대한 기여

DeepSeek-V3는 단순히 강력한 모델을 개발하는 것을 넘어서, 오픈소스 AI 생태계 전체의 발전에 기여하고자 합니다. 모델의 공개를 통해 연구 커뮤니티가 더 나은 모델을 개발할 수 있는 기반을 제공하고, 혁신적인 아키텍처와 훈련 기법들이 널리 활용될 수 있도록 지원할 것입니다.

특히 경제적인 훈련 비용과 효율적인 추론 성능을 달성한 경험을 공유함으로써, 더 많은 연구자들과 개발자들이 대규모 언어 모델 연구에 참여할 수 있는 진입 장벽을 낮추고자 합니다. 이를 통해 AI 기술의 민주화와 다양한 혁신의 촉진에 기여할 것으로 기대됩니다.

DeepSeek-V3의 개발과 공개는 AGI를 향한 여정에서 중요한 이정표가 되며, 오픈소스 모델이 클로즈드소스 모델과 경쟁할 수 있는 수준에 도달했음을 보여주는 의미 있는 성과입니다. 앞으로도 지속적인 연구와 개발을 통해 더욱 강력하고 효율적인 모델을 개발하여 인공지능 기술의 발전에 기여할 것입니다.
- - -
### References
* [DeepSeek-V3 Technical Report](http://arxiv.org/pdf/2412.19437v2)