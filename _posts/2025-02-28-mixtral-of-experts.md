---
layout: post
title: "Mixtral of Experts"
date: 2024-01-08 18:47:34
author: "Mistral AI"
categories: "Language-Models"
tags: ["Sparse-Mixture-of-Experts-Architecture", "Dynamic-Expert-Routing", "Two-Expert-Token-Processing", "Efficient-Inference-with-Reduced-Active-Parameters", "Multilingual-Performance-Scaling", "Long-Context-Retrieval-Optimization", "Direct-Preference-Optimization-for-Instruction-Tuning", "Expert-Selection-Locality-Analysis", "Adaptive-Expert-Assignment", "Computational-Efficiency-in-Large-Language-Models"]
cover: /assets/images/language-models.jpg
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
대규모 언어 모델의 지속적인 성능 향상과 계산 효율성 사이의 균형을 찾는 것이 이 연구의 핵심 동기였습니다. 기존 트랜스포머 모델들은 모델 크기를 늘릴수록 계산 비용과 메모리 요구사항이 급격히 증가하는 문제를 겪고 있었습니다. 연구진은 모델의 용량을 확장하면서도 계산 효율성을 유지할 수 있는 새로운 아키텍처의 필요성을 인식했습니다. 특히 오픈소스 AI 모델이 상용 모델과 경쟁할 수 있는 수준의 성능을 달성하는 것이 주요 목표였습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
연구진은 희소 전문가 혼합(Sparse Mixture of Experts, SMoE) 아키텍처를 도입하여 이 문제를 해결했습니다. 이 접근법의 핵심은 각 레이어에 8개의 전문가 네트워크를 두고, 각 토큰마다 단 2개의 전문가만 선택적으로 활성화하는 것입니다. 이를 통해 모델은 총 47B 파라미터를 가지면서도 실제 추론 시에는 13B의 활성 파라미터만 사용할 수 있게 되었습니다. 또한 직접 선호도 최적화(Direct Preference Optimization) 방법을 통해 모델의 지시사항 따르기 능력을 크게 향상시켰습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
Mixtral 모델은 32개의 트랜스포머 레이어로 구성되며, 각 레이어에 8개의 전문가 네트워크가 있습니다. 라우터 네트워크는 각 토큰에 대해 8개의 전문가 중 상위 2개를 동적으로 선택합니다. 모델은 32k 토큰의 긴 컨텍스트 길이를 지원하며, 지도 학습 미세 조정과 직접 선호도 최적화를 통해 훈련되었습니다. 구현 과정에서 Megablocks CUDA 커널을 활용하여 희소 전문가 혼합 레이어의 계산 효율성을 높였습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
Mixtral 8x7B는 다양한 벤치마크에서 Llama 2 70B와 GPT-3.5를 능가하는 성능을 보여주었습니다. 특히 수학, 코드 생성, 다국어 이해 분야에서 뛰어난 성과를 달성했습니다. Apache 2.0 라이선스로 공개된 이 모델은 AI 기술의 민주화에 기여하며, 오픈소스 AI 모델이 상용 모델과 경쟁할 수 있음을 입증했습니다. 희소 전문가 혼합 아키텍처의 잠재력을 보여줌으로써, 대규모 언어 모델의 미래 발전 방향에 중요한 통찰을 제공했습니다.
- - -
## Mixtral of Experts

### 소개

Mixtral 8x7B는 희소 전문가 혼합(Sparse Mixture of Experts, SMoE) 언어 모델로, 최근 인공지능 연구 분야에서 주목할 만한 발전을 이루었습니다. 이 모델은 Mistral 7B와 동일한 아키텍처를 기반으로 하지만, 각 레이어가 8개의 피드포워드 블록(전문가)으로 구성되어 있다는 중요한 차이점이 있습니다. Mixtral의 핵심 메커니즘은 각 토큰이 처리될 때마다 모든 레이어에서 라우터 네트워크가 8개의 전문가 중 2개를 선택하여 현재 상태를 처리하고 그 출력을 결합한다는 점입니다. 각 토큰은 단 2개의 전문가만 활용하지만, 선택되는 전문가는 각 타임스텝마다 달라질 수 있습니다. 이러한 동적 라우팅 방식으로 인해 각 토큰은 총 47B 파라미터에 접근할 수 있지만, 추론 과정에서는 실제로 13B 활성 파라미터만 사용합니다.

Mixtral은 32k 토큰의 컨텍스트 크기로 학습되었으며, 다양한 벤치마크에서 Llama 2 70B와 GPT-3.5를 능가하거나 동등한 성능을 보여줍니다. 특히 수학, 코드 생성, 다국어 벤치마크에서 Mixtral은 Llama 2 70B를 크게 앞서는 성능을 보여주었습니다. 이는 희소 전문가 혼합 아키텍처가 특정 도메인에 특화된 능력을 효과적으로 개발할 수 있음을 시사합니다.

실험 결과에 따르면, Mixtral은 시퀀스 길이나 정보의 위치에 관계없이 32k 토큰의 컨텍스트 윈도우에서 정보를 성공적으로 검색할 수 있습니다. 이는 긴 컨텍스트 처리 능력이 실질적으로 구현되었음을 보여주는 중요한 증거입니다.

또한 연구진은 Mixtral 8x7B – Instruct라는 지시사항을 따르도록 미세 조정된 채팅 모델도 발표했습니다. 이 모델은 지도 학습 미세 조정과 직접 선호도 최적화(Direct Preference Optimization)를 통해 훈련되었습니다. 인간 평가 벤치마크에서 Mixtral – Instruct는 GPT-3.5 Turbo, Claude-2.1, Gemini Pro, Llama 2 70B – chat 모델을 능가하는 성능을 보여주었습니다. 또한 BBQ와 BOLD와 같은 벤치마크에서 편향이 감소하고 더 균형 잡힌 감정 프로필을 보여주는 것으로 나타났습니다.

Mixtral 8x7B와 Mixtral 8x7B – Instruct 모두 Apache 2.0 라이선스로 공개되어 학술 및 상업적 용도로 자유롭게 사용할 수 있습니다. 이는 다양한 응용 분야에서 광범위한 접근성과 활용 가능성을 보장합니다. 커뮤니티가 완전히 오픈 소스 스택으로 Mixtral을 실행할 수 있도록, 연구진은 효율적인 추론을 위한 Megablocks CUDA 커널을 통합하는 vLLM 프로젝트에 변경 사항을 제출했습니다. 또한 Skypilot을 통해 클라우드의 어떤 인스턴스에서도 vLLM 엔드포인트를 배포할 수 있습니다.

Mixtral의 희소 전문가 혼합 아키텍처는 모델의 파라미터 수를 늘리면서도 비용과 지연 시간을 제어할 수 있게 해주는 혁신적인 접근 방식입니다. 각 토큰이 전체 파라미터 집합의 일부만 사용하기 때문에, 낮은 배치 크기에서는 더 빠른 추론 속도를, 큰 배치 크기에서는 더 높은 처리량을 달성할 수 있습니다. 이러한 효율성과 성능의 균형은 대규모 언어 모델의 실용적 배포에 중요한 발전을 의미합니다.

트랜스포머 아키텍처를 기반으로 하는 Mixtral은 각 레이어에서 전문가 라우팅 메커니즘을 통해 동적으로 계산 경로를 결정합니다. 이 접근 방식은 모델이 다양한 유형의 입력에 대해 서로 다른 전문가를 활용할 수 있게 하여, 특정 도메인에 대한 전문성을 개발하면서도 일반적인 언어 이해 능력을 유지할 수 있게 합니다. 이러한 아키텍처적 혁신은 모델 크기와 계산 효율성 사이의 균형을 맞추는 데 중요한 발전을 나타냅니다.

### 아키텍처 세부 사항

Mixtral은 트랜스포머 아키텍처를 기반으로 하며, Vaswani 등이 제안한 원래의 트랜스포머 모델에서 몇 가지 중요한 수정 사항을 포함하고 있습니다. 이 모델은 Mistral 7B에서 설명된 대부분의 아키텍처적 변경 사항을 그대로 유지하면서도, 두 가지 주요한 차이점을 가지고 있습니다. 첫째, Mixtral은 32k 토큰의 완전 밀집(fully dense) 컨텍스트 길이를 지원합니다. 둘째, 기존의 피드포워드 블록이 전문가 혼합(Mixture-of-Expert) 레이어로 대체되었습니다. 이러한 변경 사항은 모델의 성능과 효율성을 크게 향상시키는 데 기여합니다.

표 1은 Mixtral 모델의 주요 아키텍처 파라미터를 요약하여 보여줍니다.

| 파라미터 | 값 |
|---------|-----|
| $dim$ | 4096 |
| $n\_layers$ | 32 |
| $head\_dim$ | 128 |
| $hidden\_dim$ | 14336 |
| $n\_heads$ | 32 |
| $n\_kv\_heads$ | 8 |
| $context\_len$ | 32768 |
| $vocab\_size$ | 32000 |
| $num\_experts$ | 8 |
| $top\_k\_experts$ | 2 |

이 아키텍처 설계에서 주목할 만한 점은 $dim$ 값이 4096으로, 이는 모델의 임베딩 및 히든 레이어 차원을 나타냅니다. 모델은 총 32개의 레이어($n\_layers$)로 구성되어 있으며, 각 레이어는 32개의 어텐션 헤드($n\_heads$)를 포함하고 있습니다. 각 어텐션 헤드의 차원($head\_dim$)은 128로 설정되어 있습니다.

Mixtral은 그룹 쿼리 어텐션(Grouped-Query Attention, GQA) 메커니즘을 사용하는데, 이는 $n\_kv\_heads$ 파라미터로 표현됩니다. 이 모델에서는 8개의 키-값 헤드를 사용하여 계산 효율성을 높이고 있습니다. 이는 Mistral 7B에서 도입된 기법으로, 각 쿼리 헤드 그룹이 하나의 키-값 헤드를 공유함으로써 메모리 사용량과 계산 복잡성을 줄이는 방식입니다.

피드포워드 네트워크의 내부 차원($hidden\_dim$)은 14336으로 설정되어 있어, 모델이 복잡한 패턴을 학습하는 데 충분한 용량을 제공합니다. 또한 Mixtral은 32,000개의 어휘 토큰($vocab\_size$)을 지원하며, 이는 다양한 언어와 도메인을 처리하기에 충분한 크기입니다.

Mixtral의 가장 큰 특징 중 하나는 전문가 혼합(MoE) 레이어의 도입입니다. 각 레이어에는 8개의 전문가($num\_experts$)가 있으며, 각 토큰이 처리될 때 라우터 네트워크가 이 중 상위 2개의 전문가($top\_k\_experts$)를 선택하여 계산을 수행합니다. 이러한 희소 전문가 혼합 접근 방식은 Shazeer 등이 제안한 "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer" 연구에 기반하고 있습니다.

Mixtral의 또 다른 중요한 특징은 32,768 토큰($context\_len$)의 긴 컨텍스트 길이를 지원한다는 점입니다. 이는 기존의 많은 언어 모델들보다 훨씬 긴 컨텍스트를 처리할 수 있게 해주며, 긴 문서나 복잡한 대화를 이해하고 생성하는 데 큰 이점을 제공합니다.

이러한 아키텍처 설계는 Mixtral이 Llama 2 70B와 같은 훨씬 더 큰 모델들과 비교해도 경쟁력 있는 성능을 발휘할 수 있게 하는 핵심 요소입니다. 특히 MoE 접근 방식은 모델의 총 파라미터 수를 크게 늘리면서도(총 47B 파라미터), 각 추론 단계에서는 그 중 일부만 활성화함으로써(약 13B 활성 파라미터) 계산 효율성을 유지할 수 있게 합니다.

Mixtral의 트랜스포머 아키텍처는 기본적으로 셀프 어텐션 메커니즘을 중심으로 구성되어 있습니다. 이 메커니즘은 입력 시퀀스 내의 토큰들 간의 관계를 모델링하여 컨텍스트를 이해하는 데 핵심적인 역할을 합니다. 셀프 어텐션은 쿼리(Q), 키(K), 값(V) 행렬을 사용하여 계산되며, 다음과 같은 수식으로 표현됩니다.

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

여기서 $d_k$는 키 벡터의 차원을 나타냅니다. Mixtral은 이러한 기본 어텐션 메커니즘에 그룹 쿼리 어텐션(GQA)을 적용하여 계산 효율성을 높이고 있습니다.

또한 Mixtral은 각 레이어에서 MoE 피드포워드 네트워크를 사용합니다. 이 네트워크는 입력 $x$에 대해 다음과 같은 계산을 수행합니다.

$$\text{FFN}(x) = \sum_{i=1}^{k} G(x)_i \cdot E_i(x)$$

여기서 $G(x)$는 게이팅 네트워크의 출력으로, 각 전문가 $E_i$에 할당되는 가중치를 결정합니다. $k$는 선택되는 전문가의 수($top\_k\_experts$)를 나타냅니다. 이러한 MoE 접근 방식은 모델이 다양한 유형의 입력에 대해 특화된 처리를 할 수 있게 해주며, 전체 파라미터 수를 늘리면서도 계산 효율성을 유지할 수 있게 합니다.

Mixtral의 아키텍처는 이전 모델들의 장점을 결합하면서도 새로운 혁신을 도입하여, 효율적이면서도 강력한 성능을 발휘하는 언어 모델을 구현하고 있습니다. 특히 MoE 접근 방식과 긴 컨텍스트 길이 지원은 Mixtral이 다양한 자연어 처리 작업에서 뛰어난 성능을 보이는 핵심 요소입니다.

### 희소 전문가 혼합 레이어

![전문가 혼합 레이어](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/images/smoe.png)

그림 1: 전문가 혼합 레이어. 각 입력 벡터는 라우터에 의해 8개의 전문가 중 2개에 할당됩니다. 레이어의 출력은 선택된 두 전문가 출력의 가중 합입니다. Mixtral에서 전문가는 바닐라 트랜스포머 아키텍처의 표준 피드포워드 블록입니다.

전문가 혼합 레이어(Mixture of Experts, MoE)에 대해 간략히 살펴보겠습니다(그림 1). 더 자세한 내용은 Fedus 등의 연구를 참조하시기 바랍니다.

MoE 모듈의 출력은 주어진 입력 $x$에 대해 전문가 네트워크의 출력을 게이팅 네트워크의 출력으로 가중치를 부여한 합으로 결정됩니다. 즉, $n$개의 전문가 네트워크 $\{E_0, E_i, ..., E_{n-1}\}$가 있을 때, 전문가 레이어의 출력은 다음과 같이 주어집니다.

$$\sum_{i=0}^{n-1}G(x)_{i}\cdot E_{i}(x).$$

여기서 $G(x)_i$는 $i$번째 전문가에 대한 게이팅 네트워크의 $n$차원 출력을 나타내며, $E_i(x)$는 $i$번째 전문가 네트워크의 출력입니다. 게이팅 벡터가 희소(sparse)하다면, 게이트가 0인 전문가의 출력을 계산하지 않아도 됩니다.

$G(x)$를 구현하는 여러 대안적인 방법이 있지만, 선형 레이어의 상위 K 로짓(logit)에 소프트맥스를 적용하는 방식이 간단하면서도 성능이 좋습니다. 다음과 같이 정의합니다.

$$G(x) := \text{Softmax}(\text{TopK}(x \cdot W_g)),$$

여기서 $(\text{TopK}(\ell))_i := \ell_i$는 로짓 $\ell \in \mathbb{R}^n$의 상위 K 좌표 중 하나일 때이고, $(\text{TopK}(\ell))_i := -\infty$는 그렇지 않을 때입니다.

K 값(토큰당 사용되는 전문가의 수)은 각 토큰을 처리하는 데 사용되는 계산량을 조절하는 하이퍼파라미터입니다. K를 고정한 상태에서 $n$을 증가시키면, 계산 비용을 실질적으로 일정하게 유지하면서 모델의 파라미터 수를 증가시킬 수 있습니다. 이는 모델의 총 파라미터 수(일반적으로 희소 파라미터 수라고 함)와 개별 토큰을 처리하는 데 사용되는 파라미터 수(활성 파라미터 수라고 함) 사이의 구분을 동기화합니다. 총 파라미터 수는 $n$에 따라 증가하고, 활성 파라미터 수는 $n$까지 K에 따라 증가합니다.

MoE 레이어는 고성능 특수 커널을 사용하여 단일 GPU에서 효율적으로 실행할 수 있습니다. 예를 들어, Megablocks는 MoE 레이어의 피드포워드 네트워크(FFN) 연산을 큰 희소 행렬 곱셈으로 변환하여 실행 속도를 크게 향상시키고, 서로 다른 전문가에 할당되는 토큰 수가 가변적인 경우를 자연스럽게 처리합니다. 또한 MoE 레이어는 표준 모델 병렬화 기법과 전문가 병렬화(Expert Parallelism, EP)라는 특별한 종류의 분할 전략을 통해 여러 GPU에 분산될 수 있습니다. MoE 레이어 실행 중에 특정 전문가가 처리해야 할 토큰은 해당 GPU로 라우팅되어 처리되고, 전문가의 출력은 원래 토큰 위치로 반환됩니다. EP는 개별 GPU의 과부하를 방지하거나 계산 병목 현상을 방지하기 위해 워크로드를 GPU 전체에 균등하게 분배하는 것이 중요하므로 부하 균형 조정에 어려움이 있습니다.

트랜스포머 모델에서 MoE 레이어는 토큰별로 독립적으로 적용되며, 트랜스포머 블록의 피드포워드(FFN) 서브블록을 대체합니다. Mixtral에서는 전문가 함수 $E_i(x)$로 동일한 SwiGLU 아키텍처를 사용하고 $K=2$로 설정합니다. 이는 각 토큰이 서로 다른 가중치 세트를 가진 두 개의 SwiGLU 서브블록으로 라우팅된다는 것을 의미합니다. 이를 모두 종합하면, 입력 토큰 $x$에 대한 출력 $y$는 다음과 같이 계산됩니다.

$$y = \sum_{i=0}^{n-1}\text{Softmax}(\text{Top2}(x \cdot W_g))_{i} \cdot \text{SwiGLU}_{i}(x).$$

이 공식은 GShard 아키텍처와 유사하지만, GShard는 모든 FFN 서브블록을 MoE 레이어로 대체하는 대신 두 블록마다 대체하고, 각 토큰에 할당된 두 번째 전문가에 대해 더 정교한 게이팅 전략을 사용한다는 차이점이 있습니다.

희소 전문가 혼합 레이어의 핵심 장점은 모델의 총 파라미터 수를 크게 늘리면서도 각 토큰 처리에 필요한 계산량은 상대적으로 일정하게 유지할 수 있다는 점입니다. 이는 대규모 언어 모델의 용량을 효율적으로 확장하는 방법을 제공합니다. 전문가 네트워크들은 학습 과정에서 서로 다른 유형의 입력 패턴이나 언어적 특성을 처리하는 데 특화되어, 모델이 다양한 작업에 더 효과적으로 대응할 수 있게 합니다.

Mixtral의 MoE 구현에서는 각 레이어에 8개의 전문가가 있고 각 토큰이 2개의 전문가만 활용하므로, 모델은 총 47B 파라미터를 가지지만 각 토큰 처리에는 약 13B의 활성 파라미터만 사용합니다. 이러한 접근 방식은 계산 효율성과 모델 용량 사이의 균형을 효과적으로 맞추어, 제한된 컴퓨팅 리소스로도 더 큰 모델의 이점을 활용할 수 있게 합니다.

MoE 레이어의 구현에서 중요한 기술적 측면 중 하나는 토큰을 전문가에게 효율적으로 라우팅하는 방법입니다. Megablocks와 같은 최적화된 커널은 희소 행렬 연산을 활용하여 이 과정을 가속화합니다. 이러한 커널은 각 전문가에 할당된 토큰 수가 불균형한 경우에도 효율적으로 작동하며, 토큰 드롭핑이나 패딩과 같은 비효율적인 방법을 피할 수 있습니다.

또한 전문가 병렬화(EP)는 MoE 모델을 여러 GPU에 효율적으로 분산시키는 중요한 기술입니다. 이 방식에서는 각 전문가가 별도의 GPU에 배치되고, 토큰은 해당 전문가가 있는 GPU로 전송되어 처리된 후 결과가 다시 원래 위치로 반환됩니다. 이 과정에서 중요한 과제는 GPU 간의 부하 균형을 유지하는 것입니다. 일부 전문가가 다른 전문가보다 더 많은 토큰을 할당받을 수 있기 때문에, 효율적인 부하 분산 전략이 필요합니다.

Mixtral의 MoE 구현은 각 레이어에서 모든 FFN 블록을 MoE 레이어로 대체하는 방식을 취합니다. 이는 GShard와 같은 다른 접근 방식과 차이가 있는데, GShard는 두 블록마다 MoE 레이어를 적용합니다. 또한 Mixtral은 각 토큰에 대해 상위 2개의 전문가를 선택하는 간단한 라우팅 전략을 사용하는 반면, 다른 MoE 구현은 더 복잡한 게이팅 메커니즘을 사용할 수 있습니다.

이러한 희소 전문가 혼합 접근 방식은 모델의 표현력을 높이면서도 계산 효율성을 유지하는 데 중요한 역할을 합니다. 각 전문가가 특정 유형의 입력 패턴에 특화되면서, 모델은 다양한 언어적 현상과 작업에 더 효과적으로 대응할 수 있게 됩니다. 이는 Mixtral이 수학, 코드 생성, 다국어 이해와 같은 특정 도메인에서 뛰어난 성능을 보이는 이유 중 하나입니다.

## 결과

Mixtral 모델의 성능을 Llama와 비교하기 위해 연구진은 동일한 평가 파이프라인을 사용하여 모든 벤치마크를 재실행했습니다. 다양한 작업에 대한 성능을 측정했으며, 이는 다음과 같은 카테고리로 분류됩니다.

![Mixtral과 다양한 Llama 모델의 벤치마크 성능 비교](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/images/231209_bench_combined.png)

그림 2: Mixtral과 다양한 Llama 모델의 광범위한 벤치마크에 대한 성능 비교. 모든 모델은 정확한 비교를 위해 연구진의 평가 파이프라인으로 모든 지표에 대해 재평가되었습니다. Mixtral은 모든 벤치마크에서 Llama 2 70B와 동등하거나 더 우수한 성능을 보여줍니다. 특히 수학과 코드 생성 분야에서 월등히 우수한 성능을 보입니다.

| 모델 | 활성 파라미터 | MMLU | HellaS | WinoG | PIQA | Arc-e | Arc-c | NQ | TriQA | HumanE | MBPP | Math | GSM8K |
|------|------------|------|--------|-------|------|-------|-------|-----|-------|--------|------|------|-------|
| LLaMA 2 7B | 7B | 44.4% | 77.1% | 69.5% | 77.9% | 68.7% | 43.2% | 17.5% | 56.6% | 11.6% | 26.1% | 3.9% | 16.0% |
| LLaMA 2 13B | 13B | 55.6% | 80.7% | 72.9% | 80.8% | 75.2% | 48.8% | 16.7% | 64.0% | 18.9% | 35.4% | 6.0% | 34.3% |
| LLaMA 1 33B | 33B | 56.8% | 83.7% | 76.2% | 82.2% | 79.6% | 54.4% | 24.1% | 68.5% | 25.0% | 40.9% | 8.4% | 44.1% |
| LLaMA 2 70B | 70B | 69.9% | 85.4% | 80.4% | 82.6% | 79.9% | 56.5% | 25.4% | 73.0% | 29.3% | 49.8% | 13.8% | 69.6% |
| Mistral 7B | 7B | 62.5% | 81.0% | 74.2% | 82.2% | 80.5% | 54.9% | 23.2% | 62.5% | 26.2% | 50.2% | 12.7% | 50.0% |
| Mixtral 8x7B | 13B | 70.6% | 84.4% | 77.2% | 83.6% | 83.1% | 59.7% | 30.6% | 71.5% | 40.2% | 60.7% | 28.4% | 74.4% |

표 2: Mixtral과 Llama의 비교. Mixtral은 추론 과정에서 5배 적은 활성 파라미터를 사용하면서도 거의 모든 인기 벤치마크에서 Llama 2 70B의 성능을 능가하거나 동등한 수준을 보여줍니다.

연구진이 평가한 벤치마크는 다음과 같은 카테고리로 분류됩니다.

• 상식 추론(0-shot): Hellaswag, Winogrande, PIQA, SIQA, OpenbookQA, ARC-Easy, ARC-Challenge, CommonsenseQA
• 세계 지식(5-shot): NaturalQuestions, TriviaQA
• 독해력(0-shot): BoolQ, QuAC
• 수학: GSM8K(8-shot, maj@8 사용)와 MATH(4-shot, maj@4 사용)
• 코드: Humaneval(0-shot)과 MBPP(3-shot)
• 인기 종합 결과: MMLU(5-shot), BBH(3-shot), AGI Eval(3-5-shot, 영어 객관식 문제만)

Mixtral, Mistral 7B, Llama 2 7B/13B/70B 및 Llama 1 34B에 대한 자세한 결과는 표 2에 보고되어 있습니다. 그림 2는 다양한 카테고리에서 Mixtral과 Llama 모델의 성능을 비교합니다. Mixtral은 대부분의 지표에서 Llama 2 70B를 능가합니다. 특히 코드와 수학 벤치마크에서 우수한 성능을 보여줍니다.

![Mistral과 Llama 2 모델의 벤치마크 성능 비교](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/images/231209_scaling.png)

그림 3: Mistral(7B/8x7B)과 Llama 2(7B/13B/70B)의 MMLU, 상식 추론, 세계 지식, 독해력, 수학, 코드에 대한 결과 비교. Mixtral은 5배 적은 활성 파라미터를 사용하면서도 독해력 벤치마크를 제외한 모든 벤치마크에서 Llama 2 70B를 크게 능가합니다. 또한 코드와 수학 분야에서 Llama 2 70B보다 월등히 우수한 성능을 보입니다.

### 크기와 효율성

연구진은 Mixtral 모델의 성능을 Llama 2 제품군과 비교하여 비용-성능 스펙트럼에서의 효율성을 이해하고자 했습니다(그림 3 참조). 희소 전문가 혼합 모델인 Mixtral은 각 토큰에 대해 13B 활성 파라미터만 사용합니다. 5배 적은 활성 파라미터로도 Mixtral은 대부분의 카테고리에서 Llama 2 70B를 능가할 수 있습니다.

이 분석은 추론 계산 비용에 직접적으로 비례하는 활성 파라미터 수에 초점을 맞추고 있지만, 메모리 비용과 하드웨어 활용도는 고려하지 않았습니다. Mixtral을 서빙하기 위한 메모리 비용은 희소 파라미터 수인 47B에 비례하며, 이는 여전히 Llama 2 70B보다 작습니다.

장치 활용도 측면에서는, SMoE 레이어가 라우팅 메커니즘과 장치당 하나 이상의 전문가를 실행할 때 증가하는 메모리 로드로 인해 추가적인 오버헤드를 발생시킨다는 점에 주목할 필요가 있습니다. 이러한 모델은 좋은 수준의 산술 집약도에 도달할 수 있는 배치 워크로드에 더 적합합니다.

### Llama 2 70B 및 GPT-3.5와의 비교

표 3에서는 Mixtral 8x7B를 Llama 2 70B 및 GPT-3.5와 비교한 성능을 보고합니다. Mixtral이 두 모델과 비슷하거나 더 우수한 성능을 보이는 것을 관찰할 수 있습니다. MMLU에서 Mixtral은 훨씬 작은 용량(70B에 비해 47B 토큰)에도 불구하고 더 나은 성능을 얻었습니다. MT Bench의 경우, 연구진은 사용 가능한 최신 GPT-3.5-Turbo 모델인 gpt-3.5-turbo-1106의 성능을 보고했습니다.

| | LLaMA 2 70B | GPT-3.5 | Mixtral 8x7B |
|---|---|---|---|
| MMLU(57개 과목의 객관식 문제) | 69.9% | 70.0% | 70.6% |
| HellaSwag(10-shot) | 87.1% | 85.5% | 86.7% |
| ARC Challenge(25-shot) | 85.1% | 85.2% | 85.8% |
| WinoGrande(5-shot) | 83.2% | 81.6% | 81.2% |
| MBPP(pass@1) | 49.8% | 52.2% | 60.7% |
| GSM-8K(5-shot) | 53.6% | 57.1% | 58.4% |
| MT Bench(지시 모델용) | 6.86 | 8.32 | 8.30 |

표 3: Mixtral과 Llama 2 70B 및 GPT-3.5의 비교. Mixtral은 대부분의 지표에서 Llama 2 70B와 GPT-3.5의 성능을 능가하거나 동등한 수준을 보여줍니다.

### 평가 차이점

일부 벤치마크에서는 연구진의 평가 프로토콜과 Llama 2 논문에서 보고된 프로토콜 사이에 몇 가지 차이점이 있습니다.
1) MBPP에서는 수작업으로 검증된 하위 집합을 사용했습니다.
2) TriviaQA에서는 위키피디아 컨텍스트를 제공하지 않았습니다.

이러한 평가 방법의 차이는 모델 간의 공정한 비교를 위해 연구진이 모든 모델에 동일한 평가 파이프라인을 적용했기 때문에 발생했습니다. 이를 통해 다양한 모델 아키텍처의 상대적 성능을 더 정확하게 비교할 수 있었습니다.

Mixtral의 성능 결과는 희소 전문가 혼합 아키텍처가 효율적인 파라미터 활용을 통해 더 큰 모델과 경쟁할 수 있음을 보여줍니다. 특히 수학과 코드 생성 분야에서의 우수한 성능은 이 아키텍처가 복잡한 추론 작업에 특히 효과적임을 시사합니다. 또한 Mixtral이 GPT-3.5와 같은 상용 모델과 비교해도 경쟁력 있는 성능을 보인다는 점은 오픈 소스 AI 모델의 발전 가능성을 보여주는 중요한 지표입니다.

### 다국어 벤치마크

Mixtral 8x7B는 사전 학습 과정에서 다국어 데이터의 비율을 Mistral 7B에 비해 크게 증가시켰습니다. 이러한 추가 용량 덕분에 Mixtral은 영어에서의 높은 정확도를 유지하면서도 다국어 벤치마크에서 우수한 성능을 발휘할 수 있게 되었습니다. 특히 표 4에서 볼 수 있듯이, Mixtral은 프랑스어, 독일어, 스페인어, 이탈리아어에서 Llama 2 70B를 크게 능가하는 성능을 보여줍니다.

| 모델 | 활성 파라미터 | 프랑스어 |  | | 독일어 |  | | 스페인어 |  | | 이탈리아어 |  | |
|------|------------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|
| | | Arc-c | HellaS | MMLU | Arc-c | HellaS | MMLU | Arc-c | HellaS | MMLU | Arc-c | HellaS | MMLU |
| LLaMA 1 33B | 33B | 39.3% | 68.1% | 49.9% | 41.1% | 63.3% | 48.7% | 45.7% | 69.8% | 52.3% | 42.9% | 65.4% | 49.0% |
| LLaMA 2 70B | 70B | 49.9% | 72.5% | 64.3% | 47.3% | 68.7% | 64.2% | 50.5% | 74.5% | 66.0% | 49.4% | 70.9% | 65.1% |
| Mixtral 8x7B | 13B | 58.2% | 77.4% | 70.9% | 54.3% | 73.0% | 71.5% | 55.4% | 77.6% | 72.5% | 52.8% | 75.1% | 70.9% |

표 4: Mixtral과 Llama의 다국어 벤치마크 비교. ARC Challenge, Hellaswag, MMLU에서 Mixtral은 프랑스어, 독일어, 스페인어, 이탈리아어 등 4개 언어에서 Llama 2 70B를 능가합니다.

다국어 성능은 현대 대규모 언어 모델의 중요한 평가 지표 중 하나입니다. Mixtral 8x7B는 사전 학습 과정에서 다국어 데이터의 비중을 전략적으로 증가시킴으로써 영어 외 언어에서도 뛰어난 성능을 달성했습니다. 이는 희소 전문가 혼합(Sparse Mixture of Experts) 아키텍처의 장점을 잘 활용한 사례로 볼 수 있습니다. 각 전문가가 특정 언어나 언어적 패턴에 특화될 수 있어, 모델 전체의 다국어 이해 능력이 향상된 것으로 해석할 수 있습니다.

표 4에서 볼 수 있듯이, Mixtral 8x7B는 프랑스어, 독일어, 스페인어, 이탈리아어의 세 가지 주요 벤치마크(ARC Challenge, Hellaswag, MMLU)에서 일관되게 Llama 2 70B를 능가합니다. 특히 주목할 만한 점은 Mixtral이 13B의 활성 파라미터만 사용하면서도 70B 파라미터를 가진 Llama 2를 능가한다는 것입니다. 이는 모델 크기와 계산 효율성 측면에서 상당한 이점을 제공합니다.

프랑스어의 경우, Mixtral은 ARC Challenge에서 58.2%, Hellaswag에서 77.4%, MMLU에서 70.9%의 정확도를 보여주며, Llama 2 70B(각각 49.9%, 72.5%, 64.3%)를 크게 앞섭니다. 독일어에서도 Mixtral은 ARC Challenge 54.3%, Hellaswag 73.0%, MMLU 71.5%로 Llama 2 70B(각각 47.3%, 68.7%, 64.2%)보다 우수한 성능을 보입니다.

스페인어와 이탈리아어에서도 유사한 패턴이 관찰됩니다. 스페인어에서 Mixtral은 ARC Challenge 55.4%, Hellaswag 77.6%, MMLU 72.5%의 성능을 보이며, 이탈리아어에서는 ARC Challenge 52.8%, Hellaswag 75.1%, MMLU 70.9%의 성능을 달성했습니다. 이는 모든 측정 지표에서 Llama 2 70B를 일관되게 능가하는 결과입니다.

이러한 다국어 성능 향상은 Mixtral의 사전 학습 과정에서 다국어 데이터의 비중을 전략적으로 증가시킨 결과로 볼 수 있습니다. 또한 희소 전문가 혼합 아키텍처가 다양한 언어적 패턴을 효과적으로 학습하는 데 도움이 되었을 것으로 추정됩니다. 각 전문가가 특정 언어나 언어적 특성에 특화되어, 모델이 다양한 언어에 대한 깊은 이해를 발전시킬 수 있었을 것입니다.

Mixtral의 다국어 성능은 글로벌 응용 프로그램과 다국어 사용자를 지원하는 AI 시스템 개발에 중요한 의미를 갖습니다. 영어 중심의 기존 모델들과 달리, Mixtral은 다양한 언어에서 높은 수준의 이해력과 생성 능력을 보여주어 보다 포용적인 AI 시스템 구축에 기여할 수 있습니다. 이는 특히 비영어권 사용자들에게 더 나은 AI 경험을 제공하는 데 중요한 발전입니다.

### 장거리 성능

Mixtral 모델의 장거리 컨텍스트 처리 능력을 평가하기 위해, 연구진은 Beltagy 등이 제안한 패스키(passkey) 검색 작업을 활용했습니다. 이 작업은 모델이 긴 프롬프트 내에 무작위로 삽입된 패스키를 검색할 수 있는 능력을 측정하기 위해 설계된 합성 태스크입니다. 이러한 평가는 모델이 긴 컨텍스트에서 특정 정보를 효과적으로 기억하고 활용할 수 있는지를 검증하는 중요한 지표가 됩니다.

![패스키 검색 작업과 퍼플렉시티 결과](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/images/passkey.png)

그림 4: Mixtral의 장거리 성능. (왼쪽) Mixtral은 패스키의 위치나 입력 시퀀스의 길이에 관계없이 패스키 작업에서 100% 검색 정확도를 달성합니다. (오른쪽) proof-pile 데이터셋에 대한 Mixtral의 퍼플렉시티는 컨텍스트 길이가 증가함에 따라 단조롭게 감소합니다.

그림 4의 왼쪽 부분에서 볼 수 있듯이, Mixtral은 컨텍스트 길이나 패스키의 위치에 관계없이 100%의 검색 정확도를 달성했습니다. 이는 모델이 긴 시퀀스 내에서 중요한 정보를 효과적으로 식별하고 추출할 수 있는 강력한 능력을 갖추고 있음을 보여줍니다. 특히 패스키가 시퀀스의 어느 위치에 있든, 또는 입력 시퀀스의 길이가 얼마나 길든 상관없이 완벽한 검색 성능을 보여주었다는 점이 주목할 만합니다.

그림 4의 오른쪽 부분은 Rae 등이 제안한 proof-pile 데이터셋의 일부에 대한 Mixtral의 퍼플렉시티(perplexity)를 보여줍니다. 퍼플렉시티는 언어 모델이 텍스트를 얼마나 잘 예측하는지를 측정하는 지표로, 값이 낮을수록 모델의 예측 성능이 우수함을 의미합니다. 그래프에서 볼 수 있듯이, Mixtral의 퍼플렉시티는 컨텍스트 길이가 증가함에 따라 단조롭게 감소하는 경향을 보입니다. 이는 모델이 더 긴 컨텍스트를 활용할수록 텍스트 예측 능력이 향상된다는 것을 의미하며, 모델이 장거리 의존성을 효과적으로 포착하고 활용할 수 있음을 시사합니다.

이러한 결과는 Mixtral이 32k 토큰의 긴 컨텍스트 윈도우에서 정보를 성공적으로 처리하고 활용할 수 있는 능력을 갖추고 있음을 입증합니다. 특히 패스키 검색 작업에서의 완벽한 성능은 모델이 긴 시퀀스 내에서 특정 정보를 정확하게 기억하고 검색할 수 있음을 보여주며, proof-pile 데이터셋에서의 퍼플렉시티 감소 패턴은 모델이 더 많은 컨텍스트를 활용할수록 예측 성능이 향상됨을 나타냅니다.

Mixtral의 이러한 장거리 성능은 모델 아키텍처의 여러 요소에 기인합니다. 특히 희소 전문가 혼합(Sparse Mixture of Experts) 아키텍처는 각 레이어에서 다양한 전문가들이 서로 다른 유형의 정보를 처리하도록 함으로써, 긴 시퀀스에서도 중요한 정보를 효과적으로 유지하고 활용할 수 있게 합니다. 또한 32k 토큰의 컨텍스트 크기로 학습되었기 때문에, 모델은 긴 시퀀스에서의 패턴을 인식하고 활용하는 능력을 개발할 수 있었습니다.

![편향성 벤치마크](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/images/perplexity.png)

그림 5: 편향성 벤치마크. Llama 2 70B와 비교했을 때, Mixtral은 더 적은 편향성(BBQ에서 더 높은 정확도, BOLD에서 더 낮은 표준편차)을 보이며 더 긍정적인 감정(BOLD에서 더 높은 평균)을 표현합니다.

Mixtral의 또 다른 중요한 측면은 편향성과 관련된 성능입니다. 그림 5에서 볼 수 있듯이, Mixtral은 BBQ(Big Bench Bias Questions) 벤치마크에서 56.0%의 정확도를 달성하여 Llama 2 70B의 51.5%보다 우수한 성능을 보였습니다. BBQ는 모델의 편향성을 평가하는 벤치마크로, 더 높은 정확도는 모델이 편향된 추론을 덜 한다는 것을 의미합니다.

또한 BOLD(Bias in Open-ended Language Generation Dataset) 감정 점수에서도 Mixtral은 대체로 더 낮은 표준편차를 보여주었습니다. 특히 종교적 이데올로기 카테고리에서 Mixtral은 0.144 ± 0.089의 점수를 기록하여 Llama 2 70B의 0.188 ± 0.133보다 낮은 표준편차를 보였습니다. 이는 Mixtral이 종교적 주제에 대해 더 일관된 응답을 생성한다는 것을 시사합니다.

성별, 직업, 정치적 이데올로기 카테고리에서는 Mixtral이 Llama 2 70B보다 더 높은 평균 점수를 보였는데, 이는 Mixtral이 이러한 주제에 대해 더 긍정적인 감정을 표현한다는 것을 의미합니다. 인종 카테고리에서는 두 모델이 동일한 평균 점수(0.232)를 기록했지만, Mixtral이 약간 더 낮은 표준편차(0.052 vs 0.049)를 보였습니다.

이러한 결과는 Mixtral이 Llama 2 70B에 비해 편향성이 적고 더 일관된 응답을 생성하는 경향이 있음을 보여줍니다. 이는 모델의 학습 과정에서 다양한 데이터를 균형 있게 활용하고, 희소 전문가 혼합 아키텍처를 통해 다양한 관점을 통합할 수 있는 능력이 향상되었기 때문일 수 있습니다.

Mixtral의 장거리 성능과 편향성 측면에서의 개선은 대규모 언어 모델의 실용적 응용에 중요한 의미를 갖습니다. 긴 컨텍스트를 효과적으로 처리할 수 있는 능력은 문서 요약, 복잡한 질의응답, 장문의 콘텐츠 생성과 같은 작업에서 큰 이점을 제공합니다. 또한 편향성이 감소된 모델은 다양한 사용자와 상황에 대해 더 공정하고 균형 잡힌 응답을 생성할 수 있어, AI 시스템의 사회적 영향과 수용성을 향상시킬 수 있습니다.

### 편향성 벤치마크

모델의 미세 조정이나 선호도 모델링을 통해 수정해야 할 잠재적 결함을 식별하기 위해, 연구진은 기본 모델의 성능을 두 가지 주요 편향성 벤치마크에서 측정했습니다. 질의응답을 위한 편향성 벤치마크(Bias Benchmark for QA, BBQ)와 개방형 언어 생성의 편향성 데이터셋(Bias in Open-Ended Language Generation Dataset, BOLD)입니다.

BBQ는 수작업으로 작성된 질문 세트로 구성된 데이터셋으로, 9가지 다양한 사회적으로 관련된 범주에 대한 사회적 편향을 대상으로 합니다. 연령, 장애 상태, 성 정체성, 국적, 외모, 인종/민족, 종교, 사회경제적 지위, 성적 지향성이 포함됩니다. 이 데이터셋은 질의응답 모델이 이러한 다양한 사회적 범주에 대해 어떤 편향을 보이는지 체계적으로 평가할 수 있도록 설계되었습니다. BBQ의 각 예제는 특정 사회적 편향을 대상으로 하며, 부정적 및 비부정적 질문 변형을 통해 질문 의미와 직접적으로 연결된 편향과 질문과 무관한 편향을 구분할 수 있게 합니다.

BOLD는 편향성 벤치마킹을 위한 5개 도메인에 걸쳐 23,679개의 영어 텍스트 생성 프롬프트로 구성된 대규모 데이터셋입니다. 이 데이터셋은 다양한 작성자들의 자연스러운 텍스트를 대표하기 위해 영어 위키피디아 기사에서 추출되었습니다. BOLD는 직업, 성별, 인종, 종교, 정치적 이데올로기 등 5개 도메인에서 편향성을 측정하기 위한 프롬프트를 제공합니다. 이 데이터셋은 개방형 텍스트 생성 작업에서 언어 모델이 보이는 편향을 평가하는 데 특히 유용합니다.

연구진은 자체 평가 프레임워크를 사용하여 Llama 2와 Mixtral을 BBQ 및 BOLD에서 벤치마킹하고 그 결과를 표 5에 보고했습니다. Llama 2와 비교했을 때, Mixtral은 BBQ 벤치마크에서 더 적은 편향성을 보였습니다(56.0% 대 51.5%). BBQ에서 더 높은 정확도는 모델이 편향된 추론을 덜 한다는 것을 의미합니다. 이는 Mixtral이 다양한 사회적 범주에 대해 더 공정하고 균형 잡힌 응답을 생성할 수 있음을 시사합니다.

BOLD의 각 그룹에 대해, 더 높은 평균 감정 점수는 더 긍정적인 감정을 의미하고, 더 낮은 표준 편차는 그룹 내에서 더 적은 편향을 나타냅니다. 전반적으로, Mixtral은 Llama 2보다 더 긍정적인 감정을 표현하면서도 각 그룹 내에서 유사한 변동성을 보였습니다. 이는 Mixtral이 다양한 사회적 그룹에 대해 더 일관되고 긍정적인 표현을 생성한다는 것을 의미합니다.

BOLD 감정 점수에서 Mixtral은 대체로 더 낮은 표준편차를 보여주었습니다. 특히 종교적 이데올로기 카테고리에서 Mixtral은 0.144 ± 0.089의 점수를 기록하여 Llama 2의 0.188 ± 0.133보다 낮은 표준편차를 보였습니다. 이는 Mixtral이 종교적 주제에 대해 더 일관된 응답을 생성한다는 것을 시사합니다.

성별, 직업, 정치적 이데올로기 카테고리에서는 Mixtral이 Llama 2보다 더 높은 평균 점수를 보였는데, 이는 Mixtral이 이러한 주제에 대해 더 긍정적인 감정을 표현한다는 것을 의미합니다. 인종 카테고리에서는 두 모델이 동일한 평균 점수(0.232)를 기록했지만, Mixtral이 약간 더 낮은 표준편차(0.052 vs 0.049)를 보였습니다.

이러한 결과는 Mixtral이 Llama 2에 비해 편향성이 적고 더 일관된 응답을 생성하는 경향이 있음을 보여줍니다. 이는 모델의 학습 과정에서 다양한 데이터를 균형 있게 활용하고, 희소 전문가 혼합 아키텍처를 통해 다양한 관점을 통합할 수 있는 능력이 향상되었기 때문일 수 있습니다.

편향성 벤치마크에서의 이러한 개선은 대규모 언어 모델의 실용적 응용에 중요한 의미를 갖습니다. 편향성이 감소된 모델은 다양한 사용자와 상황에 대해 더 공정하고 균형 잡힌 응답을 생성할 수 있어, AI 시스템의 사회적 영향과 수용성을 향상시킬 수 있습니다. 특히 Mixtral이 BBQ와 같은 체계적인 편향성 평가에서 더 나은 성능을 보인다는 것은, 이 모델이 다양한 사회적 범주에 걸쳐 더 공정한 응답을 생성할 수 있음을 시사합니다.

이해를 돕기 위해 덧붙이자면, BBQ와 BOLD 같은 편향성 벤치마크는 언어 모델이 다양한 사회적 그룹에 대해 가질 수 있는 잠재적 편향을 식별하고 측정하는 데 중요한 역할을 합니다. 이러한 벤치마크는 모델이 특정 그룹에 대해 부정적이거나 고정관념적인 응답을 생성하는지, 또는 특정 그룹에 대해 일관되지 않은 응답을 생성하는지를 평가합니다. 이러한 평가를 통해 연구자들은 모델의 편향성을 식별하고, 미세 조정이나 선호도 모델링과 같은 기법을 통해 이를 완화할 수 있습니다.

Mixtral의 편향성 벤치마크 결과는 희소 전문가 혼합 아키텍처가 모델의 편향성 감소에도 기여할 수 있음을 시사합니다. 각 전문가가 다양한 유형의 입력과 주제에 특화되면서, 모델은 다양한 사회적 그룹과 주제에 대해 더 균형 잡힌 이해를 발전시킬 수 있었을 것입니다. 이는 Mixtral이 다양한 사용자와 상황에 대해 더 공정하고 유용한 AI 시스템으로 기능할 수 있게 하는 중요한 특성입니다.

## 지시 미세 조정

Mixtral 모델은 지도 학습 미세 조정(Supervised Fine-tuning, SFT)과 직접 선호도 최적화(Direct Preference Optimization, DPO)를 통해 지시사항을 따르도록 훈련되었습니다. 이 과정을 통해 탄생한 Mixtral-Instruct 모델은 2023년 12월 기준으로 오픈 웨이트 모델 중 가장 우수한 성능을 보여주고 있습니다.

### 훈련 방법론

Mixtral-Instruct 모델의 훈련은 두 단계로 진행되었습니다. 첫 번째 단계에서는 지시사항 데이터셋을 사용한 지도 학습 미세 조정이 이루어졌습니다. 이 과정에서 모델은 다양한 지시사항에 적절하게 응답하는 방법을 학습합니다. 두 번째 단계에서는 쌍으로 이루어진 피드백 데이터셋을 활용한 직접 선호도 최적화(DPO)가 적용되었습니다.

직접 선호도 최적화는 Rafailov 등이 제안한 방법으로, 기존의 강화학습 기반 인간 피드백(RLHF) 방식을 단순화한 접근법입니다. DPO는 명시적인 보상 모델을 학습하지 않고도 인간의 선호도를 직접 정책에 반영할 수 있게 해줍니다. 이 방법은 다음과 같은 목적 함수를 최적화합니다.

$$\mathcal{L}_{\text{DPO}}(\pi_{\theta}; \pi_{\text{ref}}) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_w | x)}{\pi_{\text{ref}}(y_w | x)} - \beta \log \frac{\pi_{\theta}(y_l | x)}{\pi_{\text{ref}}(y_l | x)} \right) \right]$$

여기서 $\pi_{\theta}$는 최적화하려는 정책, $\pi_{\text{ref}}$는 참조 정책, $\mathcal{D}$는 인간 선호도 데이터셋, $\beta$는 KL-발산 제약을 제어하는 하이퍼파라미터입니다. 이 접근법은 기존 RLHF 방식보다 구현이 간단하고 훈련이 안정적이며, 하이퍼파라미터 선택에 더 강건한 특성을 가집니다.

### 성능 평가

Mixtral-Instruct의 성능은 MT-Bench에서 8.30점을 기록했습니다. MT-Bench는 Zheng 등이 개발한 벤치마크로, 80개의 고품질 다중 턴 질문을 포함하고 있으며, 챗봇의 능력을 차별화하기 위해 설계되었습니다. 이 점수는 2023년 12월 기준으로 공개된 가중치를 가진 모델 중 가장 높은 점수입니다.

LMSys에서 진행한 독립적인 인간 평가에서도 Mixtral-Instruct는 뛰어난 성능을 보여주었습니다. 그림 6에서 볼 수 있듯이, Mixtral 8x7B Instruct v0.1은 Arena Elo 레이팅에서 1121점을 기록하며, Claude-2.1(1117점), GPT-3.5-Turbo의 모든 버전(최고 1117점), Gemini Pro(1111점), Llama-2-70b-chat(1077점)을 능가했습니다.

![LMSys 리더보드](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/images/lmsys_231222.png)

그림 6: LMSys 리더보드 (2023년 12월 22일 스크린샷). Mixtral 8x7B Instruct v0.1은 Arena Elo 레이팅에서 1121점을 기록하며 Claude-2.1(1117점), GPT-3.5-Turbo의 모든 버전(최고 1117점), Gemini Pro(1111점), Llama-2-70b-chat(1077점)을 능가했습니다. Mixtral은 현재 공개 가중치 모델 중 큰 차이로 가장 우수한 성능을 보여주고 있습니다.

### 평가 방법론

LMSys의 평가 시스템은 두 가지 주요 구성 요소를 포함합니다.

1. MT-Bench: 다양한 사용 사례와 도전적인 작업을 다루는 80개의 고품질 다중 턴 질문으로 구성된 벤치마크입니다. 이 벤치마크는 챗봇의 다양한 능력을 평가하기 위해 설계되었습니다.

2. Chatbot Arena: 사용자가 익명으로 챗봇 간의 대결에 참여하고 개인적인 선호도에 따라 응답을 평가할 수 있는 크라우드소싱 플랫폼입니다. 이 플랫폼은 실제 사용자 경험을 기반으로 한 평가를 제공합니다.

LMSys의 연구에 따르면, GPT-4와 같은 대규모 언어 모델을 평가자로 사용하는 "LLM-as-a-judge" 접근법은 인간 평가자와 80% 이상의 일치도를 보이며, 이는 인간-인간 간 일치도와 동일한 수준입니다. 이러한 방법론은 인간 선호도를 평가하는 확장 가능하고 설명 가능한 접근법을 제공합니다.

### Mixtral-Instruct의 의의

Mixtral-Instruct의 우수한 성능은 희소 전문가 혼합(Sparse Mixture of Experts) 아키텍처가 지시사항 따르기 작업에도 효과적으로 적용될 수 있음을 보여줍니다. 특히 주목할 만한 점은 Mixtral-Instruct가 GPT-3.5-Turbo, Claude-2.1, Gemini Pro와 같은 상용 모델들과 비교해도 경쟁력 있는 성능을 보인다는 것입니다.

이러한 결과는 오픈 소스 AI 모델의 발전 가능성을 보여주는 중요한 지표입니다. Mixtral-Instruct는 Apache 2.0 라이선스로 공개되어 있어, 학술 및 상업적 용도로 자유롭게 사용할 수 있습니다. 이는 다양한 응용 분야에서 광범위한 접근성과 활용 가능성을 보장합니다.

이해를 돕기 위해 덧붙이자면, 지시 미세 조정 과정은 기본 언어 모델이 사용자의 지시사항을 이해하고 이에 따라 응답하는 능력을 향상시키는 데 중요한 역할을 합니다. 이 과정에서 모델은 다양한 유형의 지시사항에 적절하게 응답하는 방법을 학습하며, 이는 실제 사용자와의 상호작용에서 모델의 유용성을 크게 향상시킵니다.

직접 선호도 최적화(DPO)는 기존의 강화학습 기반 인간 피드백(RLHF) 방식에 비해 여러 장점을 제공합니다. RLHF는 보상 모델을 명시적으로 학습하고 이를 기반으로 정책을 최적화하는 두 단계 과정을 거치지만, DPO는 이 과정을 단일 목적 함수로 통합하여 더 효율적인 학습을 가능하게 합니다. 또한 DPO는 구현이 더 간단하고 계산 효율성이 높으며, 하이퍼파라미터 선택에 덜 민감한 특성을 가집니다.

Mixtral-Instruct의 성공은 모델 아키텍처의 혁신(희소 전문가 혼합)과 효과적인 훈련 방법론(DPO)의 결합이 가져온 결과로 볼 수 있습니다. 이는 향후 대규모 언어 모델의 개발 방향에 중요한 시사점을 제공하며, 특히 계산 효율성과 모델 성능 사이의 균형을 맞추는 데 있어 희소 전문가 혼합 아키텍처의 잠재력을 보여줍니다.

## 라우팅 분석

이 섹션에서는 라우터에 의한 전문가 선택에 대한 간단한 분석을 수행합니다. 특히 학습 과정에서 일부 전문가들이 특정 도메인(예: 수학, 생물학, 철학 등)에 특화되었는지 여부를 조사하고자 합니다. 이를 위해 The Pile 검증 데이터셋의 다양한 하위 집합에서 선택된 전문가의 분포를 측정했습니다.

![그림 7: The Pile 데이터셋의 다양한 도메인에 대해 레이어 0, 15, 31에서 각 전문가에 할당된 토큰의 비율. 회색 점선은 균일한 샘플링 시 예상되는 비율인 1/8을 표시합니다. 여기서는 라우터에 의해 첫 번째 또는 두 번째 선택으로 선택된 전문가를 고려합니다. 각 경우에서 할당 비율의 세부 내역은 부록의 그림 9에서 확인할 수 있습니다.](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/x1.png)

그림 7은 모델의 첫 번째 레이어(레이어 0), 중간 레이어(레이어 15), 마지막 레이어(레이어 31)에서의 결과를 보여줍니다. 놀랍게도, 주제에 따른 전문가 할당에서 명확한 패턴이 관찰되지 않았습니다. 예를 들어, 모든 레이어에서 LaTeX로 작성된 ArXiv 논문, 생물학(PubMed Abstracts), 철학(PhilPapers) 문서에 대한 전문가 할당 분포가 매우 유사했습니다. 오직 DM Mathematics에서만 약간 다른 전문가 분포가 관찰되었습니다. 이러한 차이는 데이터셋의 합성적 특성과 자연어 스펙트럼의 제한된 범위 때문일 가능성이 높으며, 특히 히든 상태가 입력 및 출력 임베딩과 매우 상관관계가 높은 첫 번째와 마지막 레이어에서 두드러집니다. 이는 라우터가 일부 구조화된 구문적 동작을 보여준다는 것을 시사합니다.

![그림 8: 각 토큰이 첫 번째 전문가 선택에 따라 색상이 지정된 텍스트 샘플. 전문가 선택은 특히 초기 및 최종 레이어에서 도메인보다는 구문과 더 밀접하게 연관되어 있는 것으로 보입니다.](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/images/routing-sample.png)

그림 8은 파이썬 코드, 수학, 영어와 같은 다양한 도메인의 텍스트 예시를 보여주며, 각 토큰은 선택된 전문가에 해당하는 배경색으로 강조되어 있습니다. 이 그림에서 파이썬의 'self'와 영어의 'Question'과 같은 단어들이 여러 토큰으로 구성되어 있음에도 불구하고 종종 동일한 전문가를 통해 라우팅되는 것을 볼 수 있습니다. 마찬가지로 코드에서 들여쓰기 토큰은 항상 동일한 전문가에 할당되는데, 특히 히든 상태가 모델의 입력 및 출력과 더 상관관계가 높은 첫 번째와 마지막 레이어에서 그러합니다.

또한 그림 8에서 연속된 토큰들이 종종 동일한 전문가에 할당되는 것을 볼 수 있습니다. 실제로 The Pile 데이터셋에서 위치적 지역성(positional locality)이 어느 정도 관찰됩니다. 표 5는 도메인과 레이어별로 연속된 토큰이 동일한 전문가 할당을 받는 비율을 보여줍니다.

| | 첫 번째 선택 | | | 첫 번째 또는 두 번째 선택 | | |
|------|---------|---------|---------|---------|---------|---------|
| | 레이어 0 | 레이어 15 | 레이어 31 | 레이어 0 | 레이어 15 | 레이어 31 |
| ArXiv | 14.0% | 27.9% | 22.7% | 46.5% | 62.3% | 52.9% |
| DM Mathematics | 14.1% | 28.4% | 19.7% | 44.9% | 67.0% | 44.5% |
| Github | 14.9% | 28.1% | 19.7% | 49.9% | 66.9% | 49.2% |
| Gutenberg | 13.9% | 26.1% | 26.3% | 49.5% | 63.1% | 52.2% |
| PhilPapers | 13.6% | 25.3% | 22.1% | 46.9% | 61.9% | 51.3% |
| PubMed Abstracts | 14.2% | 24.6% | 22.0% | 48.6% | 61.6% | 51.8% |
| StackExchange | 13.6% | 27.2% | 23.6% | 48.2% | 64.6% | 53.6% |
| Wikipedia (en) | 14.4% | 23.6% | 25.3% | 49.8% | 62.1% | 51.8% |

표 5: 전문가 할당 반복 비율. 토큰 $i$와 그 다음 토큰 $i+1$에 동일한 전문가가 할당되는 비율을 평가했습니다. 첫 번째로 선택된 전문가가 동일한 경우와 연속된 토큰에서 첫 번째 또는 두 번째 선택으로 동일한 전문가가 관찰되는 경우를 보고합니다. 참고로, 무작위 할당의 경우 예상되는 반복 비율은 "첫 번째 선택"의 경우 $\frac{1}{8}=12.5\%$이고, "첫 번째 및 두 번째 선택"의 경우 $1-\frac{6}{8}\frac{5}{7}\approx 46\%$입니다. 첫 번째 레이어에서의 반복은 무작위에 가깝지만, 레이어 15와 31에서는 상당히 높습니다. 높은 반복 횟수는 이러한 레이어에서 전문가 선택이 높은 시간적 지역성을 보여준다는 것을 나타냅니다.

연속된 할당의 비율은 상위 레이어에서 무작위보다 상당히 높습니다. 이는 빠른 학습 및 추론을 위한 모델 최적화 방법에 영향을 미칩니다. 예를 들어, 높은 지역성을 가진 경우 전문가 병렬화(Expert Parallelism)를 수행할 때 특정 전문가의 과도한 구독을 유발할 가능성이 높습니다. 반대로, 이러한 지역성은 캐싱에 활용될 수 있으며, 이는 이전 연구에서도 활용된 바 있습니다. 부록의 그림 10에서는 모든 레이어와 데이터셋에 걸쳐 이러한 전문가 빈도에 대한 더 완전한 보기를 제공합니다.

이해를 돕기 위해 덧붙이자면, 이 분석 결과는 Mixtral 모델의 라우팅 메커니즘이 예상과는 달리 특정 주제 도메인에 따라 전문가를 선택하는 것이 아니라, 구문적 패턴이나 위치적 특성에 더 민감하게 반응한다는 것을 보여줍니다. 이는 희소 전문가 혼합(Sparse Mixture of Experts) 모델에서 각 전문가가 특정 주제 영역보다는 특정 언어 패턴이나 구문 구조를 처리하는 데 특화될 수 있음을 시사합니다.

특히 주목할 만한 점은 연속된 토큰들이 동일한 전문가에 할당되는 경향이 높다는 것입니다. 이러한 시간적 지역성(temporal locality)은 모델의 효율적인 구현에 중요한 의미를 갖습니다. 예를 들어, 동일한 전문가가 연속된 토큰들을 처리하는 경우가 많다면, 캐싱 메커니즘을 통해 이전에 계산된 결과를 재사용하여 계산 효율성을 높일 수 있습니다. 반면, 특정 전문가에 과도하게 많은 토큰이 할당되면 전문가 병렬화 구현 시 부하 불균형 문제가 발생할 수 있습니다.

또한 레이어 15와 같은 중간 레이어에서 연속된 토큰의 동일 전문가 할당 비율이 가장 높다는 점도 흥미롭습니다. 이는 중간 레이어에서 모델이 더 추상적인 패턴이나 구조를 포착하고, 이러한 패턴이 연속된 토큰들에 걸쳐 일관되게 처리되는 경향이 있음을 시사합니다. 반면, 첫 번째 레이어는 더 낮은 수준의 특징을 처리하고, 마지막 레이어는 출력에 더 가까운 표현을 다루기 때문에 중간 레이어보다 지역성이 낮을 수 있습니다.

이러한 라우팅 분석 결과는 Mixtral과 같은 희소 전문가 혼합 모델의 내부 작동 방식을 이해하는 데 중요한 통찰을 제공합니다. 전문가들이 주제별로 특화되기보다는 구문적 패턴이나 위치적 특성에 따라 활성화된다는 사실은 이러한 모델의 설계와 최적화에 영향을 미칠 수 있으며, 향후 연구 방향을 제시합니다.

## 결론

본 논문에서는 오픈소스 모델 중 최초로 최첨단 성능을 달성한 전문가 혼합 네트워크인 Mixtral 8x7B를 소개했습니다. Mixtral 8x7B Instruct는 인간 평가 벤치마크에서 Claude-2.1, Gemini Pro, GPT-3.5 Turbo를 능가하는 성능을 보여주었습니다. 각 타임스텝마다 단 2개의 전문가만 사용하기 때문에, Mixtral은 토큰당 13B의 활성 파라미터만 사용하면서도 토큰당 70B 파라미터를 사용하는 이전 최고 모델(Llama 2 70B)보다 우수한 성능을 달성했습니다. 연구진은 학습된 모델과 미세 조정된 모델을 Apache 2.0 라이선스 하에 공개적으로 제공하고 있습니다. 이러한 모델 공유를 통해 다양한 산업과 도메인에 혜택을 줄 수 있는 새로운 기술과 응용 프로그램의 개발을 촉진하고자 합니다.

이해를 돕기 위해 덧붙이자면, Mixtral 8x7B의 성공은 희소 전문가 혼합(Sparse Mixture of Experts) 아키텍처의 효율성과 확장성을 입증하는 중요한 사례입니다. 이 모델은 각 레이어에서 8개의 전문가 중 2개만 선택적으로 활성화함으로써, 총 파라미터 수(47B)는 크게 유지하면서도 실제 계산량은 상대적으로 적게 유지하는 방식으로 작동합니다. 이러한 접근 방식은 대규모 언어 모델의 용량과 계산 효율성 사이의 균형을 맞추는 새로운 패러다임을 제시합니다.

Mixtral의 성능은 다양한 벤치마크에서 입증되었으며, 특히 수학, 코드 생성, 다국어 이해 분야에서 두드러진 성과를 보였습니다. 이는 희소 전문가 혼합 아키텍처가 특정 도메인에 특화된 능력을 효과적으로 개발할 수 있음을 시사합니다. 또한 32k 토큰의 긴 컨텍스트 처리 능력을 갖추고 있어, 복잡한 문서나 대화를 이해하고 생성하는 데 큰 이점을 제공합니다.

Mixtral 8x7B Instruct 모델은 지도 학습 미세 조정과 직접 선호도 최적화(Direct Preference Optimization)를 통해 훈련되었으며, 인간 평가 벤치마크에서 여러 상용 모델들을 능가하는 성능을 보여주었습니다. 이는 오픈소스 AI 모델이 상용 모델과 경쟁할 수 있는 수준에 도달했음을 보여주는 중요한 이정표입니다.

Apache 2.0 라이선스로 공개된 Mixtral 모델은 학술 및 상업적 용도로 자유롭게 사용할 수 있어, AI 기술의 민주화와 혁신 촉진에 기여할 것으로 기대됩니다. 이러한 오픈소스 접근 방식은 다양한 응용 분야에서 새로운 기술과 솔루션의 개발을 가속화하는 데 중요한 역할을 할 것입니다.

Mixtral 8x7B의 개발과 공개는 대규모 언어 모델의 발전 방향에 중요한 시사점을 제공합니다. 특히 모델 크기와 계산 효율성 사이의 균형을 맞추는 데 있어 희소 전문가 혼합 아키텍처의 잠재력을 보여주며, 향후 AI 모델 개발에 있어 새로운 가능성을 열어줍니다.

- - -
### References
* [Mixtral of Experts](http://arxiv.org/pdf/2401.04088v1)