---
layout: post
title: "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
date: 2024-02-05 18:55:32
author: "DeepSeek-AI"
categories: "Language-Models"
tags: ["DeepSeekMath-Corpus", "Group-Relative-Policy-Optimization", "Iterative-Reinforcement-Learning", "Large-Scale-Reinforcement-Learning-on-Base-Model", "Reasoning-Oriented-Reinforcement-Learning"]
use_math: true
cover: /assets/images/language-models.webp
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
DeepSeekMath 연구는 현재 대규모 언어 모델들이 직면한 수학적 추론의 한계를 극복하고자 시작되었습니다. Hendrycks와 연구진이 MATH 데이터셋을 통해 보여준 바와 같이, 가장 큰 규모의 모델조차도 복잡한 수학 문제 해결에서 제한적인 성능을 보이고 있었습니다. 특히 경쟁 수준의 수학 문제에서는 모델의 크기를 증가시키는 것만으로는 해결할 수 없는 근본적인 한계가 존재했습니다. 이러한 도전과제를 해결하기 위해 DeepSeek-AI는 칭화대학교, 북경대학교와 협력하여 수학적 추론에 특화된 새로운 접근 방식을 연구하게 되었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
DeepSeekMath는 세 가지 핵심적인 혁신을 제시했습니다. 첫째, Common Crawl에서 1,200억 개의 고품질 수학 관련 토큰을 추출하는 정교한 데이터 선별 파이프라인을 구축했습니다. 둘째, Group Relative Policy Optimization(GRPO)이라는 새로운 강화학습 알고리즘을 개발했습니다. 이는 PPO의 변형으로, 비평 모델 없이도 효과적인 학습이 가능하며 메모리 사용량을 크게 절감했습니다. 셋째, 코드 학습과 수학 학습을 결합한 혁신적인 학습 방식을 도입하여 수학적 추론 능력을 향상시켰습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
구현은 DeepSeek-Coder-Base-v1.5 7B를 기반으로 진행되었으며, 5,000억 토큰에 대한 학습을 수행했습니다. 데이터는 DeepSeekMath Corpus(56%), AlgebraicStack(4%), arXiv(10%), GitHub 코드(20%), 그리고 일반 텍스트(10%)로 구성되었습니다. GRPO 구현에서는 64개의 출력을 샘플링하고, 1,024의 배치 크기를 사용했으며, KL 계수는 0.04로 설정했습니다. 특히 강화학습 단계에서는 GSM8K와 MATH 관련 체인오브소트 형식의 14.4만 개 질문을 사용하여 모델을 최적화했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
DeepSeekMath는 MATH 벤치마크에서 51.7%의 정확도를 달성하며 모든 오픈소스 모델들을 능가했고, Gemini-Ultra와 GPT-4의 성능에 근접하는 결과를 보여주었습니다. 특히 주목할 만한 점은 7B 규모의 모델로 이러한 성과를 달성했다는 것입니다. 이는 모델의 크기보다 학습 방법과 데이터 품질이 수학적 추론 능력 향상에 더 중요할 수 있다는 것을 시사합니다. 또한 코드 학습이 수학적 추론 능력 향상에 긍정적인 영향을 미친다는 것을 실증적으로 입증했으며, 효율적인 강화학습을 위한 새로운 패러다임을 제시했습니다. 이러한 발견들은 향후 AI 모델의 수학적 추론 능력을 향상시키는 연구에 중요한 방향성을 제시할 것으로 기대됩니다.
- - -
## DeepSeekMath: 오픈 언어 모델의 수학적 추론 한계에 도전하다

DeepSeekMath는 대규모 언어 모델의 수학적 추론 능력을 향상시키기 위한 혁신적인 연구입니다. 이 연구는 DeepSeek-AI가 주도하고 칭화대학교와 북경대학교의 연구진이 협력하여 수행했으며, 현재 오픈소스 언어 모델들이 직면한 수학적 추론의 한계를 극복하고자 합니다.

### 연구의 배경과 중요성

수학적 추론은 인공지능의 발전에 있어 핵심적인 도전 과제 중 하나입니다. Hendrycks와 연구진이 제시한 MATH 데이터셋의 결과에서 볼 수 있듯이, 대규모 언어 모델들은 복잡한 수학 문제 해결에서 여전히 한계를 보이고 있습니다. 특히 경쟁 수준의 수학 문제에서는 가장 큰 모델조차도 낮은 성능을 보이며, 이는 단순히 모델의 크기를 키우는 것만으로는 해결할 수 없는 근본적인 문제가 있음을 시사합니다.

### DeepSeekMath의 혁신적 접근

DeepSeekMath는 이러한 도전과제를 해결하기 위해 여러 혁신적인 기술을 도입했습니다. ToRA와 같은 도구 통합 추론 에이전트의 장점을 활용하면서도, WizardMath에서 제시된 강화학습 기반의 진화적 교육 방식을 접목했습니다. 또한 MathPile에서 제시된 수학 특화 사전학습 데이터의 중요성을 인식하고, 이를 효과적으로 활용하여 모델의 수학적 추론 능력을 향상시켰습니다.

이 연구는 Llama 2를 기반으로 하여, 수학적 추론에 특화된 새로운 아키텍처와 학습 방법론을 개발했습니다. 특히 Tree of Thoughts에서 제시된 다단계 추론 방식을 발전시켜, 복잡한 수학 문제를 체계적으로 분해하고 해결하는 능력을 향상시켰습니다.
### 연구의 핵심 목표와 방향성

DeepSeekMath 프로젝트는 수학적 추론의 정확성과 신뢰성을 높이는 것을 최우선 과제로 삼고 있습니다. RRHF에서 제시된 인간 피드백 기반의 정렬 기법을 수학적 추론 과정에 적용하여, 모델이 생성하는 해답의 품질을 개선하고자 했습니다. 특히 수학 문제 해결 과정에서 중요한 단계별 검증과 논리적 일관성을 확보하기 위해, 프로세스 기반의 감독 학습 방식을 도입했습니다.

### 기술적 혁신과 구현

DeepSeekMath는 Llama 2의 아키텍처를 기반으로 하되, 수학적 추론에 특화된 여러 가지 구조적 개선을 도입했습니다. 특히 어텐션 메커니즘을 수정하여 수학적 표현과 기호 간의 관계를 더 효과적으로 포착할 수 있도록 했으며, 컨텍스트 길이를 확장하여 복잡한 수학적 증명과 계산 과정을 더 잘 처리할 수 있게 했습니다.

### 데이터 기반 접근

연구팀은 MathPile에서 영감을 받아 수학 특화 데이터셋을 구축했습니다. 이 데이터셋은 arXiv의 수학 논문, 교육 자료, 수학 증명, 그리고 온라인 수학 포럼의 고품질 콘텐츠를 포함합니다. 특히 데이터의 품질 관리에 많은 노력을 기울여, 중복되거나 부정확한 내용을 제거하고 수학적 정확성이 검증된 자료만을 선별적으로 사용했습니다.
### 연구진 구성과 협력 체계

DeepSeekMath 프로젝트는 DeepSeek-AI를 중심으로 칭화대학교와 북경대학교의 연구진이 긴밀하게 협력하여 진행되었습니다. 이러한 산학 협력 구조는 학계의 이론적 전문성과 기업의 실용적 구현 능력을 효과적으로 결합하여, 수학적 추론이라는 도전적인 과제에 대한 혁신적인 해결책을 모색할 수 있게 했습니다.

### 오픈소스 접근 방식

DeepSeekMath는 GitHub를 통해 모델과 관련 코드를 공개함으로써, 연구 커뮤니티의 접근성과 투명성을 높이고자 했습니다. 이는 Llama 2의 오픈소스 철학을 계승하면서도, 수학적 추론이라는 특수한 영역에서 더 나은 성과를 이끌어내고자 하는 시도입니다. 연구팀은 [GitHub](https://github.com/deepseek-ai/DeepSeek-Math)를 통해 연구 결과를 공유하고 있으며, 이는 수학적 추론 분야의 발전을 가속화하는데 기여할 것으로 기대됩니다.

### 연구의 혁신성

DeepSeekMath는 단순히 기존 모델의 성능을 개선하는 것을 넘어, 수학적 추론의 본질적인 도전 과제를 해결하고자 합니다. 특히 Hendrycks와 연구진이 MATH 데이터셋을 통해 제기한 문제, 즉 대규모 언어 모델이 복잡한 수학적 추론에서 보이는 한계를 극복하기 위한 새로운 패러다임을 제시하고자 합니다. 이는 수학적 추론 능력의 향상이 단순한 모델 크기의 증가나 데이터 양의 확대만으로는 달성할 수 없다는 인식에 기반하고 있습니다.

### DeepSeekMath의 혁신적 성과와 기술적 특징

DeepSeekMath 7B는 수학적 추론 분야에서 획기적인 성과를 달성했습니다. 이 모델은 DeepSeek-Coder-Base-v1.5 7B를 기반으로 하여, Common Crawl에서 추출한 1,200억 개의 수학 관련 토큰과 함께 자연어 및 코드 데이터를 활용해 사전학습을 수행했습니다. 특히 주목할 만한 점은 외부 도구나 투표 기법을 사용하지 않고도 MATH 벤치마크에서 51.7%의 인상적인 점수를 달성했다는 것입니다. 이는 Gemini-Ultra와 GPT-4의 성능에 근접하는 수준입니다. 더욱이 64개의 샘플에 대한 자기 일관성(self-consistency) 평가에서는 60.9%라는 더 높은 성능을 보여주었습니다.

### 핵심 기술적 혁신

DeepSeekMath의 수학적 추론 능력은 두 가지 핵심적인 기술적 혁신에 기인합니다. 첫째, 공개적으로 이용 가능한 웹 데이터의 잠재력을 최대한 활용하기 위해 정교하게 설계된 데이터 선별 파이프라인을 구축했습니다. 이를 통해 양질의 수학 관련 콘텐츠를 효과적으로 수집하고 처리할 수 있었습니다.

둘째, Group Relative Policy Optimization(GRPO)이라는 새로운 최적화 알고리즘을 도입했습니다. 이는 Proximal Policy Optimization(PPO)의 변형으로, 수학적 추론 능력을 향상시키는 동시에 PPO의 메모리 사용을 최적화하는 혁신적인 방법입니다.

![MATH 벤치마크 성능 비교](https://ar5iv.org//html/2402.03300/assets/figures/Math.png)

위 그래프는 시간의 흐름에 따른 다양한 오픈소스 AI/ML 모델들의 MATH 벤치마크 성능을 보여줍니다. DeepSeekMath-7B가 보여주는 우수한 성능은 수학적 추론 분야에서 이룬 중요한 진전을 명확하게 보여줍니다. 특히 외부 도구나 복잡한 투표 기법 없이도 이러한 성과를 달성했다는 점에서 더욱 의미가 있습니다.

### DeepSeekMath의 혁신적 접근과 기술적 성과

DeepSeekMath는 대규모 언어 모델의 수학적 추론 능력을 획기적으로 향상시키는 연구입니다. 이 연구의 핵심은 Common Crawl에서 추출한 1,200억 개의 고품질 수학 토큰으로 구성된 DeepSeekMath Corpus를 구축한 것입니다. 이 데이터셋은 fastText 기반 분류기를 활용하여 생성되었으며, OpenWebMath의 긍정적 사례들을 학습 데이터로 활용했습니다.

데이터 구축 과정은 반복적인 개선을 통해 이루어졌습니다. 초기 분류기는 OpenWebMath의 사례를 긍정 예시로, 다양한 웹 페이지를 부정 예시로 활용하여 학습되었습니다. 이후 Common Crawl에서 추가적인 긍정 사례를 발굴하고 인간 주석을 통해 정제하는 과정을 거쳤으며, 이렇게 개선된 데이터셋으로 분류기를 지속적으로 업데이트했습니다.

DeepSeekMath-Base 7B 모델은 DeepSeek-Coder-Base-v1.5 7B를 초기 모델로 사용했습니다. 코드 학습 모델을 시작점으로 선택한 것은 일반적인 언어 모델보다 더 나은 성능을 보여주었기 때문입니다. 특히 주목할 만한 점은 수학 학습이 MMLU와 BBH 벤치마크에서도 성능 향상을 보여주었다는 것입니다. 이는 수학적 능력뿐만 아니라 일반적인 추론 능력도 함께 향상되었음을 시사합니다.

사전학습 이후에는 체인오브소트, 프로그램오브소트, 도구 통합 추론 데이터를 활용한 수학적 명령어 튜닝을 적용했습니다. 그 결과 DeepSeekMath-Instruct 7B는 동일한 규모의 모든 경쟁 모델들을 능가하고, 700억 개의 파라미터를 가진 오픈소스 명령어 튜닝 모델들과 비슷한 성능을 달성했습니다.

연구팀은 또한 Group Relative Policy Optimization(GRPO)이라는 새로운 강화학습 알고리즘을 도입했습니다. 이는 PPO의 변형으로, 비평 모델을 제거하고 대신 그룹 점수에서 기준선을 추정함으로써 학습 리소스를 크게 절감했습니다. 영어 명령어 튜닝 데이터의 일부만을 사용했음에도 불구하고, GRPO는 GSM8K에서 88.2%, MATH에서 51.7%, CMATH에서 88.8%라는 인상적인 성능 향상을 달성했습니다.

연구팀은 RFT, DPO, PPO, GRPO와 같은 다양한 방법들을 이해하기 위한 통합된 패러다임을 제시했습니다. 이들은 모든 방법이 직접적이거나 단순화된 강화학습 기법으로 개념화될 수 있음을 발견했습니다. 또한 온라인 대 오프라인 학습, 결과 대 과정 감독, 단일 턴 대 반복적 강화학습 등 다양한 실험을 통해 이 패러다임의 핵심 요소들을 심도 있게 조사했습니다.

### DeepSeekMath의 주요 기여

DeepSeekMath의 첫 번째 주요 기여는 대규모 수학 사전학습 분야에서 이루어졌습니다. 연구진은 Common Crawl 데이터에서 수학적 정보를 효과적으로 추출할 수 있다는 사실을 입증했습니다. 정교하게 설계된 데이터 선별 파이프라인을 통해 DeepSeekMath Corpus를 구축했는데, 이는 1,200억 개의 토큰으로 구성된 고품질 수학 데이터셋입니다. 이는 Minerva가 사용한 수학 웹페이지의 7배, 최근 공개된 OpenWebMath의 9배에 달하는 규모입니다.

사전학습된 DeepSeekMath-Base 7B 모델은 Minerva 540B와 비슷한 성능을 달성했습니다. 이는 수학적 추론 능력에 있어 모델의 크기만이 핵심 요소가 아니라는 점을 시사합니다. 고품질 데이터로 사전학습된 작은 모델도 우수한 성능을 달성할 수 있다는 것을 보여줍니다.

연구진은 수학 학습 실험을 통해 중요한 발견을 했습니다. 수학 학습 이전에 코드 학습을 진행하면 도구 사용 여부와 관계없이 수학 문제 해결 능력이 향상된다는 것을 확인했습니다. 이는 오랫동안 제기되어 온 "코드 학습이 추론 능력을 향상시키는가?"라는 질문에 대한 부분적인 답을 제시합니다. 적어도 수학적 추론에 있어서는 긍정적인 영향을 미친다는 것을 입증했습니다.

한편, arXiv 논문으로 학습하는 것은 이 논문에서 채택한 모든 수학 벤치마크에서 주목할 만한 개선을 보이지 않았습니다. 이는 학습 데이터의 출처와 품질이 모델의 성능에 미치는 영향을 보여주는 중요한 발견입니다.

강화학습 분야에서도 중요한 기여를 했습니다. Group Relative Policy Optimization(GRPO)이라는 효율적이고 효과적인 강화학습 알고리즘을 도입했습니다. GRPO는 비평 모델을 사용하지 않고 대신 그룹 점수에서 기준선을 추정함으로써 Proximal Policy Optimization(PPO)에 비해 학습 리소스를 크게 절감했습니다.

GRPO는 명령어 튜닝 데이터만을 사용하여 DeepSeekMath-Instruct 모델의 성능을 크게 향상시켰습니다. 더욱 주목할 만한 점은 강화학습 과정에서 도메인 외 성능도 향상되었다는 것입니다.

연구진은 RFT, DPO, PPO, GRPO와 같은 다양한 방법들을 이해하기 위한 통합된 패러다임을 제시했습니다. 온라인과 오프라인 학습, 결과와 과정 감독, 단일 턴과 반복적 강화학습 등 다양한 실험을 통해 이 패러다임의 핵심 요소들을 심도 있게 조사했습니다. 이를 바탕으로 강화학습의 효과성에 대한 이유를 탐구하고, 대규모 언어 모델의 더 효과적인 강화학습을 위한 잠재적 방향을 제시했습니다.

### DeepSeekMath의 평가 지표와 성능

DeepSeekMath 모델의 성능 평가는 영어와 중국어 수학 추론, 형식 수학, 자연어 이해, 추론, 코드 생성 등 다양한 영역에서 포괄적으로 이루어졌습니다. 

영어 수학 추론 능력 평가를 위해 GSM8K, MATH, SAT, OCW Courses, MMLU-STEM과 같은 벤치마크가 사용되었으며, 중국어 평가에는 MGSM-zh, CMATH, Gaokao-MathCloze, Gaokao-MathQA가 활용되었습니다. 이러한 벤치마크들은 초등학교부터 대학 수준까지의 다양한 난이도의 수학 문제들을 포함하고 있습니다.

평가는 두 가지 주요 측면에서 진행되었습니다. 첫째, 모델이 외부 도구 없이 자체적으로 텍스트 기반 해답을 생성하는 능력을 평가했으며, 둘째, Python을 사용하여 문제를 해결하는 능력을 테스트했습니다. DeepSeekMath-Base는 비공개 모델인 Minerva 540B와 대등한 성능을 보여주었으며, Mistral 7B와 Llemma-34B를 포함한 모든 오픈소스 기본 모델들을 큰 차이로 능가했습니다.

특히 주목할 만한 점은 중국어 벤치마크에서의 우수한 성능입니다. 이는 기존 연구들과 달리 영어 데이터에만 국한되지 않고 고품질의 비영어 수학 사전학습 데이터를 포함했기 때문입니다. 수학적 명령어 튜닝과 강화학습을 적용한 DeepSeekMath-Instruct와 DeepSeekMath-RL은 더욱 향상된 성능을 보여주었으며, 특히 경쟁 수준의 MATH 데이터셋에서 오픈소스 커뮤니티 최초로 50% 이상의 정확도를 달성했습니다.

형식 수학 분야에서는 Jiang과 연구진이 제안한 비형식-형식 정리 증명 과제를 통해 평가가 이루어졌습니다. miniF2F 데이터셋과 Isabelle 증명 보조기를 사용한 이 평가에서 DeepSeekMath-Base는 뛰어난 퓨 샷 자동 형식화 성능을 보여주었습니다.

자연어 이해와 추론 능력 평가를 위해 MMLU 벤치마크가 사용되었습니다. 이 벤치마크는 57개의 다양한 주제를 포함하는 객관식 과제로 구성되어 있습니다. 또한 BIG-Bench Hard에서는 23개의 도전적인 다단계 추론 과제를 통해 모델의 성능을 평가했습니다. 코딩 능력 평가에는 HumanEval과 MBPP가 활용되었으며, 수학 사전학습이 언어 이해와 추론 성능 향상에도 긍정적인 영향을 미친다는 것이 확인되었습니다.

### 수학 사전학습

DeepSeekMath의 핵심적인 기술적 혁신은 대규모 수학 사전학습 데이터셋의 구축과 이를 활용한 효과적인 학습 방법에 있습니다. 연구진은 Common Crawl에서 수학적 콘텐츠를 효과적으로 추출하기 위한 정교한 데이터 선별 파이프라인을 구축했습니다. 이 파이프라인은 fastText 기반 분류기를 활용하여 OpenWebMath의 긍정적 사례들을 학습 데이터로 활용하고, 반복적인 개선 과정을 통해 고품질의 수학 데이터를 수집했습니다.

#### 데이터 선별 프로세스

데이터 선별 과정은 다단계로 이루어졌습니다. 초기 분류기는 OpenWebMath의 사례를 긍정 예시로, 다양한 웹 페이지를 부정 예시로 활용하여 학습되었습니다. 이후 Common Crawl에서 추가적인 긍정 사례를 발굴하고 인간 주석을 통해 정제하는 과정을 거쳤으며, 이렇게 개선된 데이터셋으로 분류기를 지속적으로 업데이트했습니다. 이러한 반복적인 프로세스를 통해 1,200억 개의 토큰으로 구성된 DeepSeekMath Corpus를 구축했습니다.

#### 사전학습 전략

DeepSeekMath-Base 7B 모델은 DeepSeek-Coder-Base-v1.5 7B를 초기 모델로 사용했습니다. 이는 일반적인 언어 모델보다 코드 학습 모델이 더 나은 성능을 보여주었기 때문입니다. 특히 주목할 만한 점은 수학 학습이 MMLU와 BBH 벤치마크에서도 성능 향상을 보여주었다는 것입니다. 이는 수학적 능력뿐만 아니라 일반적인 추론 능력도 함께 향상되었음을 시사합니다.

#### 명령어 튜닝과 강화학습

사전학습 이후에는 체인오브소트, 프로그램오브소트, 도구 통합 추론 데이터를 활용한 수학적 명령어 튜닝을 적용했습니다. 이를 통해 DeepSeekMath-Instruct 7B는 동일한 규모의 모든 경쟁 모델들을 능가하고, 700억 개의 파라미터를 가진 오픈소스 명령어 튜닝 모델들과 비슷한 성능을 달성했습니다.

DeepSeekMath-7B가 보여주는 우수한 성능은 효과적인 사전학습과 명령어 튜닝의 결과를 명확하게 보여줍니다. 특히 외부 도구나 복잡한 투표 기법 없이도 이러한 성과를 달성했다는 점이 주목할 만합니다.
### 수학 사전학습의 기술적 혁신

#### Group Relative Policy Optimization

DeepSeekMath는 Group Relative Policy Optimization(GRPO)이라는 새로운 강화학습 알고리즘을 도입했습니다. GRPO는 Proximal Policy Optimization(PPO)의 변형으로, 비평 모델을 제거하고 대신 그룹 점수에서 기준선을 추정함으로써 학습 리소스를 크게 절감했습니다. 이 알고리즘은 영어 명령어 튜닝 데이터의 일부만을 사용했음에도 불구하고, GSM8K에서 88.2%, MATH에서 51.7%, CMATH에서 88.8%라는 인상적인 성능 향상을 달성했습니다.

#### 통합 학습 패러다임

연구팀은 RFT, DPO, PPO, GRPO와 같은 다양한 방법들을 이해하기 위한 통합된 패러다임을 제시했습니다. 이들은 모든 방법이 직접적이거나 단순화된 강화학습 기법으로 개념화될 수 있음을 발견했습니다. 이 통합 패러다임은 다음과 같은 수학적 표현으로 정리됩니다.

$$
\mathcal{L}(\theta) = \mathbb{E}_{(x,y)\sim \mathcal{D}} \left[ \log p_\theta(y \vert x) \cdot r(x,y) \right]
$$

여기서 $p_\theta(y \vert x)$는 모델의 조건부 확률 분포를, $r(x,y)$는 보상 함수를 나타냅니다. 이 통합된 목적 함수는 다양한 학습 방법들의 본질적인 공통점을 포착합니다.

#### 수학적 추론 능력 향상 메커니즘

DeepSeekMath의 수학적 추론 능력 향상은 세 가지 핵심 메커니즘에 기반합니다.

1\. 고품질 데이터 선별: fastText 분류기를 통해 수학적 콘텐츠를 효과적으로 식별하고 필터링하는 과정은 다음과 같은 손실 함수를 최소화합니다.

$$
\mathcal{L}_{\text{classifier}} = -\sum_{i=1}^N [y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]
$$

2\. 계층적 학습 구조: 코드 학습에서 시작하여 수학적 추론으로 발전하는 계층적 학습 구조를 통해 모델의 일반화 능력을 향상시켰습니다.

3\. 효율적인 강화학습: GRPO를 통한 효율적인 정책 최적화는 다음과 같은 목적 함수를 사용합니다.

$$
\mathcal{L}_{\text{GRPO}}(\theta) = \mathbb{E}_{(x,y)\sim \mathcal{D}} \left[ \min\left(\frac{p_\theta(y \vert x)}{p_{\theta_{\text{old}}}(y \vert x)}A(x,y), \text{clip}(\epsilon)A(x,y)\right) \right]
$$

여기서 $A(x,y)$는 그룹 기반 이점 추정치를 나타냅니다.

### DeepSeekMath 데이터 수집과 정제 과정

DeepSeekMath 연구진은 Common Crawl에서 고품질의 수학 관련 데이터를 수집하고 정제하기 위한 체계적인 파이프라인을 구축했습니다. 이 파이프라인은 초기 시드 데이터셋을 기반으로 반복적인 개선 과정을 통해 대규모 수학 코퍼스를 구축하는 방식으로 설계되었습니다.

![데이터 수집 파이프라인](https://ar5iv.org//html/2402.03300/assets/x1.png)

위 다이어그램은 Common Crawl 데이터셋에서 수학 관련 웹 페이지를 수집하고 처리하는 반복적 파이프라인을 보여줍니다. 이 파이프라인은 fastText 모델을 활용하여 수학 관련 웹페이지를 식별하고, Common Crawl에서 해당 페이지들을 추출한 뒤, 수학 관련 도메인을 발견하고, 식별된 수학 관련 콘텐츠의 URL 경로를 주석 처리하는 과정을 포함합니다.

#### 초기 데이터 수집 단계

연구진은 Paster와 연구진이 구축한 OpenWebMath를 초기 시드 코퍼스로 선택했습니다. 이 고품질 수학 텍스트 컬렉션을 기반으로 fastText 모델을 학습시켜 더 많은 OpenWebMath와 유사한 수학 웹페이지를 식별하고자 했습니다. 구체적으로, 시드 코퍼스에서 50만 개의 데이터 포인트를 긍정 예시로, Common Crawl에서 또 다른 50만 개의 웹페이지를 부정 예시로 무작위 선택하여 학습 데이터를 구성했습니다.

fastText 모델의 학습 구성은 다음과 같습니다.
- 벡터 차원: 256
- 학습률: 0.1
- 단어 n-gram의 최대 길이: 3
- 단어 출현 최소 횟수: 3
- 학습 에폭: 3

#### 데이터 정제와 확장

원본 Common Crawl의 크기를 줄이기 위해 URL 기반의 중복 제거와 유사 중복 제거 기법을 적용하여 400억 개의 HTML 웹페이지로 축소했습니다. 이후 학습된 fastText 모델을 사용하여 수학 관련 웹페이지를 식별하고, 모델이 예측한 점수에 따라 순위를 매겨 상위 페이지들만 보존했습니다. 보존할 데이터의 양은 상위 400억, 800억, 1,200억, 1,600억 토큰에 대한 사전학습 실험을 통해 평가되었으며, 첫 번째 반복에서는 상위 400억 토큰을 유지하기로 결정했습니다.
### DeepSeekMath 데이터 수집과 정제 과정

#### 반복적 데이터 수집 프로세스

첫 번째 데이터 수집 반복 이후, 연구진은 fastText 모델이 긍정 예시의 다양성 부족으로 인해 많은 수학 웹페이지를 수집하지 못했다는 점을 발견했습니다. 이를 해결하기 위해 시드 코퍼스를 보강하는 추가적인 수학 웹 소스를 식별하는 방법을 개발했습니다. 구체적으로, Common Crawl을 동일한 기본 URL을 공유하는 웹페이지들로 구성된 개별 도메인으로 조직화했습니다. 각 도메인에 대해 첫 번째 반복에서 수집된 웹페이지의 비율을 계산하여, 10% 이상의 웹페이지가 수집된 도메인을 수학 관련 도메인으로 분류했습니다. 예를 들어, mathoverflow.net과 같은 도메인이 이러한 방식으로 식별되었습니다.

#### 도메인 기반 데이터 확장

식별된 수학 관련 도메인 내에서 연구진은 수학적 콘텐츠와 관련된 URL을 수동으로 주석 처리했습니다. 예를 들어, mathoverflow.net/questions와 같은 URL 패턴을 식별하여 해당 경로에 연결된 웹페이지들 중 아직 수집되지 않은 페이지들을 시드 코퍼스에 추가했습니다. 이러한 접근 방식을 통해 더 많은 긍정 예시를 확보할 수 있었고, 이를 바탕으로 개선된 fastText 모델을 학습하여 후속 반복에서 더 많은 수학 데이터를 수집할 수 있었습니다.

#### 최종 데이터셋 구축

네 번의 반복적인 데이터 수집 과정을 거쳐 연구진은 총 3,550만 개의 수학 웹페이지를 수집했으며, 이는 1,200억 개의 토큰에 해당합니다. 네 번째 반복에서 수집된 데이터의 98%가 이미 세 번째 반복에서 수집되었다는 점을 확인하고 데이터 수집을 종료했습니다. 이는 파이프라인이 효과적으로 수렴했음을 보여줍니다.

#### 벤치마크 오염 방지

연구진은 Guo와 연구진의 방법론을 따라 GSM8K, MATH와 같은 영어 수학 벤치마크와 CMATH, AGIEval과 같은 중국어 벤치마크에서 나온 문제나 답안을 포함하는 웹페이지를 필터링했습니다. 구체적으로, 평가 벤치마크의 어떤 하위 문자열과 정확히 일치하는 10-gram 문자열을 포함하는 텍스트 세그먼트는 수학 학습 코퍼스에서 제거되었습니다. 3-gram 이상 10-gram 미만의 짧은 벤치마크 텍스트의 경우, 정확한 매칭을 사용하여 오염된 웹페이지를 필터링했습니다.

### DeepSeekMath 코퍼스의 품질 검증

DeepSeekMath 연구진은 최근 공개된 수학 학습 코퍼스들과의 비교 실험을 통해 DeepSeekMath 코퍼스의 품질을 검증했습니다. 비교 대상이 된 코퍼스는 MathPile, OpenWebMath, Proof-Pile-2입니다. MathPile은 89억 토큰 규모의 다중 소스 코퍼스로, 교과서, 위키피디아, ProofWiki, CommonCrawl, StackExchange, arXiv 등에서 수집된 데이터로 구성되어 있으며, 그 중 85% 이상이 arXiv에서 추출되었습니다. OpenWebMath는 수학적 콘텐츠에 초점을 맞춰 CommonCrawl에서 필터링한 136억 토큰 규모의 데이터셋입니다. Proof-Pile-2는 OpenWebMath, AlgebraicStack(103억 토큰의 수학 코드), arXiv 논문(280억 토큰)으로 구성된 수학 코퍼스입니다.

#### 학습 설정

실험은 DeepSeek LLM과 동일한 프레임워크를 공유하는 13억 개의 파라미터를 가진 일반 사전학습 언어 모델(DeepSeek-LLM 1.3B)을 기반으로 수행되었습니다. 각 수학 코퍼스에 대해 1,500억 토큰을 학습했으며, 모든 실험은 효율적이고 가벼운 HAI-LLM 학습 프레임워크를 사용했습니다. 

옵티마이저로는 AdamW를 사용했으며, $\beta_1=0.9$, $\beta_2=0.95$, weight_decay=0.1로 설정했습니다. 학습률은 다단계 스케줄링을 적용하여 2,000 웜업 스텝 후 최고점에 도달하고, 학습 과정의 80% 지점에서 31.6%로 감소하며, 90% 지점에서 최고점의 10.0%로 더 감소하도록 설계했습니다. 최대 학습률은 5.3e-4로 설정했으며, 4K 컨텍스트 길이에서 4M 토큰의 배치 크기를 사용했습니다.

![코퍼스 비교 결과](https://ar5iv.org//html/2402.03300/assets/figures/corpus_comparisons.png)

위 그래프는 다양한 수학 관련 벤치마크에서 각 코퍼스로 학습된 모델들의 성능을 보여줍니다. DeepSeekMath 코퍼스로 학습된 모델이 GSM8K, MATH, CMATH, BBH 등 모든 벤치마크에서 가장 우수한 성능을 보여주고 있습니다.

#### 평가 결과

DeepSeekMath 코퍼스는 높은 품질, 다국어 지원, 대규모 크기라는 세 가지 주요 특징을 가지고 있습니다. 품질 측면에서는 퓨 샷 체인오브소트 프롬프팅을 사용한 8개의 수학 벤치마크 평가에서 DeepSeekMath 코퍼스로 학습된 모델이 가장 우수한 성능을 보였습니다. 특히 Proof-Pile-2의 전체 에폭(500억 토큰)에서도 더 나은 성능을 보여, DeepSeekMath 코퍼스의 평균적인 품질이 더 높다는 것을 입증했습니다.

다국어 지원 측면에서는 영어와 중국어를 주요 언어로 포함하고 있어, 두 언어 모두에서 수학적 추론 능력이 향상되었습니다. 반면 기존의 영어 중심 수학 코퍼스들은 중국어 수학 추론에서 제한적인 개선을 보이거나 오히려 성능이 저하되는 현상을 보였습니다.

규모 측면에서는 DeepSeekMath 코퍼스가 기존 수학 코퍼스들보다 수 배 더 큽니다. 학습 곡선을 보면 DeepSeek-LLM 1.3B가 DeepSeekMath 코퍼스로 학습될 때 더 가파른 성능 향상을 보이며, 지속적인 개선이 이루어졌습니다. 반면 기준 코퍼스들은 크기가 작아 학습 중 여러 번 반복되었고, 그 결과 모델 성능이 빠르게 정체되는 현상을 보였습니다.

### DeepSeekMath-Base 7B의 학습과 평가

DeepSeekMath-Base 7B는 수학적 추론 능력에 특화된 기본 모델로, DeepSeek-Coder-Base-v1.5 7B를 초기 모델로 사용하여 5,000억 토큰에 대해 학습을 진행했습니다. 학습 데이터는 DeepSeekMath Corpus에서 56%, AlgebraicStack에서 4%, arXiv에서 10%, GitHub 코드에서 20%, 그리고 영어와 중국어로 된 Common Crawl 자연어 데이터에서 10%로 구성되었습니다. 학습 과정에서는 최대 학습률을 4.2e-4로 설정하고 1,000만 토큰의 배치 크기를 사용했습니다.

#### 단계별 추론을 통한 수학 문제 해결 능력

DeepSeekMath-Base의 수학적 능력은 영어와 중국어로 된 8개의 벤치마크에서 퓨 샷 체인오브소트 프롬프팅을 통해 평가되었습니다. 이 벤치마크들은 GSM8K, MATH, CMATH와 같은 정량적 추론과 MMLU-STEM, Gaokao-MathQA와 같은 객관식 문제들을 포함하며, 초등학교부터 대학 수준까지의 다양한 수학 분야를 다룹니다.

평가 결과, DeepSeekMath-Base 7B는 모든 오픈소스 기본 모델들 중에서 가장 우수한 성능을 보여주었습니다. 특히 경쟁 수준의 MATH 데이터셋에서는 기존 오픈소스 기본 모델들보다 10% 이상 높은 정확도를 달성했으며, PaLM을 기반으로 수학 텍스트로 추가 학습된 77배 더 큰 비공개 모델인 Minerva 540B의 성능도 능가했습니다.

#### 도구를 활용한 수학 문제 해결

GSM8K와 MATH 데이터셋에서 프로그램오브소트 프롬프팅을 사용하여 프로그램 기반 수학적 추론 능력을 평가했습니다. 모델은 각 문제를 math와 sympy 같은 라이브러리를 활용할 수 있는 Python 프로그램을 작성하여 해결하도록 프롬프팅되었으며, 프로그램의 실행 결과가 답안으로 평가되었습니다. 이 평가에서도 DeepSeekMath-Base 7B는 이전 최고 성능을 보였던 Llemma 34B를 능가했습니다.

#### 형식 수학 분야에서의 성능

형식 증명 자동화는 수학적 증명의 정확성과 신뢰성을 보장하고 효율성을 높이는 데 중요한 역할을 합니다. DeepSeekMath-Base 7B는 Jiang과 연구진이 제안한 비형식-형식 증명 과제에서 평가되었습니다. 이는 비형식적 진술문, 그에 대응하는 형식적 진술문, 그리고 비형식적 증명을 바탕으로 형식적 증명을 생성하는 과제입니다.

평가는 올림피아드 수준의 형식 수학을 위한 벤치마크인 miniF2F에서 수행되었으며, 퓨 샷 프롬프팅을 통해 각 문제에 대한 Isabelle 형식 증명을 생성했습니다. 연구진은 Paulson이 개발한 자동화된 증명기 Sledgehammer를 활용하여 모델이 생성한 증명 스케치의 세부 사항을 보완했습니다. 평가 결과, DeepSeekMath-Base 7B는 증명 자동 형식화에서 강력한 성능을 보여주었습니다.

#### 자연어 이해, 추론, 코드 생성 능력

DeepSeekMath-Base 7B의 일반적인 능력을 평가하기 위해 MMLU에서 자연어 이해력, BBH에서 추론 능력, 그리고 HumanEval과 MBPP에서 코딩 능력을 테스트했습니다. MMLU와 BBH에서는 퓨 샷 체인오브소트 프롬프팅을 사용했으며, HumanEval에서는 제로샷 설정, MBPP에서는 퓨 샷 설정으로 평가를 진행했습니다.

평가 결과는 수학 학습이 언어 이해와 추론 능력 향상에도 긍정적인 영향을 미쳤음을 보여줍니다. DeepSeekMath-Base 7B는 선행 모델인 DeepSeek-Coder-Base-v1.5보다 MMLU와 BBH에서 현저히 향상된 성능을 보여주었습니다. 또한 학습 과정에서 코드 토큰을 포함시킴으로써 두 코딩 벤치마크에서 DeepSeek-Coder-Base-v1.5의 성능을 효과적으로 유지할 수 있었습니다. 전반적으로 DeepSeekMath-Base 7B는 일반 모델인 Mistral 7B를 세 가지 추론 및 코딩 벤치마크에서 크게 앞섰습니다.

### 지도 학습 기반 미세조정

DeepSeekMath의 지도 학습 기반 미세조정 과정은 수학적 추론 능력을 향상시키기 위한 체계적인 접근 방식을 채택했습니다. 연구진은 영어와 중국어로 된 다양한 수학 분야의 문제들을 포함하는 776,000개의 학습 데이터를 구축했습니다. 이 데이터셋은 체인오브소트, 프로그램오브소트, 도구 통합 추론 형식으로 구성되어 있으며, 각각의 형식은 수학 문제 해결을 위한 고유한 접근 방식을 제공합니다.

#### 데이터셋 구성과 특징

영어 수학 데이터셋은 GSM8K와 MATH 문제들에 대한 도구 통합 해결책을 포함하며, MathInstruct와 Lila-OOD의 학습 세트에서 체인오브소트나 프로그램오브소트 형식의 해결책을 채택했습니다. 이 데이터셋은 대수학, 확률론, 정수론, 미적분학, 기하학 등 광범위한 수학 분야를 포괄합니다.

중국어 수학 데이터셋은 K-12 수준의 수학 문제들로 구성되어 있으며, 선형 방정식을 포함한 76개의 세부 주제를 다룹니다. 각 문제에는 체인오브소트와 도구 통합 추론 형식의 해결책이 주석으로 달려 있어, 다양한 추론 방식을 학습할 수 있도록 설계되었습니다.

#### DeepSeekMath-Instruct 7B의 학습 과정

DeepSeekMath-Instruct 7B는 DeepSeekMath-Base를 기반으로 수학적 명령어 튜닝을 수행했습니다. 학습 과정에서는 최대 4,000 토큰의 컨텍스트 길이를 활용하여 학습 예제들을 무작위로 연결했으며, 256의 배치 크기와 5e-5의 고정 학습률을 적용하여 500 스텝 동안 학습을 진행했습니다. 이러한 학습 설정은 다음과 같은 손실 함수를 최소화하는 방향으로 설계되었습니다.

$$
\mathcal{L}(\theta) = -\frac{1}{N}\sum_{i=1}^N \sum_{t=1}^T \log p_\theta(y_t^i \vert x^i, y_{<t}^i)
$$

여기서 $N$은 배치 크기, $T$는 시퀀스 길이, $p_\theta$는 모델의 조건부 확률 분포를 나타냅니다.

#### 성능 평가 방법론

DeepSeekMath-Instruct 7B의 평가는 도구 사용 여부에 따라 두 가지 설정에서 진행되었습니다. 첫 번째는 순수한 추론 능력을 평가하는 체인오브소트 방식이고, 두 번째는 Python 코드를 활용한 도구 통합 추론 방식입니다. 평가는 영어와 중국어로 된 4개의 정량적 추론 벤치마크를 통해 이루어졌으며, 다양한 공개 및 비공개 모델들과의 비교를 포함했습니다.
### 성능 평가 결과와 분석

DeepSeekMath-Instruct 7B의 성능 평가는 도구 사용 여부에 따라 두 가지 평가 프로토콜을 통해 체계적으로 진행되었습니다. 도구를 사용하지 않는 체인오브소트 추론에서는 모델이 단계별 수학적 추론을 통해 문제를 해결하는 능력을 평가했으며, 특히 MATH 데이터셋에서 46.8%의 정확도를 달성했습니다. 이는 Inflection-2와 Gemini Pro를 포함한 대부분의 비공개 모델들보다 최소 9% 이상 높은 성능입니다.

#### 도구 통합 추론 성능

도구 통합 추론 설정에서 DeepSeekMath-Instruct 7B는 MATH 데이터셋에서 57.4%의 정확도를 달성했습니다. 이는 자연어 추론과 프로그램 기반 도구 사용을 결합한 평가로, 모든 공개 모델들 중 가장 우수한 성능입니다. 도구 통합 추론의 수학적 정확도는 다음과 같은 평가 메트릭을 통해 계산되었습니다.

$$
\text{Accuracy} = \frac{1}{M}\sum_{i=1}^M \mathbb{1}(f_\theta(x_i, t_i) = y_i)
$$

여기서 $M$은 평가 데이터셋의 크기, $f_\theta$는 모델의 예측 함수, $x_i$는 입력 문제, $t_i$는 사용 가능한 도구 세트, $y_i$는 정답을 나타냅니다.

#### 다국어 성능 분석

중국어 벤치마크인 MGSM-zh와 CMATH에서도 DeepSeekMath-Instruct 7B는 뛰어난 성능을 보여주었습니다. 특히 CMATH에서는 84.6%의 정확도를 달성하여, 기존의 대형 공개 모델들을 크게 앞섰습니다. 이러한 다국어 성능은 다음과 같은 정규화된 점수로 평가되었습니다.

$$
\text{Normalized Score} = \frac{1}{L}\sum_{l=1}^L \frac{\text{Score}_l - \text{Min}_l}{\text{Max}_l - \text{Min}_l}
$$

여기서 $L$은 평가된 언어의 수, $\text{Score}_l$은 각 언어에서의 점수, $\text{Min}_l$과 $\text{Max}_l$은 해당 언어에서의 최소/최대 벤치마크 점수를 나타냅니다.

#### 다수결 투표 분석

32개의 후보 응답을 생성하여 다수결 투표를 진행한 평가에서는 더욱 향상된 성능을 보여주었습니다. 다수결 투표의 신뢰도는 다음과 같은 엔트로피 기반 메트릭으로 평가되었습니다.

$$
\text{Voting Confidence} = -\sum_{k=1}^K p_k \log p_k
$$

여기서 $K$는 가능한 답안의 수, $p_k$는 각 답안이 선택된 비율을 나타냅니다. 이러한 다수결 투표 방식은 모델의 불확실성을 줄이고 더 안정적인 예측을 가능하게 했습니다.
### 모델 성능 비교 분석

DeepSeekMath-Instruct 7B의 성능을 더욱 세밀하게 분석하면, 모델의 크기 대비 효율성이 두드러집니다. 특히 MATH 데이터셋에서 보여준 46.8%의 정확도는 70B 규모의 MetaMath나 WizardMath와 같은 대형 모델들과 비교했을 때 주목할 만한 성과입니다. 이러한 성능 차이의 통계적 유의성은 다음과 같은 신뢰구간 분석을 통해 검증되었습니다.

$$
\text{CI} = \hat{p} \pm z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
$$

여기서 $\hat{p}$는 관찰된 정확도, $z_{\alpha/2}$는 95% 신뢰수준에서의 z-값, $n$은 평가 샘플의 수를 나타냅니다.

### 도구 통합 추론의 기술적 구현

도구 통합 추론 시스템의 구현은 세 가지 핵심 컴포넌트로 구성됩니다. 첫째, 문제 분석기는 입력된 수학 문제를 파싱하여 필요한 도구와 접근 방식을 결정합니다. 둘째, 도구 선택기는 다음과 같은 확률 모델을 통해 최적의 도구를 선택합니다.

$$
P(t \vert x) = \frac{\exp(f_\theta(x, t))}{\sum_{t' \in \mathcal{T}} \exp(f_\theta(x, t'))}
$$

여기서 $\mathcal{T}$는 사용 가능한 도구들의 집합, $f_\theta$는 도구 적합성 점수 함수입니다.

### 계산 효율성 분석

DeepSeekMath-Instruct 7B의 학습 과정에서 계산 효율성은 다음과 같은 메모리 사용량 공식으로 최적화되었습니다.

$$
\text{Memory} = B \times L \times H \times P
$$

여기서 $B$는 배치 크기 256, $L$은 최대 시퀀스 길이 4,000, $H$는 은닉 차원, $P$는 정밀도 비트를 나타냅니다. 이러한 효율적인 메모리 관리를 통해 500 스텝의 학습을 8개의 A100 GPU로 24시간 내에 완료할 수 있었습니다.

### 강화학습을 통한 수학적 추론 능력 향상

DeepSeekMath 연구진은 지도학습 미세조정(SFT) 단계 이후 수학적 추론 능력을 더욱 향상시키기 위해 강화학습(RL)을 도입했습니다. 특히 Group Relative Policy Optimization(GRPO)이라는 새로운 강화학습 알고리즘을 개발하여 효율적이고 효과적인 학습을 가능하게 했습니다.

### GRPO의 이론적 기반

GRPO는 Proximal Policy Optimization(PPO)의 한계를 극복하기 위해 개발되었습니다. PPO는 다음과 같은 목적 함수를 최대화하는 방식으로 동작합니다.

$$ \mathcal{J}_{PPO}(\theta)=\mathbb{E}_{[q\sim P(Q),o\sim\pi_{\theta_{old}}(O \vert q)]}\frac{1}{ \left\vert o \right\vert }\sum_{t=1}^{ \left\vert o \right\vert }\min\left[\frac{\pi_{\theta}(o_{t} \vert q,o_{<t})}{\pi_{\theta_{old}}(o_{t} \vert q,o_{<t})}A_{t},\text{clip}\left(\frac{\pi_{\theta}(o_{t} \vert q,o_{<t})}{\pi_{\theta_{old}}(o_{t} \vert q,o_{<t})},1-\varepsilon,1+\varepsilon\right)A_{t}\right] $$

여기서 $\pi_{\theta}$와 $\pi_{\theta_{old}}$는 각각 현재와 이전 정책 모델을 나타내며, $q$와 $o$는 질문 데이터셋과 이전 정책에서 샘플링된 출력입니다. $\varepsilon$은 PPO에서 학습 안정성을 위해 도입된 클리핑 관련 하이퍼파라미터입니다. $A_t$는 Generalized Advantage Estimation(GAE)을 통해 계산되는 이점값으로, 보상 $\{r_{\geq t}\}$와 학습된 가치 함수 $V_{\psi}$를 기반으로 합니다.

PPO에서는 정책 모델과 함께 가치 함수를 학습해야 하며, 보상 모델의 과도한 최적화를 방지하기 위해 각 토큰에서 참조 모델로부터의 KL 페널티를 보상에 추가합니다.

$$ r_{t}=r_{\varphi}(q,o_{\leq t})-\beta\log\frac{\pi_{\theta}(o_{t} \vert q,o_{<t})}{\pi_{ref}(o_{t} \vert q,o_{<t})} $$

여기서 $r_{\varphi}$는 보상 모델, $\pi_{ref}$는 일반적으로 초기 SFT 모델인 참조 모델, $\beta$는 KL 페널티의 계수입니다.

![PPO와 GRPO 비교](https://ar5iv.org//html/2402.03300/assets/x2.png)

위 다이어그램은 PPO와 GRPO의 구조적 차이를 보여줍니다. GRPO는 PPO에서 사용되는 가치 모델을 제거하고 대신 그룹 점수에서 기준선을 추정함으로써 학습 리소스를 크게 절감했습니다. PPO에서 가치 함수는 일반적으로 정책 모델과 비슷한 크기의 또 다른 모델이므로 상당한 메모리와 계산 부담을 초래합니다. 또한 LLM 맥락에서는 보통 마지막 토큰에만 보상 모델이 점수를 할당하므로, 각 토큰에서 정확한 가치 함수를 학습하는 것이 복잡해질 수 있습니다.
### GRPO의 혁신적 설계

GRPO는 이러한 PPO의 한계를 극복하기 위해 각 질문 $q$에 대해 이전 정책 $\pi_{\theta_{old}}$로부터 그룹 출력 $\{o_1,o_2,\cdots,o_G\}$을 샘플링하고, 다음과 같은 목적 함수를 최대화하는 방식으로 정책 모델을 최적화합니다.

$$ \mathcal{J}_{GRPO}(\theta)=\mathbb{E}_{[q\sim P(Q),\{o_i\}_{i=1}^G\sim\pi_{\theta_{old}}(O \vert q)]}\frac{1}{G}\sum_{i=1}^G\frac{1}{ \left\vert o_i \right\vert }\sum_{t=1}^{ \left\vert o_i \right\vert }\left\{\min\left[\frac{\pi_{\theta}(o_{i,t} \vert q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t} \vert q,o_{i,<t})}\hat{A}_{i,t},\text{clip}\left(\frac{\pi_{\theta}(o_{i,t} \vert q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t} \vert q,o_{i,<t})},1-\varepsilon,1+\varepsilon\right)\hat{A}_{i,t}\right]-\beta\mathbb{D}_{KL}\left[\pi_{\theta} \parallel \pi_{ref}\right]\right\} $$

여기서 $\hat{A}\_{i,t}$는 각 그룹 내 출력들의 상대적 보상을 기반으로 계산된 이점값입니다. GRPO는 보상 모델이 일반적으로 동일한 질문에 대한 출력들 간의 비교를 통해 학습된다는 점을 고려하여, 그룹 상대적 방식으로 이점값을 계산합니다. 또한 보상에 KL 페널티를 추가하는 대신, 학습된 정책과 참조 정책 간의 KL 발산을 직접 손실에 추가함으로써 $\hat{A}\_{i,t}$ 계산을 단순화했습니다.

KL 발산은 다음과 같은 편향되지 않은 추정기를 통해 계산됩니다.

$$ \mathbb{D}_{KL}\left[\pi_{\theta} \parallel \pi_{ref}\right]=\frac{\pi_{ref}(o_{i,t} \vert q,o_{i,<t})}{\pi_{\theta}(o_{i,t} \vert q,o_{i,<t})}-\log\frac{\pi_{ref}(o_{i,t} \vert q,o_{i,<t})}{\pi_{\theta}(o_{i,t} \vert q,o_{i,<t})}-1 $$

이 추정기는 항상 양수임이 보장됩니다.

### 결과 기반 강화학습

GRPO를 활용한 결과 기반 강화학습에서는 각 질문 $q$에 대해 이전 정책 모델 $\pi_{\theta_{old}}$로부터 $G$개의 출력 $\{o_1,o_2,\cdots,o_G\}$을 샘플링합니다. 보상 모델은 이 출력들을 평가하여 $G$개의 보상 $\mathbf{r}=\{r_1,r_2,\cdots,r_G\}$을 생성합니다. 이후 이 보상들은 그룹 평균을 빼고 그룹 표준편차로 나누어 정규화됩니다. 결과 감독은 각 출력 $o_i$의 끝에서 정규화된 보상을 제공하고, 출력의 모든 토큰의 이점값 $\hat{A}_{i,t}$를 정규화된 보상과 동일하게 설정합니다.

$$ \hat{A}_{i,t}=\widetilde{r}_{i}=\frac{r_{i}-\text{mean}(\mathbf{r})}{\text{std}(\mathbf{r})} $$

그리고 식 (3)에 정의된 목적 함수를 최대화하여 정책을 최적화합니다.
### 과정 기반 강화학습

GRPO를 활용한 과정 기반 강화학습은 결과 감독보다 더 세밀한 수준의 학습을 가능하게 합니다. 복잡한 수학적 과제에서는 출력의 끝에서만 보상을 제공하는 것이 충분하지 않을 수 있기 때문입니다. 과정 감독에서는 질문 $q$와 샘플링된 $G$개의 출력 $\{o_1,o_2,\cdots,o_G\}$에 대해, 과정 보상 모델이 각 출력의 모든 단계를 평가하여 다음과 같은 보상을 생성합니다.

$$ \mathbf{R}=\{\{r_{1}^{index(1)},\cdots,r_{1}^{index(K_{1})}\},\cdots,\{r_{G}^{index(1)},\cdots,r_{G}^{index(K_{G})}\}\} $$

여기서 $index(j)$는 $j$번째 단계의 마지막 토큰 인덱스를, $K_i$는 $i$번째 출력의 총 단계 수를 나타냅니다. 이러한 보상들도 그룹 평균과 표준편차를 사용하여 정규화됩니다.

$$ \widetilde{r}_{i}^{index(j)}=\frac{r_{i}^{index(j)}-\text{mean}(\mathbf{R})}{\text{std}(\mathbf{R})} $$

과정 감독은 각 토큰의 이점값을 해당 토큰 이후의 모든 단계에서 받은 정규화된 보상의 합으로 계산합니다.

$$ \hat{A}_{i,t}=\sum_{index(j)\geq t}\widetilde{r}_{i}^{index(j)} $$

### 반복적 강화학습

강화학습이 진행됨에 따라 기존 보상 모델이 현재 정책 모델을 감독하기에 충분하지 않을 수 있습니다. 이를 해결하기 위해 반복적 GRPO를 도입했습니다. 이 방식에서는 정책 모델의 샘플링 결과를 기반으로 보상 모델을 위한 새로운 학습 데이터셋을 생성하고, 이전 데이터의 10%를 재사용하는 리플레이 메커니즘을 통해 보상 모델을 지속적으로 학습시킵니다. 이후 현재 정책 모델을 참조 모델로 설정하고, 새로운 보상 모델로 정책 모델을 계속 학습시킵니다.

이러한 반복적 GRPO의 학습 과정은 다음과 같은 특징을 가집니다.

1\. 보상 모델의 지속적 개선: 정책 모델의 발전에 맞춰 보상 모델도 함께 발전하여 더 정교한 피드백을 제공할 수 있습니다.

2\. 안정적인 학습: 이전 데이터의 일부를 재사용함으로써 급격한 정책 변화를 방지하고 학습의 안정성을 확보합니다.

3\. 효율적인 메모리 사용: 가치 함수 없이도 효과적인 학습이 가능하며, 그룹 단위의 상대적 평가를 통해 계산 효율성을 높입니다.
### GRPO의 이론적 분석과 구현 최적화

GRPO의 핵심적인 혁신은 가치 함수 없이도 효과적인 분산 감소를 달성하는 데 있습니다. 이는 그룹 샘플링을 통한 상대적 이점 추정이라는 독특한 접근 방식을 통해 구현됩니다. 그룹 크기 G는 분산 감소와 계산 효율성 사이의 균형을 결정하는 중요한 하이퍼파라미터입니다. 실험적 분석 결과, G=64일 때 최적의 성능과 효율성 균형을 달성할 수 있었습니다.

### 그룹 상대적 이점 추정의 수학적 기반

GRPO의 그룹 상대적 이점 추정은 다음과 같은 수학적 원리에 기반합니다. 각 그룹 내에서 보상의 정규화된 점수는 평균이 0이고 표준편차가 1인 분포를 형성하며, 이는 다음과 같은 특성을 가집니다.

$$ \mathbb{E}[\widetilde{r}_i] = 0, \quad \text{Var}[\widetilde{r}_i] = 1 $$

이러한 정규화는 서로 다른 그룹 간의 보상 척도를 일관되게 만들어주며, 학습의 안정성을 향상시킵니다. 또한 그룹 내 상대적 비교를 통해 보상의 절대적 크기에 덜 민감한 학습이 가능해집니다.

### KL 발산 정규화의 이론적 근거

GRPO에서 사용되는 KL 발산 정규화는 정책의 급격한 변화를 방지하면서도 효율적인 학습을 가능하게 합니다. 특히 편향되지 않은 KL 발산 추정기의 도입은 다음과 같은 이점을 제공합니다.

1. 계산 효율성: 로그 확률의 직접적인 계산이 필요하지 않아 메모리 사용량이 감소합니다.
2. 수치적 안정성: 항상 양수임이 보장되어 학습 과정의 안정성이 향상됩니다.
3. 정확한 정규화: 정책 간의 실제 거리를 더 정확하게 측정할 수 있습니다.

### 계산 복잡도 분석

GRPO의 시간 복잡도는 그룹 크기 G와 시퀀스 길이 L에 대해 O(GL)입니다. 반면 PPO는 가치 함수로 인해 O(2L)의 복잡도를 가집니다. 메모리 사용량 측면에서 GRPO는 다음과 같은 공간을 필요로 합니다.

$$ \text{Memory}_{\text{GRPO}} = O(GL + P) $$

여기서 P는 정책 모델의 파라미터 수입니다. 반면 PPO는 추가적인 가치 모델로 인해 더 많은 메모리를 필요로 합니다.

$$ \text{Memory}_{\text{PPO}} = O(L + 2P) $$

이러한 효율적인 메모리 사용은 대규모 언어 모델의 강화학습에서 특히 중요한 이점을 제공합니다.
### DeepSeekMath-RL의 구현과 평가

DeepSeekMath-RL은 DeepSeekMath-Instruct 7B를 기반으로 강화학습을 적용하여 개발되었습니다. 학습 데이터는 SFT 데이터에서 GSM8K와 MATH 관련 체인오브소트 형식의 질문들로 구성되었으며, 약 14.4만 개의 질문을 포함합니다. 연구진은 강화학습 단계에서 데이터가 부족한 벤치마크들에 대한 RL의 영향을 조사하기 위해 다른 SFT 질문들은 제외했습니다.

### 보상 모델 구축과 학습 설정

보상 모델은 DeepSeekMath-Base 7B를 기반으로 구축되었으며, 2e-5의 학습률로 학습되었습니다. GRPO 구현에서는 정책 모델의 학습률을 1e-6으로 설정했으며, KL 계수는 0.04로 설정했습니다. 각 질문에 대해 64개의 출력을 샘플링했으며, 최대 길이는 1,024로 제한했습니다. 학습 배치 크기는 1,024로 설정되었고, 각 탐색 단계 이후 정책 모델은 단 한 번의 업데이트만 수행했습니다.

### 성능 평가 방법론

DeepSeekMath-RL 7B는 DeepSeekMath-Instruct 7B와 동일한 벤치마크에서 평가되었습니다. GSM8K와 MATH에서의 체인오브소트 추론은 도메인 내 과제로 간주되었으며, 다른 모든 벤치마크는 도메인 외 과제로 분류되었습니다. 평가 결과, DeepSeekMath-RL 7B는 체인오브소트 추론을 사용하여 GSM8K에서 88.2%, MATH에서 51.7%의 정확도를 달성했습니다. 이는 7B에서 70B 범위의 모든 오픈소스 모델들과 대부분의 비공개 모델들의 성능을 능가하는 결과입니다.

### 도메인 일반화 능력

특히 주목할 만한 점은 DeepSeekMath-RL 7B가 GSM8K와 MATH의 체인오브소트 형식 데이터만으로 학습되었음에도 불구하고, DeepSeekMath-Instruct 7B보다 모든 평가 지표에서 더 우수한 성능을 보여주었다는 것입니다. 이는 강화학습이 모델의 일반적인 수학적 추론 능력을 효과적으로 향상시켰음을 시사합니다.

## DeepSeekMath: 사전학습과 강화학습의 주요 발견

### 사전학습 단계의 교훈

#### 코드 학습이 수학적 추론에 미치는 영향

코드 학습이 추론 능력을 향상시킨다는 가설은 오랫동안 검증되지 않은 채로 남아있었습니다. DeepSeekMath 연구진은 이 가설을 수학적 영역에서 검증하기 위해 체계적인 실험을 수행했습니다. 실험은 두 단계 학습과 단일 단계 학습 설정으로 나누어 진행되었습니다.

두 단계 학습에서는 DeepSeek-LLM 1.3B 모델을 사용하여 4,000억 개의 코드 토큰으로 학습한 후 1,500억 개의 수학 토큰으로 추가 학습을 진행했습니다. 대조군으로는 코드 대신 일반 텍스트 토큰을 사용한 실험도 함께 수행했습니다. 단일 단계 학습에서는 1,500억 개의 수학 토큰만을 사용한 경우와, 4,000억 개의 코드 토큰과 1,500억 개의 수학 토큰을 혼합하여 학습한 경우를 비교했습니다.

실험 결과는 코드 학습이 도구 사용 여부와 관계없이 수학적 추론 능력을 향상시킨다는 것을 보여주었습니다. 특히 두 단계 학습에서 코드 학습만으로도 GSM8K와 MATH 문제를 Python을 사용하여 해결하는 능력이 크게 향상되었으며, 이후 수학 학습을 통해 더욱 발전했습니다. 

단일 단계 학습에서 코드와 수학 토큰을 혼합하여 학습한 경우, 두 단계 학습에서 발생하는 재앙적 망각 문제를 효과적으로 완화하면서도 코딩과 프로그램 기반 수학적 추론 능력을 시너지 효과를 통해 향상시켰습니다.

#### arXiv 논문의 효과성 분석

연구진은 arXiv 논문이 수학적 추론 능력 향상에 미치는 영향을 분석했습니다. 이를 위해 DeepSeek-LLM 1.3B와 DeepSeek-Coder-Base-v1.5 7B 두 모델을 대상으로, 서로 다른 처리 과정을 거친 arXiv 코퍼스를 사용하여 실험을 진행했습니다.

1. MathPile: 89억 토큰 규모의 코퍼스로, 85% 이상이 과학 논문으로 구성되어 있습니다.
2. ArXiv-RedPajama: 서문, 주석, 매크로, 참고문헌이 제거된 280억 토큰 규모의 코퍼스입니다.

실험 결과, arXiv 논문만으로 학습했을 때 수학적 추론 능력이 크게 향상되지 않거나 오히려 저하되는 현상이 관찰되었습니다. 이는 GSM8K, MATH와 같은 정량적 추론, MMLU-STEM과 같은 객관식 문제, miniF2F와 같은 형식 수학 등 다양한 벤치마크에서 일관되게 나타났습니다.

![실험 결과](https://ar5iv.org//html/2402.03300/assets/x3.png)

위 그래프는 DeepSeekMath-Instruct 1.3B 모델의 다양한 학습 방법에 따른 성능을 보여줍니다. RFT, Online RFT, GRPO+OS, GRPO+PS 등 여러 평가 지표를 통해 모델의 성능과 학습 방법의 영향을 분석할 수 있습니다.
### 강화학습의 주요 발견

#### 통합 패러다임을 통한 학습 방법 분석

DeepSeekMath 연구진은 SFT, RFT, DPO, PPO, GRPO 등 다양한 학습 방법을 분석하기 위한 통합 패러다임을 제시했습니다. 이 패러다임에서 파라미터 $\theta$에 대한 학습 방법의 그래디언트는 다음과 같이 표현됩니다.

$$ \nabla_{\theta}\mathcal{J}_{\mathcal{A}}(\theta)=\mathbb{E}_{[(q,o)\sim\mathcal{D}]}\left(\frac{1}{ \left\vert o \right\vert }\sum_{t=1}^{ \left\vert o \right\vert }GC_{\mathcal{A}}(q,o,t,\pi_{rf})\nabla_{\theta}\log\pi_{\theta}(o_{t} \vert q,o_{<t})\right) $$

여기서 세 가지 핵심 요소를 확인할 수 있습니다.
1. 데이터 소스 $\mathcal{D}$: 학습 데이터를 결정합니다.
2. 보상 함수 $\pi_{rf}$: 학습 보상 신호의 출처입니다.
3. 알고리즘 $\mathcal{A}$: 학습 데이터와 보상 신호를 처리하여 그래디언트 계수 $GC$를 결정합니다.

![반복적 강화학습 성능](https://ar5iv.org//html/2402.03300/assets/x4.png)

위 그래프는 DeepSeekMath-Instruct 7B 모델의 반복적 강화학습 접근 방식의 성능을 보여줍니다. 각 반복에서 성능이 향상되는 것을 확인할 수 있으며, 이는 반복적 강화학습 기법의 효과성을 입증합니다.

#### 데이터 소스의 영향 분석

데이터 소스는 온라인 샘플링과 오프라인 샘플링으로 구분됩니다. 온라인 샘플링은 실시간 학습 정책 모델의 탐색 결과를 사용하고, 오프라인 샘플링은 초기 SFT 모델의 샘플링 결과를 활용합니다. RFT와 DPO는 오프라인 방식을, Online RFT와 GRPO는 온라인 방식을 따릅니다.

실험 결과, Online RFT가 RFT보다 우수한 성능을 보여주었습니다. 특히 학습 초기에는 비슷한 성능을 보이다가 후반부에서 절대적인 우위를 보였습니다. 이는 초기에는 액터와 SFT 모델이 유사하여 샘플링된 데이터의 차이가 미미하지만, 후반부에서는 액터에서 샘플링된 데이터가 더 큰 차이를 보이며 실시간 데이터 샘플링의 장점이 부각되기 때문입니다.
### 그래디언트 계수의 영향 분석

알고리즘은 보상 신호를 그래디언트 계수로 처리하여 모델 파라미터를 업데이트합니다. 연구진은 보상 함수를 'Rule'과 'Model' 두 가지로 구분했습니다. Rule은 응답의 정확성을 기반으로 품질을 판단하며, Model은 보상 모델을 학습시켜 각 응답을 평가합니다. 보상 모델의 학습 데이터는 Rule 판단을 기반으로 구성됩니다.

GRPO와 Online RFT의 주요 차이점은 그래디언트 계수의 조정 방식에 있습니다. GRPO는 보상 모델이 제공하는 보상값을 기반으로 그래디언트 계수를 조정하여 응답의 강화와 페널티 강도를 차별화합니다. 반면 Online RFT는 이러한 기능이 없어 잘못된 응답에 대한 페널티를 부과하지 않고 정답인 응답을 동일한 강도로 강화합니다.

![Maj@K와 Pass@K 성능](https://ar5iv.org//html/2402.03300/assets/x5.png)

위 그래프는 GSM8K와 MATH 데이터셋에서 SFT와 RL DeepSeekMath 7B의 Maj@K와 Pass@K 성능을 보여줍니다. 강화학습이 Maj@K는 향상시키지만 Pass@K는 향상시키지 않는다는 흥미로운 결과를 확인할 수 있습니다.

### 강화학습의 효과 분석

연구진은 지도 학습 데이터의 일부만을 사용한 강화학습을 통해 지도 학습 모델의 성능을 크게 향상시켰습니다. 강화학습의 효과를 더 깊이 이해하기 위해 Instruct와 RL 모델의 Pass@K와 Maj@K 정확도를 평가했습니다. 실험 결과는 강화학습이 Maj@K 성능은 향상시키지만 Pass@K는 향상시키지 않는다는 것을 보여줍니다. 이는 강화학습이 TopK 응답 중 정답의 비중을 높이는 방식으로 전반적인 성능을 향상시키며, 기본적인 능력의 향상보다는 출력 분포의 견고성을 개선하는 것으로 해석됩니다.

### 효과적인 강화학습을 위한 방향성

연구진은 수학적 추론 과제에서 강화학습이 효과적으로 작동함을 입증했으며, 다양한 학습 방법을 이해하기 위한 통합 패러다임을 제시했습니다. 이 패러다임 내에서 모든 방법은 직접적이거나 단순화된 강화학습 기법으로 개념화됩니다. 데이터 소스, 알고리즘, 보상 함수라는 세 가지 핵심 요소를 중심으로 향후 연구 방향을 제시했습니다.
### 데이터 소스의 발전 방향

데이터 소스는 모든 학습 방법의 기초가 되는 원자재입니다. 강화학습의 맥락에서 데이터 소스는 정책 모델에서 샘플링된 출력이 있는 레이블이 없는 질문들을 의미합니다. 현재 연구에서는 지도 학습 단계의 질문들만 사용하고 단순한 핵심 샘플링을 통해 출력을 생성했습니다. 이러한 접근 방식이 RL 파이프라인이 Maj@K 성능만 향상시키는 잠재적 원인일 수 있습니다.

향후 연구에서는 분포를 벗어난 질문 프롬프트에 대한 RL 파이프라인을 탐구하고, 트리 검색 방법을 기반으로 하는 고급 샘플링 전략을 도입할 필요가 있습니다. 또한 효율적인 추론 기술은 정책 모델의 탐색 효율성을 결정하는 데 매우 중요한 역할을 합니다.

### 알고리즘의 혁신 방향

현재의 알고리즘들은 보상 함수의 신호를 전적으로 신뢰하여 특정 토큰의 조건부 확률을 증가시키거나 감소시킵니다. 그러나 극도로 복잡한 과제에서는 보상 신호가 항상 신뢰할 수 있다고 보장할 수 없습니다. 예를 들어, 숙련된 주석자들이 신중하게 주석을 단 PRM800K 데이터셋조차도 약 20%의 부정확한 주석을 포함하고 있습니다.

이러한 문제를 해결하기 위해 노이즈가 있는 보상 신호에도 견고한 강화학습 알고리즘을 개발해야 합니다. 이러한 WEAK-TO-STRONG 정렬 방법은 학습 알고리즘에 근본적인 변화를 가져올 것으로 기대됩니다.

### 보상 함수의 발전 방향

보상 함수는 학습 신호의 원천이며, 강화학습에서는 일반적으로 신경망 보상 모델을 사용합니다. 보상 모델의 발전을 위해서는 세 가지 중요한 방향이 있습니다.

첫째, 보상 모델의 일반화 능력을 향상시켜야 합니다. 보상 모델은 분포를 벗어난 질문과 고급 디코딩 출력을 효과적으로 처리할 수 있어야 합니다. 그렇지 않으면 강화학습이 LLM의 분포를 안정화시키는 데 그칠 뿐, 근본적인 능력을 향상시키지 못할 수 있습니다.

둘째, 보상 모델의 불확실성을 반영하는 방법을 개발해야 합니다. 이러한 불확실성은 약한 보상 모델과 weak-to-strong 학습 알고리즘 사이의 연결 다리 역할을 할 수 있습니다.

셋째, 추론 과정에 대해 세밀한 학습 신호를 제공할 수 있는 고품질 프로세스 보상 모델을 효율적으로 구축하는 방법을 연구해야 합니다. 이는 수학적 추론의 단계별 평가와 개선을 가능하게 할 것입니다.

## DeepSeekMath: 결론과 한계점, 그리고 미래 연구 방향

### 주요 성과와 기여

DeepSeekMath는 경쟁 수준의 MATH 벤치마크에서 모든 오픈소스 모델들을 능가하는 성능을 달성하며, 비공개 모델들의 성능에 근접하는 결과를 보여주었습니다. DeepSeek-Coder-v1.5 7B를 초기 모델로 사용하여 5,000억 토큰에 대한 지속적인 학습을 수행했으며, 이 중 1,200억 토큰은 Common Crawl에서 추출한 수학 관련 데이터로 구성되었습니다.

광범위한 실험 연구를 통해 웹 페이지가 고품질 수학 데이터의 잠재력을 가지고 있음을 입증했으며, 예상과 달리 arXiv 데이터는 기대했던 만큼의 효과를 보여주지 않았습니다. 또한 Group Relative Policy Optimization(GRPO)이라는 Proximal Policy Optimization(PPO)의 변형을 도입하여 메모리 사용량을 줄이면서도 수학적 추론 능력을 크게 향상시킬 수 있었습니다.

### 한계점과 도전 과제

DeepSeekMath가 정량적 추론 벤치마크에서 인상적인 성과를 보여주었지만, 기하학과 정리 증명 분야에서는 비공개 모델들에 비해 상대적으로 약한 성능을 보였습니다. 특히 삼각형과 타원 관련 문제들을 효과적으로 처리하지 못하는 것으로 나타났는데, 이는 사전학습과 미세조정 과정에서의 데이터 선택 편향을 시사합니다.

모델 규모의 제약으로 인해 GPT-4와 같은 대형 모델들에 비해 퓨 샷 능력에서도 한계를 보였습니다. GPT-4는 퓨 샷 입력을 통해 성능이 향상되는 반면, DeepSeekMath는 제로샷과 퓨 샷 평가에서 유사한 수준의 성능을 보여주었습니다.

### 미래 연구 방향

연구진은 더 효과적인 데이터 선별 파이프라인을 구축하여 고품질의 사전학습 코퍼스를 확장하는 것을 향후 연구의 주요 방향으로 제시했습니다. 또한 앞서 논의된 강화학습의 잠재적 방향성을 탐구하여 대규모 언어 모델의 효과적인 강화학습 방법을 개발하는 데 초점을 맞출 계획입니다.

- - -
### References
* [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](http://arxiv.org/pdf/2402.03300v3)