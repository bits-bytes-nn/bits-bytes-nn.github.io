---
layout: post
title: "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"
date: 2024-05-07 15:56:43
author: "DeepSeek-AI"
categories: "Language-Models"
tags: ["Multi-Head-Latent-Attention", "DeepSeekMoE", "Economical-Training", "Efficient-Inference", "Long-Context-Adaptation"]
cover: /assets/images/language-models.webp
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
대규모 언어 모델 분야는 성능, 경제성, 효율성이라는 세 가지 핵심 과제에 직면해 있었습니다. 기존 모델들은 높은 성능을 달성하기 위해 막대한 계산 자원과 비용을 필요로 했으며, 특히 추론 과정에서 키-밸류 캐시로 인한 메모리 부담이 실용적 활용을 제한했습니다. DeepSeek-V2 연구는 이러한 한계를 극복하고 실용적이면서도 강력한 성능을 제공하는 새로운 모델 아키텍처의 필요성에서 시작되었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
DeepSeek-V2는 두 가지 혁신적인 아키텍처를 도입했습니다. 첫째, 다중 헤드 잠재 어텐션(MLA)은 저차원 키-밸류 결합 압축을 통해 KV 캐시를 93.3% 감소시키면서도 성능 저하를 최소화했습니다. 둘째, DeepSeekMoE는 세밀한 전문가 분할과 공유 전문가 격리를 통해 모델의 효율성을 극대화했습니다. 이러한 혁신으로 236B 개의 총 파라미터 중 각 토큰 처리시 21B 개만을 활성화하면서도 최상위 수준의 성능을 달성했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
모델은 60개의 트랜스포머 레이어와 5120의 은닉 차원을 가지도록 구현되었으며, MLA에서는 128개의 어텐션 헤드를 사용했습니다. DeepSeekMoE는 각 레이어마다 2개의 공유 전문가와 160개의 라우팅된 전문가로 구성되었으며, 토큰당 6개의 전문가가 활성화되도록 설계되었습니다. 8.1T 토큰의 고품질 말뭉치로 사전 학습을 진행했으며, 1.5M개의 대화 세션을 통한 지도 학습 미세조정과 그룹 상대 정책 최적화(GRPO)를 통한 강화학습을 수행했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
DeepSeek-V2는 대규모 언어 모델의 새로운 패러다임을 제시했습니다. 기존 DeepSeek 67B와 비교해 학습 비용을 42.5% 절감하고 최대 생성 처리량을 5.76배 향상시키면서도, 영어와 중국어 벤치마크에서 최상위 성능을 달성했습니다. 이는 효율성과 성능이 반드시 상충관계에 있지 않다는 것을 보여주며, 향후 대규모 언어 모델의 실용적 배포와 활용 가능성을 크게 확장했습니다. 또한 MLA와 DeepSeekMoE의 성공적인 구현은 모델 아키텍처 설계에 있어 새로운 방향을 제시했습니다.
- - -
## DeepSeek-V2: 강력하고 경제적이며 효율적인 혼합 전문가 언어 모델

DeepSeek-AI 연구팀이 개발한 DeepSeek-V2는 최신 대규모 언어 모델 분야에서 주목할 만한 혁신을 보여주는 연구입니다. 이 모델은 기존 언어 모델들이 직면한 세 가지 핵심 과제인 성능, 경제성, 그리고 효율성을 동시에 해결하고자 하는 야심찬 시도를 보여줍니다.

DeepSeek-V2는 혼합 전문가(Mixture-of-Experts, MoE) 아키텍처를 기반으로 하여, 대규모 언어 모델의 새로운 지평을 열고 있습니다. 이 모델은 단순히 기존 모델의 개선을 넘어서, 언어 모델의 구조적 효율성과 경제성을 근본적으로 재고하는 접근 방식을 취하고 있습니다. 특히 파운데이션 모델의 핵심 요소들을 재설계하고 최적화하여, 계산 자원의 효율적 활용과 높은 성능을 동시에 달성하고자 했습니다.

이 연구는 대규모 언어 모델 분야에서 중요한 전환점이 될 수 있는 잠재력을 보여주고 있습니다. 특히 모델의 규모와 성능 사이의 균형을 맞추면서도, 실제 응용 환경에서의 실용성을 고려한 혁신적인 접근 방식을 제시하고 있습니다. 이는 학술적 연구를 넘어 실제 산업 현장에서의 활용 가능성까지 고려한 포괄적인 연구라고 할 수 있습니다.

## DeepSeek-V2: 혁신적인 아키텍처를 통한 효율적인 대규모 언어 모델

![DeepSeek-V2 성능 비교](https://ar5iv.org//html/2405.04434/assets/x1.png)

DeepSeek-V2는 혼합 전문가(Mixture-of-Experts, MoE) 아키텍처를 기반으로 한 대규모 언어 모델로, 총 236B 개의 파라미터를 보유하고 있으며 각 토큰 처리 시 21B 개의 파라미터가 활성화됩니다. 이 모델은 128K 토큰의 컨텍스트 길이를 지원하며, 다중 헤드 잠재 어텐션(Multi-head Latent Attention, MLA)과 DeepSeekMoE라는 두 가지 혁신적인 아키텍처를 도입했습니다.

![DeepSeek-V2 아키텍처](https://ar5iv.org//html/2405.04434/assets/x3.png)

MLA는 키-밸류(Key-Value, KV) 캐시를 잠재 벡터로 크게 압축함으로써 효율적인 추론을 가능하게 합니다. 기존의 다중 헤드 어텐션(MHA)이 가진 KV 캐시 문제를 해결하기 위해 그룹 쿼리 어텐션(GQA)이나 다중 쿼리 어텐션(MQA) 등 다양한 접근 방식이 시도되었으나, 이러한 방법들은 성능을 희생하면서 KV 캐시를 줄이는 한계가 있었습니다. MLA는 저차원 키-밸류 결합 압축을 통해 이러한 한계를 극복하고, 우수한 성능을 유지하면서도 KV 캐시를 대폭 감소시켰습니다.

DeepSeekMoE는 피드포워드 네트워크(FFN)에 적용된 혁신적인 구조로, 세밀한 전문가 분할과 공유 전문가 격리를 통해 전문가 특화를 극대화합니다. 이는 GShard와 같은 기존의 MoE 아키텍처와 비교했을 때 큰 장점을 보여주며, 경제적인 비용으로 강력한 모델을 학습할 수 있게 합니다. 또한 학습 과정에서 전문가 병렬화를 활용하면서 통신 오버헤드를 제어하고 부하 균형을 보장하기 위한 보조 메커니즘도 도입했습니다.

DeepSeek-V2는 이러한 혁신적인 기술들을 통해 DeepSeek 67B와 비교했을 때 학습 비용을 42.5% 절감하고, KV 캐시를 93.3% 감소시켰으며, 최대 생성 처리량을 5.76배 향상시켰습니다. 또한 8.1T 토큰으로 구성된 고품질의 다중 소스 말뭉치를 사용하여 사전 학습을 진행했으며, 이는 이전 DeepSeek 67B에서 사용된 말뭉치와 비교하여 데이터의 양과 품질이 크게 향상되었습니다.
### DeepSeek-V2의 모델 학습과 정렬

DeepSeek-V2는 사전 학습 단계에서 전체 말뭉치를 활용하여 기본 모델을 학습한 후, 1.5M개의 대화 세션을 수집하여 지도 학습 미세조정(Supervised Fine-Tuning, SFT)을 수행했습니다. 이 대화 세션들은 수학, 코드, 작문, 추론, 안전성 등 다양한 영역을 포괄하며, 이를 통해 DeepSeek-V2 Chat (SFT) 모델이 개발되었습니다. 이후 DeepSeekMath의 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)를 적용하여 인간의 선호도에 더욱 부합하는 DeepSeek-V2 Chat (RL) 모델을 완성했습니다.

영어와 중국어 벤치마크에서 진행된 광범위한 평가 결과, DeepSeek-V2는 활성화되는 파라미터가 21B개에 불과함에도 오픈소스 모델들 중 최상위 성능을 달성했습니다. 특히 MMLU 평가에서는 적은 수의 활성화 파라미터로도 최고 수준의 성능을 보여주었습니다. DeepSeek-V2 Chat (RL)은 AlpacaEval 2.0에서 38.9의 길이 제어 승률을, MT-Bench에서 8.97의 종합 점수를, AlignBench에서 7.91의 종합 점수를 기록했습니다. 이는 오픈소스 챗 모델들 중 최상위권의 성능을 입증하는 결과입니다.

특히 중국어 평가에서 DeepSeek-V2 Chat (RL)은 모든 오픈소스 모델들을 능가했을 뿐만 아니라, 대부분의 비공개 모델들보다도 우수한 성능을 보여주었습니다. 이러한 성과는 DeepSeek-V2의 다국어 처리 능력과 범용성을 입증하는 중요한 지표입니다.

연구팀은 MLA와 DeepSeekMoE 기술의 추가 연구와 발전을 촉진하기 위해 DeepSeek-V2-Lite도 함께 공개했습니다. 이 모델은 총 15.7B개의 파라미터를 보유하고 있으며, 각 토큰 처리 시 2.4B개의 파라미터가 활성화되는 더 작은 규모의 모델입니다. DeepSeek-V2-Lite는 MLA와 DeepSeekMoE 아키텍처의 효과를 검증하고, 오픈소스 커뮤니티에 기여하기 위해 개발되었습니다.
### DeepSeek-V2의 아키텍처 혁신

DeepSeek-V2는 트랜스포머 프레임워크 내에서 어텐션 모듈과 피드포워드 네트워크를 최적화하여 효율적인 추론과 경제적인 학습을 실현했습니다. 이 모델의 핵심 아키텍처는 다중 헤드 잠재 어텐션(MLA)과 DeepSeekMoE를 통해 구현되었으며, 이는 기존 언어 모델들이 직면한 주요 기술적 한계를 극복하는데 중요한 역할을 했습니다.

MLA는 기존 다중 헤드 어텐션(MHA)의 한계를 극복하기 위해 개발되었습니다. MHA에서 KV 캐시는 모델의 추론 효율성을 저해하는 주요 요인이었습니다. 이를 해결하기 위해 그룹 쿼리 어텐션(GQA)이나 다중 쿼리 어텐션(MQA)과 같은 방법들이 제안되었으나, 이러한 접근법들은 KV 캐시를 줄이는 과정에서 모델의 성능을 희생해야 했습니다. MLA는 저차원 키-밸류 결합 압축 기법을 도입하여 성능 저하 없이 KV 캐시를 효과적으로 압축하는데 성공했습니다.

DeepSeekMoE는 피드포워드 네트워크를 최적화하기 위해 개발된 혁신적인 구조입니다. 이 아키텍처는 세밀한 전문가 분할과 공유 전문가 격리라는 두 가지 핵심 메커니즘을 통해 전문가 특화를 극대화합니다. 세밀한 전문가 분할은 각 전문가가 특정 도메인에 더욱 특화될 수 있도록 하며, 공유 전문가 격리는 여러 층에서 공통적으로 필요한 지식을 효율적으로 처리할 수 있게 합니다.

이러한 아키텍처적 혁신을 통해 DeepSeek-V2는 기존 DeepSeek 67B 모델과 비교하여 놀라운 효율성 향상을 달성했습니다. 학습 비용이 42.5% 절감되었고, KV 캐시는 93.3%나 감소했으며, 최대 생성 처리량은 5.76배 향상되었습니다. 이는 대규모 언어 모델의 실용적 활용 가능성을 크게 높이는 성과입니다.

DeepSeek-V2의 아키텍처는 단순히 기존 모델의 개선을 넘어서, 대규모 언어 모델의 구조적 효율성과 경제성을 근본적으로 재고하는 혁신적인 접근 방식을 보여줍니다. 특히 MLA와 DeepSeekMoE의 결합은 모델의 성능을 유지하면서도 계산 자원의 효율적 활용을 가능하게 하는 새로운 패러다임을 제시했습니다.
### DeepSeek-V2의 아키텍처 세부 구성

DeepSeek-V2의 아키텍처는 트랜스포머 기반의 혁신적인 설계를 통해 모델의 효율성과 성능을 동시에 향상시켰습니다. 이 모델은 기존 트랜스포머 구조의 핵심 요소들을 재설계하여, 특히 어텐션 메커니즘과 피드포워드 네트워크에서 중요한 개선을 이루어냈습니다.

다중 헤드 잠재 어텐션(MLA)은 기존 어텐션 메커니즘의 한계를 극복하기 위해 개발된 혁신적인 구조입니다. 표준 다중 헤드 어텐션(MHA)에서는 각 어텐션 헤드마다 별도의 키-밸류 쌍을 저장해야 했기 때문에 메모리 사용량이 크게 증가하는 문제가 있었습니다. MLA는 저차원 키-밸류 결합 압축을 통해 이 문제를 해결했습니다. 이 압축 기법은 키와 밸류 벡터를 공유된 저차원 잠재 공간으로 투영함으로써, 메모리 사용량을 크게 줄이면서도 정보의 손실을 최소화합니다.

DeepSeekMoE는 피드포워드 네트워크를 최적화하기 위한 새로운 접근 방식을 제시합니다. 이 구조는 전문가 네트워크들을 더 작은 단위로 분할하고, 이들 간의 효율적인 협업을 가능하게 합니다. 각 전문가는 특정 유형의 입력에 대해 특화된 처리를 수행하며, 공유 전문가들은 여러 층에서 공통적으로 필요한 기본적인 변환을 담당합니다.

이러한 혁신적인 아키텍처 설계를 통해 DeepSeek-V2는 236B개의 총 파라미터 중 각 토큰 처리시 21B개만을 활성화하면서도 우수한 성능을 달성할 수 있었습니다. 특히 128K 토큰이라는 긴 컨텍스트 길이를 지원하면서도, 효율적인 메모리 사용과 빠른 처리 속도를 유지할 수 있게 되었습니다.

DeepSeek-V2의 아키텍처는 단순히 기존 모델의 개선을 넘어서, 대규모 언어 모델의 설계 방식에 대한 새로운 패러다임을 제시했습니다. 특히 MLA와 DeepSeekMoE의 결합은 모델의 성능을 유지하면서도 계산 자원의 효율적 활용을 가능하게 하는 혁신적인 접근 방식을 보여줍니다.
### DeepSeek-V2의 아키텍처 기술적 혁신

DeepSeek-V2는 트랜스포머 아키텍처의 핵심 구성 요소들을 근본적으로 재설계하여 대규모 언어 모델의 새로운 지평을 열었습니다. 특히 어텐션 메커니즘과 피드포워드 네트워크의 혁신적인 설계를 통해 모델의 효율성과 성능을 획기적으로 개선했습니다.

### 다중 헤드 잠재 어텐션의 수학적 기반

다중 헤드 잠재 어텐션(MLA)은 저차원 키-밸류 결합 압축을 통해 메모리 효율성을 극대화합니다. 이는 다음과 같은 수학적 공식으로 표현됩니다.

\\[ \text{MLA}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O \\]

여기서 각 헤드는 다음과 같이 계산됩니다.

\\[ \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\]

MLA는 키와 밸류 벡터를 공유된 저차원 잠재 공간으로 투영하여 압축합니다.

\\[ K = \mathbf{M}W_k, V = \mathbf{M}W_v \\]

여기서 \\(\mathbf{M}\\)은 공유된 잠재 표현이며, \\(W_k\\)와 \\(W_v\\)는 각각 키와 밸류에 대한 투영 행렬입니다.

### DeepSeekMoE의 전문가 라우팅 메커니즘

DeepSeekMoE는 세밀한 전문가 분할과 효율적인 라우팅을 통해 모델의 성능을 최적화합니다. 각 토큰 \\(x\\)에 대한 전문가 선택 확률은 다음과 같이 계산됩니다.

\\[ p_i(x) = \frac{e^{h(x)\_i}}{\sum_{j=1}^N e^{h(x)_j}} \\]

여기서 \\(h(x)\\)는 라우터 네트워크의 출력이며, \\(N\\)은 전체 전문가의 수입니다. 부하 균형을 위한 보조 손실 함수는 다음과 같이 정의됩니다.

\\[ \mathcal{L}\_{\text{balance}} = \alpha N \sum_{i=1}^N f_i P_i \\]

여기서 \\(f_i\\)는 전문가 \\(i\\)에 할당된 토큰의 비율이며, \\(P_i\\)는 해당 전문가에 대한 라우터의 할당 확률입니다.

이러한 수학적 기반을 통해 DeepSeek-V2는 계산 효율성과 모델 성능 사이의 최적의 균형점을 찾아냈으며, 이는 대규모 언어 모델의 실용적 활용 가능성을 크게 높이는 성과입니다.
### DeepSeek-V2의 아키텍처 구현과 최적화

DeepSeek-V2의 아키텍처는 트랜스포머 모델의 핵심 구성 요소들을 재설계하여 추론 효율성과 학습 경제성을 동시에 달성했습니다. 특히 MLA와 DeepSeekMoE의 구현에서 주목할 만한 기술적 혁신이 이루어졌습니다.

### 저차원 키-밸류 결합 압축의 구현

MLA의 핵심 기술인 저차원 키-밸류 결합 압축은 다음과 같은 수학적 구조를 가집니다.

\\[ \mathbf{Z} = \text{Compress}(\mathbf{X}, \mathbf{W}_c) \\]
\\[ \mathbf{K} = \mathbf{Z}\mathbf{W}_k, \mathbf{V} = \mathbf{Z}\mathbf{W}_v \\]

여기서 \\(\mathbf{X}\\)는 입력 시퀀스, \\(\mathbf{W}_c\\)는 압축 행렬, \\(\mathbf{Z}\\)는 공유된 잠재 표현입니다. 이러한 압축 구조를 통해 KV 캐시의 크기를 93.3% 감소시키면서도 정보의 손실을 최소화할 수 있었습니다.

### 디커플드 로터리 포지션 임베딩

MLA는 위치 정보를 효율적으로 처리하기 위해 디커플드 로터리 포지션 임베딩을 도입했습니다.

\\[ \text{RoPE}(x_m, \theta) = x_m[\cos(m\theta); \sin(m\theta)] \\]

여기서 \\(x_m\\)은 \\(m\\)번째 위치의 임베딩이며, \\(\theta\\)는 회전 각도입니다. 이 방식은 위치 정보를 KV 캐시와 분리하여 처리함으로써 메모리 효율성을 더욱 향상시켰습니다.

### DeepSeekMoE의 전문가 분할 구조

DeepSeekMoE는 피드포워드 네트워크를 다음과 같이 구성합니다.

\\[ \text{FFN}(x) = \sum_{i=1}^N g_i(x)\text{Expert}_i(x) \\]

여기서 \\(g_i(x)\\)는 각 전문가에 대한 게이트 값이며, \\(\text{Expert}_i(x)\\)는 개별 전문가의 변환 함수입니다. 세밀한 전문가 분할을 통해 각 전문가는 특정 도메인에 더욱 특화된 처리를 수행할 수 있게 되었습니다.

이러한 구조적 혁신을 통해 DeepSeek-V2는 236B 개의 총 파라미터 중 각 토큰 처리시 21B 개만을 활성화하면서도 최상위 수준의 성능을 달성할 수 있었습니다. 특히 128K 토큰의 긴 컨텍스트를 처리하면서도 높은 효율성을 유지할 수 있게 되었습니다.
### DeepSeek-V2의 아키텍처 성능과 효율성 분석

DeepSeek-V2의 아키텍처는 대규모 언어 모델의 핵심 과제인 추론 효율성과 학습 경제성을 동시에 해결하는 혁신적인 접근 방식을 보여줍니다. 특히 MLA와 DeepSeekMoE의 결합을 통해 달성한 기술적 성과는 대규모 언어 모델의 실용적 활용 가능성을 크게 높였습니다.

### 추론 효율성 최적화

MLA의 저차원 키-밸류 결합 압축은 기존 어텐션 메커니즘의 한계를 극복하는 획기적인 해결책을 제시했습니다. 이 기술은 다음과 같은 수학적 최적화를 통해 구현됩니다.

\\[ \text{Attention}(\mathbf{Q}, \mathbf{Z}\mathbf{W}_k, \mathbf{Z}\mathbf{W}_v) = \text{softmax}(\frac{\mathbf{Q}(\mathbf{Z}\mathbf{W}_k)^T}{\sqrt{d_k}})\mathbf{Z}\mathbf{W}_v \\]

여기서 \\(\mathbf{Z}\\)는 공유된 잠재 표현이며, 이를 통해 키와 밸류의 차원을 효과적으로 압축합니다. 이러한 압축은 메모리 사용량을 크게 줄이면서도 어텐션의 표현력을 유지합니다.

### 전문가 특화 최적화

DeepSeekMoE의 전문가 특화는 다음과 같은 목적 함수를 최적화합니다.

\\[ \mathcal{L}\_{\text{total}} = \mathcal{L}\_{\text{task}} + \lambda\mathcal{L}\_{\text{balance}} + \gamma\mathcal{L}_{\text{diversity}} \\]

여기서 \\(\mathcal{L}\_{\text{task}}\\)는 주요 학습 목적 함수, \\(\mathcal{L}\_{\text{balance}}\\)는 부하 균형을 위한 보조 손실, \\(\mathcal{L}_{\text{diversity}}\\)는 전문가 다양성을 촉진하는 정규화 항입니다. 이러한 다중 목적 최적화를 통해 전문가들의 효율적인 특화와 협업이 가능해졌습니다.

### 시스템 통합 최적화

DeepSeek-V2는 MLA와 DeepSeekMoE의 통합을 통해 다음과 같은 계층적 처리 구조를 구현합니다.

\\[ \mathbf{h}_{t+1} = \text{MLA}(\text{DeepSeekMoE}(\mathbf{h}_t)) + \mathbf{h}_t \\]

이러한 구조는 각 계층에서 효율적인 정보 처리와 특화된 변환을 가능하게 하며, 결과적으로 모델의 전반적인 성능을 향상시킵니다. 특히 128K 토큰의 긴 컨텍스트 처리에서도 높은 효율성을 유지할 수 있게 되었습니다.

### DeepSeek-V2의 트랜스포머 기반 아키텍처 혁신

DeepSeek-V2는 기본적으로 트랜스포머 아키텍처를 기반으로 하지만, 어텐션 모듈과 피드포워드 네트워크(FFN)에서 혁신적인 구조적 변화를 도입했습니다. 이 모델의 핵심 구성 요소는 다중 헤드 잠재 어텐션(Multi-head Latent Attention, MLA)과 DeepSeekMoE입니다.

![DeepSeek-V2 아키텍처](https://ar5iv.org//html/2405.04434/assets/x2.png)

MLA는 저차원 키-밸류 결합 압축 기술을 활용하여 추론 시 발생하는 키-밸류 캐시의 병목 현상을 해결합니다. 이는 기존 트랜스포머 모델의 주요 한계점이었던 메모리 효율성 문제를 획기적으로 개선한 것입니다. MLA의 수학적 기반은 다음과 같이 표현됩니다.

\\[ \text{MLA}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V \\]

여기서 \\(Q\\), \\(K\\), \\(V\\)는 각각 쿼리, 키, 밸류 행렬을 나타내며, \\(d_k\\)는 키의 차원을 의미합니다. 저차원 압축을 통해 키-밸류 쌍을 효율적으로 저장하고 처리할 수 있게 되었습니다.

DeepSeekMoE는 FFN 구조를 개선한 혁신적인 혼합 전문가 시스템입니다. 이 구조는 경제적인 비용으로 강력한 모델을 학습할 수 있게 해주며, 다음과 같은 수식으로 표현됩니다.

\\[ \text{FFN}(x) = \sum_{i=1}^N g_i(x)\text{Expert}_i(x) \\]

여기서 \\(g_i(x)\\)는 각 전문가에 대한 게이트 값을, \\(\text{Expert}_i(x)\\)는 개별 전문가의 변환 함수를 나타냅니다.

이러한 구조적 혁신을 통해 DeepSeek-V2는 기존 DeepSeek 67B 모델의 설정을 기반으로 하면서도, 레이어 정규화와 FFN의 활성화 함수 등 세부적인 구성 요소들을 최적화했습니다. 특히 MLA를 통한 추론 효율성 향상과 DeepSeekMoE를 통한 경제적인 학습이 가능해졌으며, 이는 대규모 언어 모델의 실용적 활용 가능성을 크게 높였습니다.
### 다중 헤드 잠재 어텐션의 기술적 구현

다중 헤드 잠재 어텐션(MLA)은 저차원 키-밸류 결합 압축을 통해 추론 시간의 메모리 효율성을 획기적으로 개선합니다. MLA의 핵심 아이디어는 키와 밸류 벡터를 공유된 저차원 잠재 공간으로 투영하는 것입니다. 이는 다음과 같은 수학적 구조로 표현됩니다.

\\[ \mathbf{Z} = \text{Compress}(\mathbf{X}, \mathbf{W}_c) \\]
\\[ \mathbf{K} = \mathbf{Z}\mathbf{W}_k, \mathbf{V} = \mathbf{Z}\mathbf{W}_v \\]

여기서 \\(\mathbf{X}\\)는 입력 시퀀스, \\(\mathbf{W}_c\\)는 압축 행렬, \\(\mathbf{Z}\\)는 공유된 잠재 표현입니다. 이러한 압축 구조를 통해 KV 캐시의 크기를 93.3% 감소시키면서도 정보의 손실을 최소화할 수 있었습니다.

### DeepSeekMoE의 전문가 라우팅 메커니즘

DeepSeekMoE는 세밀한 전문가 분할과 효율적인 라우팅을 통해 모델의 성능을 최적화합니다. 각 토큰 \\(x\\)에 대한 전문가 선택 확률은 다음과 같이 계산됩니다.

\\[ p_i(x) = \frac{e^{h(x)\_i}}{\sum_{j=1}^N e^{h(x)_j}} \\]

여기서 \\(h(x)\\)는 라우터 네트워크의 출력이며, \\(N\\)은 전체 전문가의 수입니다. 부하 균형을 위한 보조 손실 함수는 다음과 같이 정의됩니다.

\\[ \mathcal{L}\_{\text{balance}} = \alpha N \sum_{i=1}^N f_i P_i \\]

여기서 \\(f_i\\)는 전문가 \\(i\\)에 할당된 토큰의 비율이며, \\(P_i\\)는 해당 전문가에 대한 라우터의 할당 확률입니다.

이러한 수학적 기반을 통해 DeepSeek-V2는 계산 효율성과 모델 성능 사이의 최적의 균형점을 찾아냈으며, 이는 대규모 언어 모델의 실용적 활용 가능성을 크게 높이는 성과입니다.

### 다중 헤드 잠재 어텐션의 혁신적 설계

기존의 트랜스포머 모델들은 다중 헤드 어텐션(Multi-Head Attention, MHA)을 주로 사용해왔으나, 생성 과정에서 무거운 키-밸류(Key-Value, KV) 캐시가 추론 효율성을 제한하는 병목 현상을 야기했습니다. 이러한 문제를 해결하기 위해 다중 쿼리 어텐션(Multi-Query Attention, MQA)과 그룹 쿼리 어텐션(Grouped-Query Attention, GQA)이 제안되었습니다. 이들은 더 작은 규모의 KV 캐시를 요구하지만, MHA에 비해 성능이 떨어진다는 한계가 있었습니다.

DeepSeek-V2는 이러한 한계를 극복하기 위해 다중 헤드 잠재 어텐션(Multi-head Latent Attention, MLA)이라는 혁신적인 어텐션 메커니즘을 도입했습니다. MLA는 저차원 키-밸류 결합 압축을 통해 MHA보다 우수한 성능을 달성하면서도 KV 캐시의 크기를 대폭 감소시켰습니다.

### 표준 다중 헤드 어텐션의 기본 원리

![어텐션 메커니즘 비교](https://ar5iv.org//html/2405.04434/assets/x4.png)

표준 MHA의 작동 원리를 이해하기 위해, 먼저 핵심 구성 요소들을 살펴보겠습니다. 임베딩 차원을 $d$, 어텐션 헤드의 수를 $n_h$, 헤드당 차원을 $d_h$라고 하고, $t$번째 토큰의 어텐션 입력을 $\mathbf{h}_t \in \mathbb{R}^d$라고 합니다.

표준 MHA는 먼저 세 개의 행렬 $W^Q, W^K, W^V \in \mathbb{R}^{d_hn_h \times d}$를 통해 $\mathbf{q}_t, \mathbf{k}_t, \mathbf{v}_t \in \mathbb{R}^{d_hn_h}$를 생성합니다.

$\mathbf{q}_t = W^Q\mathbf{h}_t$
$\mathbf{k}_t = W^K\mathbf{h}_t$
$\mathbf{v}_t = W^V\mathbf{h}_t$

이후 $\mathbf{q}_t, \mathbf{k}_t, \mathbf{v}_t$는 $n_h$개의 헤드로 분할되어 다중 헤드 어텐션 연산을 수행합니다.

$[\mathbf{q}\_{t,1}; \mathbf{q}\_{t,2}; ...; \mathbf{q}\_{t,n_h}] = \mathbf{q}_t$
$[\mathbf{k}\_{t,1}; \mathbf{k}\_{t,2}; ...; \mathbf{k}\_{t,n_h}] = \mathbf{k}_t$
$[\mathbf{v}\_{t,1}; \mathbf{v}\_{t,2}; ...; \mathbf{v}\_{t,n_h}] = \mathbf{v}_t$

각 헤드에서의 출력은 다음과 같이 계산됩니다.

$\mathbf{o}\_\{t,i} = \sum_{j=1}^t \text{Softmax}\_j(\frac{\mathbf{q}\_{t,i}^T\mathbf{k}\_{j,i}}{\sqrt{d_h}})\mathbf{v}_{j,i}$

최종적으로 모든 헤드의 출력을 결합하여 출력 투영 행렬 $W^O \in \mathbb{R}^{d \times d_hn_h}$를 통해 최종 출력을 생성합니다.

$\mathbf{u}\_t = W^O[\mathbf{o}\_{t,1}; \mathbf{o}\_{t,2}; ...; \mathbf{o}_{t,n_h}]$

추론 과정에서는 모든 키와 밸류를 캐시해야 하므로, MHA는 각 토큰마다 $2n_hd_hl$ 개의 요소를 캐시해야 합니다. 이러한 무거운 KV 캐시는 모델 배포 시 최대 배치 크기와 시퀀스 길이를 제한하는 큰 병목 현상이 됩니다.
### 저차원 키-밸류 결합 압축의 구현

MLA의 핵심은 키와 밸류를 위한 저차원 결합 압축을 통해 KV 캐시를 줄이는 것입니다. 이는 다음과 같은 수학적 구조로 구현됩니다.

$\mathbf{c}_t^{KV} = W^{DKV}\mathbf{h}_t$
$\mathbf{k}_t^C = W^{UK}\mathbf{c}_t^{KV}$
$\mathbf{v}_t^C = W^{UV}\mathbf{c}_t^{KV}$

여기서 $\mathbf{c}_t^{KV} \in \mathbb{R}^{d_c}$는 키와 밸류를 위한 압축된 잠재 벡터이며, $d_c(\ll d_hn_h)$는 KV 압축 차원을 나타냅니다. $W^{DKV} \in \mathbb{R}^{d_c \times d}$는 다운 프로젝션 행렬이고, $W^{UK}, W^{UV} \in \mathbb{R}^{d_hn_h \times d_c}$는 각각 키와 밸류를 위한 업 프로젝션 행렬입니다.

추론 과정에서 MLA는 $\mathbf{c}_t^{KV}$만을 캐시하면 되므로, KV 캐시는 $d_cl$ 개의 요소만을 필요로 합니다. 여기서 $l$은 레이어의 수를 나타냅니다. 또한 추론 시에는 $W^{UK}$가 $W^Q$에 흡수될 수 있고, $W^{UV}$가 $W^O$에 흡수될 수 있어서 어텐션을 위한 키와 밸류를 따로 계산할 필요가 없습니다.

학습 중의 활성화 메모리를 줄이기 위해, MLA는 쿼리에 대해서도 저차원 압축을 수행합니다.

$\mathbf{c}_t^Q = W^{DQ}\mathbf{h}_t$
$\mathbf{q}_t^C = W^{UQ}\mathbf{c}_t^Q$

여기서 $\mathbf{c}_t^Q \in \mathbb{R}^{d_c'}$는 쿼리를 위한 압축된 잠재 벡터이고, $d_c'(\ll d_hn_h)$는 쿼리 압축 차원입니다. $W^{DQ} \in \mathbb{R}^{d_c' \times d}$와 $W^{UQ} \in \mathbb{R}^{d_hn_h \times d_c'}$는 각각 쿼리를 위한 다운 프로젝션과 업 프로젝션 행렬입니다.

### 디커플드 로터리 포지션 임베딩

DeepSeek 67B를 따라 DeepSeek-V2는 로터리 포지션 임베딩(RoPE)을 사용하고자 했습니다. 그러나 RoPE는 저차원 KV 압축과 호환되지 않는 문제가 있었습니다. RoPE는 키와 쿼리 모두에 대해 위치 민감성을 가지기 때문에, 키 $\mathbf{k}_t^C$에 RoPE를 적용하면 식 10의 $W^{UK}$가 위치 민감한 RoPE 행렬과 결합됩니다. 이로 인해 $W^{UK}$를 $W^Q$에 흡수할 수 없게 되는데, 이는 현재 생성되는 토큰과 관련된 RoPE 행렬이 $W^Q$와 $W^{UK}$ 사이에 위치하게 되고 행렬 곱셈이 교환법칙을 따르지 않기 때문입니다.
### 디커플드 로터리 포지션 임베딩의 해결책

이러한 문제를 해결하기 위해, DeepSeek-V2는 추가적인 다중 헤드 쿼리 $\mathbf{q}_{t,i}^R \in \mathbb{R}^{d_h^R}$와 공유 키 $\mathbf{k}_t^R \in \mathbb{R}^{d_h^R}$를 사용하는 디커플드 RoPE 전략을 제안했습니다. 여기서 $d_h^R$는 디커플드 쿼리와 키의 헤드당 차원을 나타냅니다. 이러한 디커플드 RoPE 전략을 통해 MLA는 다음과 같은 연산을 수행합니다.

$[\mathbf{q}\_{t,1}^R; \mathbf{q}\_{t,2}^R; ...; \mathbf{q}\_{t,n_h}^R] = \mathbf{q}_t^R = \text{RoPE}(W^{QR}\mathbf{c}_t^Q)$
$\mathbf{k}_t^R = \text{RoPE}(W^{KR}\mathbf{h}_t)$

$\mathbf{q}\_{t,i} = [\mathbf{q}\_{t,i}^C; \mathbf{q}_{t,i}^R]$
$\mathbf{k}\_{t,i} = [\mathbf{k}\_{t,i}^C; \mathbf{k}_t^R]$

$\mathbf{o}\_{t,i} = \sum_{j=1}^t \text{Softmax}\_j(\frac{\mathbf{q}\_{t,i}^T\mathbf{k}\_{j,i}}{\sqrt{d_h + d_h^R}})\mathbf{v}_{j,i}^C$

$\mathbf{u}\_t = W^O[\mathbf{o}\_{t,1}; \mathbf{o\}\_{t,2}; ...; \mathbf{o}_{t,n_h}]$

추론 과정에서는 디커플드 키도 캐시해야 하므로, DeepSeek-V2는 각 토큰마다 총 $(d_c + d_h^R)l$ 개의 요소를 포함하는 KV 캐시가 필요합니다.

### 어텐션 메커니즘의 KV 캐시 비교

다양한 어텐션 메커니즘의 토큰당 KV 캐시를 비교해보면, MHA는 $2n_hd_hl$ 개, GQA는 $2n_gd_hl$ 개, MQA는 $2d_hl$ 개의 요소를 필요로 합니다. 반면 MLA는 $(d_c + d_h^R)l \approx \frac{9}{2}d_hl$ 개의 요소만을 필요로 하며, 이는 GQA에서 2.25개의 그룹만 사용하는 것과 동등한 수준입니다. 그럼에도 불구하고 MLA는 MHA보다 더 강력한 성능을 보여줍니다.

DeepSeek-V2에서는 $d_c$를 $4d_h$로, $d_h^R$를 $\frac{d_h}{2}$로 설정했습니다. 이러한 설정을 통해 KV 캐시의 크기를 크게 줄이면서도 모델의 성능을 향상시킬 수 있었습니다.

### DeepSeek-V2의 장치 제한 라우팅과 부하 균형

DeepSeek-V2는 MoE 관련 통신 비용을 제한하기 위해 장치 제한 라우팅 메커니즘을 도입했습니다. 전문가 병렬화가 적용될 때 라우팅된 전문가들은 여러 장치에 분산되어 배치됩니다. 각 토큰의 MoE 관련 통신 빈도는 해당 토큰의 대상 전문가들이 분포된 장치 수에 비례합니다. DeepSeekMoE의 세밀한 전문가 분할로 인해 활성화되는 전문가의 수가 많아질 수 있어, 전문가 병렬화를 적용할 경우 MoE 관련 통신 비용이 더욱 증가할 수 있습니다.

이러한 문제를 해결하기 위해 DeepSeek-V2는 기본적인 상위 K 전문가 선택 방식에 추가하여, 각 토큰의 대상 전문가들이 최대 \\(M\\)개의 장치에만 분산되도록 제한합니다. 구체적으로, 각 토큰에 대해 먼저 가장 높은 친화도 점수를 가진 전문가들이 있는 \\(M\\)개의 장치를 선택한 후, 이 \\(M\\)개 장치에 있는 전문가들 중에서 상위 K개를 선택합니다. 실제 구현에서는 \\(M \geq 3\\)일 때 장치 제한 라우팅이 제한 없는 상위 K 라우팅과 비슷한 성능을 달성할 수 있음을 확인했습니다.

자동으로 학습된 라우팅 전략의 부하 균형을 위해 세 가지 보조 손실 함수를 설계했습니다. 전문가 수준 부하 균형(\\(\mathcal{L}\_{\mathrm{ExpBal}}\\)), 장치 수준 부하 균형(\\(\mathcal{L}\_{\mathrm{DevBal}}\\)), 그리고 통신 균형(\\(\mathcal{L}\_{\mathrm{CommBal}}\\))을 위한 손실 함수들입니다.

전문가 수준 부하 균형 손실은 라우팅 붕괴를 방지하고 모든 전문가가 충분히 학습되고 활용되도록 하기 위해 도입되었습니다. 이는 다음과 같이 정의됩니다.

\\[ \mathcal{L}\_{\mathrm{ExpBal}} = \alpha_1\sum_{i=1}^{N_r}{f_i P_i} \\]
\\[ f_i = \frac{N_r}{K_rT}\sum_{t=1}^{T}{\mathbb{1}(\text{Token $t$ selects Expert $i$})} \\]
\\[ P_i = \frac{1}{T}\sum_{t=1}^{T}{s_{i,t}} \\]

여기서 \\(\alpha_1\\)은 전문가 수준 균형 인자이며, \\(\mathbb{1}(\cdot)\\)은 지시 함수, \\(T\\)는 시퀀스의 토큰 수를 나타냅니다.

### DeepSeek-V2의 데이터 처리와 학습 방법론

DeepSeek-V2는 기존 DeepSeek 67B의 데이터 처리 단계를 유지하면서도 데이터의 양과 품질을 크게 향상시켰습니다. 인터넷 데이터의 잠재력을 최대한 활용하기 위해 데이터 정제 프로세스를 최적화하여 이전에 실수로 삭제된 많은 데이터를 복구했으며, 중국어 인터넷의 데이터를 더 효과적으로 활용하기 위해 중국어 데이터의 비중을 늘렸습니다.

데이터 품질 향상을 위해 다양한 소스에서 수집한 고품질 데이터를 사전 학습 말뭉치에 추가했으며, 품질 기반 필터링 알고리즘을 개선했습니다. 이 개선된 알고리즘은 유용하지 않은 데이터는 제거하면서도 가치 있는 데이터는 최대한 보존하도록 설계되었습니다. 또한 특정 지역 문화에서 기인하는 데이터 편향을 완화하기 위해 논란의 여지가 있는 콘텐츠를 사전 학습 말뭉치에서 제외했습니다.

토크나이저는 DeepSeek 67B와 동일하게 바이트 레벨 바이트 페어 인코딩(BBPE) 알고리즘을 기반으로 하며, 어휘 크기는 100K입니다. 토큰화된 사전 학습 말뭉치는 총 8.1T 토큰을 포함하며, 중국어 토큰이 영어 토큰보다 약 12% 더 많습니다.

### 하이퍼파라미터 설정

모델 하이퍼파라미터의 경우, 트랜스포머 레이어 수는 60개, 은닉 차원은 5120으로 설정했습니다. 모든 학습 가능한 파라미터는 표준편차 0.006으로 무작위 초기화되었습니다. 다중 헤드 잠재 어텐션(MLA)에서는 어텐션 헤드 수(\\(n_h\\))를 128로, 헤드당 차원(\\(d_h\\))을 128로 설정했습니다. KV 압축 차원(\\(d_c\\))은 512, 쿼리 압축 차원(\\(d_c'\\))은 1536으로 설정했습니다. 디커플드 쿼리와 키의 경우, 헤드당 차원(\\(d_h^R\\))을 64로 설정했습니다.

Dai와 연구진의 접근 방식을 따라, 첫 번째 레이어를 제외한 모든 피드포워드 네트워크(FFN)를 MoE 레이어로 대체했습니다. 각 MoE 레이어는 2개의 공유 전문가와 160개의 라우팅된 전문가로 구성되며, 각 전문가의 중간 은닉 차원은 1536입니다. 라우팅된 전문가 중에서 각 토큰마다 6개의 전문가가 활성화됩니다.

저차원 압축과 세밀한 전문가 분할은 레이어의 출력 스케일에 영향을 미칠 수 있습니다. 이를 해결하기 위해 압축된 잠재 벡터 이후에 추가적인 RMS 정규화 레이어를 사용하고, 폭 병목 지점(압축된 잠재 벡터와 라우팅된 전문가의 중간 은닉 상태)에서 추가적인 스케일링 팩터를 곱하여 안정적인 학습을 보장했습니다.

이러한 구성에서 DeepSeek-V2는 총 236B개의 파라미터를 가지며, 각 토큰 처리시 21B개의 파라미터가 활성화됩니다.

### DeepSeek-V2의 지도 학습 미세조정과 평가 방법론

DeepSeek-V2는 이전 연구를 기반으로 지도 학습 미세조정(Supervised Fine-Tuning, SFT)을 위한 데이터셋을 더욱 발전시켰습니다. 총 1.5M개의 인스턴스로 구성된 이 데이터셋은 1.2M개의 유용성 관련 인스턴스와 0.3M개의 안전성 관련 인스턴스를 포함합니다. 초기 버전과 비교하여 환각 응답을 줄이고 작문 능력을 향상시키는 데 중점을 두어 데이터 품질을 개선했습니다.

미세조정 과정에서는 학습률을 \\( 5 \times 10^{-6} \\)으로 설정하고 2 에포크 동안 학습을 진행했습니다. 이는 모델이 과적합되지 않으면서도 효과적으로 학습할 수 있는 최적의 설정으로 확인되었습니다.

DeepSeek-V2 Chat (SFT)의 평가는 다양한 벤치마크를 통해 종합적으로 이루어졌습니다. 주로 생성 기반 벤치마크를 사용했으며, MMLU와 ARC와 같은 대표적인 객관식 과제도 포함했습니다. Zhou와 연구진이 개발한 IFEval을 통해 지시사항 준수 능력을 평가했으며, 프롬프트 수준의 느슨한 정확도를 평가 지표로 사용했습니다.

코드 관련 능력 평가를 위해 LiveCodeBench의 2023년 9월 1일부터 2024년 4월 1일까지의 문제들을 활용했습니다. 이는 데이터 오염을 방지하면서도 모델의 코딩 능력을 공정하게 평가할 수 있는 방법을 제공합니다.

또한 MT-Bench, AlpacaEval 2.0, AlignBench와 같은 개방형 대화 벤치마크를 통해 모델의 대화 능력을 평가했습니다. 이러한 벤치마크들은 모델의 자연스러운 대화 능력과 다양한 상황에서의 응답 품질을 종합적으로 평가할 수 있게 해줍니다.

비교 평가를 위해 Qwen1.5 72B Chat, LLaMA-3-70B Instruct, Mistral-8x22B Instruct 모델들도 동일한 평가 프레임워크와 설정에서 평가했습니다. DeepSeek 67B Chat의 경우에는 이전 공개 버전에서 보고된 평가 결과를 직접 참조했습니다.

### DeepSeek-V2의 강화학습 기반 선호도 조정

DeepSeek-V2는 모델의 잠재력을 더욱 발휘하고 인간의 선호도와 일치시키기 위해 강화학습(Reinforcement Learning, RL)을 통한 선호도 조정을 수행합니다. 이 과정에서 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)를 채택하여 학습 비용을 절감했습니다.

GRPO는 기존의 정책 모델과 동일한 크기의 비평자 모델을 사용하지 않고, 대신 그룹 점수로부터 기준선을 추정합니다. 구체적으로, 각 질문 \\(q\\)에 대해 GRPO는 이전 정책 \\(\pi_{\theta_{old}}\\)로부터 출력 그룹 \\(\{o_1,o_2,\cdots,o_G\}\\)을 샘플링하고, 다음과 같은 목적 함수를 최대화하여 정책 모델 \\(\pi_\theta\\)를 최적화합니다.

\\[ \begin{split}\mathcal{J}\_{GRPO}(\theta)&=\mathbb{E}\_{[q\sim P(Q),\{o_{i}\}\_{i=1}^{G}\sim\pi_{\theta_{old}}(O \vert q)]}\\ &\frac{1}{G}\sum\_{i=1}^{G}\left(\min\left(\frac{\pi_{\theta}(o_{i} \vert q)}{\pi_{\theta_{old}}(o_{i} \vert q)}A_{i},\text{clip}\left(\frac{\pi_{\theta}(o_{i} \vert q)}{\pi_{\theta_{old}}(o_{i} \vert q)},1-\varepsilon,1+\varepsilon\right)A_{i}\right)-\beta\mathbb{D}\_{KL}\left(\pi_{\theta} \parallel \pi_{ref}\right)\right) \\end{split} \\]

여기서 \\(\varepsilon\\)과 \\(\beta\\)는 하이퍼파라미터이며, \\(A_i\\)는 각 그룹 내 출력에 해당하는 보상 그룹 \\(\{r_1,r_2,\ldots,r_G\}\\)을 사용하여 계산된 이점(advantage)입니다.

\\[ A_i=\frac{r_i-\text{mean}(\{r_1,r_2,\cdots,r_G\})}{\text{std}(\{r_1,r_2,\cdots,r_G\})} \\]

DeepSeek-V2의 강화학습은 두 단계로 구성된 전략을 채택합니다. 첫 번째 단계는 추론 정렬(reasoning alignment) 단계로, 코드와 수학적 추론 작업을 위한 보상 모델 \\(RM_{reasoning}\\)을 학습하고 이를 통해 정책 모델을 최적화합니다.

\\[ r_i = RM_{reasoning}(o_i) \\]

두 번째 단계는 인간 선호도 정렬(human preference alignment) 단계로, 다중 보상 프레임워크를 채택합니다. 이 단계에서는 유용성 보상 모델 \\(RM_{helpful}\\), 안전성 보상 모델 \\(RM_{safety}\\), 그리고 규칙 기반 보상 모델 \\(RM_{rule}\\)로부터 보상을 획득합니다. 응답 \\(o_i\\)에 대한 최종 보상은 다음과 같이 계산됩니다.

\\[ r_i = c_1 \cdot RM_{helpful}(o_i) + c_2 \cdot RM_{safety}(o_i) + c_3 \cdot RM_{rule}(o_i) \\]

여기서 \\(c_1\\), \\(c_2\\), \\(c_3\\)는 각각의 가중치 계수입니다.

### DeepSeek-V2의 벤치마크 평가 결과

DeepSeek-V2 Chat (SFT)와 DeepSeek-V2 Chat (RL)의 표준 벤치마크 평가 결과는 주목할 만한 성과를 보여주었습니다. 특히 DeepSeek-V2 Chat (SFT)는 GSM8K, MATH, HumanEval 평가에서 기본 버전과 비교하여 상당한 성능 향상을 달성했습니다. 이러한 발전은 수학과 코드 관련 콘텐츠가 상당 부분 포함된 SFT 데이터의 활용 덕분입니다. 또한 DeepSeek-V2 Chat (RL)은 수학과 코드 벤치마크에서 더욱 향상된 성능을 보여주었습니다.

다른 모델들과의 비교에서, DeepSeek-V2 Chat (SFT)는 Qwen1.5 72B Chat과 비교했을 때 거의 모든 영어, 수학, 코드 벤치마크에서 우수한 성능을 보였습니다. 중국어 벤치마크에서는 다중 주제 객관식 과제에서 Qwen1.5 72B Chat보다 약간 낮은 점수를 기록했는데, 이는 기본 버전에서도 관찰된 패턴과 일치합니다.

최신 오픈소스 MoE 모델인 Mixtral 8x22B Instruct와 비교했을 때, DeepSeek-V2 Chat (SFT)는 NaturalQuestions와 IFEval을 제외한 대부분의 벤치마크에서 더 나은 성능을 보여주었습니다. 또한 최신 오픈소스 모델인 LLaMA3 70B Chat과 비교했을 때, DeepSeek-V2 Chat (SFT)는 코드와 수학 관련 벤치마크에서 유사한 성능을 보였습니다. LLaMA3 70B Chat이 MMLU와 IFEval에서 더 우수한 성능을 보인 반면, DeepSeek-V2 Chat (SFT)는 중국어 과제에서 더 강력한 성능을 보여주었습니다.

### 개방형 대화 생성 평가

영어 개방형 대화 생성에 대한 평가는 MT-Bench와 AlpacaEval 2.0을 활용했습니다. 평가 결과는 DeepSeek-V2 Chat (RL)이 DeepSeek-V2 Chat (SFT)보다 상당히 우수한 성능을 보여주었으며, 이는 RL 학습을 통한 정렬 개선의 효과를 입증합니다. 다른 오픈소스 모델들과 비교했을 때, DeepSeek-V2 Chat (RL)은 MT-Bench와 AlpacaEval 2.0 모두에서 Mistral 8x22B Instruct와 Qwen1.5 72B Chat을 능가했습니다. LLaMA3 70B Instruct와 비교했을 때는 MT-Bench에서 경쟁력 있는 성능을 보여주었고, AlpacaEval 2.0에서는 더 우수한 성능을 달성했습니다.

중국어 개방형 생성 능력은 AlignBench를 기반으로 평가되었습니다. DeepSeek-V2 Chat (RL)은 DeepSeek-V2 Chat (SFT)보다 약간 더 나은 성능을 보여주었으며, DeepSeek-V2 Chat (SFT)는 모든 오픈소스 중국어 모델들을 상당한 차이로 능가했습니다. 특히 중국어 추론과 언어 이해 측면에서 두 번째로 우수한 오픈소스 모델인 Qwen1.5 72B Chat을 크게 앞섰습니다.

## DeepSeek-V2의 결론과 향후 연구 방향

DeepSeek-V2는 128K 컨텍스트 길이를 지원하는 대규모 혼합 전문가(MoE) 언어 모델로서, 강력한 성능과 함께 경제적인 학습 및 효율적인 추론이라는 세 가지 핵심 목표를 동시에 달성했습니다. 이는 다중 헤드 잠재 어텐션(MLA)과 DeepSeekMoE라는 혁신적인 아키텍처를 통해 가능해졌습니다.

실제 성능 측면에서 DeepSeek-V2는 기존의 DeepSeek 67B와 비교했을 때 현저히 향상된 성능을 보여주면서도, 학습 비용을 42.5% 절감하고 KV 캐시를 93.3% 감소시켰으며, 최대 생성 처리량을 5.76배 향상시켰습니다. 특히 주목할 만한 점은 각 토큰 처리시 21B개의 파라미터만을 활성화하면서도 오픈소스 모델들 중 최상위 성능을 달성했다는 것입니다.

DeepSeek-V2와 그 챗 버전들은 다른 대규모 언어 모델들과 마찬가지로 몇 가지 한계점을 가지고 있습니다. 사전 학습 이후 지식 업데이트가 불가능하다는 점, 검증되지 않은 조언과 같은 사실이 아닌 정보를 생성할 수 있다는 점, 그리고 환각 현상이 발생할 수 있다는 점이 그것입니다. 또한 학습 데이터가 주로 중국어와 영어로 구성되어 있어 다른 언어에 대해서는 제한된 능력을 보일 수 있으므로, 중국어와 영어 이외의 상황에서는 주의해서 사용해야 합니다.

DeepSeek은 장기적인 관점에서 인공 일반 지능(AGI)이라는 목표에 점진적으로 접근하기 위해 오픈소스 대규모 모델에 지속적으로 투자할 계획입니다. 연구팀은 현재 경제적인 학습과 추론 비용을 유지하면서도 MoE 모델의 규모를 더욱 확장할 수 있는 방법을 연구하고 있으며, 다음 버전에서는 GPT-4와 동등한 수준의 성능 달성을 목표로 하고 있습니다.

또한 정렬 연구팀은 모델이 도움이 될 뿐만 아니라 정직하고 안전하게 작동하도록 지속적으로 개선하고 있습니다. 궁극적인 목표는 인간의 감독을 최소화하면서도 모델의 가치를 인간의 가치와 일치시키는 것입니다. 윤리적 고려사항과 책임감 있는 개발을 우선시함으로써 사회에 긍정적이고 유익한 영향을 미치고자 합니다.

현재 DeepSeek-V2는 텍스트 모달리티만을 지원하도록 설계되어 있습니다. 향후 연구에서는 모델이 다중 모달리티를 지원할 수 있도록 하여 더 넓은 범위의 시나리오에서 활용성과 유용성을 높이는 것을 목표로 하고 있습니다.

- - -
### References
* [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/pdf/2405.04434v5)
