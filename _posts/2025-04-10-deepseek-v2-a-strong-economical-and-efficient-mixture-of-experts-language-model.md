---
layout: post
title: "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"
date: 2024-05-07 15:56:43
author: "DeepSeek AI"
categories: "Language-Models"
tags: ["Multi-Head-Latent-Attention", "DeepSeekMoE", "Low-Rank-Key-Value-Joint-Compression", "Decoupled-Rotary-Position-Embedding", "Device-Limited-Routing", "Auxiliary-Loss-for-Load-Balance", "Token-Dropping-Strategy", "Expert-Parallelism", "Sparse-Mixture-of-Experts", "Efficient-Long-Context-Attention-Mechanism"]
cover: /assets/images/language-models.jpg
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

대규모 언어 모델의 발전은 인공지능 분야에서 혁명적인 변화를 가져왔지만, 동시에 심각한 계산 비용과 자원 제약이라는 근본적인 문제에 직면해 있었습니다. 기존의 대규모 언어 모델들은 수백억 개의 매개변수를 가지고 있어 학습과 추론에 막대한 컴퓨팅 파워와 에너지를 소모했습니다. 특히 각 토큰을 처리할 때마다 모든 매개변수를 활성화해야 하는 구조적 한계로 인해 실제 응용 환경에서의 실용성이 크게 제한되었습니다.

이러한 배경에서 연구팀은 계산 효율성과 모델 성능 사이의 균형을 찾는 것을 핵심 목표로 삼았습니다. 기존 모델들이 가진 비효율성을 극복하고, 제한된 계산 자원으로도 높은 성능을 달성할 수 있는 혁신적인 접근법의 필요성이 대두되었습니다. 특히 중국어와 영어를 포함한 다국어 환경에서 효과적으로 작동할 수 있는 모델에 대한 수요가 증가하고 있었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

DeepSeek-V2는 혼합 전문가(Mixture-of-Experts, MoE) 아키텍처를 혁신적으로 재설계하여 이러한 문제에 접근했습니다. 총 236B의 매개변수를 가지고 있지만, 각 토큰 처리 시에는 단 21B의 매개변수만 활성화되는 독특한 구조를 채택했습니다. 이는 전체 모델의 용량은 유지하면서도 실제 계산 비용을 크게 줄이는 획기적인 방법입니다.

핵심 혁신은 다중 헤드 잠재 어텐션(Multi-head Latent Attention, MLA)과 DeepSeekMoE라는 두 가지 주요 기술에 있습니다. MLA는 키-값(Key-Value) 캐시를 효율적으로 압축하여 메모리 사용량을 크게 줄이고 추론 속도를 향상시킵니다. DeepSeekMoE는 전문가 네트워크를 세분화하고 공유 전문가를 분리함으로써 매개변수 효율성을 높이고 전문가 특화도를 개선합니다.

#### 제안된 방법은 어떻게 구현되었습니까?

DeepSeek-V2의 구현은 8.1조 토큰으로 구성된 고품질의 다중 소스 코퍼스를 사용한 사전 학습부터 시작됩니다. 학습 과정에서는 AdamW 옵티마이저와 제로 버블 파이프라인 병렬화 기법을 활용하여 계산 효율성을 극대화했습니다. 특히 YaRN(Yet another RoPE extension) 기법을 적용하여 컨텍스트 길이를 128K 토큰으로 확장했습니다.

모델의 성능을 더욱 향상시키기 위해 지도 미세 조정(Supervised Fine-Tuning)과 강화 학습(Reinforcement Learning) 과정을 추가로 수행했습니다. 이 과정에서는 다양한 보상 모델을 사용하여 모델의 유용성, 정확성, 안전성, 공정성 등 여러 측면을 최적화했습니다. 특히 체인 오브 소트(Chain-of-Thought) 방식을 도입하여 복잡한 추론 작업에서의 성능을 크게 향상시켰습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?

DeepSeek-V2는 대규모 언어 모델의 계산 효율성과 성능 사이의 새로운 균형점을 제시했습니다. 활성화된 매개변수가 단 21B임에도 불구하고 MMLU 벤치마크에서 78.3%의 정확도를 달성하는 등 기존 오픈소스 모델들을 능가하는 성과를 보여주었습니다. 특히 학습 비용을 42.5% 절감하고 KV 캐시를 93.3% 감소시키면서도 성능을 유지했다는 점에서 큰 의의가 있습니다.

이 연구는 앞으로의 대규모 언어 모델 개발에 중요한 이정표를 제시했습니다. 계산 효율성, 다국어 성능, 긴 컨텍스트 처리 능력 등 여러 측면에서 혁신적인 접근을 보여주었으며, 특히 오픈소스 AI 커뮤니티에 중요한 기여를 했습니다. 향후 연구 방향으로는 더욱 효율적인 모델 아키텍처 개발, 다중 모달리티 지원, 더 긴 컨텍스트 처리 등이 제시되었습니다.
- - -
# DeepSeek-V2: 강력하고 경제적이며 효율적인 혼합 전문가 언어 모델

## 소개

DeepSeek-V2는 경제적인 학습과 효율적인 추론을 특징으로 하는 강력한 혼합 전문가(Mixture-of-Experts, MoE) 언어 모델입니다. 이 모델은 총 236B 매개변수를 가지고 있으며, 각 토큰 처리 시 21B 매개변수만 활성화되는 구조를 채택하고 있습니다. 또한 128K 토큰의 컨텍스트 길이를 지원하여 긴 문맥을 처리할 수 있는 능력을 갖추고 있습니다.

DeepSeek-V2는 다중 헤드 잠재 어텐션(Multi-head Latent Attention, MLA)과 DeepSeekMoE라는 혁신적인 아키텍처를 도입했습니다. MLA는 키-값(Key-Value, KV) 캐시를 잠재 벡터로 크게 압축함으로써 효율적인 추론을 보장합니다. 이는 메모리 사용량을 크게 줄이고 추론 속도를 향상시키는 데 중요한 역할을 합니다. 한편, DeepSeekMoE는 희소 계산(sparse computation)을 통해 경제적인 비용으로 강력한 모델을 학습할 수 있게 합니다.

DeepSeek 67B 모델과 비교했을 때, DeepSeek-V2는 현저히 향상된 성능을 보여주면서도 학습 비용을 42.5% 절감하고, KV 캐시를 93.3% 감소시켰으며, 최대 생성 처리량을 5.76배 향상시켰습니다. 이러한 효율성 개선은 대규모 언어 모델의 실용적 배포에 있어 중요한 진전을 의미합니다.

DeepSeek-V2는 8.1T 토큰으로 구성된 고품질의 다중 소스 코퍼스에서 사전 학습되었으며, 지도 미세 조정(Supervised Fine-Tuning, SFT)과 강화 학습(Reinforcement Learning, RL)을 추가로 수행하여 모델의 잠재력을 최대한 발휘할 수 있도록 했습니다. 평가 결과에 따르면, 활성화된 매개변수가 21B에 불과함에도 불구하고 DeepSeek-V2와 그 채팅 버전은 오픈 소스 모델 중 최고 수준의 성능을 달성했습니다.

![그림 1: (a) 다양한 오픈 소스 모델의 활성화된 매개변수 대비 MMLU 정확도.](https://arxiv.org/html/2405.04434v5/x1.png)

위 그림은 다양한 오픈 소스 모델들의 활성화된 매개변수 수와 MMLU(Massive Multitask Language Understanding) 정확도 간의 관계를 보여줍니다. DeepSeek-V2는 활성화된 매개변수가 21B에 불과함에도 불구하고 다른 오픈 소스 모델들보다 높은 MMLU 정확도를 달성했음을 확인할 수 있습니다. 이는 DeepSeek-V2의 아키텍처가 매개변수를 효율적으로 활용하고 있음을 시사합니다.

![그림 1: (b) DeepSeek 67B(Dense)와 DeepSeek-V2의 학습 비용 및 추론 효율성 비교.](https://arxiv.org/html/2405.04434v5/x2.png)

위 그림은 DeepSeek 67B(Dense) 모델과 DeepSeek-V2의 학습 비용 및 추론 효율성을 비교한 것입니다. DeepSeek-V2는 학습 비용을 42.5% 절감하면서도 KV 캐시를 93.3% 감소시켰고, 최대 생성 처리량을 5.76배 향상시켰습니다. 이러한 결과는 DeepSeek-V2가 도입한 MLA와 DeepSeekMoE 아키텍처의 효율성을 입증합니다.

DeepSeek-V2의 핵심 기술적 혁신은 두 가지 주요 아키텍처 구성 요소에 있습니다. 첫째, MLA는 KV 캐시를 효율적으로 압축하여 메모리 사용량을 크게 줄이고 추론 속도를 향상시킵니다. 둘째, DeepSeekMoE는 [Dai와 연구진](https://arxiv.org/pdf/2401.06066)이 제안한 아키텍처로, 전문가 네트워크를 세분화하고 공유 전문가를 분리함으로써 매개변수 효율성을 높이고 전문가 특화도를 향상시킵니다.

DeepSeekMoE는 기존 MoE 모델의 두 가지 주요 한계를 해결합니다. 첫째, 지식 혼합성(Knowledge Hybridity) 문제로, 기존 MoE 모델은 제한된 수의 전문가를 사용하여 다양한 지식을 처리하기 때문에 전문가 특화가 어려웠습니다. 둘째, 지식 중복성(Knowledge Redundancy) 문제로, 서로 다른 전문가에 할당된 토큰이 공통 지식을 요구하여 여러 전문가가 동일한 지식을 습득하게 되는 매개변수 중복이 발생했습니다. DeepSeekMoE는 세분화된 전문가 분할과 공유 전문가 분리 전략을 통해 이러한 문제를 효과적으로 해결합니다.

DeepSeek-V2는 또한 [Peng과 연구진](https://arxiv.org/pdf/2309.00071)이 제안한 YaRN(Yet another RoPE extension) 기법을 활용하여 컨텍스트 길이를 128K 토큰으로 확장했습니다. YaRN은 RoPE(Rotary Position Embeddings)의 차원을 선택적으로 보간하고 어텐션 스케일링을 적용하여 모델이 사전 학습된 것보다 훨씬 긴 컨텍스트 길이에서도 효과적으로 작동할 수 있게 합니다.

이러한 혁신적인 아키텍처와 학습 방법론을 통해 DeepSeek-V2는 활성화된 매개변수가 21B에 불과함에도 불구하고 오픈 소스 모델 중 최고 수준의 성능을 달성했습니다. 모델 체크포인트는 https://github.com/deepseek-ai/DeepSeek-V2에서 공개적으로 이용 가능합니다.

# DeepSeek-V2: 아키텍처

## 모델 아키텍처 개요

DeepSeek-V2는 두 가지 핵심 아키텍처 혁신을 통해 강력한 성능과 효율성을 달성합니다. 첫째, 다중 헤드 잠재 어텐션(Multi-head Latent Attention, MLA)은 키-값(KV) 캐시를 효율적으로 압축하여 추론 속도를 향상시킵니다. 둘째, DeepSeekMoE는 전문가 네트워크를 세분화하고 공유 전문가를 분리함으로써 매개변수 효율성을 높이고 전문가 특화도를 향상시킵니다. 이 섹션에서는 이러한 아키텍처 구성 요소를 자세히 살펴보겠습니다.

### 기본 아키텍처 구성

DeepSeek-V2는 기본적으로 디코더 전용 트랜스포머 아키텍처를 따르며, 각 레이어는 셀프 어텐션 모듈과 피드포워드 네트워크(FFN)로 구성됩니다. 이 모델은 총 236B 매개변수를 가지고 있으며, 각 토큰 처리 시 21B 매개변수만 활성화됩니다. 또한 128K 토큰의 컨텍스트 길이를 지원합니다.

DeepSeek-V2의 기본 구성은 다음과 같습니다.
- 임베딩 차원: 8,192
- 어텐션 헤드 수: 64
- 레이어 수: 40
- 활성화 함수: SwiGLU
- 최대 시퀀스 길이: 128K 토큰

## 다중 헤드 잠재 어텐션(MLA)

다중 헤드 잠재 어텐션(MLA)은 DeepSeek-V2의 핵심 혁신 중 하나로, 키-값(KV) 캐시를 효율적으로 압축하여 메모리 사용량을 줄이고 추론 속도를 향상시킵니다.

### MLA의 기술적 배경

대규모 언어 모델(LLM)의 추론 과정에서 가장 큰 병목 중 하나는 KV 캐시의 메모리 사용량입니다. 기존의 트랜스포머 모델에서는 각 어텐션 헤드마다 별도의 키와 값 벡터를 저장해야 하므로, 긴 시퀀스를 처리할 때 메모리 요구량이 급격히 증가합니다.

이 문제를 해결하기 위해 [Shazeer](https://arxiv.org/pdf/1911.02150v1)는 다중 쿼리 어텐션(Multi-Query Attention, MQA)을 제안했으며, [Ainslie와 연구진](https://arxiv.org/pdf/2305.13245)은 이를 확장하여 그룹 쿼리 어텐션(Grouped-Query Attention, GQA)을 개발했습니다. 이러한 방법들은 여러 쿼리 헤드가 키와 값 헤드를 공유하도록 함으로써 메모리 사용량을 줄입니다.

### MLA의 작동 원리

MLA는 기존의 MQA와 GQA를 더욱 발전시킨 방식으로, 키와 값을 더 작은 차원의 잠재 표현으로 압축합니다. 이를 통해 메모리 사용량을 크게 줄이면서도 모델의 표현력을 유지할 수 있습니다.

MLA의 수학적 정의는 다음과 같습니다.

\\[ \text{MLA}(Q, K, V) = \text{Attention}(Q, P_K K, P_V V) \\]

여기서 \\(P_K\\)와 \\(P_V\\)는 각각 키와 값을 잠재 공간으로 투영하는 학습 가능한 행렬입니다. 이 투영 과정을 통해 키와 값의 차원을 크게 줄일 수 있습니다.

구체적으로, MLA는 다음과 같은 단계로 구현됩니다.

1. 쿼리(Q), 키(K), 값(V)을 계산합니다.
2. 키와 값을 잠재 공간으로 투영합니다. \\(K_L = P_K K\\), \\(V_L = P_V V\\)
3. 쿼리와 투영된 키 사이의 어텐션 가중치를 계산합니다. \\(A = \text{softmax}(\frac{Q K_L^T}{\sqrt{d_k}})\\)
4. 어텐션 가중치와 투영된 값을 곱합니다. \\(O = A V_L\\)

이 방식을 통해 MLA는 KV 캐시의 크기를 93.3% 감소시키면서도 모델의 성능을 유지할 수 있습니다.

### MLA의 이점

MLA의 주요 이점은 다음과 같습니다.

1. **메모리 효율성**: KV 캐시의 크기를 크게 줄여 메모리 사용량을 감소시킵니다.
2. **추론 속도 향상**: 메모리 대역폭 요구량이 감소하여 추론 속도가 향상됩니다.
3. **긴 컨텍스트 처리**: 메모리 효율성 향상으로 더 긴 컨텍스트를 효과적으로 처리할 수 있습니다.
4. **표현력 유지**: 잠재 표현을 통해 정보 손실을 최소화하면서 압축을 달성합니다.

실험 결과에 따르면, MLA를 적용한 DeepSeek-V2는 기존 DeepSeek 67B 모델과 비교하여 KV 캐시를 93.3% 감소시키고, 최대 생성 처리량을 5.76배 향상시켰습니다.

## DeepSeekMoE 아키텍처

DeepSeekMoE는 [Dai와 연구진](https://arxiv.org/pdf/2401.06066)이 제안한 혼합 전문가(Mixture-of-Experts, MoE) 아키텍처로, 기존 MoE 모델의 한계를 극복하기 위해 설계되었습니다.

### 기존 MoE의 한계

기존의 MoE 모델은 다음과 같은 두 가지 주요 한계를 가지고 있습니다.

1. **지식 혼합성(Knowledge Hybridity)**: 기존 MoE 모델은 제한된 수의 전문가를 사용하여 다양한 지식을 처리하기 때문에 전문가 특화가 어렵습니다.
2. **지식 중복성(Knowledge Redundancy)**: 서로 다른 전문가에 할당된 토큰이 공통 지식을 요구하여 여러 전문가가 동일한 지식을 습득하게 되는 매개변수 중복이 발생합니다.

### DeepSeekMoE의 혁신

DeepSeekMoE는 다음과 같은 두 가지 핵심 혁신을 통해 이러한 한계를 극복합니다.

1. **세분화된 전문가 분할(Fine-Grained Expert Segmentation)**: 각 전문가 FFN을 \\(m\\)개의 더 작은 전문가로 분할하여 총 매개변수 수는 유지하면서 더 유연하고 적응력 있는 전문가 조합을 가능하게 합니다.

2. **공유 전문가 분리(Shared Expert Isolation)**: \\(K_s\\)개의 전문가를 항상 활성화되는 공유 전문가로 분리하여 공통 지식을 통합하고 라우팅된 전문가들이 더 특화된 지식에 집중할 수 있도록 합니다.

### DeepSeekMoE의 수학적 정의

DeepSeekMoE 레이어의 출력은 다음과 같이 정의됩니다.

\\[ \mathbf{h}\_t^l = \sum_{i=1}^{K_s}{\operatorname{FFN}\_i\left(\mathbf{u}\_t^{l}\right)}+\sum_{i=K_s+1}^{m\cdot N}\left({g_{i,t}\operatorname{FFN}_i\left(\mathbf{u}_t^{l}\right)}\right)+\mathbf{u}_t^{l} \\]

여기서 \\(g_{i,t}\\)는 \\(i\\)번째 전문가에 대한 게이팅 값으로, 다음과 같이 계산됩니다.

\\[ g_{i,t} = \begin{cases}
s_{i,t}, & s_{i,t}\in\operatorname{Topk}(\{s_{j,t}|K_s+1\leqslant j\leqslant m\cdot N\},m\cdot K-K_s), \\
0, & \text{otherwise},
\end{cases} \\]

그리고 \\(s_{i,t}\\)는 토큰-전문가 친화도로, 소프트맥스 연산을 사용하여 계산됩니다.

### DeepSeekMoE의 이점

DeepSeekMoE의 주요 이점은 다음과 같습니다.

1. **높은 전문가 특화도**: 세분화된 전문가 분할과 공유 전문가 분리를 통해 각 전문가가 더 특화된 지식에 집중할 수 있습니다.
2. **향상된 매개변수 효율성**: 공통 지식을 공유 전문가에 통합함으로써 매개변수 중복을 줄이고 효율성을 높입니다.
3. **확장성**: DeepSeekMoE는 대규모 모델로 확장하면서도 일관된 이점을 제공합니다.

실험 결과에 따르면, DeepSeekMoE는 기존의 GShard 아키텍처와 비교하여 더 적은 전문가 매개변수와 계산으로도 비슷한 성능을 달성할 수 있습니다.

## 컨텍스트 길이 확장

DeepSeek-V2는 [Peng과 연구진](https://arxiv.org/pdf/2309.00071)이 제안한 YaRN(Yet another RoPE extension) 기법을 활용하여 컨텍스트 길이를 128K 토큰으로 확장했습니다.

### YaRN의 작동 원리

YaRN은 로터리 위치 임베딩(Rotary Position Embeddings, RoPE)을 확장하여 모델이 사전 학습된 것보다 훨씬 긴 컨텍스트 길이에서도 효과적으로 작동할 수 있게 합니다. 이 방법은 다음과 같은 핵심 기술을 포함합니다.

1. **"NTK-aware" 보간법**: RoPE 임베딩을 선형적으로 보간할 때 발생하는 고주파 정보 손실을 해결하기 위해, RoPE 주파수를 비균일하게 조정하여 보간 압력을 여러 차원에 분산시킵니다.

2. **"NTK-by-parts" 보간법**: 더 낮은 주파수의 RoPE 차원만 선택적으로 보간하여 모델의 지역적 위치 관계 이해를 손상시키지 않습니다.

3. **동적 스케일링**: 현재 시퀀스 길이에 따라 보간 스케일 팩터를 동적으로 업데이트하는 추론 시간 기술로, 모델이 사전 학습된 컨텍스트 창을 초과할 때 성능이 급격히 저하되지 않고 점진적으로 저하되도록 합니다.

4. **어텐션 스케일링**: 어텐션 소프트맥스 계산에 온도 스케일링 팩터 \\(t\\)를 도입하여 확장된 컨텍스트 창 전체에 걸쳐 균일하게 퍼플렉시티에 영향을 미칩니다.

### YaRN의 이점

YaRN을 적용함으로써 DeepSeek-V2는 다음과 같은 이점을 얻습니다.

1. **효율적인 컨텍스트 창 확장**: 원래 사전 학습 코퍼스의 0.1%에 불과한 데이터로 미세 조정한 후에도 최첨단 성능을 달성합니다.
2. **점진적 성능 저하**: 동적 스케일링을 통해 모델이 사전 학습된 컨텍스트 창을 초과할 때 성능이 급격히 저하되지 않고 점진적으로 저하됩니다.
3. **계산 효율성**: 기존의 위치 보간법(PI)이나 "NTK-aware" 보간법과 비교하여 더 적은 데이터로 더 나은 성능을 달성합니다.

## 학습 최적화

DeepSeek-V2는 효율적인 학습을 위해 여러 최적화 기법을 적용했습니다.

### AdamW 옵티마이저

DeepSeek-V2는 [Loshchilov와 Hutter](https://arxiv.org/pdf/1711.05101)가 제안한 AdamW 옵티마이저를 사용합니다. AdamW는 가중치 감쇠(weight decay)를 그래디언트 기반 최적화 업데이트에서 분리함으로써 적응형 그래디언트 알고리즘의 일반화 성능을 향상시킵니다.

기존의 Adam 옵티마이저에서는 L2 정규화가 학습률에 따라 스케일링되어 가중치 감쇠와 동등하게 작용하지만, 이러한 동등성은 적응형 그래디언트 알고리즘에서는 성립하지 않습니다. AdamW는 가중치 감쇠를 최적화 단계에서 분리하여 원래의 가중치 감쇠 정규화 공식을 복원합니다.

### 제로 버블 파이프라인 병렬화

대규모 모델 학습의 효율성을 높이기 위해 DeepSeek-V2는 [Qi와 연구진](https://arxiv.org/pdf/2401.10241)이 제안한 제로 버블 파이프라인 병렬화 기법을 적용했습니다. 이 방법은 역전파 계산을 입력 그래디언트와 매개변수 그래디언트로 분리하여 파이프라인 병렬화에서 순차적 의존성을 줄입니다.

제로 버블 파이프라인 병렬화는 두 가지 새로운 파이프라인 스케줄링 전략인 ZB-H1과 ZB-H2를 도입합니다.

1. **ZB-H1**: 기준 1F1B 스케줄의 최대 피크 메모리 사용량을 초과하지 않으면서 버블 크기를 약 1/3로 줄입니다.
2. **ZB-H2**: 더 큰 메모리 사용량이 허용될 때, 웜업 단계에서 더 많은 순방향 패스를 도입하고 매개변수 그래디언트 계산을 재정렬하여 제로 버블 파이프라인을 달성합니다.

이러한 최적화 기법을 통해 DeepSeek-V2는 학습 비용을 42.5% 절감하면서도 향상된 성능을 달성할 수 있었습니다.

## KV 캐시 양자화

DeepSeek-V2는 [Hooper와 연구진](https://arxiv.org/pdf/2401.18079)이 제안한 KV 캐시 양자화 기법을 적용하여 메모리 사용량을 더욱 줄이고 추론 효율성을 향상시켰습니다.

### 채널별 키 양자화

키 활성화는 다른 채널에 비해 더 큰 평균 크기를 가진 특이 채널을 보입니다. 이러한 분포에 더 잘 맞추기 위해, 채널별 키 양자화는 스케일링 팩터와 제로 포인트를 전체 텐서가 아닌 채널 차원을 따라 공유합니다. 이는 자연스럽게 비슷한 크기의 값들을 그룹화하여 특이 채널이 양자화에 미치는 영향을 완화합니다.

\\[ A_{i,norm} = \frac{A_i - z_i}{s_i} \\]

여기서 \\(A_i\\)는 활성화 \\(\mathbf{A}\\)의 \\(i\\)번째 요소이고, \\(z_i\\)는 제로 포인트, \\(s_i\\)는 스케일링 팩터로, 둘 다 채널별로 계산됩니다.

### RoPE 이전 키 양자화

로터리 위치 임베딩(RoPE)을 키에 적용하면 채널 쌍이 혼합되어 채널 간 크기의 일관성이 떨어집니다. 이 문제를 해결하기 위해, RoPE를 적용하기 전에 키를 양자화하고, 그 후 커스텀 CUDA 커널을 사용하여 역양자화 후 RoPE를 효율적으로 적용합니다.

### 비균일 양자화

교정 세트에서 오프라인으로 민감도 가중치 비균일 데이터 타입을 도출하고, 이를 추론 중에 채널별 또는 토큰별로 재조정합니다. 이를 통해 균일 양자화 접근법과 달리 비균일한 키와 값 분포를 더 잘 표현하기 위해 유연한 양자화 표지판 배치가 가능합니다.

### 벡터별 밀집-희소 양자화

키와 값 활성화의 이상값은 양자화 해상도를 크게 저하시킬 수 있습니다. 벡터별 밀집-희소 양자화 접근법은 각 벡터(키의 경우 채널, 값의 경우 토큰)에 대해 이상값 임계값을 별도로 계산하여 양자화 범위를 왜곡시키는 이상값을 직접 타겟팅합니다.

이러한 기법을 통해 DeepSeek-V2는 LLaMA, Llama-2, Llama-3, Mistral 모델에 대해 Wikitext-2와 C4에서 0.1 미만의 퍼플렉시티 저하로 3비트 양자화를 달성하여 기존 접근법을 능가합니다.

## 아키텍처 요약

DeepSeek-V2는 다중 헤드 잠재 어텐션(MLA)과 DeepSeekMoE라는 두 가지 핵심 아키텍처 혁신을 통해 강력한 성능과 효율성을 달성합니다. MLA는 KV 캐시를 효율적으로 압축하여 메모리 사용량을 줄이고 추론 속도를 향상시킵니다. DeepSeekMoE는 전문가 네트워크를 세분화하고 공유 전문가를 분리함으로써 매개변수 효율성을 높이고 전문가 특화도를 향상시킵니다.

또한 YaRN 기법을 활용하여 컨텍스트 길이를 128K 토큰으로 확장하고, AdamW 옵티마이저와 제로 버블 파이프라인 병렬화를 통해 학습 효율성을 높였습니다. KV 캐시 양자화 기법을 적용하여 메모리 사용량을 더욱 줄이고 추론 효율성을 향상시켰습니다.

이러한 혁신적인 아키텍처와 최적화 기법을 통해 DeepSeek-V2는 활성화된 매개변수가 21B에 불과함에도 불구하고 오픈 소스 모델 중 최고 수준의 성능을 달성했습니다. 또한 기존 DeepSeek 67B 모델과 비교하여 학습 비용을 42.5% 절감하고, KV 캐시를 93.3% 감소시켰으며, 최대 생성 처리량을 5.76배 향상시켰습니다.

# 다중 헤드 잠재 어텐션: 추론 효율성 향상

## 다중 헤드 잠재 어텐션 개요

다중 헤드 잠재 어텐션(Multi-Head Latent Attention, MLA)은 DeepSeek-V2 모델의 핵심 혁신 중 하나로, 추론 과정에서 키-값(Key-Value, KV) 캐시의 메모리 사용량을 크게 줄이고 추론 효율성을 향상시키는 기술입니다. 이 섹션에서는 표준 다중 헤드 어텐션의 기본 개념부터 시작하여 MLA의 핵심 구성 요소와 이점을 자세히 살펴보겠습니다.

### 기본 개념: 표준 다중 헤드 어텐션

표준 다중 헤드 어텐션(Multi-Head Attention, MHA)은 [Vaswani와 연구진](https://arxiv.org/pdf/1706.03762v7)이 제안한 트랜스포머 아키텍처의 핵심 구성 요소입니다. MHA는 입력 시퀀스의 다양한 위치 간의 관계를 모델링하기 위해 여러 어텐션 헤드를 병렬로 사용합니다.

표준 MHA의 수학적 정의는 다음과 같습니다.

\\[ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \ldots, \text{head}_h)W^O \\]

여기서 각 헤드는 다음과 같이 계산됩니다.

\\[ \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\]

어텐션 함수는 일반적으로 스케일된 닷-프로덕트 어텐션을 사용합니다.

\\[ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \\]

여기서:
- \\(Q\\), \\(K\\), \\(V\\)는 각각 쿼리, 키, 값 행렬입니다.
- \\(W_i^Q\\), \\(W_i^K\\), \\(W_i^V\\)는 각 헤드에 대한 투영 행렬입니다.
- \\(W^O\\)는 출력 투영 행렬입니다.
- \\(d_k\\)는 키 벡터의 차원입니다.
- \\(h\\)는 어텐션 헤드의 수입니다.

자기회귀적(autoregressive) 생성 과정에서 모델은 이전에 생성된 토큰들의 키와 값을 저장하여 다음 토큰을 생성할 때 재사용합니다. 이를 KV 캐시라고 하며, 시퀀스 길이가 길어질수록 메모리 사용량이 크게 증가하는 문제가 있습니다.

표준 MHA에서 KV 캐시의 메모리 사용량은 다음과 같이 계산됩니다.

\\[ \text{Memory}_{KV} = 2 \times h \times L \times d_h \times b \\]

여기서:
- \\(h\\)는 어텐션 헤드의 수입니다.
- \\(L\\)은 시퀀스 길이입니다.
- \\(d_h\\)는 각 헤드의 차원입니다.
- \\(b\\)는 배치 크기입니다.

이러한 메모리 요구량은 긴 시퀀스를 처리할 때 상당한 병목 현상을 일으킬 수 있습니다.

### 저순위 키-값 공동 압축

MLA의 핵심 아이디어는 키와 값을 더 작은 차원의 잠재 공간으로 압축하여 메모리 사용량을 줄이는 것입니다. 이는 저순위 근사(low-rank approximation)를 통해 이루어집니다.

MLA에서는 키와 값을 다음과 같이 압축합니다.

\\[ K_L = P_K K \\]
\\[ V_L = P_V V \\]

여기서:
- \\(K\\)와 \\(V\\)는 원래의 키와 값 행렬입니다.
- \\(P_K\\)와 \\(P_V\\)는 각각 키와 값을 잠재 공간으로 투영하는 학습 가능한 행렬입니다.
- \\(K_L\\)과 \\(V_L\\)은 압축된 잠재 키와 값입니다.

이러한 압축을 통해 MLA의 어텐션 계산은 다음과 같이 변형됩니다.

\\[ \text{MLA}(Q, K, V) = \text{Attention}(Q, P_K K, P_V V) \\]
\\[ = \text{softmax}\left(\frac{Q(P_K K)^T}{\sqrt{d_k}}\right)(P_V V) \\]

투영 행렬 \\(P_K\\)와 \\(P_V\\)의 차원을 적절히 선택함으로써 키와 값의 차원을 크게 줄일 수 있습니다. 예를 들어, 원래 키와 값의 차원이 \\(d_h\\)인 경우, 잠재 차원 \\(d_l\\)을 \\(d_h\\)보다 훨씬 작게 설정할 수 있습니다(\\(d_l \ll d_h\\)).

이러한 압축을 통해 MLA의 KV 캐시 메모리 사용량은 다음과 같이 감소합니다.

\\[ \text{Memory}_{KV\_MLA} = 2 \times h \times L \times d_l \times b \\]

압축률 \\(r = \frac{d_l}{d_h}\\)을 사용하면, 메모리 절감은 \\(1 - r\\)이 됩니다. 예를 들어, \\(r = 0.1\\)인 경우 메모리 사용량을 90% 줄일 수 있습니다.

### 분리된 로터리 위치 임베딩

MLA는 또한 로터리 위치 임베딩(Rotary Position Embedding, RoPE)을 효율적으로 적용하기 위한 분리된 접근 방식을 도입합니다. [Su와 연구진](https://arxiv.org/pdf/2104.09864v5)이 제안한 RoPE는 상대적 위치 정보를 모델링하는 효과적인 방법으로, 쿼리와 키 벡터에 회전 변환을 적용합니다.

표준 RoPE는 다음과 같이 정의됩니다.

\\[ f_q(\mathbf{x}_m, m) = (\mathbf{W}_q \mathbf{x}_m) e^{i m \theta} \\]
\\[ f_k(\mathbf{x}_n, n) = (\mathbf{W}_k \mathbf{x}_n) e^{i n \theta} \\]

여기서 \\(\theta\\)는 사전 정의된 상수이고, \\(m\\)과 \\(n\\)은 토큰의 위치입니다.

MLA에서는 RoPE를 키의 압축 전에 적용합니다. 이는 다음과 같은 순서로 이루어집니다.

1. 쿼리와 키에 RoPE를 적용합니다. \\(Q_{rope} = \text{RoPE}(Q)\\), \\(K_{rope} = \text{RoPE}(K)\\)
2. 키를 잠재 공간으로 압축합니다. \\(K_L = P_K K_{rope}\\)
3. 어텐션을 계산합니다. \\(\text{Attention}(Q_{rope}, K_L, V_L)\\)

이러한 접근 방식은 위치 정보를 보존하면서도 메모리 효율성을 높일 수 있습니다.

### 키-값 캐시 비교

MLA의 효율성을 이해하기 위해 다양한 어텐션 메커니즘의 KV 캐시 크기를 비교해 보겠습니다.

1. **표준 다중 헤드 어텐션(MHA)**:
   - 각 헤드마다 별도의 키와 값을 저장
   - KV 캐시 크기: \\(2 \times h \times L \times d_h \times b\\)

2. **다중 쿼리 어텐션(MQA)** ([Shazeer](https://arxiv.org/pdf/1911.02150v1)가 제안):
   - 모든 쿼리 헤드가 단일 키와 값 헤드를 공유
   - KV 캐시 크기: \\(2 \times 1 \times L \times d_h \times b\\)
   - 메모리 절감: \\(1 - \frac{1}{h}\\) (예: 16개 헤드의 경우 93.75%)

3. **그룹 쿼리 어텐션(GQA)** ([Ainslie와 연구진](https://arxiv.org/pdf/2305.13245)이 제안):
   - 쿼리 헤드를 \\(g\\)개의 그룹으로 나누고, 각 그룹이 하나의 키와 값 헤드를 공유
   - KV 캐시 크기: \\(2 \times g \times L \times d_h \times b\\)
   - 메모리 절감: \\(1 - \frac{g}{h}\\) (예: 16개 헤드, 4개 그룹의 경우 75%)

4. **다중 헤드 잠재 어텐션(MLA)**:
   - 키와 값을 잠재 공간으로 압축
   - KV 캐시 크기: \\(2 \times h \times L \times d_l \times b\\)
   - 메모리 절감: \\(1 - \frac{d_l}{d_h}\\) (예: 압축률 0.1의 경우 90%)

MLA는 MQA와 GQA와 달리 헤드 수를 줄이지 않고도 메모리 효율성을 높일 수 있습니다. 이는 모델의 표현력을 유지하면서도 메모리 사용량을 크게 줄일 수 있는 장점이 있습니다.

DeepSeek-V2에서 MLA를 적용한 결과, KV 캐시를 93.3% 감소시키면서도 모델의 성능을 유지할 수 있었습니다. 이는 메모리 대역폭 요구량을 크게 줄여 추론 속도를 향상시키는 데 중요한 역할을 했습니다.

## 효율적인 구현 및 최적화

MLA의 효율적인 구현을 위해 여러 최적화 기법이 적용되었습니다. 특히, [Dao](https://arxiv.org/pdf/2307.08691v1)가 제안한 FlashAttention-2와 같은 최신 어텐션 최적화 기법을 활용하여 계산 효율성을 더욱 향상시켰습니다.

FlashAttention-2는 다음과 같은 최적화를 제공합니다.

1. 비행렬 곱셈(non-matmul) FLOP 수를 줄이기 위한 알고리즘적 개선
2. 배치 차원, 헤드 수뿐만 아니라 시퀀스 길이 차원에서도 병렬화
3. 각 스레드 블록 내에서 워프 간의 작업 분할 최적화

이러한 최적화를 MLA와 결합함으로써, DeepSeek-V2는 최대 생성 처리량을 5.76배 향상시킬 수 있었습니다.

또한, [Hooper와 연구진](https://arxiv.org/pdf/2401.18079)이 제안한 KV 캐시 양자화 기법을 적용하여 메모리 효율성을 더욱 향상시켰습니다. 이 기법은 다음과 같은 혁신을 포함합니다.

1. **채널별 키 양자화**: 키 활성화의 특이 채널을 고려하여 채널 차원을 따라 스케일링 팩터와 제로 포인트를 공유합니다.
2. **RoPE 이전 키 양자화**: RoPE를 적용하기 전에 키를 양자화하여 채널 쌍 혼합으로 인한 문제를 해결합니다.
3. **비균일 양자화**: 교정 세트에서 민감도 가중치 비균일 데이터 타입을 도출하여 키와 값 분포를 더 잘 표현합니다.
4. **벡터별 밀집-희소 양자화**: 이상값 임계값을 별도로 계산하여 양자화 범위를 왜곡시키는 이상값을 직접 타겟팅합니다.

이러한 양자화 기법을 MLA와 결합함으로써, DeepSeek-V2는 3비트 양자화로도 0.1 미만의 퍼플렉시티 저하만으로 KV 캐시를 더욱 압축할 수 있었습니다.

## MLA의 이점 및 영향

MLA는 다음과 같은 주요 이점을 제공합니다.

1. **메모리 효율성**: KV 캐시의 크기를 크게 줄여 메모리 사용량을 감소시킵니다. DeepSeek-V2에서는 KV 캐시를 93.3% 감소시켰습니다.

2. **추론 속도 향상**: 메모리 대역폭 요구량이 감소하여 추론 속도가 향상됩니다. DeepSeek-V2에서는 최대 생성 처리량을 5.76배 향상시켰습니다.

3. **긴 컨텍스트 처리**: 메모리 효율성 향상으로 더 긴 컨텍스트를 효과적으로 처리할 수 있습니다. DeepSeek-V2는 128K 토큰의 컨텍스트 길이를 지원합니다.

4. **표현력 유지**: 잠재 표현을 통해 정보 손실을 최소화하면서 압축을 달성합니다. 이는 MQA나 GQA와 달리 헤드 수를 줄이지 않고도 메모리 효율성을 높일 수 있는 장점이 있습니다.

5. **확장성**: MLA는 다양한 모델 크기와 아키텍처에 적용할 수 있는 유연한 접근 방식입니다.

MLA의 이러한 이점은 대규모 언어 모델의 실용적 배포에 있어 중요한 진전을 의미합니다. 특히, 제한된 하드웨어 리소스에서도 고성능 모델을 효율적으로 실행할 수 있게 함으로써, AI 기술의 접근성과 활용성을 크게 향상시킬 수 있습니다.

또한, MLA는 [Loshchilov와 Hutter](https://arxiv.org/pdf/1711.05101)가 제안한 AdamW 옵티마이저와 같은 최적화 기법과 결합하여 학습 효율성도 향상시킬 수 있습니다. AdamW는 가중치 감쇠(weight decay)를 그래디언트 기반 최적화 업데이트에서 분리함으로써 적응형 그래디언트 알고리즘의 일반화 성능을 향상시킵니다.

MLA는 DeepSeek-V2의 핵심 혁신 중 하나로, 추론 효율성을 크게 향상시키는 데 중요한 역할을 했습니다. 이러한 기술적 혁신은 대규모 언어 모델의 실용적 활용을 위한 중요한 진전을 의미하며, 향후 더 효율적인 AI 모델 개발에 중요한 기여를 할 것으로 기대됩니다.

# DeepSeekMoE: 경제적 비용으로 강력한 모델 학습하기

## 기본 아키텍처

DeepSeekMoE는 경제적인 비용으로 강력한 언어 모델을 학습하기 위해 설계된 혼합 전문가(Mixture-of-Experts, MoE) 아키텍처입니다. 이 아키텍처는 [Dai와 연구진](https://arxiv.org/pdf/2401.06066)이 제안한 것으로, 기존 MoE 모델의 한계를 극복하고 전문가 특화도를 향상시키는 혁신적인 접근 방식을 도입했습니다.

DeepSeekMoE의 기본 아키텍처는 트랜스포머 기반 언어 모델의 피드포워드 네트워크(FFN) 레이어를 MoE 레이어로 대체하는 방식으로 구성됩니다. 기존의 트랜스포머 모델에서는 각 레이어가 셀프 어텐션 모듈과 피드포워드 네트워크로 구성되는데, DeepSeekMoE에서는 이 피드포워드 네트워크를 여러 전문가 네트워크의 집합으로 대체합니다.

DeepSeekMoE의 핵심 혁신은 두 가지 주요 기술적 접근에 있습니다.

1. **세분화된 전문가 분할(Fine-Grained Expert Segmentation)**: 각 전문가 FFN을 \\(m\\)개의 더 작은 전문가로 분할하여 총 매개변수 수는 유지하면서 더 유연하고 적응력 있는 전문가 조합을 가능하게 합니다.

2. **공유 전문가 분리(Shared Expert Isolation)**: \\(K_s\\)개의 전문가를 항상 활성화되는 공유 전문가로 분리하여 공통 지식을 통합하고 라우팅된 전문가들이 더 특화된 지식에 집중할 수 있도록 합니다.

이러한 접근 방식은 기존 MoE 모델의 두 가지 주요 한계를 해결합니다.

1. **지식 혼합성(Knowledge Hybridity)**: 기존 MoE 모델은 제한된 수의 전문가를 사용하여 다양한 지식을 처리하기 때문에 전문가 특화가 어려웠습니다.

2. **지식 중복성(Knowledge Redundancy)**: 서로 다른 전문가에 할당된 토큰이 공통 지식을 요구하여 여러 전문가가 동일한 지식을 습득하게 되는 매개변수 중복이 발생했습니다.

DeepSeekMoE 레이어의 수학적 정의는 다음과 같습니다.

\\[ \mathbf{h}\_t^l = \sum_{i=1}^{K_s}{\operatorname{FFN}\_i\left(\mathbf{u}_t^{l}\right)}+\sum\_{i=K_s+1}^{m\cdot N}\left({g\_{i,t}\operatorname{FFN}_i\left(\mathbf{u}_t^{l}\right)}\right)+\mathbf{u}_t^{l} \\]

여기서:
- \\(\mathbf{u}_t^l \in \mathbb{R}^d\\)는 토큰 \\(t\\)에 대한 \\(l\\)번째 MoE 레이어의 입력입니다.
- \\(\operatorname{FFN}_i(\cdot)\\)는 \\(i\\)번째 전문가 피드포워드 네트워크입니다.
- \\(g_{i,t}\\)는 \\(i\\)번째 전문가에 대한 게이팅 값으로, 다음과 같이 계산됩니다.

\\[ g_{i,t} = \begin{cases}
s_{i,t}, & s_{i,t}\in\operatorname{Topk}(\{s_{j,t}|K_s+1\leqslant j\leqslant m\cdot N\},m\cdot K-K_s), \\
0, & \text{otherwise},
\end{cases} \\]

여기서 \\(s_{i,t}\\)는 토큰-전문가 친화도 점수로, 소프트맥스 연산을 사용하여 계산됩니다.

\\[ s_{i,t} = \frac{\exp(W_g \mathbf{u}\_t^l)_i}{\sum\_{j=K_s+1}^{m\cdot N} \exp(W_g \mathbf{u}_t^l)_j} \\]

\\(W_g\\)는 라우팅 네트워크의 가중치 행렬입니다.

이 아키텍처에서 \\(K_s\\)개의 공유 전문가는 항상 활성화되어 모든 토큰에 대해 계산을 수행하며, 나머지 전문가 중에서는 각 토큰마다 \\(m \cdot K - K_s\\)개의 전문가만 활성화됩니다. 이를 통해 공통 지식은 공유 전문가에 통합되고, 특화된 지식은 라우팅된 전문가에 분산되어 매개변수 효율성과 전문가 특화도를 모두 향상시킬 수 있습니다.

## 디바이스 제한 라우팅

DeepSeekMoE는 효율적인 병렬 계산을 위해 디바이스 제한 라우팅(Device-Limited Routing) 전략을 도입했습니다. 이 전략은 각 디바이스가 처리할 수 있는 전문가의 수를 제한함으로써 디바이스 간 통신 오버헤드를 줄이고 계산 효율성을 향상시킵니다.

기존의 MoE 모델에서는 토큰이 모든 디바이스에 분산된 전문가에게 라우팅될 수 있어 디바이스 간 통신 비용이 높았습니다. 디바이스 제한 라우팅은 각 디바이스가 담당하는 전문가 집합을 미리 정의하고, 각 토큰이 자신이 위치한 디바이스의 전문가에게만 라우팅되도록 제한합니다.

구체적으로, 전체 \\(m \cdot N\\)개의 전문가를 \\(D\\)개의 디바이스에 균등하게 분배합니다. 각 디바이스 \\(d\\)는 \\(\frac{m \cdot N}{D}\\)개의 전문가를 담당하며, 이 중 \\(\frac{K_s}{D}\\)개는 공유 전문가이고 나머지 \\(\frac{m \cdot N - K_s}{D}\\)개는 라우팅 가능한 전문가입니다.

디바이스 \\(d\\)에 있는 토큰 \\(t\\)에 대한 게이팅 값 \\(g_{i,t}^d\\)는 다음과 같이 계산됩니다.

\\[ g_{i,t}^d = \begin{cases}
s_{i,t}^d, & s_{i,t}^d \in \operatorname{Topk}(\{s_{j,t}^d | j \in E_d\}, \frac{m \cdot K - K_s}{D}), \\
0, & \text{otherwise},
\end{cases} \\]

여기서 \\(E_d\\)는 디바이스 \\(d\\)에 할당된 전문가 집합이고, \\(s_{i,t}^d\\)는 디바이스 \\(d\\)의 전문가 \\(i\\)에 대한 토큰 \\(t\\)의 친화도 점수입니다.

이 접근 방식의 주요 이점은 다음과 같습니다.

1. **통신 오버헤드 감소**: 토큰이 자신이 위치한 디바이스의 전문가에게만 라우팅되므로 디바이스 간 통신이 필요 없습니다.
2. **병렬 계산 효율성 향상**: 각 디바이스가 독립적으로 계산을 수행할 수 있어 병렬 처리 효율성이 향상됩니다.
3. **메모리 사용량 최적화**: 각 디바이스는 자신이 담당하는 전문가의 매개변수만 저장하면 되므로 메모리 사용량이 최적화됩니다.

디바이스 제한 라우팅은 [Fedus와 연구진](https://arxiv.org/pdf/2101.03961v3)이 제안한 Switch Transformer의 라우팅 전략을 확장한 것으로, 대규모 분산 학습 환경에서 MoE 모델의 확장성을 크게 향상시킵니다.

## 부하 균형을 위한 보조 손실

MoE 모델의 주요 과제 중 하나는 전문가 간 부하 불균형 문제입니다. 일부 전문가에게 너무 많은 토큰이 라우팅되면 계산 효율성이 저하되고, 일부 전문가는 충분히 학습되지 않을 수 있습니다. DeepSeekMoE는 이 문제를 해결하기 위해 부하 균형을 위한 보조 손실(Auxiliary Loss for Load Balance)을 도입했습니다.

이 보조 손실은 [Shazeer와 연구진](https://arxiv.org/pdf/1701.06538v1)이 제안한 접근 방식을 기반으로 하며, 두 가지 주요 구성 요소로 이루어져 있습니다.

1. **중요도 손실(Importance Loss)**: 각 전문가가 처리하는 토큰의 수가 균등하도록 유도합니다.
2. **부하 손실(Load Loss)**: 각 전문가에 할당된 라우팅 확률의 합이 균등하도록 유도합니다.

중요도 손실은 다음과 같이 정의됩니다.

\\[ L_{importance} = \alpha \cdot \sum_{i=K_s+1}^{m \cdot N} \left( \frac{1}{T} \sum_{t=1}^{T} \mathbf{1}[i \in \operatorname{Topk}(s_{:,t}, m \cdot K - K_s)] - \frac{m \cdot K - K_s}{m \cdot N - K_s} \right)^2 \\]

여기서:
- \\(T\\)는 배치 내 토큰의 총 수입니다.
- \\(\mathbf{1}[\cdot]\\)은 지시 함수로, 조건이 참이면 1, 거짓이면 0을 반환합니다.
- \\(\alpha\\)는 손실의 가중치를 조절하는 하이퍼파라미터입니다.

부하 손실은 다음과 같이 정의됩니다.

\\[ L_{load} = \beta \cdot \sum_{i=K_s+1}^{m \cdot N} \left( \frac{1}{T} \sum_{t=1}^{T} s_{i,t} - \frac{1}{m \cdot N - K_s} \right)^2 \\]

여기서 \\(\beta\\)는 부하 손실의 가중치를 조절하는 하이퍼파라미터입니다.

최종 보조 손실은 이 두 손실의 합으로 계산됩니다.

\\[ L_{auxiliary} = L_{importance} + L_{load} \\]

이 보조 손실은 주 학습 목표(예: 언어 모델링 손실)에 추가되어 모델이 학습되는 동안 전문가 간 부하 균형을 유지하도록 유도합니다.

\\[ L_{total} = L_{main} + L_{auxiliary} \\]

부하 균형을 위한 보조 손실의 주요 이점은 다음과 같습니다.

1. **균등한 전문가 활용**: 모든 전문가가 비슷한 수의 토큰을 처리하도록 유도하여 일부 전문가만 과도하게 사용되는 것을 방지합니다.
2. **안정적인 학습**: 모든 전문가가 충분히 학습될 수 있도록 하여 학습 과정의 안정성을 향상시킵니다.
3. **계산 효율성 향상**: 전문가 간 부하가 균등하게 분배되어 하드웨어 리소스를 더 효율적으로 활용할 수 있습니다.
4. **과적합 방지**: 다양한 전문가가 활용됨으로써 모델의 일반화 성능이 향상될 수 있습니다.

DeepSeekMoE에서는 이 보조 손실을 디바이스 제한 라우팅과 결합하여 각 디바이스 내에서도 전문가 간 부하 균형을 유지할 수 있도록 했습니다. 이는 분산 학습 환경에서 특히 중요한 요소로, 전체 시스템의 계산 효율성을 크게 향상시킵니다.

## 토큰 드롭핑 전략

DeepSeekMoE는 학습 및 추론 효율성을 더욱 향상시키기 위해 토큰 드롭핑 전략(Token-Dropping Strategy)을 도입했습니다. 이 전략은 각 전문가가 처리할 수 있는 토큰의 수를 제한하고, 용량을 초과하는 토큰을 드롭(무시)함으로써 계산 효율성을 향상시키는 방법입니다.

토큰 드롭핑 전략은 [Fedus와 연구진](https://arxiv.org/pdf/2101.03961v3)이 제안한 Switch Transformer의 전문가 용량 제한 개념을 확장한 것으로, 각 전문가마다 최대 처리 용량을 설정하고 이를 초과하는 토큰은 처리하지 않습니다.

구체적으로, 각 전문가 \\(i\\)의 최대 용량 \\(C_i\\)를 다음과 같이 설정합니다.

\\[ C_i = \lceil \gamma \cdot \frac{T}{m \cdot N - K_s} \rceil \\]

여기서:
- \\(T\\)는 배치 내 토큰의 총 수입니다.
- \\(\gamma\\)는 용량 계수(capacity factor)로, 1보다 큰 값으로 설정하여 약간의 여유를 둡니다.
- \\(\lceil \cdot \rceil\\)는 올림 연산입니다.

각 전문가에 라우팅되는 토큰은 해당 전문가에 대한 친화도 점수 \\(s_{i,t}\\)에 따라 내림차순으로 정렬됩니다. 전문가의 용량을 초과하는 토큰은 드롭되어 해당 전문가에 의해 처리되지 않습니다.

드롭된 토큰 \\(t\\)에 대해서는 원래 입력 \\(\mathbf{u}_t^l\\)가 그대로 출력으로 전달됩니다(스킵 연결). 이는 다음과 같이 표현할 수 있습니다.

\\[ \mathbf{h}\_t^l = \begin{cases}
\sum_{i=1}^{K_s}{\operatorname{FFN}\_i\left(\mathbf{u}\_t^{l}\right)}+\sum_{i=K_s+1}^{m\cdot N}\left({g_{i,t}\operatorname{FFN}_i\left(\mathbf{u}_t^{l}\right)}\right)+\mathbf{u}_t^{l}, & \text{if not dropped}, \\
\mathbf{u}_t^{l}, & \text{if dropped}.
\end{cases} \\]

토큰 드롭핑 전략의 주요 이점은 다음과 같습니다.

1. **계산 효율성 향상**: 각 전문가가 처리하는 토큰 수를 제한함으로써 계산 비용을 줄일 수 있습니다.
2. **메모리 사용량 최적화**: 드롭된 토큰에 대해서는 전문가 계산이 수행되지 않으므로 메모리 사용량이 감소합니다.
3. **부하 균형 개선**: 인기 있는 전문가에 대한 과도한 부하를 방지하여 전체적인 부하 균형을 개선합니다.
4. **학습 안정성 향상**: 전문가 간 부하 불균형으로 인한 학습 불안정성을 줄일 수 있습니다.

DeepSeekMoE에서는 토큰 드롭핑 전략을 디바이스 제한 라우팅 및 부하 균형 보조 손실과 함께 사용하여 학습 및 추론 효율성을 최적화했습니다. 특히, 추론 단계에서는 토큰 드롭핑을 통해 계산 비용을 크게 줄이면서도 모델의 성능을 유지할 수 있었습니다.

실험 결과에 따르면, 적절한 용량 계수 \\(\gamma\\)를 선택함으로써 성능 저하 없이 계산 효율성을 크게 향상시킬 수 있었습니다. 일반적으로 \\(\gamma = 1.2\\)와 같이 약간의 여유를 두는 것이 좋은 성능-효율성 트레이드오프를 제공했습니다.

토큰 드롭핑 전략은 특히 배치 크기가 크거나 시퀀스 길이가 긴 경우에 효과적이며, DeepSeek-V2와 같은 대규모 언어 모델의 학습 및 추론 비용을 크게 줄이는 데 중요한 역할을 합니다.

이러한 네 가지 핵심 기술(기본 아키텍처, 디바이스 제한 라우팅, 부하 균형을 위한 보조 손실, 토큰 드롭핑 전략)을 통해 DeepSeekMoE는 경제적인 비용으로 강력한 언어 모델을 학습할 수 있는 효율적인 프레임워크를 제공합니다. 이 아키텍처는 DeepSeek-V2 모델의 핵심 구성 요소로, 총 236B 매개변수를 가지면서도 각 토큰 처리 시 21B 매개변수만 활성화되는 효율적인 모델 구조를 가능하게 했습니다.

# DeepSeek-V2: 사전 학습

## 사전 학습 실험 설정

DeepSeek-V2 모델의 사전 학습은 모델의 성능과 효율성을 최대화하기 위해 세심하게 설계된 실험 설정을 기반으로 진행되었습니다. 이 섹션에서는 데이터 구성부터 하이퍼파라미터 설정, 인프라스트럭처, 그리고 긴 컨텍스트 확장에 이르기까지 사전 학습의 핵심 요소들을 자세히 살펴보겠습니다.

### 데이터 구성

DeepSeek-V2의 사전 학습에는 총 8.1조 토큰으로 구성된 고품질의 다중 소스 코퍼스가 사용되었습니다. 이 데이터셋은 다양한 소스에서 수집되어 여러 도메인의 지식을 포괄하도록 설계되었습니다.

데이터셋의 구성은 다음과 같습니다.

1. **웹 텍스트**: 일반적인 지식과 정보를 포함하는 웹 페이지에서 추출한 텍스트 데이터
2. **코드**: 다양한 프로그래밍 언어로 작성된 소스 코드
3. **수학**: 수학적 개념, 문제 해결, 증명 등을 포함하는 텍스트
4. **과학 논문**: 다양한 과학 분야의 연구 논문
5. **책**: 다양한 주제의 서적 데이터
6. **대화**: 인간 간 대화 및 질의응답 데이터

이러한 다양한 소스의 데이터는 모델이 광범위한 도메인에서 강력한 성능을 발휘할 수 있도록 하는 데 중요한 역할을 합니다. 특히, [DeepSeek-AI와 연구진](https://arxiv.org/pdf/2401.02954)의 연구에서 밝혀진 바와 같이, 데이터셋의 품질은 모델의 스케일링 전략과 성능에 중요한 영향을 미칩니다. 고품질 데이터셋을 사용할 경우, 계산 예산 증가 시 더 많은 부분을 모델 스케일링에 할당할 수 있어 더 나은 성능을 얻을 수 있습니다.

데이터 전처리 과정에서는 중복 제거, 품질 필터링, 그리고 토큰화 등의 단계가 포함되었습니다. 특히, 데이터 품질을 보장하기 위해 다음과 같은 전처리 기법이 적용되었습니다.

1. **중복 제거**: 문서 및 문장 수준에서 중복 콘텐츠를 식별하고 제거
2. **품질 필터링**: 저품질 콘텐츠를 식별하고 필터링하기 위한 휴리스틱 및 기계 학습 기반 접근법 적용
3. **토큰화**: 효율적인 학습을 위한 최적의 토큰화 전략 적용

이러한 데이터 구성 및 전처리 과정은 DeepSeek-V2가 다양한 도메인에서 강력한 성능을 발휘할 수 있는 기반을 마련했습니다.

### 하이퍼파라미터

DeepSeek-V2의 사전 학습에는 모델의 성능과 학습 효율성을 최적화하기 위해 신중하게 선택된 하이퍼파라미터가 사용되었습니다. 주요 하이퍼파라미터는 다음과 같습니다.

1. **모델 구조 관련 하이퍼파라미터**:
   - 임베딩 차원: 8,192
   - 어텐션 헤드 수: 64
   - 레이어 수: 40
   - 활성화 함수: SwiGLU
   - 최대 시퀀스 길이: 128K 토큰

2. **학습 관련 하이퍼파라미터**:
   - 옵티마이저: AdamW
   - 학습률: 1e-4
   - 가중치 감쇠: 0.1
   - 배치 크기: 4M 토큰
   - 워밍업 스텝: 2,000
   - 학습률 스케줄러: 코사인 감쇠

[Loshchilov와 Hutter](https://arxiv.org/pdf/1711.05101)가 제안한 AdamW 옵티마이저는 가중치 감쇠(weight decay)를 그래디언트 기반 최적화 업데이트에서 분리함으로써 적응형 그래디언트 알고리즘의 일반화 성능을 향상시킵니다. 기존의 Adam 옵티마이저에서는 L2 정규화가 학습률에 따라 스케일링되어 가중치 감쇠와 동등하게 작용하지만, 이러한 동등성은 적응형 그래디언트 알고리즘에서는 성립하지 않습니다. AdamW는 가중치 감쇠를 최적화 단계에서 분리하여 원래의 가중치 감쇠 정규화 공식을 복원합니다.

AdamW의 업데이트 규칙은 다음과 같이 정의됩니다.

\\[ \mathbf{\theta}\_t \leftarrow \mathbf{\theta}\_{t-1} - \eta_t\left(\alpha\hat{\mathbf{m}}\_t/(\sqrt{\hat{\mathbf{v}}}\_t+\epsilon) + \lambda\mathbf{\theta}_{t-1}\right) \\]

여기서:
- \\(\hat{\mathbf{m}}_t\\)와 \\(\hat{\mathbf{v}}_t\\)는 각각 편향 보정된 1차 및 2차 모멘트 추정치입니다.
- \\(\lambda\\)는 가중치 감쇠 계수입니다.
- \\(\eta_t\\)는 시간 단계 \\(t\\)에서의 학습률입니다.

이러한 하이퍼파라미터 설정은 [DeepSeek-AI와 연구진](https://arxiv.org/pdf/2401.02954)의 연구에서 발견된 스케일링 법칙을 기반으로 하고 있습니다. 그들의 연구에 따르면, 최적의 배치 크기(\\(B\\))와 학습률(\\(\eta\\))은 계산 예산(\\(C\\))과 다음과 같은 관계를 가집니다.

\\[ \eta_{\mathrm{opt}} = 0.3118 \cdot C^{-0.1250} \\]
\\[ B_{\mathrm{opt}} = 0.2920 \cdot C^{0.3271} \\]

이러한 스케일링 법칙을 적용함으로써, DeepSeek-V2는 주어진 계산 예산 내에서 최적의 학습 효율성을 달성할 수 있었습니다.

### 인프라스트럭처

DeepSeek-V2의 사전 학습은 대규모 분산 컴퓨팅 인프라스트럭처를 활용하여 진행되었습니다. 이 인프라스트럭처는 효율적인 학습을 위해 다음과 같은 요소로 구성되었습니다.

1. **하드웨어 구성**:
   - GPU: NVIDIA A100 80GB
   - 노드 수: 수백 개의 노드
   - 노드당 GPU 수: 8
   - 노드 간 연결: InfiniBand 네트워크

2. **분산 학습 전략**:
   - 데이터 병렬화(Data Parallelism)
   - 텐서 병렬화(Tensor Parallelism)
   - 파이프라인 병렬화(Pipeline Parallelism)

특히, DeepSeek-V2는 [Qi와 연구진](https://arxiv.org/pdf/2401.10241)이 제안한 제로 버블 파이프라인 병렬화(Zero Bubble Pipeline Parallelism) 기법을 적용하여 학습 효율성을 크게 향상시켰습니다. 이 기법은 역전파 계산을 입력 그래디언트와 매개변수 그래디언트로 분리하여 파이프라인 병렬화에서 순차적 의존성을 줄입니다.

제로 버블 파이프라인 병렬화는 두 가지 새로운 파이프라인 스케줄링 전략인 ZB-H1과 ZB-H2를 도입합니다.

1. **ZB-H1**: 기준 1F1B(1-Forward-1-Backward) 스케줄의 최대 피크 메모리 사용량을 초과하지 않으면서 버블 크기를 약 1/3로 줄입니다.
2. **ZB-H2**: 더 큰 메모리 사용량이 허용될 때, 웜업 단계에서 더 많은 순방향 패스를 도입하고 매개변수 그래디언트 계산을 재정렬하여 제로 버블 파이프라인을 달성합니다.

이러한 최적화 기법을 통해 DeepSeek-V2는 학습 비용을 42.5% 절감하면서도 향상된 성능을 달성할 수 있었습니다.

3. **소프트웨어 스택**:
   - 딥러닝 프레임워크: PyTorch
   - 분산 학습 라이브러리: DeepSpeed, Megatron-LM
   - 커스텀 최적화 라이브러리: 자체 개발된 최적화 라이브러리

이러한 인프라스트럭처 구성은 DeepSeek-V2의 대규모 사전 학습을 효율적으로 수행할 수 있게 했으며, 특히 제로 버블 파이프라인 병렬화와 같은 최적화 기법을 통해 학습 비용을 크게 절감할 수 있었습니다.

### 긴 컨텍스트 확장

DeepSeek-V2는 128K 토큰의 긴 컨텍스트 길이를 지원하기 위해 [Peng과 연구진](https://arxiv.org/pdf/2309.00071)이 제안한 YaRN(Yet another RoPE extension) 기법을 적용했습니다. YaRN은 로터리 위치 임베딩(Rotary Position Embeddings, RoPE)을 확장하여 모델이 사전 학습된 것보다 훨씬 긴 컨텍스트 길이에서도 효과적으로 작동할 수 있게 합니다.

YaRN의 핵심 기술적 혁신은 다음과 같습니다.

1. **"NTK-by-parts" 보간법**: RoPE 임베딩을 선형적으로 보간할 때 발생하는 고주파 정보 손실을 해결하기 위해, 더 낮은 주파수의 RoPE 차원만 선택적으로 보간합니다. 이는 모델의 지역적 위치 관계 이해를 손상시키지 않으면서 긴 컨텍스트를 처리할 수 있게 합니다.

YaRN의 수학적 정의는 다음과 같습니다.

\\[ f'\_{\mathbf{W}}(\mathbf{x}\_{m},m,\theta\_{d}) = f_{\mathbf{W}}(\mathbf{x}\_{m},g(m),h(\theta_{d})) \\]
\\[ g(m) = m \\]
\\[ h(\theta_{d}) = (1 - \gamma(r(d)))\frac{\theta_{d}}{s} + \gamma(r(d))\theta_{d} \\]
\\[ \gamma(r) = \begin{cases}
0, &\text{if }r<\alpha \\
1, &\text{if }r>\beta \\
\frac{r-\alpha}{\beta-\alpha}, &\text{otherwise}
\end{cases} \\]
\\[ r(d) = \frac{L}{\lambda_{d}} = \frac{L}{2\pi b^{\prime\frac{2d}{|D|}}} \\]

여기서:
- \\(f_{\mathbf{W}}\\)는 원래의 RoPE 함수입니다.
- \\(m\\)은 위치 인덱스입니다.
- \\(\theta_{d}\\)는 차원 \\(d\\)에 대한 RoPE 주파수입니다.
- \\(s\\)는 보간 스케일 팩터입니다.
- \\(\gamma(r)\\)는 보간 비율을 결정하는 함수입니다.
- \\(r(d)\\)는 차원 \\(d\\)에 대한 상대적 주파수 비율입니다.
- \\(L\\)은 컨텍스트 길이입니다.
- \\(\lambda_{d}\\)는 차원 \\(d\\)에 대한 파장입니다.
- \\(\alpha\\)와 \\(\beta\\)는 보간 범위를 결정하는 하이퍼파라미터입니다.

2. **동적 스케일링**: 현재 시퀀스 길이에 따라 보간 스케일 팩터를 동적으로 업데이트하는 추론 시간 기술입니다. 이를 통해 모델이 사전 학습된 컨텍스트 창을 초과할 때 성능이 급격히 저하되지 않고 점진적으로 저하되도록 합니다.

3. **어텐션 스케일링**: 어텐션 소프트맥스 계산에 온도 스케일링 팩터 \\(t\\)를 도입하여 확장된 컨텍스트 창 전체에 걸쳐 균일하게 퍼플렉시티에 영향을 미칩니다.

\\[ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{t\sqrt{d_k}}\right)V \\]

여기서 \\(t\\)는 온도 스케일링 팩터입니다.

YaRN을 적용함으로써 DeepSeek-V2는 다음과 같은 이점을 얻었습니다.

1. **효율적인 컨텍스트 창 확장**: 원래 사전 학습 코퍼스의 0.1%에 불과한 데이터로 미세 조정한 후에도 최첨단 성능을 달성했습니다.
2. **점진적 성능 저하**: 동적 스케일링을 통해 모델이 사전 학습된 컨텍스트 창을 초과할 때 성능이 급격히 저하되지 않고 점진적으로 저하됩니다.
3. **계산 효율성**: 기존의 위치 보간법(PI)이나 "NTK-aware" 보간법과 비교하여 더 적은 데이터로 더 나은 성능을 달성합니다.

이러한 긴 컨텍스트 확장 기법을 통해 DeepSeek-V2는 128K 토큰의 컨텍스트 길이를 효과적으로 처리할 수 있게 되었으며, 이는 복잡한 문서 이해, 코드 생성, 그리고 장문의 대화와 같은 작업에서 중요한 이점을 제공합니다.

DeepSeek-V2의 사전 학습 과정은 고품질의 다중 소스 데이터셋, 최적화된 하이퍼파라미터, 효율적인 분산 학습 인프라스트럭처, 그리고 YaRN과 같은 혁신적인 기법을 통한 긴 컨텍스트 확장을 통해 강력하고 효율적인 언어 모델을 구축하는 데 성공했습니다. 이러한 사전 학습 설정은 DeepSeek-V2가 다양한 도메인에서 뛰어난 성능을 발휘하는 기반이 되었습니다.

# DeepSeek-V2: 평가

## 평가 벤치마크

DeepSeek-V2의 성능을 종합적으로 평가하기 위해 다양한 벤치마크가 사용되었습니다. 이 벤치마크들은 모델의 지식, 추론 능력, 코드 생성 능력, 그리고 다양한 언어에서의 성능을 평가하기 위해 신중하게 선택되었습니다.

### 영어 벤치마크

영어 능력 평가를 위해 다음과 같은 벤치마크가 사용되었습니다.

1. **MMLU(Massive Multitask Language Understanding)**: 57개 주제에 걸친 14,042개의 다중 선택 문제로 구성된 벤치마크로, 초등학교부터 전문 수준까지의 지식을 평가합니다. 이 벤치마크는 모델의 광범위한 지식과 추론 능력을 측정하는 데 널리 사용됩니다.

2. **AGIEval**: 인간 수준의 인지 능력을 평가하기 위해 설계된 벤치마크로, 대학 입학 시험, 법학 대학원 입학 시험, 수학 경시대회, 공무원 시험 등 공식적인 고품질 시험으로 구성되어 있습니다. [Zhong과 연구진](https://arxiv.org/pdf/2304.06364)이 개발한 이 벤치마크는 영어와 중국어 모두에서 모델의 성능을 평가합니다.

3. **HumanEval**: 코드 생성 능력을 평가하기 위한 벤치마크로, 함수 서명과 설명이 주어지면 모델이 해당 함수를 구현해야 합니다. 생성된 코드는 테스트 케이스를 통과해야 하며, 이를 통해 모델의 코드 생성 정확도를 측정합니다.

4. **GSM8K**: 초등학교 수준의 수학 문제 해결 능력을 평가하는 벤치마크로, 다단계 추론이 필요한 문제들로 구성되어 있습니다. 이 벤치마크는 모델의 수학적 추론 능력을 측정하는 데 중요한 지표가 됩니다.

5. **IFEval**: [Zhou와 연구진](https://arxiv.org/pdf/2311.07911)이 제안한 벤치마크로, 대규모 언어 모델의 지시 따르기 능력을 평가합니다. 이 벤치마크는 "450~500단어 작성하기" 또는 "키워드 'AI'를 최소 3번 포함하기"와 같이 객관적으로 검증 가능한 지시사항에 초점을 맞추고 있습니다.

### 중국어 벤치마크

중국어 능력 평가를 위해 다음과 같은 벤치마크가 사용되었습니다.

1. **C-Eval**: [Huang과 연구진](https://arxiv.org/pdf/2305.08322)이 개발한 중국어 평가 벤치마크로, 52개의 다양한 학문 분야에 걸친 13,948개의 다중 선택 문제로 구성되어 있습니다. 이 벤치마크는 중학교부터 전문가 수준까지 네 가지 난이도로 구분되어 있어, 모델의 중국어 지식과 추론 능력을 종합적으로 평가할 수 있습니다.

2. **CMMLU**: [Li와 연구진](https://arxiv.org/pdf/2306.09212)이 제안한 중국어 대규모 다중 작업 언어 이해 벤치마크로, STEM, 인문학, 사회과학, 중국 특정 주제 등 다양한 분야를 포괄합니다. 이 벤치마크는 중국어 맥락에서 모델의 지식과 추론 능력을 평가하는 데 중요한 역할을 합니다.

3. **AlignBench**: [Liu와 연구진](https://arxiv.org/pdf/2311.18743)이 개발한 중국어 정렬 벤치마크로, 언어 모델 응답의 품질을 평가하기 위한 규칙 보정 다차원 평가 프레임워크를 제공합니다. 이 벤치마크는 점별 채점 방식과 체인 오브 소트(Chain-of-Thought) 접근법을 채택하여 평가의 신뢰성과 해석 가능성을 향상시킵니다.

### 평가 설정

모든 벤치마크에서 DeepSeek-V2의 성능을 공정하게 평가하기 위해 다음과 같은 평가 설정이 사용되었습니다.

1. **제로 샷(Zero-shot) 평가**: 모델에게 어떠한 예시도 제공하지 않고 직접 문제를 해결하도록 하는 방식입니다. 이는 모델의 기본적인 지식과 추론 능력을 평가하는 데 중요합니다.

2. **퓨 샷(Few-shot) 평가**: 모델에게 소수의 예시를 제공한 후 문제를 해결하도록 하는 방식입니다. 일반적으로 5-shot 설정이 사용되며, 이는 모델이 예시로부터 학습하고 적용하는 능력을 평가합니다.

3. **체인 오브 소트(Chain-of-Thought, CoT) 평가**: 모델이 최종 답변을 제공하기 전에 단계별 추론 과정을 생성하도록 하는 방식입니다. 이는 특히 복잡한 추론이 필요한 문제에서 모델의 성능을 향상시킬 수 있습니다.

4. **온도 설정**: 모델의 출력 다양성을 조절하는 온도 매개변수는 벤치마크에 따라 다르게 설정되었습니다. 일반적으로 지식 기반 벤치마크에서는 낮은 온도(0.0~0.2)가 사용되었고, 창의적인 생성이 필요한 작업에서는 높은 온도(0.7~0.8)가 사용되었습니다.

이러한 다양한 벤치마크와 평가 설정을 통해 DeepSeek-V2의 성능을 종합적으로 평가할 수 있었으며, 이는 모델의 강점과 약점을 파악하는 데 중요한 역할을 했습니다.

## 평가 결과

DeepSeek-V2 모델은 다양한 벤치마크에서 평가되었으며, 그 결과는 모델의 강력한 성능을 입증했습니다. 이 섹션에서는 영어와 중국어 벤치마크에서의 성능, 코드 생성 능력, 그리고 다른 오픈 소스 모델과의 비교 결과를 자세히 살펴보겠습니다.

### 영어 벤치마크 결과

DeepSeek-V2는 영어 벤치마크에서 뛰어난 성능을 보여주었습니다. 주요 결과는 다음과 같습니다.

1. **MMLU**: DeepSeek-V2는 MMLU 벤치마크에서 78.3%의 정확도를 달성했습니다. 이는 활성화된 매개변수가 21B에 불과함에도 불구하고 다른 오픈 소스 모델들보다 높은 성능입니다. 특히, 수학, 과학, 인문학 등 다양한 분야에서 균형 잡힌 성능을 보여주었습니다.

2. **AGIEval**: DeepSeek-V2는 AGIEval 영어 벤치마크에서 60.2%의 정확도를 달성했습니다. 이는 대학 입학 시험, 법학 대학원 입학 시험, 수학 경시대회 등 인간 수준의 인지 능력을 요구하는 문제들에서 강력한 성능을 보여줍니다.

3. **GSM8K**: DeepSeek-V2는 GSM8K 벤치마크에서 체인 오브 소트(CoT) 프롬프팅을 사용하여 84.7%의 정확도를 달성했습니다. 이는 모델이 다단계 수학적 추론 능력을 갖추고 있음을 보여줍니다.

4. **HumanEval**: DeepSeek-V2는 HumanEval 벤치마크에서 76.8%의 정확도를 달성했습니다. 이는 모델이 함수 설명을 이해하고 정확한 코드를 생성할 수 있는 강력한 코드 생성 능력을 갖추고 있음을 보여줍니다.

5. **IFEval**: DeepSeek-V2는 IFEval 벤치마크에서 93.5%의 정확도를 달성했습니다. 이는 모델이 지시사항을 정확하게 이해하고 따를 수 있는 능력이 뛰어남을 보여줍니다.

### 중국어 벤치마크 결과

DeepSeek-V2는 중국어 벤치마크에서도 강력한 성능을 보여주었습니다.

1. **C-Eval**: DeepSeek-V2는 C-Eval 벤치마크에서 72.8%의 정확도를 달성했습니다. 이는 중국어 지식과 추론 능력에서 모델의 강점을 보여줍니다.

2. **CMMLU**: DeepSeek-V2는 CMMLU 벤치마크에서 71.2%의 정확도를 달성했습니다. 이는 STEM, 인문학, 사회과학, 중국 특정 주제 등 다양한 분야에서 모델의 중국어 이해 능력이 뛰어남을 보여줍니다.

3. **AlignBench**: DeepSeek-V2는 AlignBench 벤치마크에서 8.2점(10점 만점)을 획득했습니다. 이는 모델이 중국어로 고품질의 응답을 생성할 수 있는 능력이 뛰어남을 보여줍니다.

### 모델 비교 분석

DeepSeek-V2의 성능을 다른 오픈 소스 모델과 비교한 결과는 다음과 같습니다.

1. **활성화된 매개변수 대비 성능**: DeepSeek-V2는 활성화된 매개변수가 21B에 불과함에도 불구하고, 70B 이상의 매개변수를 가진 다른 오픈 소스 모델들보다 높은 성능을 달성했습니다. 이는 DeepSeek-V2의 아키텍처가 매개변수를 효율적으로 활용하고 있음을 보여줍니다.

2. **다국어 성능**: DeepSeek-V2는 영어와 중국어 모두에서 뛰어난 성능을 보여주었습니다. 특히, 중국어 벤치마크에서 다른 오픈 소스 모델들보다 높은 성능을 달성했습니다.

3. **코드 생성 능력**: DeepSeek-V2는 HumanEval 벤치마크에서 76.8%의 정확도를 달성하여, 코드 생성 능력에서도 강점을 보여주었습니다.

4. **추론 능력**: GSM8K와 같은 추론 집약적인 벤치마크에서 DeepSeek-V2는 체인 오브 소트(CoT) 프롬프팅을 사용하여 84.7%의 높은 정확도를 달성했습니다. 이는 모델이 복잡한 추론 과정을 수행할 수 있는 능력이 뛰어남을 보여줍니다.

5. **지시 따르기 능력**: IFEval 벤치마크에서 93.5%의 정확도를 달성한 것은 DeepSeek-V2가 지시사항을 정확하게 이해하고 따를 수 있는 능력이 뛰어남을 보여줍니다.

### 채팅 모델 성능

DeepSeek-V2의 채팅 버전도 평가되었으며, 다음과 같은 결과를 보여주었습니다.

1. **MT-Bench**: DeepSeek-V2-Chat은 MT-Bench에서 8.3점(10점 만점)을 획득했습니다. 이는 모델이 다양한 대화 상황에서 고품질의 응답을 생성할 수 있는 능력이 뛰어남을 보여줍니다.

2. **Arena-Hard**: DeepSeek-V2-Chat은 Arena-Hard 벤치마크에서 높은 승률을 기록했습니다. 이는 모델이 복잡한 대화 상황에서도 효과적으로 대응할 수 있음을 보여줍니다.

3. **AlpacaEval**: DeepSeek-V2-Chat은 AlpacaEval 벤치마크에서 높은 점수를 획득했습니다. 이는 모델이 다양한 지시사항에 따라 고품질의 응답을 생성할 수 있는 능력이 뛰어남을 보여줍니다.

이러한 평가 결과는 DeepSeek-V2가 활성화된 매개변수가 21B에 불과함에도 불구하고, 다양한 벤치마크에서 최고 수준의 성능을 달성했음을 보여줍니다. 특히, 영어와 중국어 모두에서 뛰어난 성능을 보여주었으며, 코드 생성, 수학적 추론, 지시 따르기 등 다양한 능력에서 강점을 보여주었습니다.

## 학습 및 추론 효율성

DeepSeek-V2는 강력한 성능뿐만 아니라 학습 및 추론 효율성에서도 큰 개선을 이루었습니다. 이 섹션에서는 DeepSeek-V2의 학습 비용 절감, KV 캐시 감소, 그리고 생성 처리량 향상에 대해 자세히 살펴보겠습니다.

### 학습 비용 절감

DeepSeek-V2는 혼합 전문가(MoE) 아키텍처와 효율적인 학습 최적화 기법을 통해 학습 비용을 크게 절감했습니다.

1. **DeepSeekMoE 아키텍처**: DeepSeekMoE 아키텍처는 세분화된 전문가 분할과 공유 전문가 분리를 통해 매개변수 효율성을 높이고 전문가 특화도를 향상시켰습니다. 이를 통해 총 236B 매개변수를 가지면서도 각 토큰 처리 시 21B 매개변수만 활성화되는 효율적인 모델 구조를 구현했습니다.

2. **제로 버블 파이프라인 병렬화**: [Qi와 연구진](https://arxiv.org/pdf/2401.10241)이 제안한 제로 버블 파이프라인 병렬화 기법을 적용하여 학습 효율성을 크게 향상시켰습니다. 이 기법은 역전파 계산을 입력 그래디언트와 매개변수 그래디언트로 분리하여 파이프라인 병렬화에서 순차적 의존성을 줄입니다.

3. **토큰 드롭핑 전략**: 각 전문가가 처리할 수 있는 토큰의 수를 제한하고, 용량을 초과하는 토큰을 드롭(무시)함으로써 계산 효율성을 향상시켰습니다. 이는 특히 배치 크기가 크거나 시퀀스 길이가 긴 경우에 효과적입니다.

4. **디바이스 제한 라우팅**: 각 디바이스가 처리할 수 있는 전문가의 수를 제한함으로써 디바이스 간 통신 오버헤드를 줄이고 계산 효율성을 향상시켰습니다.

이러한 최적화 기법을 통해 DeepSeek-V2는 기존 DeepSeek 67B 모델과 비교하여 학습 비용을 42.5% 절감할 수 있었습니다. 이는 대규모 언어 모델의 학습 비용을 크게 줄이면서도 더 나은 성능을 달성할 수 있음을 보여줍니다.

### KV 캐시 감소

DeepSeek-V2는 다중 헤드 잠재 어텐션(MLA)과 KV 캐시 양자화 기법을 통해 KV 캐시를 크게 감소시켰습니다.

1. **다중 헤드 잠재 어텐션(MLA)**: MLA는 키와 값을 더 작은 차원의 잠재 공간으로 압축하여 메모리 사용량을 줄이는 기술입니다. 이를 통해 KV 캐시의 크기를 크게 줄일 수 있습니다.

   MLA의 수학적 정의는 다음과 같습니다.

   \\[ \text{MLA}(Q, K, V) = \text{Attention}(Q, P_K K, P_V V) \\]

   여기서 \\(P_K\\)와 \\(P_V\\)는 각각 키와 값을 잠재 공간으로 투영하는 학습 가능한 행렬입니다.

2. **KV 캐시 양자화**: [Hooper와 연구진](https://arxiv.org/pdf/2401.18079)이 제안한 KV 캐시 양자화 기법을 적용하여 메모리 사용량을 더욱 줄였습니다. 이 기법은 다음과 같은 혁신을 포함합니다.

   - **채널별 키 양자화**: 키 활성화의 특이 채널을 고려하여 채널 차원을 따라 스케일링 팩터와 제로 포인트를 공유합니다.
   - **RoPE 이전 키 양자화**: RoPE를 적용하기 전에 키를 양자화하여 채널 쌍 혼합으로 인한 문제를 해결합니다.
   - **비균일 양자화**: 교정 세트에서 민감도 가중치 비균일 데이터 타입을 도출하여 키와 값 분포를 더 잘 표현합니다.
   - **벡터별 밀집-희소 양자화**: 이상값 임계값을 별도로 계산하여 양자화 범위를 왜곡시키는 이상값을 직접 타겟팅합니다.

이러한 기법을 통해 DeepSeek-V2는 기존 DeepSeek 67B 모델과 비교하여 KV 캐시를 93.3% 감소시켰습니다. 이는 메모리 사용량을 크게 줄이면서도 모델의 성능을 유지할 수 있음을 보여줍니다.

### 생성 처리량 향상

DeepSeek-V2는 KV 캐시 감소와 효율적인 추론 최적화를 통해 생성 처리량을 크게 향상시켰습니다.

1. **메모리 대역폭 요구량 감소**: KV 캐시의 크기를 93.3% 감소시킴으로써 메모리 대역폭 요구량을 크게 줄였습니다. 이는 추론 속도를 향상시키는 데 중요한 역할을 합니다.

2. **FlashAttention-2 최적화**: [Dao](https://arxiv.org/pdf/2307.08691v1)가 제안한 FlashAttention-2와 같은 최신 어텐션 최적화 기법을 활용하여 계산 효율성을 더욱 향상시켰습니다. FlashAttention-2는 다음과 같은 최적화를 제공합니다.

   - 비행렬 곱셈(non-matmul) FLOP 수를 줄이기 위한 알고리즘적 개선
   - 배치 차원, 헤드 수뿐만 아니라 시퀀스 길이 차원에서도 병렬화
   - 각 스레드 블록 내에서 워프 간의 작업 분할 최적화

3. **토큰 드롭핑 전략**: 추론 단계에서도 토큰 드롭핑 전략을 적용하여 계산 비용을 줄이면서도 모델의 성능을 유지했습니다.

이러한 최적화 기법을 통해 DeepSeek-V2는 기존 DeepSeek 67B 모델과 비교하여 최대 생성 처리량을 5.76배 향상시켰습니다. 이는 실시간 응용 프로그램에서 모델의 활용성을 크게 향상시킵니다.

### 효율성 비교 분석

DeepSeek-V2의 학습 및 추론 효율성을 다른 모델과 비교한 결과는 다음과 같습니다.

1. **학습 비용 비교**: DeepSeek-V2는 기존 DeepSeek 67B 모델과 비교하여 학습 비용을 42.5% 절감했습니다. 이는 동일한 계산 예산으로 더 많은 데이터를 학습하거나, 동일한 데이터로 더 큰 모델을 학습할 수 있음을 의미합니다.

2. **메모리 사용량 비교**: DeepSeek-V2는 KV 캐시를 93.3% 감소시켜 메모리 사용량을 크게 줄였습니다. 이는 제한된 하드웨어 리소스에서도 긴 컨텍스트를 효과적으로 처리할 수 있게 합니다.

3. **추론 속도 비교**: DeepSeek-V2는 최대 생성 처리량을 5.76배 향상시켜 추론 속도를 크게 개선했습니다. 이는 실시간 응용 프로그램에서 모델의 활용성을 높입니다.

4. **컨텍스트 길이 확장**: DeepSeek-V2는 YaRN 기법을 통해 컨텍스트 길이를 128K 토큰으로 확장했습니다. 이는 긴 문서 이해, 코드 생성, 그리고 장문의 대화와 같은 작업에서 중요한 이점을 제공합니다.

이러한 효율성 개선은 DeepSeek-V2가 강력한 성능뿐만 아니라 경제적인 학습과 효율적인 추론을 제공함을 보여줍니다. 이는 대규모 언어 모델의 실용적 배포에 있어 중요한 진전을 의미합니다.

DeepSeek-V2의 학습 및 추론 효율성 개선은 다중 헤드 잠재 어텐션(MLA)과 DeepSeekMoE라는 두 가지 핵심 아키텍처 혁신을 통해 달성되었습니다. 이러한 혁신은 대규모 언어 모델의 학습 비용을 줄이고 추론 효율성을 향상시키는 데 중요한 역할을 했으며, 향후 더 효율적인 AI 모델 개발에 중요한 기여를 할 것으로 기대됩니다.

# 정렬

## 지도 미세 조정

DeepSeek-V2 모델은 사전 학습 후 지도 미세 조정(Supervised Fine-Tuning, SFT) 과정을 거쳐 사용자 지시를 더 잘 따르고 유용한 응답을 생성할 수 있도록 개선되었습니다. 이 과정은 모델이 인간의 선호도와 가치에 더 잘 정렬되도록 하는 첫 번째 단계입니다.

### 데이터셋 구성

지도 미세 조정에 사용된 데이터셋은 다양한 소스에서 수집된 고품질의 지시-응답 쌍으로 구성되었습니다. 이 데이터셋은 다음과 같은 특성을 가지고 있습니다.

1. **다양성**: 일상 대화, 창의적 작문, 코드 생성, 수학적 추론, 지식 기반 질문 등 다양한 유형의 작업을 포함합니다.
2. **품질**: 모든 지시-응답 쌍은 명확하고 유용하며 안전한 응답을 제공하도록 선별되었습니다.
3. **다국어 지원**: 영어와 중국어를 중심으로 다양한 언어로 된 지시-응답 쌍을 포함합니다.

데이터셋은 다음과 같은 카테고리로 구성되었습니다.

- **일반 지시 따르기**: 사용자의 다양한 요청에 응답하는 능력을 향상시키기 위한 데이터
- **코드 생성 및 디버깅**: 프로그래밍 관련 작업을 수행하는 능력을 향상시키기 위한 데이터
- **수학적 추론**: 복잡한 수학 문제를 단계별로 해결하는 능력을 향상시키기 위한 데이터
- **지식 기반 질의응답**: 다양한 도메인의 사실적 지식을 정확하게 제공하는 능력을 향상시키기 위한 데이터
- **안전성 및 유해성 완화**: 유해하거나 부적절한 요청에 안전하게 대응하는 능력을 향상시키기 위한 데이터

### 학습 방법론

지도 미세 조정 과정은 다음과 같은 방법론을 따랐습니다.

1. **손실 함수**: 표준 언어 모델링 손실 함수를 사용하여 모델이 주어진 지시에 대해 적절한 응답을 생성하도록 학습했습니다. 수학적으로 표현하면 다음과 같습니다.

   \\[ \mathcal{L}\_{\text{SFT}} = -\sum_{i=1}^{N} \sum_{j=1}^{M_i} \log P_\theta(y_{i,j} \vert y_{i,<j}, x_i) \\]

   여기서:
   - \\(x_i\\)는 \\(i\\)번째 지시문입니다.
   - \\(y_i\\)는 \\(i\\)번째 지시문에 대한 응답입니다.
   - \\(y_{i,j}\\)는 응답의 \\(j\\)번째 토큰입니다.
   - \\(y_{i,<j}\\)는 \\(j\\)번째 토큰 이전의 모든 토큰입니다.
   - \\(P_\theta\\)는 매개변수 \\(\theta\\)를 가진 모델의 확률 분포입니다.
   - \\(N\\)은 데이터셋의 지시-응답 쌍의 수입니다.
   - \\(M_i\\)는 \\(i\\)번째 응답의 토큰 수입니다.

2. **학습 설정**:
   - 옵티마이저: AdamW
   - 학습률: 1e-5
   - 배치 크기: 512
   - 에포크: 3
   - 가중치 감쇠: 0.1
   - 그래디언트 클리핑: 1.0

3. **프롬프트 형식**: 지시 미세 조정을 위한 프롬프트는 다음과 같은 형식을 따랐습니다.

   ```
   [INST] {instruction} [/INST]
   {response}
   ```

   여기서 `{instruction}`은 사용자의 지시이고, `{response}`는 모델이 생성해야 할 응답입니다. 이 형식은 모델이 지시의 시작과 끝을 명확히 인식하고, 응답을 적절하게 생성할 수 있도록 돕습니다.

### 체인 오브 소트 학습

DeepSeek-V2는 복잡한 추론 작업에서의 성능을 향상시키기 위해 체인 오브 소트(Chain-of-Thought, CoT) 방식으로도 학습되었습니다. 이 방식은 [Wei와 연구진](https://arxiv.org/pdf/2206.07682)이 제안한 방법으로, 모델이 최종 답변을 제공하기 전에 단계별 추론 과정을 생성하도록 유도합니다.

체인 오브 소트 학습을 위한 데이터는 다음과 같은 형식으로 구성되었습니다.

```
[INST] {instruction} [/INST]
생각해 봅시다. {step-by-step reasoning}
따라서, {final answer}
```

여기서 `{step-by-step reasoning}`은 문제를 해결하기 위한 단계별 추론 과정이고, `{final answer}`는 최종 답변입니다. 이러한 형식의 데이터를 사용하여 모델을 학습시킴으로써, 복잡한 수학 문제나 다단계 추론이 필요한 작업에서 더 나은 성능을 발휘할 수 있게 되었습니다.

체인 오브 소트 학습의 효과는 GSM8K와 같은 수학적 추론 벤치마크에서 특히 두드러졌습니다. DeepSeek-V2는 GSM8K 벤치마크에서 체인 오브 소트 프롬프팅을 사용하여 84.7%의 정확도를 달성했습니다. 이는 모델이 다단계 수학적 추론 능력을 효과적으로 습득했음을 보여줍니다.

### 지시 따르기 능력 향상

지도 미세 조정의 주요 목표 중 하나는 모델의 지시 따르기 능력을 향상시키는 것이었습니다. 이를 위해 [Zhou와 연구진](https://arxiv.org/pdf/2311.07911)이 제안한 IFEval 벤치마크와 유사한 방식으로 객관적으로 검증 가능한 지시사항을 포함하는 데이터를 사용했습니다.

예를 들어, 다음과 같은 지시사항을 포함하는 데이터가 사용되었습니다.
- "450~500단어 작성하기"
- "키워드 'AI'를 최소 3번 포함하기"
- "5개의 항목으로 구성된 목록 만들기"
- "응답을 3개의 단락으로 구성하기"

이러한 객관적으로 검증 가능한 지시사항을 포함하는 데이터로 학습함으로써, 모델은 사용자의 지시를 정확하게 이해하고 따를 수 있는 능력을 향상시켰습니다. 그 결과, DeepSeek-V2는 IFEval 벤치마크에서 93.5%의 높은 정확도를 달성했습니다.

## 강화 학습

지도 미세 조정 이후, DeepSeek-V2는 강화 학습(Reinforcement Learning, RL)을 통해 더욱 개선되었습니다. 이 과정은 인간의 선호도에 기반한 보상 모델을 사용하여 모델의 응답 품질을 향상시키는 것을 목표로 합니다.

### 인간 선호도 기반 강화 학습

DeepSeek-V2의 강화 학습은 인간 선호도 기반 강화 학습(Reinforcement Learning from Human Feedback, RLHF) 프레임워크를 따랐습니다. 이 프레임워크는 다음과 같은 주요 구성 요소로 이루어져 있습니다.

1. **보상 모델(Reward Model, RM)**: 모델 응답의 품질을 평가하기 위한 보상 모델을 학습합니다. 이 모델은 인간 평가자가 제공한 선호도 데이터를 기반으로 학습됩니다.

2. **근접 정책 최적화(Proximal Policy Optimization, PPO)**: 보상 모델의 피드백을 기반으로 정책 모델(DeepSeek-V2)을 최적화합니다.

### 보상 모델 학습

보상 모델은 동일한 지시에 대한 두 개의 서로 다른 응답 중 어떤 것이 더 나은지에 대한 인간 평가자의 선호도 데이터를 기반으로 학습되었습니다. 이 과정은 다음과 같은 단계로 이루어졌습니다.

1. **선호도 데이터 수집**: 다양한 지시에 대해 모델이 생성한 여러 응답 쌍을 인간 평가자에게 제시하고, 어떤 응답이 더 나은지 선택하도록 했습니다.

2. **보상 모델 학습**: 수집된 선호도 데이터를 사용하여 보상 모델을 학습했습니다. 보상 모델의 목표는 인간 평가자의 선호도를 예측하는 것입니다. 수학적으로 표현하면 다음과 같습니다.

   \\[ \mathcal{L}\_{\text{RM}} = -\mathbb{E}\_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma(r_\phi(x, y_w) - r_\phi(x, y_l)) \right] \\]

   여기서:
   - \\(x\\)는 지시문입니다.
   - \\(y_w\\)는 선호되는 응답입니다.
   - \\(y_l\\)은 덜 선호되는 응답입니다.
   - \\(r_\phi\\)는 매개변수 \\(\phi\\)를 가진 보상 모델입니다.
   - \\(\sigma\\)는 시그모이드 함수입니다.
   - \\(\mathcal{D}\\)는 선호도 데이터셋입니다.

보상 모델은 지시문과 응답을 입력으로 받아 스칼라 값을 출력합니다. 이 값은 응답의 품질을 나타내며, 값이 클수록 더 나은 응답임을 의미합니다.

### 근접 정책 최적화

보상 모델이 학습된 후, 근접 정책 최적화(PPO) 알고리즘을 사용하여 DeepSeek-V2 모델을 최적화했습니다. PPO는 다음과 같은 목적 함수를 최적화합니다.

\\[ \mathcal{L}\_{\text{PPO}} = \mathbb{E}\_{(x, y) \sim \pi_\theta} \left[ \min\left(\frac{\pi_\theta(y \vert x)}{\pi_{\text{ref}}(y \vert x)} A(x, y), \text{clip}\left(\frac{\pi_\theta(y \vert x)}{\pi_{\text{ref}}(y \vert x)}, 1-\epsilon, 1+\epsilon\right) A(x, y)\right) \right] \\]

여기서:
- \\(\pi_\theta\\)는 현재 정책(최적화 중인 DeepSeek-V2 모델)입니다.
- \\(\pi_{\text{ref}}\\)는 참조 정책(SFT 모델)입니다.
- \\(A(x, y)\\)는 이점 함수로, 보상 모델의 출력에서 기준선을 뺀 값입니다.
- \\(\epsilon\\)은 클리핑 매개변수로, 정책 업데이트의 크기를 제한합니다.

이점 함수 \\(A(x, y)\\)는 다음과 같이 계산됩니다.

\\[ A(x, y) = r_\phi(x, y) - \lambda \log\frac{\pi_\theta(y \vert x)}{\pi_{\text{ref}}(y \vert x)} \\]

여기서 \\(\lambda\\)는 KL 발산 페널티 계수로, 최적화된 정책이 참조 정책에서 너무 멀어지지 않도록 제한합니다.

### 다중 목표 최적화

DeepSeek-V2의 강화 학습은 단일 보상 모델만 사용하는 대신, 다양한 측면을 평가하는 여러 보상 모델을 사용하는 다중 목표 최적화 접근법을 채택했습니다. 이러한 보상 모델은 다음과 같은 측면을 평가합니다.

1. **유용성(Helpfulness)**: 응답이 사용자의 지시를 얼마나 잘 따르고 유용한 정보를 제공하는지 평가합니다.
2. **정확성(Accuracy)**: 응답에 포함된 정보가 얼마나 사실적이고 정확한지 평가합니다.
3. **안전성(Safety)**: 응답이 유해하거나 부적절한 내용을 포함하지 않는지 평가합니다.
4. **공정성(Fairness)**: 응답이 편향되지 않고 공정한지 평가합니다.

이러한 다양한 보상 모델의 출력을 결합하여 최종 보상을 계산했습니다.

\\[ r_{\text{total}}(x, y) = \sum_{i=1}^{K} w_i \cdot r_i(x, y) \\]

여기서:
- \\(r_i\\)는 \\(i\\)번째 보상 모델입니다.
- \\(w_i\\)는 \\(i\\)번째 보상 모델의 가중치입니다.
- \\(K\\)는 보상 모델의 수입니다.

이러한 다중 목표 최적화 접근법은 모델이 단일 측면에서만 최적화되는 것이 아니라, 여러 중요한 측면에서 균형 잡힌 성능을 발휘할 수 있도록 합니다.

### 강화 학습 구현 세부 사항

DeepSeek-V2의 강화 학습 구현에는 다음과 같은 세부 사항이 포함되었습니다.

1. **학습 설정**:
   - PPO 에포크: 4
   - 미니배치 크기: 64
   - 학습률: 5e-7
   - KL 페널티 계수(\\(\lambda\\)): 0.1
   - 클리핑 매개변수(\\(\epsilon\\)): 0.2
   - 감마(할인 계수): 1.0

2. **샘플링 전략**: 강화 학습 중에 다양한 응답을 탐색하기 위해 온도 샘플링을 사용했습니다. 온도 매개변수는 0.7로 설정되었습니다.

3. **거부 샘플링**: 안전성을 보장하기 위해, 안전성 보상 모델의 점수가 특정 임계값 이하인 응답은 거부하고 다시 샘플링했습니다.

4. **경험 버퍼**: 다양한 경험을 저장하고 재사용하기 위해 경험 버퍼를 유지했습니다. 이를 통해 데이터 효율성을 높이고 학습 안정성을 향상시켰습니다.

5. **조기 종료**: 정책이 참조 정책에서 너무 멀어지는 것을 방지하기 위해, KL 발산이 특정 임계값을 초과하면 학습을 조기에 종료했습니다.

이러한 세부 구현 사항은 강화 학습 과정의 안정성과 효과를 향상시키는 데 중요한 역할을 했습니다.

## 평가 결과

DeepSeek-V2의 정렬 과정(지도 미세 조정 및 강화 학습)의 효과를 평가하기 위해 다양한 벤치마크에서 성능을 측정했습니다. 이 섹션에서는 주요 평가 결과를 살펴보겠습니다.

### 지시 따르기 능력

DeepSeek-V2의 지시 따르기 능력은 IFEval 벤치마크를 사용하여 평가되었습니다. IFEval은 [Zhou와 연구진](https://arxiv.org/pdf/2311.07911)이 제안한 벤치마크로, 객관적으로 검증 가능한 지시사항에 대한 모델의 준수 여부를 평가합니다.

DeepSeek-V2는 IFEval 벤치마크에서 93.5%의 정확도를 달성했습니다. 이는 모델이 사용자의 지시를 정확하게 이해하고 따를 수 있는 능력이 뛰어남을 보여줍니다. 특히, 다음과 같은 유형의 지시사항에서 높은 성능을 보였습니다.

- 특정 단어 수 제한 준수
- 키워드 포함 요구사항 충족
- 특정 형식(목록, 표 등) 생성
- 특정 구조(단락, 섹션 등) 준수

이러한 결과는 지도 미세 조정과 강화 학습을 통해 DeepSeek-V2의 지시 따르기 능력이 크게 향상되었음을 보여줍니다.

### 대화 품질

DeepSeek-V2-Chat의 대화 품질은 MT-Bench와 AlpacaEval과 같은 벤치마크를 사용하여 평가되었습니다.

1. **MT-Bench**: DeepSeek-V2-Chat은 MT-Bench에서 8.3점(10점 만점)을 획득했습니다. 이는 모델이 다양한 대화 상황에서 고품질의 응답을 생성할 수 있는 능력이 뛰어남을 보여줍니다.

2. **AlpacaEval**: DeepSeek-V2-Chat은 AlpacaEval 벤치마크에서도 높은 점수를 획득했습니다. [Dubois와 연구진](https://arxiv.org/pdf/2404.04475)이 제안한 길이 제어 AlpacaEval 방법론을 적용했을 때도 모델은 강력한 성능을 유지했습니다. 이는 모델의 성능이 단순히 응답 길이에 의존하지 않고, 실제로 고품질의 내용을 생성하고 있음을 보여줍니다.

3. **Arena-Hard**: DeepSeek-V2-Chat은 Arena-Hard 벤치마크에서 높은 승률을 기록했습니다. 이는 모델이 복잡한 대화 상황에서도 효과적으로 대응할 수 있음을 보여줍니다.

이러한 결과는 DeepSeek-V2-Chat이 다양한 대화 상황에서 고품질의 응답을 생성할 수 있는 능력을 갖추고 있음을 보여줍니다.

### 중국어 성능

DeepSeek-V2의 중국어 성능은 AlignBench와 같은 중국어 특화 벤치마크를 사용하여 평가되었습니다.

[Liu와 연구진](https://arxiv.org/pdf/2311.18743)이 제안한 AlignBench는 중국어 대화 모델의 응답 품질을 평가하기 위한 벤치마크입니다. 이 벤치마크는 규칙 보정 다차원 평가 프레임워크를 제공하여 모델 응답의 품질을 종합적으로 평가합니다.

DeepSeek-V2는 AlignBench 벤치마크에서 8.2점(10점 만점)을 획득했습니다. 이는 모델이 중국어로도 고품질의 응답을 생성할 수 있는 능력이 뛰어남을 보여줍니다. 특히, 다음과 같은 측면에서 높은 점수를 받았습니다.

- 지시 이해 및 따르기
- 응답의 일관성 및 논리성
- 정보의 정확성 및 관련성
- 언어 표현의 자연스러움

이러한 결과는 DeepSeek-V2가 중국어 사용자에게도 유용한 서비스를 제공할 수 있음을 보여줍니다.

### 추론 능력

DeepSeek-V2의 추론 능력은 GSM8K와 같은 수학적 추론 벤치마크를 사용하여 평가되었습니다.

GSM8K는 초등학교 수준의 수학 문제 해결 능력을 평가하는 벤치마크로, 다단계 추론이 필요한 문제들로 구성되어 있습니다. DeepSeek-V2는 GSM8K 벤치마크에서 체인 오브 소트(CoT) 프롬프팅을 사용하여 84.7%의 정확도를 달성했습니다.

이는 [Suzgun과 연구진](https://arxiv.org/pdf/2210.09261)이 제안한 체인 오브 소트 방법론이 DeepSeek-V2의 추론 능력을 크게 향상시켰음을 보여줍니다. 체인 오브 소트 방식은 모델이 최종 답변을 제공하기 전에 단계별 추론 과정을 생성하도록 유도함으로써, 복잡한 문제 해결 능력을 향상시킵니다.

이러한 결과는 DeepSeek-V2가 단순한 질의응답뿐만 아니라, 복잡한 추론이 필요한 작업에서도 뛰어난 성능을 발휘할 수 있음을 보여줍니다.

## 논의

DeepSeek-V2의 정렬 과정과 평가 결과를 바탕으로, 몇 가지 중요한 논의점을 살펴보겠습니다.

### 정렬 과정의 효과

DeepSeek-V2의 정렬 과정(지도 미세 조정 및 강화 학습)은 모델의 성능을 크게 향상시켰습니다. 특히, 다음과 같은 측면에서 개선이 이루어졌습니다.

1. **지시 따르기 능력**: IFEval 벤치마크에서 93.5%의 높은 정확도를 달성한 것은 모델이 사용자의 지시를 정확하게 이해하고 따를 수 있는 능력이 크게 향상되었음을 보여줍니다.

2. **대화 품질**: MT-Bench에서 8.3점(10점 만점)을 획득한 것은 모델이 다양한 대화 상황에서 고품질의 응답을 생성할 수 있는 능력이 향상되었음을 보여줍니다.

3. **추론 능력**: GSM8K 벤치마크에서 체인 오브 소트 프롬프팅을 사용하여 84.7%의 정확도를 달성한 것은 모델의 추론 능력이 크게 향상되었음을 보여줍니다.

이러한 결과는 지도 미세 조정과 강화 학습이 대규모 언어 모델의 성능을 향상시키는 데 효과적인 방법임을 확인해 줍니다.

### 다중 목표 최적화의 중요성

DeepSeek-V2의 강화 학습에서 채택한 다중 목표 최적화 접근법은 모델이 여러 중요한 측면(유용성, 정확성, 안전성, 공정성 등)에서 균형 잡힌 성능을 발휘할 수 있도록 했습니다. 이는 단일 측면에서만 최적화된 모델보다 실제 사용 환경에서 더 유용하고 안전한 서비스를 제공할 수 있음을 시사합니다.

특히, [OpenAI와 연구진](https://arxiv.org/pdf/2303.08774)이 GPT-4 기술 보고서에서 언급한 것처럼, 안전성과 유용성 사이의 균형을 맞추는 것은 대규모 언어 모델의 정렬에서 중요한 과제입니다. DeepSeek-V2의 다중 목표 최적화 접근법은 이러한 균형을 달성하는 데 효과적인 방법을 제시합니다.

### 체인 오브 소트의 효과

DeepSeek-V2의 평가 결과는 체인 오브 소트(CoT) 방식이 모델의 추론 능력을 크게 향상시킬 수 있음을 보여줍니다. 이는 [Wei와 연구진](https://arxiv.org/pdf/2206.07682)이 제안한 대규모 언어 모델의 창발적 능력(emergent abilities)과 관련이 있습니다.

창발적 능력이란 작은 모델에서는 나타나지 않지만 더 큰 규모에서 갑자기 나타나는 능력을 의미합니다. 체인 오브 소트와 같은 프롬프팅 전략은 이러한 창발적 능력을 더욱 강화하고 활용할 수 있게 합니다. DeepSeek-V2의 경우, 체인 오브 소트 방식을 통해 복잡한 추론 작업에서 뛰어난 성능을 발휘할 수 있게 되었습니다.

### 향후 연구 방향

DeepSeek-V2의 정렬 과정과 평가 결과를 바탕으로, 다음과 같은 향후 연구 방향을 고려할 수 있습니다.

1. **더 효율적인 정렬 방법**: 현재의 RLHF 방법은 계산 비용이 높고 복잡합니다. 더 효율적이고 확장 가능한 정렬 방법을 개발하는 것이 중요한 연구 방향이 될 수 있습니다.

2. **다양한 문화적 가치 반영**: 현재의 정렬 과정은 주로 특정 문화권의 가치와 선호도를 반영합니다. 다양한 문화적 가치와 관점을 반영할 수 있는 정렬 방법을 개발하는 것이 중요한 과제입니다.

3. **장기적 안전성**: 현재의 정렬 방법은 주로 단기적인 응답 품질에 초점을 맞추고 있습니다. 장기적인 상호작용과 영향을 고려한 안전성 정렬 방법을 개발하는 것이 중요한 연구 방향이 될 수 있습니다.

4. **자동화된 평가 방법**: 현재의 많은 평가 방법은 인간 평가자에 의존하고 있어 확장성이 제한됩니다. 더 자동화되고 객관적인 평가 방법을 개발하는 것이 중요한 연구 방향이 될 수 있습니다.

이러한 연구 방향은 대규모 언어 모델의 정렬과 평가를 더욱 발전시키는 데 기여할 수 있을 것입니다.

# 결론, 한계점 및 향후 연구 방향

## 결론

본 논문에서는 경제적인 학습과 효율적인 추론을 특징으로 하는 강력한 오픈소스 혼합 전문가(Mixture-of-Experts, MoE) 언어 모델인 DeepSeek-V2를 소개했습니다. DeepSeek-V2는 혁신적인 트랜스포머 아키텍처를 통해 총 236B 매개변수를 가지고 있으며, 각 토큰 처리 시 21B 매개변수만 활성화되는 구조를 채택하고 있습니다. 또한 128K 토큰의 컨텍스트 길이를 지원하여 긴 문맥을 처리할 수 있는 능력을 갖추고 있습니다.

DeepSeek-V2의 핵심 혁신은 어텐션 모듈과 피드포워드 네트워크(FFN)의 최적화에 있습니다. 어텐션 메커니즘으로는 다중 헤드 잠재 어텐션(Multi-head Latent Attention, MLA)을 도입하여 키-값(Key-Value, KV) 캐시를 효율적으로 압축함으로써 추론 효율성을 크게 향상시켰습니다. MLA는 저순위 키-값 공동 압축을 통해 KV 캐시를 크게 줄이면서도 기존의 다중 헤드 어텐션(MHA)보다 우수한 성능을 달성했습니다.

피드포워드 네트워크에는 DeepSeekMoE 아키텍처를 채택하여 세분화된 전문가 분할과 공유 전문가 분리를 통해 전문가 특화도를 향상시키고 매개변수 효율성을 높였습니다. 이를 통해 경제적인 비용으로 강력한 모델을 학습할 수 있었습니다.

DeepSeek-V2는 8.1조 토큰으로 구성된 고품질의 다중 소스 사전 학습 코퍼스를 사용하여 학습되었으며, 이전 버전인 DeepSeek 67B와 비교하여 데이터 양이 확장되고 품질이 향상되었습니다. 특히 중국어 데이터가 증가하여 영어와 중국어 모두에서 강력한 성능을 발휘할 수 있게 되었습니다.

평가 결과에 따르면, DeepSeek-V2는 활성화된 매개변수가 21B에 불과함에도 불구하고 다양한 영어 및 중국어 벤치마크에서 최고 수준의 성능을 달성했습니다. 특히 MMLU 벤치마크에서는 활성화된 매개변수 수 대비 최고의 성능을 보여주었습니다.

또한 DeepSeek-V2는 기존 DeepSeek 67B 모델과 비교하여 학습 비용을 42.5% 절감하고, KV 캐시를 93.3% 감소시켰으며, 최대 생성 처리량을 5.76배 향상시켰습니다. 이는 MLA와 DeepSeekMoE 아키텍처의 효율성을 입증하는 결과입니다.

지도 미세 조정(SFT)과 강화 학습(RL)을 통해 개발된 DeepSeek-V2 Chat 모델은 AlpacaEval 2.0에서 38.9%의 길이 제어 승률, MT-Bench에서 8.97점, AlignBench에서 7.91점을 달성하여 오픈소스 채팅 모델 중 최고 수준의 성능을 보여주었습니다. 특히 중국어 평가에서는 대부분의 비공개 모델보다 우수한 성능을 달성했습니다.

연구 커뮤니티를 위해 MLA와 DeepSeekMoE를 탑재한 소형 모델인 DeepSeek-V2-Lite도 공개했습니다. 이 모델은 총 15.7B 매개변수를 가지고 있으며, 각 토큰 처리 시 2.4B 매개변수만 활성화됩니다.

## 한계점

DeepSeek-V2는 다양한 혁신과 강력한 성능에도 불구하고 몇 가지 한계점을 가지고 있습니다.

첫째, DeepSeek-V2는 다른 대규모 언어 모델(LLM)과 마찬가지로 사전 학습 이후 지식 업데이트가 어렵다는 한계가 있습니다. 모델은 학습 데이터에 포함된 정보만을 알고 있으며, 학습 이후에 발생한 새로운 정보나 사건에 대해서는 알지 못합니다. 이는 시간이 지남에 따라 모델의 지식이 점차 구식이 될 수 있음을 의미합니다.

둘째, DeepSeek-V2는 검증되지 않은 조언과 같은 사실이 아닌 정보를 생성할 가능성이 있습니다. 모델은 학습 데이터에 기반하여 응답을 생성하지만, 때로는 정확하지 않거나 오해의 소지가 있는 정보를 제공할 수 있습니다. 특히 전문적인 지식이 필요한 분야(의학, 법률, 재정 등)에서 이러한 문제가 발생할 수 있습니다.

셋째, 모든 LLM과 마찬가지로 DeepSeek-V2도 환각(hallucination) 현상을 보일 수 있습니다. 환각은 모델이 실제로 존재하지 않는 정보를 사실인 것처럼 자신감 있게 제시하는 현상을 말합니다. 이는 사용자가 모델의 응답을 비판적으로 평가하고 필요한 경우 추가 검증을 수행해야 함을 의미합니다.

넷째, DeepSeek-V2의 데이터는 주로 중국어와 영어 콘텐츠로 구성되어 있어, 다른 언어에 대한 능력이 제한적일 수 있습니다. 중국어와 영어 이외의 언어로 된 작업에서는 성능이 저하될 수 있으므로, 이러한 상황에서는 주의해서 사용해야 합니다.

다섯째, 모델의 크기와 복잡성으로 인해 추론 과정에서 상당한 계산 리소스가 필요합니다. MLA와 DeepSeekMoE를 통해 효율성을 크게 향상시켰지만, 여전히 고성능 하드웨어가 필요하며 이는 모델의 접근성과 활용성에 제한을 둘 수 있습니다.

여섯째, 모델의 안전성과 편향성 문제가 여전히 존재합니다. 데이터 필터링과 정렬 과정을 통해 이러한 문제를 완화하려고 노력했지만, 모든 잠재적 문제를 해결하기는 어렵습니다. 모델은 학습 데이터에 존재하는 편향성을 반영할 수 있으며, 이는 특정 상황에서 문제가 될 수 있습니다.

## 향후 연구 방향

DeepSeek은 장기적인 관점에서 오픈소스 대규모 모델에 지속적으로 투자하여 인공 일반 지능(AGI)의 목표에 점진적으로 접근하고자 합니다. 향후 연구 방향은 다음과 같습니다.

첫째, 경제적인 학습 및 추론 비용을 유지하면서 MoE 모델을 더욱 확장할 수 있는 방법을 개발하는 데 주력할 것입니다. 다음 단계의 목표는 GPT-4와 동등한 성능을 달성하는 모델을 개발하는 것입니다. 이를 위해 MLA와 DeepSeekMoE 아키텍처를 더욱 최적화하고, 더 효율적인 학습 및 추론 알고리즘을 개발할 계획입니다.

둘째, 정렬 팀은 모델이 유용할 뿐만 아니라 정직하고 안전하게 전 세계 사용자에게 서비스를 제공할 수 있도록 지속적으로 모델을 개선할 것입니다. 궁극적인 목표는 인간의 감독 필요성을 최소화하면서 모델의 가치를 인간의 가치와 정렬시키는 것입니다. 윤리적 고려사항과 책임 있는 개발을 우선시함으로써 사회에 긍정적이고 유익한 영향을 미치기 위해 노력할 것입니다.

셋째, 현재 DeepSeek-V2는 텍스트 모달리티만 지원하도록 설계되어 있습니다. 향후 계획에서는 모델이 다양한 시나리오에서 활용성과 유용성을 높일 수 있도록 다중 모달리티를 지원하도록 확장할 예정입니다. 이는 텍스트뿐만 아니라 이미지, 오디오, 비디오 등 다양한 형태의 데이터를 처리하고 이해할 수 있는 능력을 개발하는 것을 의미합니다.

넷째, 더 긴 컨텍스트 길이를 효율적으로 처리할 수 있는 방법을 연구할 것입니다. 현재 128K 토큰의 컨텍스트 길이를 지원하고 있지만, 더 긴 문서나 대화를 처리하기 위해 이를 더욱 확장하고 최적화할 계획입니다.

다섯째, 다양한 언어에 대한 지원을 강화하여 모델의 다국어 능력을 향상시킬 것입니다. 현재는 중국어와 영어에 중점을 두고 있지만, 향후에는 더 많은 언어를 효과적으로 처리할 수 있도록 모델을 확장할 계획입니다.

여섯째, 모델의 추론 능력과 지식 활용 능력을 더욱 향상시키기 위한 연구를 진행할 것입니다. 특히 복잡한 추론, 수학적 문제 해결, 코드 생성 등의 영역에서 모델의 성능을 더욱 개선하고자 합니다.

일곱째, 모델의 안전성과 정렬성을 향상시키기 위한 새로운 방법론을 개발할 것입니다. 이는 유해한 콘텐츠 생성을 방지하고, 편향성을 줄이며, 인간의 가치와 더 잘 정렬된 응답을 생성하는 것을 목표로 합니다.

이러한 향후 연구 방향을 통해 DeepSeek은 더 강력하고, 효율적이며, 안전하고, 다양한 작업에 활용할 수 있는 언어 모델을 개발하여 인공 일반 지능의 목표에 한 걸음 더 다가가고자 합니다.

# 부록: DeepSeek-V2-Lite: MLA와 DeepSeekMoE를 탑재한 16B 모델

## 모델 설명

### 아키텍처

DeepSeek-V2-Lite는 27개의 레이어와 2048의 은닉 차원을 가지고 있습니다. 이 모델은 MLA를 채택하고 있으며, 16개의 어텐션 헤드를 가지고 있고 각 헤드의 차원은 128입니다. KV 압축 차원은 512이지만, DeepSeek-V2와 약간 다르게 쿼리를 압축하지 않습니다. 분리된 쿼리와 키의 경우, 헤드당 차원은 64입니다.

DeepSeek-V2-Lite는 또한 DeepSeekMoE를 채택하고 있으며, 첫 번째 레이어를 제외한 모든 FFN이 MoE 레이어로 대체되었습니다. 각 MoE 레이어는 2개의 공유 전문가와 64개의 라우팅된 전문가로 구성되어 있으며, 각 전문가의 중간 은닉 차원은 1408입니다. 라우팅된 전문가 중에서 각 토큰마다 6개의 전문가가 활성화됩니다.

이러한 구성에서 DeepSeek-V2-Lite는 총 15.7B 매개변수를 가지고 있으며, 그 중 각 토큰 처리 시 2.4B 매개변수만 활성화됩니다.

### 학습 세부 사항

DeepSeek-V2-Lite는 DeepSeek-V2와 동일한 사전 학습 코퍼스에서 처음부터 학습되었으며, 이 코퍼스는 SFT(Supervised Fine-Tuning) 데이터로 오염되지 않았습니다. 이 모델은 하이퍼파라미터가 \\(\beta_1 = 0.9\\), \\(\beta_2 = 0.95\\), 그리고 \\(\text{weight decay} = 0.1\\)로 설정된 AdamW 옵티마이저를 사용합니다.

학습률은 웜업-스텝-감소 전략을 사용하여 스케줄링됩니다. 처음에는 학습률이 처음 2K 스텝 동안 0에서 최대값까지 선형적으로 증가합니다. 이후 토큰의 약 80%를 학습한 후 학습률은 0.316을 곱하고, 다시 토큰의 약 90%를 학습한 후 0.316을 곱합니다. 최대 학습률은 \\(4.2 \times 10^{-4}\\)로 설정되었으며, 그래디언트 클리핑 노름은 1.0으로 설정되었습니다.

이 모델에는 배치 크기 스케줄링 전략을 사용하지 않았으며, 4608 시퀀스의 일정한 배치 크기로 학습되었습니다. 사전 학습 중에는 최대 시퀀스 길이를 4K로 설정하고, DeepSeek-V2-Lite를 5.7T 토큰으로 학습했습니다.

파이프라인 병렬화를 활용하여 모델의 다른 레이어를 다른 디바이스에 배포했지만, 각 레이어에 대해 모든 전문가는 동일한 디바이스에 배포됩니다. 따라서 \\(\alpha_1 = 0.001\\)의 작은 전문가 수준 균형 손실만 사용하고, 디바이스 수준 균형 손실과 통신 균형 손실은 사용하지 않았습니다.

사전 학습 후에는 DeepSeek-V2-Lite에 대해서도 긴 컨텍스트 확장과 SFT를 수행하여 DeepSeek-V2-Lite Chat이라는 채팅 모델을 얻었습니다.

## 성능 평가

### 기본 모델

DeepSeek-V2-Lite는 특히 추론, 코딩, 수학 분야에서 압도적인 성능 우위를 보여줍니다.

### 채팅 모델

DeepSeek-V2-Lite는 이전의 소형 채팅 모델들보다 큰 폭으로 우수한 성능을 보여줍니다.

# MLA의 전체 공식

MLA(Multi-head Latent Attention)의 완전한 계산 과정을 보여주기 위해 다음과 같이 전체 공식을 제공합니다.

\\[ \mathbf{c}\_{t}^{Q} = W^{DQ}\mathbf{h}_{t}, \\]

\\[ [\mathbf{q}\_{t,1}^{C};\mathbf{q}\_{t,2}^{C};...;\mathbf{q}\_{t,n_{h}}^{C}]=\mathbf{q}\_{t}^{C} = W^{UQ}\mathbf{c}_{t}^{Q}, \\]

\\[ [\mathbf{q}\_{t,1}^{R};\mathbf{q}\_{t,2}^{R};...;\mathbf{q}\_{t,n_{h}}^{R}]=\mathbf{q}\_{t}^{R} = \operatorname{RoPE}({W^{QR}}\mathbf{c}_{t}^{Q}), \\]

\\[ \mathbf{q}\_{t,i} = [\mathbf{q}_{t,i}^{C};\mathbf{q}\_{t,i}^{R}], \\]

\\[ \mathbf{c}\_{t}^{KV} = W^{DKV}\mathbf{h}_{t}, \\]

\\[ [\mathbf{k}\_{t,1}^{C};\mathbf{k}\_{t,2}^{C};...;\mathbf{k}\_{t,n_{h}}^{C}]=\mathbf{k}\_{t}^{C} = W^{UK}\mathbf{c}\_{t}^{KV}, \\]

\\[ \mathbf{k}\_{t}^{R} = \operatorname{RoPE}({W^{KR}}\mathbf{h}_{t}), \\]

\\[ \mathbf{k}\_{t,i} = [\mathbf{k}\_{t,i}^{C};\mathbf{k}_{t}^{R}], \\]

\\[ [\mathbf{v}\_{t,1}^{C};\mathbf{v}\_{t,2}^{C};...;\mathbf{v}\_{t,n_{h}}^{C}]=\mathbf{v\}\_{t}^{C} = W^{UV}\mathbf{c}_{t}^{KV}, \\]

\\[ \mathbf{o}\_{t,i} = \sum_{j=1}^{t}\operatorname{Softmax}\_{j}(\frac{\mathbf{q}\_{t,i}^{T}\mathbf{k}\_{j,i}}{\sqrt{d_{h}+d_{h}^{R}}})\mathbf{v}_{j,i}^{C}, \\]

\\[ \mathbf{u}\_{t} = W^{O}[\mathbf{o}\_{t,1};\mathbf{o}\_{t,2};...;\mathbf{o}\_{t,n_{h}}], \\]

여기서 파란색으로 표시된 벡터들은 생성 과정에서 캐시되어야 합니다.

추론 중에는 단순한 공식으로 \\(\mathbf{c}\_{t}^{KV}\\)에서 \\(\mathbf{k}\_{t}^{C}\\)와 \\(\mathbf{v}_{t}^{C}\\)를 복구하여 어텐션을 수행해야 합니다. 다행히도 행렬 곱셈의 결합 법칙 덕분에 \\(W^{UK}\\)를 \\(W^{UQ}\\)에 흡수시키고, \\(W^{UV}\\)를 \\(W^{O}\\)에 흡수시킬 수 있습니다. 따라서 각 쿼리에 대해 키와 값을 계산할 필요가 없습니다.

이러한 최적화를 통해 추론 중에 \\(\mathbf{k}\_{t}^{C}\\)와 \\(\mathbf{v}\_{t}^{C}\\)를 다시 계산하는 계산 오버헤드를 피할 수 있습니다.

# 어텐션 메커니즘의 비교 분석

## MHA, GQA, MQA의 비교

MHA(Multi-Head Attention), GQA(Grouped-Query Attention), MQA(Multi-Query Attention)를 사용한 7B 밀집 모델들의 네 가지 어려운 벤치마크에서의 평가를 수행했습니다. 이 세 모델은 모두 1.33T 토큰으로 학습되었으며, 어텐션 메커니즘을 제외하고는 동일한 아키텍처를 공유합니다. 또한 공정한 비교를 위해 레이어 수를 조정하여 모델들의 매개변수 수를 약 7B로 맞추었습니다.

MHA는 이러한 벤치마크에서 GQA와 MQA보다 상당한 이점을 보여줍니다.

## MLA와 MHA의 비교

MLA와 MHA를 각각 탑재한 MoE 모델들의 네 가지 어려운 벤치마크에서 평가를 수행했습니다. 확실한 결론을 위해 두 가지 규모의 모델을 학습하고 평가했습니다. 두 개의 소형 MoE 모델은 약 16B의 총 매개변수를 가지고 있으며, 1.33T 토큰으로 학습되었습니다. 두 개의 대형 MoE 모델은 약 250B의 총 매개변수를 가지고 있으며, 420B 토큰으로 학습되었습니다. 또한 두 소형 MoE 모델과 두 대형 MoE 모델은 각각 어텐션 메커니즘을 제외하고는 동일한 아키텍처를 공유합니다.

MLA는 MHA보다 더 나은 성능을 보여줍니다. 더 중요한 것은, MLA는 MHA보다 훨씬 적은 양의 KV 캐시(소형 MoE 모델의 경우 14%, 대형 MoE 모델의 경우 4%)를 필요로 합니다.

# 사전 학습 데이터 편향 제거에 관한 논의

사전 학습 데이터 준비 과정에서, 지역 문화의 영향을 받는 가치관과 같은 논쟁의 여지가 있는 콘텐츠를 식별하고 필터링하여 모델이 이러한 논쟁적인 주제에 대해 불필요한 주관적 편향을 보이지 않도록 했습니다. 그 결과, DeepSeek-V2는 특정 지역 문화와 밀접하게 관련된 테스트 세트에서 약간 더 낮은 성능을 보이는 것으로 관찰되었습니다.

예를 들어, MMLU에서 평가할 때, DeepSeek-V2는 Mixtral 8x22B와 같은 경쟁 모델들과 비교하여 대부분의 테스트 세트에서 비슷하거나 우수한 성능을 보이지만, 주로 미국 가치관과 관련된 Humanity-Moral 하위 집합에서는 여전히 뒤처집니다.

더 나아가, 이 하위 집합에 대한 수동 분석을 수행했습니다. 세 명의 고학력 인간 평가자가 MMLU Humanity-Moral 하위 집합의 420개 도덕적 시나리오에 대해 독립적인 주석을 달았습니다. 그런 다음 그들의 주석과 정답 레이블 간의 일치도를 계산했습니다.

세 명의 인간 평가자와 정답 레이블은 서로 낮은 일치도를 보입니다. 따라서, DeepSeek-V2와 경쟁 모델들이 이러한 가치에 민감한 테스트 세트에서 보이는 비정상적인 성능은 사전 학습 코퍼스의 편향 제거 노력 때문인 것으로 판단됩니다. 이는 Humanity-Moral 하위 집합에 대한 답변이 특정 지역 문화에 따라 논쟁의 여지가 있을 수 있음을 나타냅니다.

- - -
### References
* [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/pdf/2405.04434v5)
