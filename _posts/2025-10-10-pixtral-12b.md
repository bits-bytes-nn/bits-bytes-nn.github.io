---
layout: post
title: "Pixtral 12B"
date: 2024-10-09 17:16:22
author: "Mistral AI"
categories: ["Paper Reviews", "Multimodal-Learning"]
tags: ["RoPE-2D-Positional-Encoding", "Block-Diagonal-Attention-Masking", "Flexible-Vision-Encoder-Architecture", "Natively-Multimodal-Transformer-Architecture", "Multi-Turn-Instruction-Tuning", "Sequence-Packing-Optimization", "Variable-Image-Resolution-Processing", "Break-Tokens-for-Image-Tokenization", "Standardized-Multimodal-Evaluation", "MM-MT-Bench-Benchmark"]
cover: /assets/images/multimodal-learning.jpg
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

현대 인공지능 연구에서 멀티모달 언어 모델의 발전은 매우 중요한 과제로 대두되고 있습니다. 기존의 대부분 멀티모달 모델들은 이미지 이해 능력과 텍스트 처리 능력 사이에 심각한 성능 불균형을 보였으며, 특히 오픈소스 모델들은 상업용 클로즈드 모델들에 비해 현저히 낮은 성능을 나타냈습니다. 연구진은 이러한 한계를 극복하고, 실제 사용자 환경에서 유용하게 활용될 수 있는 멀티모달 AI 시스템의 필요성을 깊이 인식했습니다.

특히 기존 멀티모달 벤치마크들이 단순한 질의응답 형태에 국한되어 실제 사용자와의 복잡한 상호작용을 제대로 평가하지 못한다는 점이 중요한 동기가 되었습니다. 연구진은 이미지 이해, 추론, 대화 능력을 종합적으로 평가할 수 있는 새로운 접근법이 필요하다고 판단했으며, 이는 멀티모달 AI 기술의 실질적인 발전을 위해 반드시 해결해야 할 과제였습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

Pixtral 12B는 혁신적인 비전 인코더와 멀티모달 디코더를 결합한 완전히 새로운 아키텍처를 제안합니다. 핵심 혁신은 **RoPE-2D** 기반의 비전 인코더로, 이는 기존 모델들과 달리 가변적인 이미지 해상도와 종횡비를 자연스럽게 처리할 수 있습니다. 특히 이미지의 원본 구조를 보존하면서 다양한 크기와 형태의 시각적 콘텐츠를 효과적으로 이해할 수 있는 능력을 갖추고 있습니다.

또한 연구진은 **MM-MT-Bench**라는 새로운 멀티모달 벤치마크를 개발하여 모델의 실제 사용성을 평가하는 혁신적인 방법론을 제시했습니다. 이 벤치마크는 단순한 객관식 질의응답을 넘어서 멀티턴 대화, 복잡한 추론, 그리고 실제 사용자 시나리오를 반영하는 평가 방식을 도입했습니다. 특히 독립적인 LLM 판정자를 통해 모델의 응답을 정확성과 완전성 측면에서 심층적으로 평가하는 접근법은 기존 평가 방식에 대한 중요한 대안을 제시합니다.

#### 제안된 방법은 어떻게 구현되었습니까?

Pixtral 12B의 구현은 크게 두 가지 주요 컴포넌트로 구성됩니다. 첫째, **Pixtral-ViT**라는 4억 개의 파라미터를 가진 비전 인코더는 **RoPE-2D** 위치 인코딩, 특수 토큰([IMAGE BREAK], [IMAGE END]), 그리고 게이팅 메커니즘을 통합하여 이미지 처리의 유연성을 극대화했습니다. 둘째, Mistral Nemo 12B를 기반으로 한 120억 개의 파라미터를 가진 멀티모달 디코더는 그룹화된 쿼리 어텐션 방식을 채택하여 메모리 효율성을 높였습니다.

모델의 학습 과정에서는 대규모 이미지-텍스트 인터리브 문서로 사전 훈련을 수행한 후, 인스트럭션 튜닝을 통해 멀티턴, 멀티이미지 대화 능력을 강화했습니다. 특히 128K 토큰의 긴 컨텍스트 윈도우를 활용하여 복잡한 멀티모달 작업을 원활하게 처리할 수 있도록 설계했습니다. 비전 인코더와 언어 디코더 사이의 연결은 2층 완전 연결 네트워크를 통해 이루어지며, GeLU 활성화 함수를 사용하여 효과적인 특성 변환을 수행합니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?

Pixtral 12B의 연구 결과는 멀티모달 AI 분야에 매우 중요한 의미를 지닙니다. 다양한 벤치마크에서 비슷한 크기의 오픈소스 모델들을 상당한 차이로 능가했으며, 심지어 더 큰 클로즈드 모델들과도 경쟁할 수 있는 성능을 보였습니다. MM-MT-Bench에서 6.05점, LMSys Vision Arena에서 1076점의 ELO 점수를 기록하며 오픈소스 멀티모달 모델의 새로운 가능성을 제시했습니다.

특히 주목할 만한 점은 멀티모달 능력을 확보하면서도 텍스트 전용 작업에서의 성능을 전혀 저하시키지 않았다는 것입니다. MT-Bench에서 7.68점, MMLU에서 69.2%, HumanEval에서 72.0%의 성과를 달성하여 실제 워크플로우에 바로 적용 가능한 모델임을 입증했습니다. Apache 2.0 라이선스로 공개됨에 따라 연구자와 개발자들이 자유롭게 활용하고 개선할 수 있는 길을 열었다는 점에서도 큰 의의가 있습니다.
- - -
# Pixtral 12B

## 초록

Pixtral 12B는 120억 개의 파라미터를 가진 멀티모달 언어 모델로, 자연 이미지와 문서를 모두 이해할 수 있는 혁신적인 AI 시스템입니다. 이 모델은 다양한 멀티모달 벤치마크에서 최고 수준의 성능을 달성하며, 훨씬 더 큰 모델들을 능가하는 놀라운 결과를 보여줍니다.

기존의 많은 오픈소스 모델들과 달리, Pixtral은 멀티모달 작업에서 뛰어난 성능을 보이면서도 자연어 처리 능력을 전혀 희생하지 않는 균형잡힌 설계를 특징으로 합니다. 이는 텍스트 전용 작업과 멀티모달 작업 모두에서 일관되게 높은 성능을 유지한다는 의미로, 실용적인 응용에서 매우 중요한 특성입니다.

모델의 핵심 혁신 중 하나는 처음부터 새롭게 훈련된 비전 인코더입니다. 이 비전 인코더는 이미지를 원본 해상도와 종횡비 그대로 처리할 수 있는 능력을 갖추고 있어, 사용자가 이미지 처리에 사용되는 토큰 수를 유연하게 조절할 수 있습니다. 예를 들어, 지연 시간이 중요한 상황에서는 낮은 해상도로 빠르게 처리하고, 세밀한 추론이 필요한 경우에는 높은 해상도로 정밀하게 분석할 수 있습니다.

또한 Pixtral은 128K 토큰이라는 긴 컨텍스트 윈도우 내에서 임의의 개수의 이미지를 처리할 수 있는 능력을 보유하고 있습니다. 이는 복잡한 멀티턴 대화나 여러 이미지가 포함된 문서 분석 등의 실제 사용 시나리오에서 매우 유용한 기능입니다.

성능 측면에서 Pixtral 12B는 비슷한 크기의 다른 오픈 모델들(Llama-3.2 11B, Qwen-2-VL 7B)을 상당한 차이로 능가합니다. 더욱 놀라운 것은 7배나 더 큰 Llama-3.2 90B와 같은 대형 모델들도 뛰어넘는 성능을 보인다는 점입니다. 이는 모델 크기 대비 효율성 측면에서 획기적인 발전을 의미합니다.

연구진은 또한 실용적인 시나리오에서 비전-언어 모델을 평가하기 위한 새로운 오픈소스 벤치마크인 MM-MT-Bench를 제공합니다. 이 벤치마크는 기존의 단순한 질의응답 형태를 넘어서 실제 사용자가 멀티모달 AI와 상호작용하는 방식을 더 잘 반영하도록 설계되었습니다.

평가 프로토콜의 표준화에 대한 상세한 분석과 코드도 함께 제공되어, 멀티모달 대규모 언어 모델의 공정하고 일관된 평가를 위한 기반을 마련합니다. 이는 연구 커뮤니티 전체의 발전에 기여할 수 있는 중요한 기여입니다.

Pixtral 12B는 Apache 2.0 라이선스 하에 공개되어, 연구자들과 개발자들이 자유롭게 사용하고 개선할 수 있습니다. 이러한 오픈소스 접근 방식은 AI 기술의 민주화와 투명성 증진에 기여하는 의미있는 선택입니다.

## 서론

![Pixtral 성능 비교](https://arxiv.org/html/2410.07073/extracted/5915552/images/pareto_mm_mt_bench_oct.png)

![LMSys 리더보드 성능](https://arxiv.org/html/2410.07073/extracted/5915552/images/pareto_lmsys_oct.png)

Pixtral 12B는 이미지와 텍스트를 모두 이해할 수 있는 멀티모달 언어 모델로, Apache 2.0 라이선스 하에 오픈 웨이트로 공개되었습니다. 이 모델은 대규모 이미지-텍스트 인터리브 문서로 사전 훈련된 후 인스트럭션 튜닝을 거쳐, 멀티턴, 멀티이미지 대화가 가능한 시스템으로 발전했습니다.

모델의 가장 혁신적인 특징 중 하나는 새로운 RoPE-2D 구현을 통해 훈련된 비전 인코더입니다. 이 인코더는 이미지를 원본 해상도와 종횡비로 처리할 수 있어, 상황에 따라 유연한 처리가 가능합니다. 지연 시간이 제약된 환경에서는 낮은 해상도로 빠르게 처리하고, 세밀한 추론이 필요한 경우에는 높은 해상도로 정밀하게 분석할 수 있습니다.

비슷한 크기의 모델들과 동일한 평가 환경에서 비교했을 때, Pixtral은 텍스트 전용 추론 성능을 희생하지 않으면서도 강력한 멀티모달 추론 능력을 제공합니다. 예를 들어, [Qwen2-VL 7B](https://arxiv.org/pdf/2409.12191)와 [Llama-3.2 11B](https://arxiv.org/pdf/2407.21783) 같은 모델들과 비교했을 때, MMMU와 MathVista 같은 인기 있는 멀티모달 벤치마크에서 동등하거나 더 나은 성능을 보입니다. 동시에 MATH와 HumanEval 같은 텍스트 전용 작업에서도 대부분의 오픈소스 모델들을 능가하는 성능을 달성합니다.

더욱 인상적인 것은 Pixtral이 [Llama-3.2 90B](https://arxiv.org/pdf/2407.21783)처럼 훨씬 더 큰 모델들과 [Claude-3 Haiku](https://www.anthropic.com/news/claude-3-family), [Gemini-1.5 Flash 8B](https://arxiv.org/pdf/2403.05530) 같은 클로즈드 모델들도 멀티모달 벤치마크에서 능가한다는 점입니다. 이는 모델 크기와 성능 간의 효율성 측면에서 상당한 발전을 의미합니다.

Pixtral과 기준 모델들을 평가하는 과정에서, 연구진은 멀티모달 언어 모델의 평가 프로토콜이 표준화되지 않았으며, 설정의 작은 변화가 일부 모델의 성능을 극적으로 변화시킬 수 있다는 중요한 발견을 했습니다. 이에 대한 철저한 분석을 통해 공통 평가 프로토콜 하에서 비전-언어 모델을 재평가한 경험을 상세히 제공합니다.

구체적으로 평가에서 두 가지 주요 문제를 식별했습니다.

**프롬프트 문제**: 여러 벤치마크의 기본 프롬프트들이 명세가 부족하여, [GPT-4](https://arxiv.org/pdf/2303.08774)나 [Claude-3](https://www.anthropic.com/news/claude-3-family) 같은 최고 수준의 클로즈드 소스 모델들의 성능을 보고된 수치에 비해 극적으로 감소시킵니다. 이는 모델의 실제 능력을 제대로 평가하지 못하게 하는 심각한 문제입니다.

**평가 메트릭 문제**: 공식 메트릭들은 일반적으로 정확한 일치(exact match)를 요구하는데, 이는 실질적으로 정확하지만 형식이 약간 다른 답변들("6.0" vs "6" 같은 경우)에 대해 부당하게 낮은 점수를 부여합니다. 이러한 엄격한 형식 요구사항은 모델의 실제 이해 능력보다는 출력 형식 준수 능력을 평가하게 됩니다.

이러한 문제들을 해결하기 위해, 연구진은 참조 답변에서 요구하는 형식을 명시적으로 지정하는 'Explicit' 프롬프트를 제안합니다. 또한 다양한 모델에 대한 유연한 파싱의 영향을 분석하고, 공정하고 표준화된 평가 프로토콜을 확립하기 위한 노력으로 평가 코드와 프롬프트를 공개합니다.

현재의 멀티모달 벤치마크들이 주로 입력 이미지에 대한 단답형 또는 객관식 질의응답을 평가하는 반면, 실제 사용 사례(예: 멀티턴, 장문형 어시스턴트 설정)에서의 모델 유용성을 완전히 포착하지 못한다는 한계를 인식하여, 새로운 멀티모달, 멀티턴 평가인 MM-MT-Bench를 오픈소스로 제공합니다.

MM-MT-Bench에서의 성능이 LMSys Vision Leaderboard의 ELO 순위와 높은 상관관계를 보인다는 발견은 이 벤치마크의 실용적 가치를 입증합니다. Pixtral은 멀티모달 인스트럭션 팔로잉에서 뛰어난 성능을 보이며, MM-MT-Bench 벤치마크에서 비교 가능한 오픈소스 모델들을 능가합니다.

LMSys Vision Leaderboard의 인간 선호도 기반 평가에서, Pixtral 12B는 현재 Apache 2.0 모델 중 최고 순위를 기록하고 있으며, [Llama-3.2 11B](https://arxiv.org/pdf/2407.21783)와 [Qwen2-VL 7B](https://arxiv.org/pdf/2409.12191) 같은 다른 오픈 모델들을 상당한 차이로 능가합니다. 더욱 놀라운 것은 [Claude-3 Opus & Claude-3 Sonnet](https://www.anthropic.com/news/claude-3-family) 같은 여러 클로즈드 모델들과 [Llama-3.2 90B](https://arxiv.org/pdf/2407.21783) 같은 더 큰 모델들보다도 높은 순위를 기록한다는 점입니다.

이러한 성과는 단순히 모델 크기를 늘리는 것보다 효율적인 아키텍처 설계와 훈련 방법론이 더 중요할 수 있음을 시사합니다. Pixtral 12B의 성공은 오픈소스 AI 모델이 상업적 클로즈드 모델들과 경쟁할 수 있는 수준에 도달했음을 보여주는 중요한 이정표입니다.
## 아키텍처 세부사항

Pixtral 12B는 트랜스포머 아키텍처를 기반으로 구축된 멀티모달 언어 모델로, 고수준 추론을 수행하는 멀티모달 디코더와 이미지를 처리할 수 있게 해주는 비전 인코더로 구성됩니다. 모델의 주요 파라미터들은 다음 표에 정리되어 있습니다.

| 구성요소 | 파라미터 | 디코더 | 인코더 |
|---------|---------|--------|--------|
| dim | 차원 | 5120 | 1024 |
| n_layers | 레이어 수 | 40 | 24 |
| head_dim | 헤드 차원 | 128 | 64 |
| hidden_dim | 은닉 차원 | 14336 | 4096 |
| n_heads | 헤드 수 | 32 | 16 |
| n_kv_heads | KV 헤드 수 | 8 | 16 |
| context_len | 컨텍스트 길이 | 131072 | 4096 |
| vocab_size | 어휘 크기 | 131072 | - |
| patch_size | 패치 크기 | - | 16 |

### 멀티모달 디코더

Pixtral 12B는 Mistral Nemo 12B를 기반으로 구축되었습니다. Mistral Nemo 12B는 120억 개의 파라미터를 가진 디코더 전용 언어 모델로, 다양한 지식과 추론 작업에서 강력한 성능을 달성합니다. 이 모델은 텍스트 전용 작업에서 이미 검증된 성능을 보유하고 있어, 멀티모달 확장의 견고한 기반을 제공합니다.

디코더는 표준적인 트랜스포머 아키텍처를 따르며, 40개의 레이어로 구성되어 있습니다. 각 레이어는 5120차원의 임베딩 공간에서 작동하며, 32개의 어텐션 헤드를 사용합니다. 특히 주목할 점은 8개의 키-값(KV) 헤드만을 사용하는 그룹화된 쿼리 어텐션(Grouped Query Attention) 방식을 채택했다는 것입니다. 이는 메모리 효율성을 크게 향상시키면서도 성능 저하를 최소화하는 현대적인 최적화 기법입니다.

### 비전 인코더

![Pixtral 비전 인코더](https://arxiv.org/html/2410.07073/extracted/5915552/images/pixtral_vit.png)

Pixtral 12B가 이미지를 처리할 수 있도록 하기 위해, 연구진은 Pixtral-ViT라는 새로운 비전 인코더를 처음부터 훈련시켰습니다. 이 비전 인코더의 목표는 광범위한 해상도와 종횡비의 이미지를 처리할 수 있는 간단한 아키텍처를 구현하는 것입니다.

Pixtral-ViT는 4억 개의 파라미터를 가진 비전 트랜스포머로, 표준 아키텍처에 비해 네 가지 핵심적인 변화를 도입했습니다.

**Break 토큰**: 동일한 패치 수(동일한 면적)를 가지지만 서로 다른 종횡비를 가진 이미지들을 모델이 구별할 수 있도록 돕기 위해, 이미지 행 사이에 [IMAGE BREAK] 토큰을 포함시켰습니다. 또한 이미지 시퀀스의 끝에는 [IMAGE END] 토큰을 추가했습니다. 이러한 특수 토큰들은 모델이 이미지의 공간적 구조를 더 잘 이해할 수 있게 해줍니다.

**FFN에서의 게이팅**: 어텐션 블록의 표준 피드포워드 레이어 대신, 은닉층에서 게이팅을 사용합니다. 이는 [GLU Variants Improve Transformer](https://arxiv.org/pdf/2002.05202v1)에서 제안된 기법으로, 정보의 흐름을 더 효과적으로 제어할 수 있게 해줍니다. 게이팅 메커니즘은 입력의 일부를 선택적으로 통과시키거나 차단함으로써 모델의 표현력을 향상시킵니다.

**시퀀스 패킹**: 단일 배치 내에서 이미지들을 효율적으로 처리하기 위해, 이미지들을 시퀀스 차원을 따라 평면화하고 연결합니다. 서로 다른 이미지의 패치들 간에 어텐션 누출이 발생하지 않도록 블록 대각선 마스크를 구성합니다. 이 방식은 가변 크기의 이미지들을 배치로 처리할 때 메모리와 계산 효율성을 크게 향상시킵니다.

**RoPE-2D**: 이미지 패치에 대한 전통적인 학습된 절대 위치 임베딩을 상대적, 회전 위치 인코딩으로 대체했습니다. 학습된 위치 임베딩은 새로운 이미지 크기를 다룰 때 보간이 필요하며(종종 성능 저하를 수반), 상대적 위치 인코딩은 가변 이미지 크기에 자연스럽게 적응할 수 있습니다.

RoPE-2D 변환의 수학적 정의를 살펴보면, $d$차원 패치 벡터 $x$가 이미지의 위치 $(i,j)$에 나타날 때 $x^{(i,j)}$로 표기됩니다. 그러면 RoPE-2D 변환은 다음과 같이 표현됩니다.

$$\text{RoPE-2D}(x^{(i,j)}, \Theta) = M^{(i,j)}_{\Theta}x^{(i,j)}$$

여기서 $M^{(i,j)}_{\Theta}$는 다음과 같은 블록 대각선 행렬입니다.

$$M^{(i,j)}_{\Theta} = \begin{pmatrix}
\cos i\theta_{1} & -\sin i\theta_{1} & 0 & 0 & \cdots & 0 & 0 \\
\sin i\theta_{1} & \cos i\theta_{1} & 0 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cos j\theta_{2} & -\sin j\theta_{2} & \cdots & 0 & 0 \\
0 & 0 & \sin j\theta_{2} & \cos j\theta_{2} & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cos j\theta_{\frac{d}{2}} & -\sin j\theta_{\frac{d}{2}} \\
0 & 0 & 0 & 0 & \cdots & \sin j\theta_{\frac{d}{2}} & \cos j\theta_{\frac{d}{2}}
\end{pmatrix}$$

이 행렬에서 부분 행렬 $M^{(i,j)}_{\Theta}[k:k+2,k:k+2]$는 차원 $k$가 홀수일 때 특성의 높이 위치($i$)를 포착하고, $k$가 짝수일 때 너비 위치($j$)를 포착합니다(1-기반 인덱싱). 또한 $\Theta = [\theta_{1}\dots\theta_{d/2}]$는 $x$의 다양한 차원에 대한 주파수 벡터이며, $\theta_m$은 RoPE-1D의 표준 관행을 따라 정의됩니다.

이 RoPE-2D 변환의 핵심적인 특성은 "상대적" 속성을 만족한다는 것입니다. 즉, 두 벡터 간의 내적이 절대 위치가 아닌 높이와 너비 위치의 상대적 차이에만 의존한다는 것입니다. 이러한 특성은 모델이 이미지 크기에 관계없이 일관된 공간적 관계를 학습할 수 있게 해줍니다.

### 완전한 아키텍처

![완전한 Pixtral 아키텍처](https://arxiv.org/html/2410.07073/extracted/5915552/images/full_arch.png)

Pixtral 비전 인코더는 2층 완전 연결 네트워크를 통해 멀티모달 디코더와 연결됩니다. 이 네트워크는 비전 인코더의 출력을 디코더가 요구하는 입력 임베딩 크기로 변환하며, 동일한 크기의 중간 은닉층을 거쳐 GeLU 활성화 함수를 사용합니다.

이미지 토큰들은 멀티모달 디코더에서 텍스트 토큰들과 동일하게 처리되며, 모든 토큰에 대해 RoPE-1D 위치 인코딩이 적용됩니다. 특히 디코더는 인과적 셀프 어텐션 메커니즘을 사용하여 멀티이미지 대화와 같은 기능을 원활하게 지원합니다.

**논의**: 이 비전 인코더는 멀티모달 모델링을 위해 특별히 설계되었습니다. 전통적인 인코더들은 일반적으로 224×224 또는 336×336 픽셀과 같은 해상도에서 ImageNet 성능에 최적화되어 있습니다. 표준 분류부터 광학 문자 인식까지 다양한 작업을 유연하게 수행하는 멀티모달 언어 모델에 통합될 때, 기존 연구들은 일반적으로 이미지를 더 작은 (정사각형) 타일로 나누어 각 타일을 비전 인코더에 독립적으로 입력하는 방식을 사용했습니다.

반면 Pixtral의 비전 인코더는 고해상도와 저해상도 이미지 모두를 원본 종횡비 그대로 자연스럽게 적응할 수 있어, 멀티모달 작업에서 상당히 향상된 성능을 제공합니다. 이러한 접근 방식은 이미지의 원본 구조와 비율을 보존함으로써 더 정확한 시각적 이해를 가능하게 하며, 특히 문서나 차트와 같이 공간적 관계가 중요한 콘텐츠에서 큰 장점을 보입니다.
## MM-MT-Bench: 멀티모달 인스트럭션 팔로잉 벤치마크

기존의 대부분 멀티모달 벤치마크들은 입력 이미지에 대한 객관식 질의응답 형태의 모델 능력을 측정합니다. 이러한 평가 방식은 모델의 이미지 이해 능력에 대한 유용한 신호를 제공하지만, 실제 사용자에게 제공하는 모델의 유용성(예: 멀티모달 어시스턴트나 챗봇으로서의 활용도)을 완전히 포착하지는 못합니다. 이러한 실용적 품질을 측정하기 위해, 인스트럭션 튜닝된 텍스트 전용 모델들은 일반적으로 [MT-Bench](https://arxiv.org/html/2410.07073v2#bib.bib25)에서 평가되는데, 여기서는 독립적인 LLM 판정자가 참조 답변과 비교하여 모델의 출력을 평가합니다.

연구진은 텍스트 전용 변형과 유사한 방식으로 인스트럭션 튜닝된 멀티모달 모델의 성능을 평가하기 위해 Multimodal MT-Bench(MM-MT-Bench)라는 새로운 벤치마크를 구축하고 공개했습니다. 이 벤치마크는 실제 비전-언어 모델의 사용 패턴을 반영하여 설계되었으며, 단순한 질의응답을 넘어서 실제 사용자와의 상호작용에서 나타나는 복잡한 요구사항들을 평가할 수 있습니다.

### 벤치마크 설계

MM-MT-Bench는 총 92개의 대화로 구성되어 있으며, 다섯 가지 이미지 카테고리에 걸쳐 광범위한 실용적 사용 사례를 다룹니다. 차트(21개), 표(19개), PDF 페이지(24개), 다이어그램(20개), 그리고 기타(8개)입니다. 이러한 다양한 카테고리는 실제 사용자들이 멀티모달 AI와 상호작용할 때 마주치는 다양한 시각적 콘텐츠를 포괄적으로 반영합니다.

대화 구조 측면에서 보면, 69개의 단일 턴 대화, 18개의 2턴 대화, 4개의 3턴 대화, 그리고 1개의 4턴 대화로 구성되어 있습니다. 이러한 멀티턴 구조는 실제 사용자와 AI 어시스턴트 간의 자연스러운 대화 흐름을 모방하며, 모델이 이전 대화 맥락을 유지하면서 일관된 응답을 생성할 수 있는지를 평가합니다.

모델을 평가할 때는 대화의 모든 턴에 대해 병렬로 쿼리를 수행하며, 과거 턴에 대한 참조 답변을 히스토리로 제공합니다. 각 턴은 전체 대화 히스토리가 제공된 상태에서 판정자에 의해 독립적으로 평가됩니다. 판정자는 정확성(추출된 정보가 올바른가)과 완전성(모델의 답변이 참조에서 제기된 모든 요점을 다루는가)을 기준으로 1점에서 10점 척도로 대화를 평가하도록 지시받습니다.

![MM-MT-Bench 평가 과정](https://arxiv.org/html/2410.07073/x1.png)

평가 과정은 위 그림에서 보듯이 체계적으로 구성되어 있습니다. 입력 이미지, 참조 답변, 그리고 모델 응답이 주어지면, 독립적인 LLM 판정자가 1점부터 10점까지의 척도로 모델의 응답을 평가합니다. 이러한 평가 방식은 인간의 주관적 판단을 모방하면서도 일관성 있는 평가 기준을 제공합니다.

MM-MT-Bench의 신뢰성을 검증하기 위해 연구진은 LMSys-Vision ELO 평점과의 상관관계를 분석했습니다. 그 결과 0.91의 피어슨 상관계수를 보여, 이 벤치마크가 실제 사용자 선호도와 높은 일치성을 가진다는 것을 확인했습니다. 이는 MM-MT-Bench가 단순히 학술적 평가를 위한 도구가 아니라, 실제 사용자 경험을 예측할 수 있는 실용적인 평가 도구임을 의미합니다.

### 벤치마크 예시와 특징

MM-MT-Bench는 실제 비전-언어 모델의 사용 패턴을 모방하도록 설계되었으며, 이미지 내용에 대한 추출, 요약, 추론 작업을 포함합니다. 각 카테고리의 대표적인 이미지들과 비전-언어 모델들의 평가된 응답 예시들이 제공되어, 벤치마크의 실제 적용 양상을 구체적으로 확인할 수 있습니다.

연구진은 이미지, 프롬프트, 답변을 수동으로 큐레이션했으며, 두 번째 라벨러 그룹으로부터 답변을 검증받았습니다. 모든 프롬프트가 올바르게 답변되기 위해서는 이미지 입력에 대한 참조가 필요하도록 보장했습니다. 이는 모델이 단순히 텍스트 정보만으로는 답변할 수 없고, 반드시 시각적 정보를 이해하고 활용해야 한다는 것을 의미합니다.

### 성능 평가 결과

| Model | MathVista | MMMU | ChartQA | DocVQA | VQAv2 | MM-MT-Bench | LMSys-Vision |
|-------|-----------|------|---------|--------|-------|-------------|--------------|
| Pixtral 12B | 58.3 | 52.0 | 81.8 | 90.7 | 78.6 | 6.05 | 1076 |
| Qwen-2-VL 7B | 53.7 | 48.1 | 41.2 | 94.5 | 75.9 | 5.45 | 1040 |
| → w/ Flexible Parsing | 55.2 | 48.7 | 77.5 | - | - | - | - |
| Llama-3.2 11B | 24.3 | 23.0 | 14.8 | 91.1 | 67.1 | 4.79 | 1032 |
| → w/ Flexible Parsing | 47.9 | 45.3 | 78.5 | - | - | - | - |
| Molmo-D 7B | 12.3 | 24.3 | 27.0 | 72.2 | 57.1 | 3.72 | - |
| LLaVA-OneVision 7B | 36.1 | 45.1 | 67.2 | 90.5 | 78.4 | 4.12 | - |
| Claude-3 Haiku | 44.8 | 50.4 | 69.6 | 74.6 | 68.4 | 5.46 | 1000 |
| Gemini-1.5-Flash 8B | 56.9 | 50.7 | 78.0 | 79.5 | 65.5 | 5.93 | 1111 |
| Molmo 72B | 52.2 | 52.7 | 75.6 | 86.5 | 75.2 | 3.51 | - |
| LLaVA-OneVision 72B | 57.2 | 54.4 | 66.9 | 91.6 | 83.8 | 4.95 | 992 |
| Qwen-2-VL 72B | 68.2 | 60.3 | 66.6 | 96.3 | 81.6 | 6.59 | 1104 |
| Llama-3.2 90B | 49.1 | 53.7 | 33.8 | 85.7 | 67.0 | 5.50 | 1071 |
| GPT-4o (0513) | 64.6 | 68.6 | 85.1 | 88.9 | 77.8 | 7.72 | 1208 |
| Claude-3.5 Sonnet | 64.4 | 68.0 | 87.6 | 90.3 | 70.7 | 7.50 | 1189 |

위 표에서 보듯이 Pixtral 12B는 비슷한 크기의 오픈소스 모델들과 여러 클로즈드 소스 모델들을 상당한 차이로 능가합니다. 특히 주목할 점은 MM-MT-Bench에서 6.05점을 기록하여 같은 크기 범주의 다른 모델들을 크게 앞서는 성능을 보인다는 것입니다.

연구진은 투명한 비교를 위해 모든 모델을 동일한 프롬프트와 평가 메트릭으로 재평가했습니다. [Qwen2-VL 7B](https://arxiv.org/pdf/2409.12191)와 [Llama-3.2 11B](https://arxiv.org/pdf/2407.21783)에 대해서는 완화된 평가 제약 조건 하에서의 성능도 추가로 보고했습니다. 이는 일부 오픈소스 모델들의 보고된 수치와의 격차를 더 자세히 조사하기 위한 것입니다.

### 언어 벤치마크에서의 성능

| Model | MT-Bench | MMLU | Math | HumanEval |
|-------|----------|------|------|-----------|
| Pixtral 12B | 7.68 | 69.2 | 48.1 | 72.0 |
| LLaVA-OneVision 7B | 6.94 | 67.9 | 38.6 | 65.9 |
| Molmo-D 7B | 4.53 | 61.2 | 10.2 | 3.7 |
| Qwen-2-VL 7B | 6.41 | 68.5 | 27.9 | 62.2 |
| Llama-3.2 11B | 7.51 | 68.5 | 48.3 | 62.8 |

언어 벤치마크 결과에서도 Pixtral 12B는 비교 가능한 크기의 오픈소스 모델들을 일관되게 능가하는 성능을 보입니다. 이는 Pixtral이 멀티모달 능력을 획득하면서도 기존의 텍스트 전용 배포에서 드롭인 대체재로 사용될 수 있음을 의미합니다.

특히 MT-Bench에서 7.68점을 기록하여 텍스트 전용 인스트럭션 팔로잉에서도 뛰어난 성능을 보이며, MMLU에서 69.2%, Math에서 48.1%, HumanEval에서 72.0%의 성과를 달성했습니다. 이러한 결과는 멀티모달 확장이 기존의 언어 이해 능력을 손상시키지 않았음을 명확히 보여줍니다.

MM-MT-Bench의 도입은 멀티모달 AI 모델 평가에 있어서 중요한 진전을 나타냅니다. 기존의 단순한 질의응답 형태를 넘어서 실제 사용자와의 복잡한 상호작용을 평가할 수 있는 도구를 제공함으로써, 연구 커뮤니티가 더욱 실용적이고 유용한 멀티모달 AI 시스템을 개발할 수 있는 기반을 마련했습니다.
## 결과

이 섹션에서는 Pixtral 12B를 다양한 크기의 클로즈드 소스 및 오픈소스 모델들과 비교하여 포괄적인 평가를 제공하며, 모든 모델을 동일한 평가 프레임워크를 통해 재평가했습니다. 특히 각 데이터셋에 대해 [GPT-4o](https://arxiv.org/pdf/2303.08774)와 [Claude-3.5 Sonnet](https://www.anthropic.com/news/claude-3-family) 같은 최고 수준의 멀티모달 모델들의 보고된 결과를 재현할 수 있도록 프롬프트를 설계했습니다. 이러한 프롬프트들은 '명시적(Explicit)'이며 출력 형식을 완전히 지정하여, 프롬프트 지시사항을 따르는 모델들이 테스트 시점에서 정확하게 평가될 수 있도록 합니다.

### 주요 결과

**멀티모달 성능**: Pixtral은 비슷한 규모의 모든 오픈 모델들을 상당한 차이로 능가하며, [Claude-3 Haiku](https://www.anthropic.com/news/claude-3-family)와 [Gemini-1.5 Flash 8B](https://arxiv.org/pdf/2403.05530) 같은 클로즈드 소스 모델들도 뛰어넘습니다. 특히 Pixtral은 실제 사용 사례를 대상으로 하는 MM-MT-Bench에서 비교 가능한 크기의 모든 모델들을 능가하는 성능을 보이며, 이는 LMSys Vision Arena에서의 강력한 성능으로도 입증됩니다.

이 공개 리더보드에서 Pixtral 12B는 [Qwen2-VL 72B](https://arxiv.org/pdf/2409.12191)와 [Llama-3.2 90B](https://arxiv.org/pdf/2407.21783) 같은 가장 큰 오픈 웨이트 모델들의 성능에 근접합니다. 연구진은 '명시적' 프롬프트를 사용했을 때 일부 오픈소스 모델들의 성능이 보고된 수치보다 상당히 낮아진다는 점을 강조합니다.

가장 가까운 오픈소스 모델들인 [Qwen2-VL 7B](https://arxiv.org/pdf/2409.12191)와 [Llama-3.2 11B](https://arxiv.org/pdf/2407.21783)의 경우, 이러한 성능 저하는 주로 모델들이 답변 형식에 대한 지시사항을 따르지 않기 때문입니다. 예를 들어, "Final answer: 6" 대신 "The answer is 6."을 생성하는 경우가 이에 해당합니다. 이러한 모델들과의 투명한 비교를 위해, 연구진은 더 유연한 파싱을 사용한 평가 결과도 회색으로 추가 보고했습니다.

**언어 성능**: Pixtral 12B는 멀티모달 능력을 추구하면서도 텍스트 이해 능력을 전혀 희생하지 않아, 텍스트와 비전 작업 모두에서 적합한 드롭인 대체재가 됩니다. 이는 실용적인 배포 시나리오에서 매우 중요한 특성으로, 기존의 텍스트 전용 워크플로우를 방해하지 않으면서도 멀티모달 기능을 추가할 수 있음을 의미합니다.

### 프롬프트 선택 방법론

연구진의 평가 프레임워크에서는 최고 수준의 클로즈드 소스 모델들인 [GPT-4o](https://arxiv.org/pdf/2303.08774)와 [Claude-3.5-Sonnet](https://www.anthropic.com/news/claude-3-family)의 보고된 결과를 재현할 수 있는 프롬프트를 선택했습니다. 이러한 프롬프트들은 부록에 제공되며, 10개 프롬프트에 대한 평균 결과도 별도로 보고했습니다.

연구진은 일반적으로 사용되는 프롬프트들이 출력 형식을 제대로 지정하지 않는다는 중요한 발견을 했습니다. 예를 들어, 객관식 질문에 대해 오픈소스 프롬프트들은 "위 옵션에서 정답을 선택하세요"와 같은 모호한 지시사항을 포함합니다. 이 경우 모델들은 답변이 인덱스("Option A", "Option B" 등)로 제시되어야 하는지 자연어 응답으로 제시되어야 하는지 알 수 없습니다. 그 결과 모델들은 잘못된 형식으로 인해 불이익을 받게 됩니다.

![프롬프트 효과 비교](https://arxiv.org/html/2410.07073/x2.png)

위 그림은 MMMU의 실제 예시를 통해 '순진한(Naive)' 프롬프트와 '명시적(Explicit)' 프롬프트의 효과를 보여줍니다. 최고 수준의 모델들은 출력 형식에 대한 세부사항을 제공하는 '명시적' 프롬프트로부터 큰 이익을 얻습니다. 이는 실질적으로 정확한 응답이 평가 중에 부정확한 것으로 표시되는 것을 방지하기 때문입니다.

| 데이터셋 | VQAv2 | | ChartQA | | MMMU | |
|---------|-------|-------|---------|-------|------|-------|
| 프롬프트 | Naive | Explicit | Naive | Explicit | Naive | Explicit |
| GPT-4o (0513) | 64.2 | 77.8 | 58.0 | 85.1 | 55.0 | 68.6 |
| Sonnet-3.5 | 50.2 | 70.7 | 39.6 | 87.6 | 48.6 | 68.0 |
| Qwen-2-VL 7B | 82.1 | 75.9 | 83.4 | 41.2 | 46.7 | 48.1 |
| Llama-3.2 11B | 29.5 | 67.1 | 0.0 | 14.8 | 20.7 | 23.0 |
| Llama-3.2 90B | 52.6 | 67.0 | 3.9 | 33.8 | 27.0 | 53.7 |
| Pixtral 12B | 78.9 | 78.6 | 84.3 | 81.8 | 45.8 | 52.0 |

위 표는 프롬프트 절제 실험 결과를 보여줍니다. 최고 수준의 모델들은 출력 형식을 명시적으로 지정하는 프롬프트가 필요하며, Pixtral 12B는 '명시적' 프롬프트와 '순진한' 프롬프트 모두에서 잘 작동하고 ChartQA에서만 약간의 성능 저하를 보입니다.

### 평가 메트릭에 대한 민감도

연구진은 명시적 프롬프트를 사용하더라도 많은 모델들이 여전히 다양한 형식으로 출력을 제공하며, 이는 참조 답변과 정확히 일치해야 하는 메트릭에 의해 불이익을 받는다는 것을 발견했습니다. 이를 조사하기 위해, 모델들의 생성 결과를 점진적으로 더 느슨한 파싱 제약 조건 하에서 평가했습니다.

예를 들어, 정답이 "6"인 경우, 유연한 메트릭은 "6.0"이나 "The answer is 6"과 같은 답변에 대해 불이익을 주지 않습니다. '유연한 레벨 3'은 참조 답변이 생성 결과 어디에든 나타나면 응답을 정답으로 표시하는 지나치게 관대한 메트릭으로, 참조 답변이 "6"일 때 "6000"과 같은 답변도 허용하기 때문에 상한선을 보여주기 위해서만 포함되었습니다.

| 모델 | Llama-3.2 11B | Llama-3.2 90B | Qwen2-VL 7B | Pixtral 12B |
|------|---------------|---------------|-------------|-------------|
| **MathVista** | | | | |
| Baseline | 24.3 | 49.1 | 53.7 | 58.3 |
| Flexible level 1 | 25.9 | 50.3 | 54.3 | 58.3 |
| Flexible level 2 | 40.2 | 54.7 | 54.3 | 58.3 |
| Flexible level 3 | 47.9 | 57.3 | 55.2 | 58.5 |
| **MMMU** | | | | |
| Baseline | 23.0 | 53.7 | 48.1 | 52.0 |
| Flexible level 1 | 23.4 | 53.7 | 48.1 | 52.0 |
| Flexible level 2 | 41.0 | 55.7 | 48.1 | 52.0 |
| Flexible level 3 | 45.3 | 56.7 | 48.7 | 52.0 |
| **ChartQA** | | | | |
| Baseline | 14.8 | 33.8 | 41.2 | 81.8 |
| Flexible level 1 | 20.4 | 33.9 | 73.8 | 81.9 |
| Flexible level 2 | 29.9 | 35.6 | 73.8 | 81.9 |
| Flexible level 3 | 78.5 | 79.1 | 77.5 | 82.0 |

위 표는 유연한 파싱 절제 실험 결과를 보여줍니다. 느슨한 파싱 제약 조건 하에서 일부 모델들의 성능이 극적으로 향상되는 것을 확인할 수 있습니다. Pixtral 12B의 성능은 모든 파싱 조건에서 안정적이며, 유연한 파싱을 고려하더라도 계속해서 다른 모델들을 앞서는 성능을 보입니다.

이러한 결과는 일부 모델들의 낮은 점수가 이해 부족이 아니라 프롬프트 지시사항을 제대로 따르지 못하는 능력 때문임을 나타냅니다. Pixtral 12B는 유연한 파싱으로부터 거의 이익을 얻지 않아 지시사항을 따르는 능력을 입증하며, 유연한 메트릭을 사용한 후에도 일반적으로 다른 모델들을 능가할 수 있습니다.

### 비전 인코더 절제 실험

![비전 인코더 성능 비교](https://arxiv.org/html/2410.07073/extracted/5915552/images/vit_ablation.png)

비전 인코더의 설계 선택을 검증하기 위해, 연구진은 [Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485)을 통한 소규모 절제 실험을 수행했습니다. Pixtral-ViT와 [CLIPA](https://arxiv.org/pdf/2209.14169) 백본을 기준선으로 하여 단기간 멀티모달 인스트럭션 튜닝을 실행했습니다. 두 비전 인코더 모두에 대해 [Mistral-Nemo 12B-Instruct](https://arxiv.org/pdf/2407.21783)를 사용하여 멀티모달 디코더를 초기화했습니다.

많은 오픈소스 비전 인코더들과 마찬가지로, CLIPA는 $224 \times 224$ 픽셀의 고정 해상도에서 훈련됩니다. 비전-언어 모델에서 해상도를 확장하기 위해, 기존 방법들은 이미지에서 여러 타일 크롭을 구성하고 각 크롭을 사전 훈련 해상도에서 비전 인코더에 독립적으로 전달합니다.

연구진은 CLIPA로 두 가지 절제 실험을 수행했습니다. (a) 전체 이미지를 $224 \times 224$로 크기 조정, (b) 입력 이미지의 25개 크롭을 구성하여 총 해상도 $1120 \times 1120$을 달성. 이러한 모델들은 각각 224픽셀과 1120픽셀에서 평가되었으며, 유연한 인코더는 최대 해상도 1024픽셀로 가변 이미지 해상도에서 평가되었습니다.

위 그림에서 보듯이, 연구진의 모델은 차트와 문서 이해와 같은 세밀한 이해가 필요한 설정에서 CLIPA를 상당히 능가하면서도 VQAv2와 같은 자연어 벤치마크에서는 동등한 성능을 유지합니다. 이는 Pixtral-ViT가 문서 이해 작업에서 특히 강력한 성능을 보이면서도 일반적인 시각적 이해 능력을 손상시키지 않는다는 것을 보여줍니다.

이러한 결과는 처음부터 멀티모달 작업을 위해 설계된 비전 인코더의 중요성을 강조합니다. 기존의 ImageNet 분류에 최적화된 인코더들과 달리, Pixtral-ViT는 다양한 해상도와 종횡비의 이미지를 자연스럽게 처리할 수 있어 실제 멀티모달 응용에서 상당한 이점을 제공합니다.
## 정성적 예시

이 섹션에서는 Pixtral의 실제 응용 사례를 정성적 예시를 통해 살펴봅니다. 특히 Pixtral은 복잡한 도표에 대한 추론, 멀티이미지 인스트럭션 팔로잉, 차트 이해 및 분석, 그리고 이미지를 코드로 변환하는 작업에서 뛰어난 성능을 보여줍니다.

### 복잡한 도표에 대한 추론

![GDP 세계 지도](https://arxiv.org/html/2410.07073/extracted/5915552/images/gdp.png)

![유럽 GDP 파이 차트](https://arxiv.org/html/2410.07073/extracted/5915552/images/lechat_gdp.png)

Pixtral의 복잡한 도표 이해 능력을 보여주는 대표적인 예시입니다. 이 예시에서 Pixtral은 녹색 박스가 유럽 국가들을 나타낸다는 것을 정확히 식별하고, 모든 유럽 국가들의 GDP를 읽어서 정렬한 후 상위 5개국을 정확한 GDP 수치와 함께 나열했습니다.

이러한 작업은 단순한 텍스트 인식을 넘어서 공간적 관계 이해, 색상 코딩 해석, 수치 데이터 추출 및 정렬 등 여러 복합적인 인지 능력을 요구합니다. Pixtral은 첫 번째 이미지에서 전 세계 GDP 분포를 파악하고, 두 번째 이미지에서 유럽 상위 5개국의 구체적인 GDP 정보를 정확히 추출하여 일관된 분석을 제공했습니다.

### 멀티이미지 인스트럭션 팔로잉

![멀티이미지 테이블 통합](https://arxiv.org/html/2410.07073/extracted/5915552/images/lechat_multi_image_oct.png)

Pixtral은 128K 토큰의 컨텍스트 윈도우 내에서 임의의 개수의 이미지를 처리할 수 있는 능력을 보유하고 있습니다. 이 예시는 Pixtral이 여러 이미지의 정보를 성공적으로 결합하여 하나의 마크다운 테이블로 통합할 수 있음을 보여줍니다.

이러한 멀티이미지 처리 능력은 실제 사용 시나리오에서 매우 중요합니다. 예를 들어, 여러 페이지에 걸친 문서 분석, 다양한 차트와 그래프의 비교 분석, 또는 시계열 데이터의 종합적 해석 등에서 활용될 수 있습니다. Pixtral은 각 이미지에서 추출한 정보를 논리적으로 구조화하여 사용자가 요청한 형식으로 정확히 제시할 수 있습니다.

### 차트 이해 및 분석

![훈련 손실 차트](https://arxiv.org/html/2410.07073/extracted/5915552/images/lechat_chart.png)

Pixtral은 복잡한 차트를 해석하고 분석하는 뛰어난 능력을 보여줍니다. 이 예시에서 Pixtral은 "dark-dragon"이 빨간색 선에 해당한다는 것을 정확히 식별했습니다. 더 나아가, 훈련 손실이 부드럽게 감소해야 한다는 일반적인 기대와 달리, 10K 스텝 지점에서 손실이 급격히 증가하여 훈련이 불안정해졌다는 중요한 패턴을 인식했습니다.

이러한 분석 능력은 단순한 데이터 읽기를 넘어서 도메인 지식을 바탕으로 한 해석을 포함합니다. Pixtral은 머신러닝 훈련 과정에서 손실 함수의 정상적인 행동 패턴을 이해하고 있으며, 이를 바탕으로 비정상적인 패턴을 감지하고 그 의미를 설명할 수 있습니다. 이는 연구자들이 실험 결과를 분석하고 문제점을 파악하는 데 매우 유용한 기능입니다.

### 이미지를 코드로 변환

![웹사이트 인터페이스 스케치](https://arxiv.org/html/2410.07073/extracted/5915552/images/lechat_website.png)

Pixtral의 가장 인상적인 기능 중 하나는 손으로 그린 웹사이트 인터페이스를 실행 가능한 HTML 코드로 변환하는 능력입니다. 이 예시는 아이스크림 맛을 선택하는 간단한 웹 인터페이스 스케치를 완전히 기능하는 웹사이트로 변환하는 과정을 보여줍니다.

이러한 기능은 프로토타이핑과 개발 과정을 혁신적으로 단축시킬 수 있습니다. 디자이너나 개발자가 아이디어를 종이에 스케치하면, Pixtral이 이를 즉시 실행 가능한 코드로 변환하여 빠른 프로토타입 제작을 가능하게 합니다. 이는 UI/UX 디자인 프로세스에서 아이디어 검증과 반복 개발을 크게 가속화할 수 있는 혁신적인 도구입니다.

### 모델 간 성능 비교

![모델 응답 비교](https://arxiv.org/html/2410.07073/x3.png)

MM-MT-Bench의 실제 예시를 통해 Pixtral 12B, QwenVL-7B, Gemini-1.5 Flash-8B 간의 성능을 비교한 결과입니다. 이 예시는 미국 내 세대별 직업 안정성에 대한 우려를 보여주는 복잡한 차트를 분석하는 작업으로, 정확한 이해, 추론, 그리고 분석이 요구됩니다.

Pixtral의 응답은 완전하고 정확하여 8점의 높은 평가를 받았습니다. 반면 Gemini-Flash-8B는 잘못된 정보를 추출했고, QwenVL은 트렌드에 대한 자세한 설명을 제공하지 못했습니다. 이는 Pixtral이 단순히 데이터를 읽는 것을 넘어서 의미 있는 패턴을 파악하고 종합적인 분석을 제공할 수 있음을 보여줍니다.

이러한 정성적 예시들은 Pixtral이 다양한 실제 사용 시나리오에서 어떻게 활용될 수 있는지를 구체적으로 보여줍니다. 복잡한 시각적 정보의 이해부터 실용적인 코드 생성까지, Pixtral은 멀티모달 AI의 실질적인 가능성을 입증하는 포괄적인 능력을 제공합니다. 특히 다른 모델들과의 직접적인 비교를 통해 Pixtral의 우수성이 명확히 드러나며, 이는 실제 사용자 경험에서도 상당한 차이를 만들어낼 것으로 기대됩니다.
## 결론

Pixtral 12B는 텍스트 전용 작업과 멀티모달 작업 모두에서 최고 수준의 성능을 달성하는 혁신적인 멀티모달 언어 모델입니다. 이 모델은 4억 개의 파라미터를 가진 비전 인코더와 120억 개의 파라미터를 가진 멀티모달 디코더로 구성된 새로운 아키텍처를 특징으로 합니다.

Pixtral 12B의 가장 주목할 만한 혁신은 처음부터 새롭게 훈련된 비전 인코더입니다. 이 인코더는 가변적인 이미지 크기를 지원하여 사용자가 상황에 따라 이미지 처리에 사용되는 토큰 수를 유연하게 조절할 수 있게 해줍니다. 이러한 설계는 실시간 응용에서 지연 시간을 최소화하면서도 정밀한 분석이 필요한 경우에는 높은 해상도로 처리할 수 있는 실용적인 장점을 제공합니다.

모델의 뛰어난 인스트럭션 팔로잉 능력은 복잡한 멀티모달 응용에서 특히 유용합니다. 128K 토큰이라는 긴 컨텍스트 윈도우를 통해 여러 이미지가 포함된 복잡한 대화나 문서 분석 작업을 원활하게 처리할 수 있습니다. 이는 실제 사용 시나리오에서 사용자가 연속적인 질의를 통해 점진적으로 더 깊은 분석을 요청할 수 있게 해주는 중요한 기능입니다.

다양한 벤치마크에서의 성능 평가 결과, Pixtral 12B는 다른 오픈 모델들을 상당한 차이로 능가하며, 훨씬 더 큰 모델들과도 경쟁할 수 있는 수준의 성능을 보여줍니다. 특히 비슷한 크기의 모델들과 비교했을 때 멀티모달 추론 능력에서 현저한 우위를 보이면서도, 텍스트 전용 작업에서의 성능 저하는 전혀 나타나지 않았습니다.

이러한 균형잡힌 성능은 Pixtral 12B가 기존의 텍스트 전용 워크플로우에서 드롭인 대체재로 사용될 수 있음을 의미합니다. 조직이나 개발자들은 기존 시스템을 크게 변경하지 않고도 멀티모달 기능을 추가할 수 있어, 실용적인 배포와 채택에서 상당한 이점을 얻을 수 있습니다.

Pixtral 12B는 Apache 2.0 라이선스 하에 공개되어 연구자들과 개발자들이 자유롭게 사용하고 개선할 수 있습니다. 이러한 오픈소스 접근 방식은 AI 기술의 민주화에 기여하며, 멀티모달 AI 연구 분야의 발전을 가속화할 것으로 기대됩니다. 특히 상업적 제약 없이 연구와 상업적 응용 모두에서 활용할 수 있어, 다양한 혁신적인 응용 사례들이 등장할 수 있는 기반을 제공합니다.
## 부록

이 부록에서는 Pixtral 12B 논문의 평가와 분석에 사용된 상세한 기술적 자료들을 제공합니다. 특히 평가 프롬프트의 공개, RoPE-2D의 수학적 증명, 유연한 파싱 설정, 그리고 기존 모델들의 보고된 성능을 재현하기 위한 방법론이 포함되어 있습니다.

### 평가 프롬프트 공개

연구진은 투명성과 재현성을 위해 모든 평가에 사용된 프롬프트를 오픈소스로 공개했습니다. 이러한 프롬프트들은 GPT-4o와 Claude-3.5 Sonnet의 보고된 성능을 재현할 수 있도록 신중하게 설계되었습니다.

**MMMU와 MathVista 프롬프트**는 단계별 추론을 요구하는 구조로 설계되었습니다.

```
Analyze the image and question carefully, using step-by-step reasoning. First, describe any image provided in detail. Then, present your reasoning. And finally your final answer in this format: Final Answer: <answer> where <answer> is:
- The single correct letter choice A, B, C, D, E, F, etc. when options are provided. Only include the letter.
- Your direct answer if no options are given, as a single phrase or number.
- If your answer is a number, only include the number without any unit.
- If your answer is a word or phrase, do not paraphrase or reformat the text you see in the image.
- You cannot answer that the question is unanswerable. You must either pick an option or provide a direct answer.
IMPORTANT: Remember, to end your answer with Final Answer: <answer>.
```

이 프롬프트는 모델이 먼저 이미지를 상세히 설명하고, 추론 과정을 명시적으로 제시한 후, 정확한 형식으로 최종 답변을 제공하도록 구조화되어 있습니다. 특히 답변 형식에 대한 명확한 지침을 제공하여 평가 시 형식 오류로 인한 불이익을 방지합니다.

**ChartQA 프롬프트**는 차트 분석에 특화된 지침을 포함합니다.

```
Analyze the image and question carefully, using step-by-step reasoning. First, describe any image provided in detail. Then, present your reasoning. And finally your final answer in this format: Final Answer: <answer> where <answer> follows the following instructions:
- <answer> should be a single phrase or number.
- <answer> should not paraphrase or reformat the text in the image.
- If <answer> is a ratio, it should be a decimal value like 0.25 instead of 1:4.
- If the question is a Yes/No question, <answer> should be Yes/No.
- If <answer> is a number, it should not contain any units.
- If <answer> is a percentage, it should include a % sign.
- If <answer> is an entity, it should include the full label from the graph.
```

이 프롬프트는 차트에서 추출되는 다양한 데이터 형태(비율, 백분율, 엔티티 라벨 등)에 대한 구체적인 형식 요구사항을 명시하여 일관된 평가를 가능하게 합니다.

**VQAv2와 DocVQA 프롬프트**는 간결성을 강조합니다. VQAv2의 경우 "Answer the question using a single word, number, or short phrase"로 시작하여 최대한 간결한 답변을 요구하며, DocVQA는 "Answer the question using a single word or phrase"라는 더욱 단순한 지침을 사용합니다.

**MM-MT-Bench 판정자 프롬프트**는 GPT-4o가 공정한 평가자 역할을 수행하도록 설계되었습니다.

```
SYSTEM: Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the most recent question given the previous conversation as context. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: "[[rating]]", for example: "Rating: [[5]]".
```

이 프롬프트는 판정자가 정확성과 유용성을 기준으로 객관적인 평가를 수행하도록 지시하며, 1점에서 10점까지의 척도로 일관된 평가를 보장합니다.

### RoPE-2D의 상대적 위치 인코딩 특성 증명

연구진은 RoPE-2D가 상대적 위치 인코딩 특성을 만족한다는 것을 수학적으로 증명했습니다. 이 특성은 다음과 같이 표현됩니다.

$$\langle\textsc{RoPE-2D}(x^{(p,q)},\Theta),\textsc{RoPE-2D}(y^{(r,s)},\Theta)\rangle=\langle\textsc{RoPE-2D}(x^{(p-r,q-s)},\Theta),\textsc{RoPE-2D}(y^{(0,0)},\Theta)\rangle$$

이는 임의의 특성 벡터 $x, y \in \mathbb{R}^d$와 모든 위치 $p, r \in \{0\dots H\}$, $q, s \in \{0\dots W\}$에 대해 성립합니다.

증명을 위해 $d = 4$인 경우를 예시로 살펴보겠습니다. RoPE-2D 변환은 다음과 같이 정의됩니다.

$$\textsc{RoPE-2D}(x^{(p,q)},\Theta) = \begin{pmatrix}
\cos p\theta_{1} & -\sin p\theta_{1} & 0 & 0\\
\sin p\theta_{1} & \cos p\theta_{1} & 0 & 0\\
0 & 0 & \cos q\theta_{2} & -\sin q\theta_{2}\\
0 & 0 & \sin q\theta_{2} & \cos q\theta_{2}
\end{pmatrix}\cdot\begin{pmatrix}x_{1}\\x_{2}\\x_{3}\\x_{4}\end{pmatrix}$$

두 벡터 간의 내적을 계산하면, 삼각함수의 덧셈 공식에 의해:

$$\cos p\theta_{1}\cos r\theta_{1} + \sin p\theta_{1}\sin r\theta_{1} = \cos((p-r)\theta_{1})$$
$$\cos q\theta_{2}\cos s\theta_{2} + \sin q\theta_{2}\sin s\theta_{2} = \cos((q-s)\theta_{2})$$

이를 통해 최종적으로 내적이 절대 위치 $(p,q)$, $(r,s)$가 아닌 상대적 차이 $(p-r, q-s)$에만 의존한다는 것을 보일 수 있습니다. 이러한 특성은 모델이 이미지 크기에 관계없이 일관된 공간적 관계를 학습할 수 있게 해주는 핵심적인 수학적 기반입니다.

### 유연한 파싱 설정

연구진은 모델 평가에서 점진적으로 완화된 제약 조건을 적용하는 세 가지 유연한 파싱 레벨을 도입했습니다.

**기준선(Baseline)**은 프롬프트 지시사항의 정확한 준수를 요구하며, 모델 응답이 "Final Answer: <ANSWER>" 문자열로 끝나야 합니다.

**유연한 파싱 레벨 1**은 "Answer: <ANSWER>"로 끝나는 경우도 허용합니다. 이는 일부 모델들이 "Final" 단어를 생략하는 경향을 고려한 것입니다.

**유연한 파싱 레벨 2**는 추가적으로 마크다운 형식을 제거합니다. "**Answer**", "**Answer:**", "*Answer: <ANSWER>*" 같은 형식을 처리하며, 특히 Llama-3.2 모델들에서 자주 나타나는 마크다운 형식을 고려합니다.

**유연한 파싱 레벨 3**은 가장 관대한 평가 설정으로, 정답이 모델 응답 어디에든 나타나면 정답으로 처리합니다. 단일 문자 답변의 경우 "is <A>", "are <A>", "<A>" 패턴을 검색하고, 숫자 답변의 경우 쉼표가 있거나 없는 형태 모두를 검색합니다. 이 레벨은 상한선 역할을 하며, 잘못된 답변을 정답으로 표시할 수 있어 주의가 필요합니다.

### 프롬프팅에 대한 견고성 분석

**Llama 전용 프롬프트** 분석에서 연구진은 Llama-3.2 모델 계열이 기본적으로 "**Answer:** <ANSWER>" 형식(마크다운 형식과 'Final' 생략)으로 응답하는 경향을 발견했습니다. 명시적 지시에도 불구하고 이러한 기본 출력 형식을 유지하는 특성을 보였습니다.

| 모델 | MathVista | MMMU | ChartQA |
|------|-----------|------|---------|
| **Llama-3.2 11B** | | | |
| 기본 프롬프트 | 24.3 | 23.0 | 14.8 |
| Llama 전용 프롬프트 | 41.6 | 41.9 | 33.7 |
| **Llama-3.2 90B** | | | |
| 기본 프롬프트 | 49.1 | 53.7 | 33.8 |
| Llama 전용 프롬프트 | 57.6 | 58.6 | 34.8 |

Llama 전용 프롬프트를 사용했을 때 특히 11B 변형에서 MathVista와 MMMU에서 15% 이상의 성능 향상을 보였습니다. 반면 Pixtral은 두 프롬프트 모두에서 안정적인 성능을 유지하며 11B 변형을 상당한 차이로 앞섰습니다.

**다중 프롬프트 평균 성능** 분석에서는 Mistral Large v2를 사용하여 기본 프롬프트의 10가지 변형을 생성하고 평가했습니다. 모든 모델을 '유연한 파싱 레벨 3'으로 평가한 결과, 주요 논문의 경향이 그대로 유지되었으며 Pixtral이 비교 가능한 크기의 모델들을 능가하고 Llama-3.2 90B를 MathVista와 ChartQA에서 뛰어넘었습니다. 특히 Pixtral은 프롬프트 간 성능 변동성이 낮아 더욱 안정적인 특성을 보였습니다.

### 보고된 성능 재현 방법론

연구진은 각 모델의 보고된 성능을 재현하기 위해 필요한 구체적인 개입 방법들을 문서화했습니다. 이 분석을 통해 최고 수준의 모델들과 소규모 오픈소스 모델들 간의 중요한 차이점을 발견했습니다.

**요약 분석**에 따르면, 최고 수준의 모델들과 소규모 클로즈드 소스 모델들은 명시적 프롬프트 하에서 지시사항을 정확히 따라 강력한 성능을 보고할 수 있습니다. 반면 소규모 오픈소스 모델들은 일반적으로 모델별 맞춤형 프롬프트 튜닝과 평가 메트릭 조정이 필요합니다. Pixtral 12B는 강력한 클로즈드 소스 모델들처럼 이러한 개입 없이도 강력한 성능을 보고할 수 있습니다.

**클로즈드 모델들**(Claude-3 Haiku, Gemini-Flash-8B)의 경우 표준화된 평가 프로토콜이 보고된 수치와 대략 일치하거나 이를 초과하며, 유연한 파싱을 통해 소폭의 개선을 얻을 수 있습니다. 예외적으로 Claude Haiku의 ChartQA에서는 보고된 성능에 근접하기 위해 유연한 파싱 레벨 3이 필요했습니다.

**Qwen2-VL 7B**는 프롬프트를 ChartQA 훈련 세트와 유사한 한 줄 지시사항으로 단순화하고, 예상되는 답변 형식에 따라 다른 프롬프트를 제공해야 했습니다. 예를 들어, 부동소수점 답변의 경우 "Answer with a two decimal place floating point"를 지정하고, 정수 및 객관식 질문에 대해서도 유사한 프롬프트를 사용했습니다. 모든 형식 사양이 포함된 통합 프롬프트는 성능을 저하시키는 것으로 나타났습니다.

**Llama-3.2 모델들**은 "**Answer**", "**Answer:**", "*Answer: <ANSWER>*" 같은 마크다운 형식으로 기본 응답하는 특성을 보였습니다. 이러한 형식을 요청하도록 '명시적' 프롬프트를 변경함으로써 상당한 개선을 얻을 수 있었고, 유연한 레벨 3으로 평가한 후 보고된 성능을 회복했습니다. Llama-3.2 90B의 DocVQA 평가에서는 많은 생성 결과가 'The answer is <ANSWER>' 형태였는데, 이는 ANLS 메트릭에 의해 불이익을 받았습니다. 이러한 접두사를 제거함으로써 DocVQA 성능이 4.8점 향상되었습니다.

이러한 상세한 분석은 멀티모달 언어 모델 평가에서 프롬프트 설계와 평가 메트릭의 중요성을 강조하며, 공정하고 일관된 비교를 위한 표준화된 프로토콜의 필요성을 보여줍니다.
- - -
### References
* [Pixtral 12B](http://arxiv.org/pdf/2410.07073v2)