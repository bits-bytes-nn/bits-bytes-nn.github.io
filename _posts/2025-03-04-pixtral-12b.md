---
layout: post
title: "Pixtral 12B"
date: 2024-10-09 17:16:22
author: "Mistral AI"
categories: "Multimodal-Learning"
tags: ["RoPE-2D-Positional-Encoding", "Flexible-Vision-Encoder-Architecture", "Multi-Image-Instruction-Following", "Standardized-Multimodal-Evaluation", "MM-MT-Bench-Benchmark", "Variable-Image-Resolution-Processing", "Block-Diagonal-Attention-Masking", "Vision-Encoder-with-Break-Tokens", "Explicit-Prompt-Engineering", "Multimodal-Instruction-Tuning"]
cover: /assets/images/multimodal-learning.jpg
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
멀티모달 언어 모델의 발전에도 불구하고, 대부분의 오픈소스 모델들은 텍스트와 이미지를 동시에 처리할 때 자연어 처리 능력을 희생하거나 제한된 이미지 해상도와 종횡비만 처리할 수 있었습니다. 특히 기존 비전 인코더들은 일반적으로 224×224 또는 336×336 픽셀과 같은 고정된 해상도에서 학습되어, 다양한 크기와 형태의 이미지를 효과적으로 처리하는 데 한계가 있었습니다. 또한 멀티모달 모델의 평가 방법도 표준화되어 있지 않아 모델 간 공정한 비교가 어려웠습니다.
이 연구팀은 텍스트 전용 작업의 능력을 유지하면서도 뛰어난 비전-언어 이해 능력을 갖춘 오픈소스 멀티모달 모델을 개발하고자 했습니다. 특히 다양한 해상도와 종횡비의 이미지를 원본 그대로 처리할 수 있는 유연한 비전 인코더와, 실제 사용 사례를 더 잘 반영하는 새로운 멀티모달 벤치마크의 필요성이 연구의 주요 동기가 되었습니다. 또한 멀티모달 모델 평가를 위한 표준화된 프로토콜을 확립하고자 했습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
Pixtral 12B는 120억 개의 파라미터를 가진 멀티모달 언어 모델로, 완전히 새롭게 설계된 비전 인코더를 통해 이미지를 원본 해상도와 종횡비 그대로 처리할 수 있습니다. 이 모델의 핵심 혁신은 RoPE-2D 구현을 통한 유연한 비전 인코더로, 이미지를 작은 타일로 분할하여 처리하는 기존 방식과 달리 다양한 크기의 이미지를 직접 처리할 수 있습니다. 또한 시퀀스 패킹과 블록 대각선 어텐션 마스크를 도입하여 배치 처리 효율성을 높였습니다.
연구팀은 또한 멀티모달 모델의 실제 사용 사례를 평가하기 위한 새로운 벤치마크인 MM-MT-Bench를 제안했습니다. 이 벤치마크는 단일 턴 및 다중 턴 대화를 포함하며, 차트, 표, PDF 페이지, 다이어그램 등 다양한 유형의 이미지에 대한 모델의 이해력을 평가합니다. 또한 멀티모달 모델 평가를 위한 표준화된 프로토콜을 제시하고, 프롬프트와 평가 지표의 작은 변화가 모델 성능에 미치는 영향을 철저히 분석했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
Pixtral 12B는 Mistral Nemo 12B를 기반으로 한 멀티모달 디코더와, 4억 개의 파라미터를 가진 새로운 비전 인코더로 구성되어 있습니다. 비전 인코더는 토큰 분리 방식으로 이미지 행 사이에 [IMAGE BREAK] 토큰을 삽입하여 종횡비가 다른 이미지를 구분하고, 이미지 시퀀스 끝에는 [IMAGE END] 토큰을 추가합니다. 또한 게이팅된 피드포워드 네트워크와 RoPE-2D 위치 인코딩을 사용하여 다양한 이미지 크기를 자연스럽게 처리합니다.
비전 인코더와 멀티모달 디코더는 두 개의 완전 연결 레이어로 구성된 네트워크를 통해 연결됩니다. 이 네트워크는 비전 인코더의 출력을 디코더가 요구하는 입력 임베딩 크기로 변환합니다. 전체 모델은 128K 토큰의 컨텍스트 윈도우를 지원하여 여러 이미지와 텍스트를 포함한 긴 대화를 처리할 수 있습니다. 모델은 대규모 이미지-텍스트 데이터로 사전 학습되고 지시 조정되어, 다중 턴, 다중 이미지 대화가 가능합니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
Pixtral 12B는 MathVista, MMMU, ChartQA 등 다양한 멀티모달 벤치마크에서 비슷한 크기의 모든 오픈소스 모델(Qwen2-VL 7B, Llama-3.2 11B 등)을 능가하는 성능을 보여주었으며, 심지어 Claude-3 Haiku와 Gemini-1.5 Flash 8B와 같은 비공개 모델보다도 우수한 성능을 달성했습니다. 특히 주목할 만한 점은 Pixtral 12B가 7배나 더 큰 Llama-3.2 90B보다도 멀티모달 작업에서 더 나은 성능을 보였다는 것입니다. 동시에 MATH, HumanEval, MT-Bench와 같은 텍스트 전용 벤치마크에서도 동급 모델보다 우수한 성능을 유지했습니다.
이 연구는 멀티모달 모델 설계와 평가에 있어 중요한 통찰을 제공합니다. 특히 다양한 이미지 크기와 종횡비를 원본대로 처리할 수 있는 유연한 비전 인코더의 중요성을 입증했으며, 프롬프트 설계와 평가 지표가 모델 성능에 미치는 영향을 체계적으로 분석했습니다. Apache 2.0 라이선스로 공개된 Pixtral 12B는 텍스트와 이미지를 모두 효과적으로 처리할 수 있는 강력한 오픈소스 멀티모달 모델로, 다양한 실제 응용 시나리오에서 중요한 도구가 될 것입니다.
- - -
## Pixtral 12B: 멀티모달 언어 모델의 새로운 지평

### 소개

Pixtral 12B는 120억 개의 파라미터를 가진 멀티모달 언어 모델로, 자연 이미지와 문서를 모두 이해할 수 있도록 설계되었습니다. 이 모델은 다양한 멀티모달 벤치마크에서 뛰어난 성능을 보이며, 많은 대형 모델들을 능가하는 결과를 보여주고 있습니다. 특히 주목할 점은 Pixtral이 다른 오픈소스 모델들과 달리 멀티모달 작업에서 뛰어난 성능을 보이면서도 자연어 처리 능력을 희생하지 않았다는 것입니다.

Pixtral은 처음부터 새롭게 학습된 비전 인코더를 사용하여 이미지를 원본 해상도와 종횡비 그대로 처리할 수 있습니다. 이는 사용자에게 이미지 처리에 사용되는 토큰 수를 유연하게 조절할 수 있는 자유를 제공합니다. 또한 Pixtral은 128K 토큰의 긴 컨텍스트 윈도우 내에서 여러 이미지를 처리할 수 있는 능력을 갖추고 있습니다.

Pixtral 12B는 비슷한 크기의 다른 오픈 모델들(Llama-3.2 11B 및 Qwen-2-VL 7B)보다 상당히 우수한 성능을 보여줍니다. 더 놀라운 점은 7배 더 작은 크기임에도 불구하고 Llama-3.2 90B와 같은 훨씬 더 큰 오픈 모델보다 뛰어난 성능을 발휘한다는 것입니다.

연구진은 또한 실제 시나리오에서 비전-언어 모델을 평가하기 위한 오픈소스 벤치마크인 MM-MT-Bench를 제공하고, 멀티모달 LLM에 대한 표준화된 평가 프로토콜을 위한 상세한 분석과 코드를 제공합니다. Pixtral 12B는 Apache 2.0 라이선스로 공개되었습니다.

![Pixtral 성능](https://ar5iv.labs.arxiv.org//html/2410.07073/assets/images/pareto_mm_mt_bench_oct.png)

그림 1: Pixtral 성능. Pixtral은 멀티모달 작업에서 동일한 크기 범주의 모든 오픈 모델보다 상당한 차이로 우수한 성능을 보여줍니다. 왼쪽: MM-MT-Bench에서의 성능, 이는 멀티모달 언어 모델의 실제 사용을 반영하도록 설계된 새로운 멀티모달, 멀티턴, 지시 따르기 벤치마크입니다. 오른쪽: 공개 LMSys 리더보드(Vision arena, 2024년 10월)에서의 성능.

이 논문은 이미지와 텍스트를 모두 이해하도록 학습된 멀티모달 언어 모델인 Pixtral 12B에 대해 설명합니다. 이 모델은 Apache 2.0 라이선스 하에 오픈 웨이트로 공개되었습니다. Pixtral은 대규모 이미지와 텍스트 문서가 혼합된 데이터로 사전 학습된 지시 조정 모델로, 다중 턴, 다중 이미지 대화가 가능합니다.

Pixtral은 새로운 RoPE-2D 구현으로 학습된 새로운 비전 인코더를 탑재하고 있어 이미지를 원본 해상도와 종횡비로 처리할 수 있습니다. 이를 통해 모델은 지연 시간이 제한된 환경에서는 저해상도로 이미지를 처리하고, 세밀한 추론이 필요할 때는 고해상도로 이미지를 처리하는 유연성을 갖게 됩니다.

동일한 평가 환경에서 비슷한 크기의 모델과 비교했을 때, Pixtral은 텍스트 전용 추론 성능을 희생하지 않으면서도 강력한 멀티모달 추론 능력을 제공합니다. 예를 들어, 이 모델은 MMMU와 MathVista와 같은 인기 있는 멀티모달 벤치마크에서 Qwen2-VL 7B와 Llama-3.2 11B와 같은 모델의 성능을 일치시키거나 능가하면서도, MATH와 HumanEval과 같은 인기 있는 텍스트 전용 작업에서 대부분의 오픈소스 모델보다 우수한 성능을 보입니다. Pixtral은 심지어 Llama-3.2 90B와 같은 훨씬 더 큰 모델뿐만 아니라 Claude-3 Haiku와 Gemini-1.5 Flash 8B와 같은 비공개 모델보다도 멀티모달 벤치마크에서 더 나은 성능을 보입니다.

Pixtral과 기준 모델을 평가하는 과정에서, 연구진은 멀티모달 언어 모델에 대한 평가 프로토콜이 표준화되어 있지 않으며, 설정의 작은 변화가 일부 모델의 성능을 극적으로 변화시킬 수 있다는 것을 발견했습니다. 연구진은 공통 평가 프로토콜 하에서 비전-언어 모델을 재평가한 경험에 대한 철저한 분석을 제공합니다. 구체적으로, 평가에 관한 두 가지 문제를 확인했습니다.

• 프롬프트: 여러 벤치마크에는 명확하지 않게 지정된 기본 프롬프트가 있어, 보고된 수치에 비해 주요 비공개 소스 모델의 성능을 크게 감소시킵니다.

• 평가 지표: 공식 지표는 일반적으로 정확한 일치를 요구하며, 이는 참조 답변과 정확히 일치하는 경우에만 모델 생성을 정확하다고 평가합니다. 그러나 이 지표는 실질적으로 정확하지만 약간 다른 형식(예: "6.0" 대 "6")으로 된 답변에 불이익을 줍니다.

이러한 문제를 완화하기 위해, 연구진은 참조 답변이 요구하는 형식을 명시적으로 지정하는 '명시적' 프롬프트를 제안합니다. 또한 다양한 모델에 대한 유연한 구문 분석의 영향을 분석하고, 공정하고 표준화된 평가 프로토콜을 확립하기 위한 노력으로 평가 코드와 프롬프트를 공개합니다.

더욱이, 현재의 멀티모달 벤치마크는 대부분 입력 이미지가 주어진 짧은 형식 또는 객관식 질문 답변을 평가하지만, 실제 사용 사례(예: 다중 턴, 장문 어시스턴트 설정)에서 모델의 유용성을 완전히 포착하지 못합니다. 이를 해결하기 위해, 연구진은 새로운 멀티모달, 멀티턴 평가인 MM-MT-Bench를 오픈소스로 공개합니다. MM-MT-Bench에서의 성능이 LMSys Vision 리더보드의 ELO 랭킹과 높은 상관관계가 있음을 발견했습니다.

Pixtral은 멀티모달 지시 따르기에 탁월하며, MM-MT-Bench 벤치마크에서 비슷한 오픈소스 모델을 능가합니다(그림 1 참조). LMSys Vision 리더보드의 인간 선호도에 기반하여, Pixtral 12B는 현재 가장 높은 순위의 Apache 2.0 모델로, Llama-3.2 11B와 Qwen2-VL 7B와 같은 다른 오픈 모델보다 상당히 우수한 성능을 보입니다. 심지어 Claude-3 Opus & Claude-3 Sonnet과 같은 여러 비공개 모델과 Llama-3.2 90B와 같은 더 큰 모델보다도 더 높은 순위를 차지합니다.

## Pixtral 12B: 아키텍처 세부 사항

### 아키텍처 개요

Pixtral 12B는 트랜스포머 아키텍처를 기반으로 하며, 고수준 추론을 수행하는 멀티모달 디코더와 이미지를 처리하는 비전 인코더로 구성되어 있습니다. 표 1은 디코더와 인코더의 주요 파라미터를 요약하고 있습니다.

| 파라미터 | 디코더 | 인코더 |
|---------|-------|-------|
| dim | 5120 | 1024 |
| n_layers | 40 | 24 |
| head_dim | 128 | 64 |
| hidden_dim | 14336 | 4096 |
| n_heads | 32 | 16 |
| n_kv_heads | 8 | 16 |
| context_len | 131072 | 4096 |
| vocab_size | 131072 | - |
| patch_size | - | 16 |

표 1: 디코더와 인코더 파라미터.

### 멀티모달 디코더

Pixtral 12B는 Mistral Nemo 12B를 기반으로 구축되었습니다. Mistral Nemo 12B는 120억 개의 파라미터를 가진 디코더 전용 언어 모델로, 다양한 지식 및 추론 작업에서 뛰어난 성능을 보여줍니다.

### 비전 인코더

![Pixtral 비전 인코더](https://ar5iv.labs.arxiv.org//html/2410.07073/assets/images/pixtral_vit.png)

그림 2: Pixtral 비전 인코더. Pixtral은 다양한 이미지 크기와 종횡비를 기본적으로 지원하도록 처음부터 학습된 새로운 비전 인코더를 사용합니다. 블록 대각선 어텐션 마스크는 배치 처리를 위한 시퀀스 패킹을 가능하게 하고, RoPE-2D 인코딩은 다양한 이미지 크기를 처리할 수 있게 합니다. 어텐션 마스크와 위치 인코딩은 비전 트랜스포머에 추가 입력으로 제공되며, 셀프 어텐션 레이어에서만 활용됩니다.

Pixtral 12B가 이미지를 처리할 수 있도록 Pixtral-ViT라는 새로운 비전 인코더를 처음부터 학습시켰습니다. 이 인코더의 목표는 다양한 해상도와 종횡비의 이미지를 처리할 수 있는 간단한 아키텍처를 구현하는 것입니다. 이를 위해 4억 개의 파라미터를 가진 비전 트랜스포머를 구축하고(표 1 참조), 표준 아키텍처에 네 가지 주요 변경사항을 적용했습니다.

1. **토큰 분리**: 동일한 수의 패치(동일한 면적)를 가지지만 종횡비가 다른 이미지를 구분하기 위해 이미지 행 사이에 [IMAGE BREAK] 토큰을 포함합니다. 또한 이미지 시퀀스의 끝에 [IMAGE END] 토큰을 추가합니다.

2. **FFN의 게이팅**: 어텐션 블록에서 표준 피드포워드 레이어 대신 은닉층에 게이팅을 사용합니다.

3. **시퀀스 패킹**: 단일 배치 내에서 이미지를 효율적으로 처리하기 위해 이미지를 시퀀스 차원을 따라 평탄화하고 연결합니다. 서로 다른 이미지의 패치 간에 어텐션 누출이 없도록 블록 대각선 마스크를 구성합니다.

4. **RoPE-2D**: 이미지 패치에 대한 기존의 학습된 절대 위치 임베딩을 셀프 어텐션 레이어에서 상대적인 회전 위치 인코딩으로 대체합니다. 학습된 위치 임베딩은 새로운 이미지 크기를 처리하기 위해 보간해야 하지만(종종 성능 저하를 초래함), 상대적 위치 인코딩은 다양한 이미지 크기에 자연스럽게 적용됩니다.

특히, $d$-차원 패치 벡터 $x$가 있다고 가정합시다(키 또는 쿼리 특징). 이 특징이 이미지의 위치 $(i,j)$에 나타날 때 $x^{(i,j)}$로 표시합니다. 그러면 $x^{(i,j)}$의 RoPE-2D 변환은 다음과 같이 표현됩니다.

$$\text{RoPE-2D}(x^{(i,j)},\Theta)=M^{(i,j)}_{\Theta}x^{(i,j)}\,,$$

여기서 $M^{(i,j)}_{\Theta}=$

$$\begin{pmatrix}\cos i\theta_{1}&-\sin i\theta_{1}&0&0&\cdots&0&0\\ \sin i\theta_{1}&\cos i\theta_{1}&0&0&\cdots&0&0\\ 0&0&\cos j\theta_{2}&-\sin j\theta_{2}&\cdots&0&0\\ 0&0&\sin j\theta_{2}&\cos j\theta_{2}&\cdots&0&0\\ \vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\ 0&0&0&0&\cdots&\cos j\theta_{\frac{d}{2}}&-\sin j\theta_{\frac{d}{2}}\\ 0&0&0&0&\cdots&\sin j\theta_{\frac{d}{2}}&\cos j\theta_{\frac{d}{2}}\end{pmatrix}\,.$$

여기서 하위 행렬 $M^{(i,j)}\_{\Theta}[k:k+2,k:k+2]$는 차원 $k$의 홀수 값에 대해 특징의 높이 위치($i$)를 캡처하고, $k$의 짝수 값에 대해 너비 위치($j$)를 캡처합니다(1부터 시작하는 인덱싱). 또한, $\Theta=[\theta_{1}\dots\theta_{d/2}]$는 $x$의 다양한 차원에 대한 주파수 벡터로, $\theta_{m}$은 RoPE-1D에 대한 표준 관행을 따라 정의됩니다.

중요하게도, RoPE-2D 변환의 이러한 간단한 구현은 "상대적" 속성을 만족합니다. 두 벡터 사이의 내적은 그들의 절대 위치가 아닌 높이와 너비 위치의 상대적 차이에만 의존합니다.

![Pixtral 전체 아키텍처](https://ar5iv.labs.arxiv.org//html/2410.07073/assets/images/full_arch.png)

그림 3: 완전한 Pixtral 아키텍처. Pixtral은 두 가지 구성 요소를 가지고 있습니다. 이미지를 토큰화하는 비전 인코더와 텍스트와 이미지의 시퀀스가 주어졌을 때 다음 텍스트 토큰을 예측하는 멀티모달 디코더. Pixtral은 128K 컨텍스트 윈도우 내에 들어가는 한 임의의 수의 이미지를 입력으로 받을 수 있습니다.

#### 논의: 멀티모달 모델링을 위한 비전 인코더

Pixtral의 비전 인코더는 특별히 멀티모달 모델링을 위해 설계되었습니다. 전통적인 인코더는 일반적으로 $224 \times 224$ 또는 $336 \times 336$ 픽셀과 같은 해상도에서 ImageNet 성능을 최적화하도록 설계되었습니다. 이러한 인코더를 멀티모달 언어 모델에 통합할 때, 기존 연구들은 일반적으로 이미지를 작은 (정사각형) 타일로 나누어 각 타일을 독립적으로 비전 인코더에 공급합니다. 반면, Pixtral의 비전 인코더는 고해상도와 저해상도 이미지를 원래의 종횡비로 자연스럽게 처리할 수 있어 멀티모달 작업에서 상당히 향상된 성능을 제공합니다.

### 완전한 아키텍처

Pixtral 비전 인코더는 두 개의 완전 연결 레이어로 구성된 네트워크를 통해 멀티모달 디코더와 연결됩니다. 이 네트워크는 비전 인코더의 출력을 디코더가 요구하는 입력 임베딩 크기로 변환하며, 동일한 크기의 중간 은닉층을 통해 GeLU 활성화 함수를 사용합니다.

이미지 토큰은 멀티모달 디코더에 의해 텍스트 토큰과 동일하게 처리되며, 모든 토큰에 대해 RoPE-1D 위치 인코딩이 적용됩니다. 특히, 디코더는 인과적(causal) 셀프 어텐션 메커니즘을 사용하여 다중 이미지 대화와 같은 기능을 원활하게 지원합니다. 전체 아키텍처는 그림 3에 나와 있습니다.

### MM-MT-Bench: 멀티모달 지시 따르기를 위한 벤치마크

기존의 대부분 멀티모달 벤치마크는 입력 이미지가 주어졌을 때 모델이 객관식 질문에 답변하는 능력을 측정합니다. 이러한 방식은 모델이 이미지를 이해하는 능력을 평가하는 데 유용하지만, 실제 사용자에게 멀티모달 어시스턴트나 챗봇으로서 제공할 수 있는 유용성을 완전히 측정하지는 못합니다. 텍스트 전용 지시 조정 모델은 일반적으로 MT-Bench에서 평가되는데, 이는 독립적인 LLM 심사관이 참조 답변을 기준으로 모델의 출력을 평가하는 방식입니다.

이러한 맥락에서, 연구진은 텍스트 전용 변형과 유사한 방식으로 지시 조정된 멀티모달 모델의 성능을 평가하기 위한 새로운 벤치마크인 Multimodal MT-Bench(MM-MT-Bench)를 구축하고 공개했습니다.

#### 설계

MM-MT-Bench는 총 92개의 대화로 구성되어 있습니다. 이 벤치마크는 다양한 실용적 사용 사례를 포괄하며, 다섯 가지 이미지 카테고리를 다룹니다.

- 차트(21개)
- 표(19개)
- PDF 페이지(24개)
- 다이어그램(20개)
- 기타(8개)

대화 구성 측면에서는 다음과 같이 분류됩니다.
- 단일 턴 대화: 69개
- 2턴 대화: 18개
- 3턴 대화: 4개
- 4턴 대화: 1개

모델을 평가하기 위해, 연구진은 대화의 모든 턴에 대해 모델에 병렬로 쿼리하며, 과거 턴에 대한 참조 답변을 대화 기록으로 제공합니다. 각 턴은 전체 대화 기록이 제공된 상태에서 심사관에 의해 독립적으로 평가됩니다. 심사관은 다음 두 가지 기준에 따라 1점부터 10점까지의 척도로 대화를 평가하도록 지시받습니다.

1. 정확성(correctness): 추출된 정보가 정확한지 여부
2. 완전성(completeness): 모델의 답변이 참조에서 제기된 모든 요점을 다루는지 여부

![MM-MT-Bench: 멀티모달 모델을 위한 지시 따르기 벤치마크로, LMSys ELO 평가와 높은 상관관계를 보입니다. 입력 이미지, 참조 답변, 모델 응답이 주어지면 독립적인 LLM 심사관이 모델의 응답을 1-10 척도로 평가합니다.](https://ar5iv.labs.arxiv.org//html/2410.07073/assets/x1.png)

그림 4: MM-MT-Bench: 연구진은 멀티모달 모델을 위한 새로운 지시 따르기 벤치마크를 오픈소스로 공개했으며, 이는 LMSys ELO 평가와 높은 상관관계를 보입니다. 입력 이미지, 참조 답변, 모델 응답이 주어지면 독립적인 LLM 심사관이 모델의 응답을 1점부터 10점까지의 척도로 평가합니다.

평가 과정은 그림 4에 설명되어 있으며, 심사관 프롬프트는 부록 A.5에 제공되어 있습니다. 표 2에 나타난 결과에 따르면, MM-MT-Bench는 LMSys-Vision ELO 평가와 0.91의 피어슨 상관 계수를 보여, 이 벤치마크가 모델의 전반적인 성능을 잘 반영함을 알 수 있습니다.

#### 예시

MM-MT-Bench는 이미지 내용에 대한 추출, 요약, 추론과 같은 비전-언어 모델의 실제 사용 사례를 모방하도록 설계되었습니다. 각 카테고리의 대표적인 이미지는 그림 12에 제공되어 있으며, 비전-언어 모델의 평가된 응답 예시는 그림 11에 제공되어 있습니다.

연구진은 이미지, 프롬프트, 답변을 수동으로 큐레이션했으며, 두 번째 그룹의 라벨러를 통해 답변을 검증했습니다. 모든 프롬프트는 올바르게 답변하기 위해 이미지 입력을 참조해야 하도록 설계되었습니다.

#### 벤치마크 결과

표 2는 다양한 멀티모달 모델의 벤치마크 결과를 보여줍니다. Pixtral 12B는 비슷한 크기의 오픈 모델뿐만 아니라 여러 비공개 모델보다 상당히 우수한 성능을 보입니다.

| 모델 | Mathvista | MMMU | ChartQA | DocVQA | VQAv2 | MM-MT-Bench | LMSys-Vision |
|------|-----------|------|---------|--------|-------|-------------|--------------|
| Pixtral 12B | 58.3 | 52.0 | 81.8 | 90.7 | 78.6 | 6.05 | 1076 |
| Qwen-2-VL 7B | 53.7 | 48.1 | 41.2 | 94.5 | 75.9 | 5.45 | 1040 |
| → w/ Flexible Parsing | 55.2 | 48.7 | 77.5 | – | – | – | – |
| Llama-3.2 11B | 24.3 | 23.0 | 14.8 | 91.1 | 67.1 | 4.79 | 1032 |
| → w/ Flexible Parsing | 47.9 | 45.3 | 78.5 | – | – | – | – |
| Molmo-D 7B | 12.3 | 24.3 | 27.0 | 72.2 | 57.1 | 3.72 | – |
| LLaVA-OneVision 7B | 36.1 | 45.1 | 67.2 | 90.5 | 78.4 | 4.12 | – |
| Claude-3 Haiku | 44.8 | 50.4 | 69.6 | 74.6 | 68.4 | 5.46 | 1000 |
| Gemini-1.5-Flash 8B(0827) | 56.9 | 50.7 | 78.0 | 79.5 | 65.5 | 5.93 | 1111 |
| Molmo 72B | 52.2 | 52.7 | 75.6 | 86.5 | 75.2 | 3.51 | – |
| LLaVA-OneVision 72B | 57.2 | 54.4 | 66.9 | 91.6 | 83.8 | 4.95 | 992 |
| Qwen-2-VL 72B | 68.2 | 60.3 | 66.6 | 96.3 | 81.6 | 6.59 | 1104 |
| Llama-3.2 90B | 49.1 | 53.7 | 33.8 | 85.7 | 67.0 | 5.50 | 1071 |
| GPT-4o (0513) | 64.6 | 68.6 | 85.1 | 88.9 | 77.8 | 7.72 | 1208 |
| Claude-3.5 Sonnet | 64.4 | 68.0 | 87.6 | 90.3 | 70.7 | 7.50 | 1189 |

표 2: 멀티모달 벤치마크. Pixtral은 비슷한 크기의 오픈 모델뿐만 아니라 여러 비공개 모델보다 상당히 우수한 성능을 보입니다. 모든 모델은 동일한 프롬프트와 평가 지표로 재평가되었습니다. Qwen2-VL 7B와 Llama-3.2 11B와의 투명한 비교를 위해, 완화된 평가 제약 조건 하에서 이들의 성능도 (회색으로) 추가로 보고합니다. 일부 오픈소스 모델에 대한 보고된 수치와의 차이를 더 조사하기 위해, 부록 E에 분석을 제공합니다.

또한 표 3은 텍스트 전용 벤치마크에서의 성능을 보여줍니다. Pixtral 12B는 텍스트 전용 벤치마크에서도 비슷한 크기의 오픈소스 모델보다 일관되게 우수한 성능을 보여, 기존 텍스트 전용 배포를 대체할 수 있는 멀티모달 모델임을 입증합니다.

| 모델 | MT-Bench | MMLU | Math | HumanEval |
|------|----------|------|------|-----------|
| | | | 5-shot | Maj@1 | Pass@1 |
| Pixtral 12B | 7.68 | 69.2 | 48.1 | 72.0 |
| LLaVA-OneVision 7B | 6.94 | 67.9 | 38.6 | 65.9 |
| Molmo-D 7B | 4.53 | 61.2 | 10.2 | 3.7 |
| Qwen-2-VL 7B | 6.41 | 68.5 | 27.9 | 62.2 |
| Llama-3.2 11B | 7.51 | 68.5 | 48.3 | 62.8 |

표 3: 언어 벤치마크. Pixtral 12B는 텍스트 전용 벤치마크에서도 비슷한 크기의 오픈소스 모델보다 일관되게 우수한 성능을 보여, 기존 텍스트 전용 배포를 대체할 수 있는 멀티모달 모델임을 입증합니다.

MM-MT-Bench는 멀티모달 모델의 실제 사용 사례를 평가하는 데 중요한 도구로, 단순한 객관식 질문 응답 능력을 넘어 모델이 실제 사용자에게 얼마나 유용한지를 측정합니다. 이 벤치마크는 LMSys-Vision ELO 평가와 높은 상관관계를 보이며, 다양한 이미지 유형과 복잡한 질의에 대한 모델의 성능을 종합적으로 평가합니다.

## 결과

이 섹션에서는 Pixtral 12B를 다양한 크기의 비공개 및 오픈소스 모델과 비교하여 평가한 결과를 제시합니다. 모든 모델은 동일한 평가 환경을 통해 재평가되었습니다. 특히, 각 데이터셋에 대해 GPT-4o와 Claude-3.5 Sonnet과 같은 선도적인 멀티모달 모델의 결과를 재현할 수 있도록 프롬프트를 설계했습니다. 이러한 프롬프트는 '명시적(Explicit)'이며 출력 형식을 완전히 지정하여 프롬프트 지침을 따르는 모델이 테스트 시간에 정확하게 평가될 수 있도록 합니다. 모든 모델은 부록 A에 명시된 동일한 프롬프트로 평가되었습니다. 또한 다양한 프롬프트와 평가 지표에 따른 모델 재평가에 대한 추가 분석을 뒤에서 제공합니다.

### 주요 결과

#### 멀티모달 성능

표 2는 Pixtral이 멀티모달 벤치마크에서 비슷한 규모의 모든 오픈 모델뿐만 아니라 Claude-3 Haiku와 Gemini-1.5 Flash 8B와 같은 비공개 소스 모델보다 상당히 우수한 성능을 보여줌을 나타냅니다. 특히 Pixtral은 실제 사용 사례를 대상으로 하는 MM-MT-Bench에서 비슷한 크기의 모든 모델보다 우수한 성능을 보이며, 이는 LMSys Vision Arena에서의 강력한 성능으로도 확인됩니다. 이 공개 리더보드에서 Pixtral 12B는 Qwen2-VL 72B와 Llama-3.2 90B와 같은 가장 큰 오픈 웨이트 모델들의 성능에 근접합니다.

'명시적' 프롬프트를 사용할 때 일부 오픈소스 모델의 성능이 보고된 수치보다 상당히 낮다는 점을 강조합니다. 가장 유사한 오픈소스 모델인 Qwen2-VL 7B와 Llama-3.2 11B의 경우, 이는 주로 모델이 답변 형식에 대한 지침을 따르지 않기 때문입니다(예: "Final answer: 6" 대신 "The answer is 6."을 생성). 이러한 모델과의 투명한 비교를 위해, 더 유연한 구문 분석을 사용한 평가 결과를 회색으로 추가로 보고합니다. 부록 D에서는 다양한 프롬프트에 따른 이러한 모델의 성능을 분석하고, 부록 E에서는 각 모델에 맞게 평가를 사용자 지정하여 보고된 성능과의 격차를 해소하는 데 필요한 변경 사항을 설명합니다.

#### 언어 성능

표 3은 Pixtral 12B를 일반적인 텍스트 전용 벤치마크에서 비슷한 크기의 오픈소스 모델과 비교하여 평가합니다(역시 공통 프롬프팅 및 평가 프로토콜 사용). Pixtral은 멀티모달 기능을 추구하면서도 텍스트 이해 능력을 손상시키지 않아, 텍스트 및 비전 작업 모두에 적합한 대체 모델이 됩니다.

### 프롬프트 선택

여기서는 평가 프롬프트를 설계하는 방법론에 대해 논의합니다. 평가 환경에서는 GPT-4o와 Claude-3.5-Sonnet과 같은 선도적인 비공개 모델의 보고된 결과를 재현할 수 있는 프롬프트를 선택했습니다. 이러한 프롬프트는 부록 A에 제공되어 있으며, 부록 D에서는 10개의 프롬프트에 걸친 평균 결과를 보고합니다.

일반적으로 사용되는 프롬프트는 출력 형식을 제대로 지정하지 않는다는 것을 발견했습니다. 예를 들어, 객관식 질문의 경우 오픈소스 프롬프트에는 "Select the correct answer from the options above"와 같은 모호한 지침이 포함되어 있습니다. 이 경우 모델은 답변을 인덱스("Option A", "Option B" 등)로 제시해야 하는지 또는 자연어 응답으로 제시해야 하는지 알 수 없습니다. 그 결과 모델은 잘못된 형식으로 인해 불이익을 받게 됩니다. 따라서 선도적인 모델은 필요한 출력 형식을 명시적으로 지정하는 프롬프트가 필요합니다.

그림 5에서 MMMU의 실제 예를 통해 이를 설명합니다. 표 4에서는 '명시적' 프롬프트가 '단순' 프롬프트에 비해 선도적인 모델의 성능을 상당히 향상시킨다는 것을 보여줍니다. 또한 일부 경우에는 더 작은 모델의 성능이 명시적 프롬프트 형식으로 감소하는데, 이는 아마도 이러한 벤치마크의 훈련 세트에서 프롬프트 스타일과의 불일치 때문일 수 있습니다. Pixtral 12B는 일반적으로 명시적 프롬프트로 더 나은 성능을 보이며, ChartQA에서만 약간의 성능 저하가 있습니다.

![이 그림은 시각적 질문-답변 작업에서 선도적인 AI 모델의 성능을 비교하여 "단순" 대 "명시적" 프롬프트의 영향을 평가합니다. 시각화는 입력 이미지, 질문, 두 프롬프트 조건에서의 모델 응답을 보여줍니다. 주요 발견은 모델이 예상 출력 형식에 대한 세부 정보를 제공하는 "명시적" 프롬프트에서 상당한 이점을 얻는다는 것입니다. 이는 "단순" 프롬프트에 비해 더 정확한 응답으로 이어집니다. 이는 모델이 명시적 지침 없이 올바른 응답 형식을 추론하는 데 어려움을 겪는다는 것을 시사하며, 이러한 작업에서 강력한 성능을 달성하는 데 프롬프트 설계의 중요성을 강조합니다.](https://ar5iv.labs.arxiv.org//html/2410.07073/assets/x2.png)

그림 5: '단순' 대 '명시적' 프롬프트가 선도적인 모델에 미치는 영향. 선도적인 모델은 출력 형식에 대한 세부 정보를 제공하는 '명시적' 프롬프트에서 큰 이점을 얻습니다. 이는 실질적으로 올바른 응답이 평가 중에 잘못된 것으로 표시되기 때문에 이해가 됩니다(상단 행, 오른쪽).

| | VQAv2 | | ChartQA | | MMMU | |
|---|---|---|---|---|---|---|
|프롬프트 → | 단순 | 명시적 | 단순 | 명시적 | 단순 | 명시적 |
|GPT-4o(0513) | 64.2 | 77.8 | 58.0 | 85.1 | 55.0 | 68.6 |
|Sonnet-3.5 | 50.2 | 70.7 | 39.6 | 87.6 | 48.6 | 68.0 |
|Qwen-2-VL 7B | 82.1 | 75.9 | 83.4 | 41.2 | 46.7 | 48.1 |
|Llama-3.2 11B | 29.5 | 67.1 | 0.0 | 14.8 | 20.7 | 23.0 |
|Llama-3.2 90B | 52.6 | 67.0 | 3.9 | 33.8 | 27.0 | 53.7 |
|Pixtral 12B | 78.9 | 78.6 | 84.3 | 81.8 | 45.8 | 52.0 |

표 4: 프롬프트 변형 실험. 선도적인 모델은 좋은 성능을 내기 위해 출력 형식을 명시적으로 지정하는 프롬프트가 필요합니다. Pixtral 12B는 '명시적'과 '단순' 프롬프트 모두에서 잘 작동하며, ChartQA에서만 약간의 성능 저하가 있습니다.

### 평가 지표에 대한 민감도

앞선 절에서는 출력 형식을 적절히 지정하는 프롬프트의 중요성에 대해 논의했습니다. 그러나 평가 중에 명시적 프롬프트를 사용하더라도 많은 모델이 여전히 다양한 형식으로 출력을 제공하며, 이는 참조 답변과 정확히 일치하는 응답을 요구하는 지표에 의해 불이익을 받게 됩니다.

이를 조사하기 위해 모델의 생성물을 가져와 점진적으로 더 느슨한 구문 분석 제약 조건에서 평가합니다. 예를 들어, 정답이 "6"인 경우, 유연한 지표는 "6.0" 또는 "The answer is 6"과 같은 답변에 불이익을 주지 않습니다. 이러한 구문 분석 설정의 세부 사항은 부록 C에 제공되어 있지만, 여기서는 '유연한 레벨 3'이 생성물 어디에서든 참조 답변이 발생하면 응답을 정확한 것으로 표시한다는 점을 언급합니다. 이는 참조 답변이 "6"일 때 "6000"과 같은 답변을 허용하기 때문에 과도하게 관대한 지표이며, 상한선을 보여주기 위해서만 포함되었습니다.

표 5에 분석 결과를 제공합니다. 일부 모델의 성능이 더 유연한 구문 분석 지표로 극적으로 향상된다는 것을 발견했습니다. 이는 낮은 점수가 프롬프트 지침을 제대로 따르지 못하는 모델의 능력 부족에 기인할 수 있음을 나타냅니다. 또한 Pixtral 12B는 유연한 구문 분석으로 거의 이점을 얻지 못하며(지침을 따르는 능력을 입증함), 더욱이 유연한 지표가 사용된 후에도 일반적으로 다른 모델보다 우수한 성능을 발휘할 수 있습니다.

| | Llama-3.2 11B | Llama-3.2 90B | Qwen2-VL 7B | Pixtral 12B |
|---|---|---|---|---|
|**Mathvista** | | | | |
|기준선 | 24.3 | 49.1 | 53.7 | 58.3 |
|유연한 레벨 1 | 25.9 | 50.3 | 54.3 | 58.3 |
|유연한 레벨 2 | 40.2 | 54.7 | 54.3 | 58.3 |
|유연한 레벨 3 | 47.9 | 57.3 | 55.2 | 58.5 |
|**MMMU** | | | | |
|기준선 | 23.0 | 53.7 | 48.1 | 52.0 |
|유연한 레벨 1 | 23.4 | 53.7 | 48.1 | 52.0 |
|유연한 레벨 2 | 41.0 | 55.7 | 48.1 | 52.0 |
|유연한 레벨 3 | 45.3 | 56.7 | 48.7 | 52.0 |
|**ChartQA** | | | | |
|기준선 | 14.8 | 33.8 | 41.2 | 81.8 |
|유연한 레벨 1 | 20.4 | 33.9 | 73.8 | 81.9 |
|유연한 레벨 2 | 29.9 | 35.6 | 73.8 | 81.9 |
|유연한 레벨 3 | 78.5 | 79.1 | 77.5 | 82.0 |

표 5: 유연한 구문 분석 변형 실험. 점진적으로 더 느슨한 구문 분석 제약 조건에서 모델을 평가합니다(세부 사항은 부록 C 참조). 느슨한 구문 분석 제약 조건에서 일부 모델의 성능이 극적으로 향상됩니다. Pixtral 12B 성능은 모든 구문 분석 조건에서 안정적이며, 유연한 구문 분석이 고려되더라도 계속해서 선두를 유지합니다. '유연한 레벨 3'은 일부 잘못된 답변이 정확한 것으로 표시되는 것을 허용하기 때문에 설명용으로만 포함되었습니다.

### 비전 인코더 변형 실험

![이 시각화는 다양한 문서 이해 작업(ChartQA, DocVQA, VQAv2)과 자연 이미지 작업(AI2D)에서 CLIPA(224px 및 1120px), Pixtral-ViT(1024px)를 포함한 다양한 비전 인코더의 성능을 비교합니다. 주요 발견은 224px CLIPA 인코더가 자연 이미지에 대해서는 동등한 성능을 유지하면서 세밀한 문서 이해 작업에서 강력한 CLIPA 기준선보다 상당히 우수한 성능을 보인다는 것입니다. 이는 제안된 인코더 아키텍처가 시각적 지시 조정 및 문서 중심 응용 프로그램에 적합하다는 것을 시사합니다.](https://ar5iv.labs.arxiv.org//html/2410.07073/assets/images/vit_ablation.png)

그림 6: 비전 인코더 변형 실험: 시각적 지시 조정에 활용될 때, 우리의 인코더는 자연 이미지에 대해서는 동등한 성능을 유지하면서 세밀한 문서 이해가 필요한 작업에서 강력한 CLIPA 기준선보다 상당히 우수한 성능을 보입니다.

비전 인코더에 대한 설계 선택을 검증하기 위해 시각적 지시 조정을 통한 소규모 변형 실험을 수행했습니다. 우리의 비전 인코더(Pixtral-ViT)와 기준선으로 CLIPA 백본을 모두 사용하여 짧은 기간의 멀티모달 지시 조정 실행을 수행했습니다. 두 비전 인코더 모두 멀티모달 디코더를 초기화하기 위해 Mistral-Nemo 12B-Instruct를 사용했습니다.

많은 오픈소스 비전 인코더와 마찬가지로 CLIPA는 고정된 $224 \times 224$ 픽셀 해상도에서 훈련됩니다. 비전-언어 모델에서 해상도를 확장하기 위해 기존 방법은 이미지에서 여러 타일 크롭을 구성하고 각 크롭을 사전 훈련 해상도에서 독립적으로 비전 인코더에 통과시킵니다. CLIPA로 두 가지 변형 실험을 수행했습니다. (a) 전체 이미지를 $224 \times 224$로 크기 조정; (b) 입력 이미지에서 $25$개의 크롭을 구성하여 총 해상도 $1120 \times 1120$을 만듭니다. 이러한 모델은 각각 $224$ 픽셀과 $1120$ 픽셀에서 평가되는 반면, 우리의 유연한 인코더는 최대 해상도 $1024$ 픽셀로 다양한 이미지 해상도에서 평가됩니다.

그림 6에서 우리의 모델이 차트 및 문서 이해와 같은 세밀한 이해가 필요한 설정에서 CLIPA보다 상당히 우수한 성능을 보이면서도 VQAv2와 같은 자연어 벤치마크에서는 동등한 성능을 보인다는 것을 발견했습니다.

## Pixtral의 정성적 예시 분석

Pixtral 모델의 실제 응용 사례를 살펴보면서 이 멀티모달 언어 모델의 실질적인 능력을 확인해보겠습니다. Pixtral은 복잡한 그림에 대한 추론, 다중 이미지 지시 따르기, 차트 이해 및 분석, 이미지를 코드로 변환하는 등 다양한 작업에서 뛰어난 성능을 보여줍니다.

### 복잡한 그림에 대한 추론 능력

![복잡한 그림에 대한 추론](https://ar5iv.labs.arxiv.org//html/2410.07073/assets/images/gdp.png)

그림 7은 Pixtral이 복잡한 그림에 대해 추론하는 능력을 보여줍니다. 이 시각화는 다양한 국가와 지역이 세계 경제에 기여하는 GDP를 육각형 레이아웃으로 표현하고 있습니다. 주요 기술적 구성 요소로는 육각형 레이아웃, GDP 값, 각 국가/지역에 대한 백분율이 포함됩니다. 중요한 발견은 미국이 세계 GDP의 가장 큰 비중인 23.89%를 차지하고, 중국이 15.86%로 그 뒤를 잇는다는 것입니다.

![Pixtral의 복잡한 그림 추론 능력](https://ar5iv.labs.arxiv.org//html/2410.07073/assets/images/lechat_gdp.png)

Pixtral은 이 복잡한 그림을 정확하게 이해하고 추론할 수 있습니다. 특히 녹색 상자가 유럽 국가들을 나타낸다는 것을 올바르게 식별하고, 모든 유럽 국가들의 GDP를 읽고 정렬하여 정확한 GDP 수치와 함께 상위 5개 국가를 나열할 수 있습니다. 이는 Pixtral이 시각적 정보를 단순히 인식하는 것을 넘어 그 정보를 분석하고 추론할 수 있는 능력을 갖추고 있음을 보여줍니다.

### 다중 이미지 지시 따르기

![다중 이미지 지시 따르기](https://ar5iv.labs.arxiv.org//html/2410.07073/assets/images/lechat_multi_image_oct.png)

그림 8은 Pixtral의 다중 이미지 지시 따르기 능력을 보여줍니다. 이 그림은 Mathvista, MMMU, ChartQA, DocVQA, VOA2를 포함한 여러 벤치마크에 걸쳐 다양한 AI/ML 모델의 성능 지표를 통합한 마크다운 테이블을 제시합니다. 이 테이블은 다양한 작업에서 모델 성능을 쉽게 비교할 수 있게 해주며, 다양한 모델과 벤치마크를 포함하는 것은 이 분석이 멀티모달 AI 시스템과 여러 소스에서 정보를 처리하고 결합하는 능력에 대한 포괄적인 평가의 일부임을 시사합니다.

Pixtral은 컨텍스트 윈도우 내에서 임의의 수의 이미지를 처리할 수 있습니다. 이 예시는 Pixtral이 두 이미지의 정보를 성공적으로 결합하여 단일 마크다운 테이블로 만들 수 있음을 보여줍니다. 이는 모델이 여러 소스에서 정보를 통합하고 구조화된 형식으로 출력할 수 있는 능력을 갖추고 있음을 입증합니다.

### 차트 이해 및 분석

![차트 이해 및 분석](https://ar5iv.labs.arxiv.org//html/2410.07073/assets/images/lechat_chart.png)

그림 9는 Pixtral의 차트 이해 및 분석 능력을 보여줍니다. 이 이미지는 네 가지 다른 기계 학습 모델에 대한 훈련 손실을 단계별로 보여주는 선 그래프입니다. 주요 발견은 "dark-dragon-50" 모델의 경우, 손실이 처음에는 감소하지만 약 10k 단계 지점에서 크게 급증하기 시작하여 모델의 성능이 저하되고 있으며 과적합이나 다른 문제가 발생하고 있음을 나타냅니다. 이 모델에서 문제가 발생하기 시작한 중요한 지점은 약 10k 단계 지점으로 식별됩니다.

Pixtral은 복잡한 차트를 높은 정확도로 해석하고 분석하는 능력을 보여줍니다. 이 예시에서 Pixtral은 "dark-dragon"이 빨간색 선에 해당한다는 것을 정확하게 식별합니다. 또한 훈련 손실이 부드럽게 감소할 것으로 예상되며, 약 10K 단계 지점에서 손실이 크게 급증하여 훈련 실행이 불안정해졌다는 것을 인식합니다. 이는 Pixtral이 시각적 데이터를 단순히 설명하는 것을 넘어 그 의미를 해석하고 분석할 수 있는 능력을 갖추고 있음을 보여줍니다.

### 이미지를 코드로 변환

![이미지를 코드로 변환](https://ar5iv.labs.arxiv.org//html/2410.07073/assets/images/lechat_website.png)

그림 10은 Pixtral의 이미지를 코드로 변환하는 능력을 보여줍니다. 이 그림은 Pixtral이 손으로 그린 웹사이트 인터페이스를 실행 가능한 HTML 코드로 변환하는 능력을 보여주며, 손으로 그린 디자인을 완전히 기능하는 웹사이트로 변환할 수 있게 합니다. 표시된 주요 기술적 구성 요소에는 아이스크림 맛을 선택하기 위한 드롭다운 메뉴와 "Next" 버튼이 포함되며, 이는 HTML 요소와 JavaScript 기능을 사용하여 구현됩니다. 이 연구의 중요성은 손으로 그린 디자인과 기능적인 웹 개발 사이의 격차를 좁히는 능력에 있으며, 디자인에서 구현까지의 과정을 간소화할 수 있는 가능성을 제시합니다.

이 시연은 Pixtral이 손으로 그린 웹사이트 인터페이스를 실행 가능한 HTML 코드로 변환하는 능력을 보여주며, 손으로 그린 디자인을 완전히 기능하는 웹사이트로 구현할 수 있음을 입증합니다. 이는 디자이너와 개발자 간의 협업을 간소화하고, 프로토타입 제작 과정을 가속화할 수 있는 중요한 응용 사례입니다.

### 모델 응답 비교

![모델 응답 비교](https://ar5iv.labs.arxiv.org//html/2410.07073/assets/x3.png)

그림 11은 작업 전망 우려를 판단하는 작업에서 세 가지 대형 언어 모델(Pixral-12B, QwenVL-7B, Gemini-1.5 Flash-8B)의 모델 응답을 비교합니다. 주요 목적은 모델이 데이터의 주요 트렌드를 정확하게 추출하고 설명하는 능력을 평가하는 것입니다. 시각화는 품질 평가와 함께 샘플 모델 출력을 보여주며, Pixral-12B가 완전하고 정확한 응답을 제공하는 반면, QwenVL-7B는 설명이 부족하고 Gemini-1.5 Flash-8B는 잘못된 정보를 추출한다는 점을 강조합니다. 이 분석은 복잡한 업무 관련 트렌드를 이해하고 전달하는 데 있어 이러한 모델의 상대적 강점과 한계에 대한 통찰력을 제공합니다.

MM-MT-Bench의 예시에서 Pixtral-12B, QwenVL-7B, Gemini-1.5 Flash-8B(0827)의 모델 응답과 LLM-as-a-judge 점수를 비교한 결과, Pixtral의 응답이 완전하고 정확하여 8점을 받은 반면, Gemini-Flash-8B는 잘못된 정보를 추출하고, QwenVL은 트렌드에 대한 설명이 부족했습니다. 이는 Pixtral이 복잡한 차트와 데이터를 이해하고 분석하는 데 있어 다른 모델들보다 우수한 능력을 갖추고 있음을 보여줍니다.

![MM-MT-Bench 예시 이미지](https://ar5iv.labs.arxiv.org//html/2410.07073/assets/x4.png)

그림 12는 MM-MT-Bench에서 사용된 예시 이미지를 보여줍니다. 이 그림은 의료 영상 연구에서 가져온 일련의 뇌 MRI 스캔을 보여줍니다. 스캔은 이미지의 빨간색 표시로 표시된 다양한 유형의 뇌 종양이나 병변을 묘사하는 것으로 보입니다. 이 시각화의 목적은 의료 영상 기술을 통해 감지할 수 있는 다양한 뇌 이상을 보여주는 것이며, 이는 임상 환경에서 정확한 진단과 치료 계획을 위해 중요합니다.

이러한 다양한 예시들은 Pixtral이 단순한 이미지 인식을 넘어 복잡한 시각적 정보를 이해하고, 분석하며, 추론할 수 있는 능력을 갖추고 있음을 보여줍니다. 특히 복잡한 그림에 대한 추론, 다중 이미지 처리, 차트 분석, 이미지를 코드로 변환하는 능력은 실제 응용 시나리오에서 매우 유용하게 활용될 수 있습니다. 또한 다른 최신 모델들과의 비교를 통해 Pixtral이 복잡한 시각적 정보를 이해하고 분석하는 데 있어 경쟁력 있는 성능을 보여준다는 것을 확인할 수 있습니다.

이러한 정성적 예시들은 Pixtral의 다양한 응용 가능성을 보여주며, 특히 데이터 분석, 웹 개발, 의료 영상 분석 등 다양한 분야에서 활용될 수 있는 잠재력을 시사합니다. Pixtral의 이러한 능력은 멀티모달 AI 시스템이 실제 세계의 복잡한 문제를 해결하는 데 어떻게 기여할 수 있는지를 보여주는 좋은 예시입니다.

## 결론

본 논문에서는 텍스트 전용 및 멀티모달 작업 모두에서 뛰어난 성능을 보이는 최첨단 멀티모달 모델인 Pixtral 12B를 소개했습니다. 4억 개의 파라미터를 가진 비전 인코더와 120억 개의 파라미터를 가진 멀티모달 디코더로 구성된 혁신적인 아키텍처를 특징으로 하는 Pixtral 12B는 다양한 벤치마크에서 강력한 성능을 보여주며, 다른 오픈 모델들을 능가하고 더 큰 모델들과 대등한 성능을 발휘합니다.

Pixtral 12B의 뛰어난 지시 따르기 능력, 다양한 이미지 크기 지원, 그리고 긴 컨텍스트 윈도우는 복잡한 멀티모달 애플리케이션에 매우 유용하게 활용될 수 있습니다. 이 모델은 Apache 2.0 라이선스 하에 공개되었습니다.

## 참고 문헌

Pixtral 12B 모델의 개발에는 다양한 연구와 기술적 혁신이 기반이 되었습니다. 이 모델은 트랜스포머 아키텍처 ([Vaswani와 연구진](https://arxiv.org/abs/1706.03762))를 기반으로 하며, 비전 트랜스포머 ([Dosovitskiy](https://arxiv.org/abs/2010.11929))의 개념을 확장하여 다양한 이미지 크기와 종횡비를 처리할 수 있는 능력을 갖추었습니다.

특히 Pixtral의 비전 인코더는 Patch n'Pack 접근법 ([Dehghani와 연구진](https://arxiv.org/abs/2312.00079))에서 영감을 받아 다양한 해상도의 이미지를 효과적으로 처리할 수 있도록 설계되었습니다. 또한 CLIP ([Radford와 연구진](https://arxiv.org/abs/2103.00020))과 같은 자연어 지도 학습을 통한 시각적 모델 학습 방법론을 활용하여 텍스트와 이미지 간의 연결성을 강화했습니다.

Pixtral의 지시 조정 방법론은 시각적 지시 조정 ([Liu와 연구진](https://arxiv.org/abs/2304.08485))의 접근법을 확장하여 멀티모달 컨텍스트에서 더 나은 지시 따르기 능력을 구현했습니다. 이러한 다양한 연구 성과들을 통합하여 Pixtral 12B는 텍스트 전용 및 멀티모달 작업 모두에서 뛰어난 성능을 발휘하는 강력한 모델로 개발되었습니다.

참고 문헌 목록에는 Anthropic의 Claude 3 모델 계열, Fuyu-8b 멀티모달 아키텍처, Llama 3 모델군, MATH 데이터셋, Gaussian Error Linear Units (GELUs), LLaVA-OneVision, 역 스케일링 법칙, 부분 포인트 클라우드 매칭, 시각적 지시 조정, MathVista, Mistral NeMo 12B, GPT-4, CLIP, Gemini 1.5, GLU 변형, RoFormer, Llama, Qwen2-VL, MMMU, MT-bench 및 ChatBot Arena, AGIEval 등 다양한 연구가 포함되어 있습니다.

이러한 참고 문헌들은 Pixtral 12B의 개발에 직간접적으로 영향을 미쳤으며, 멀티모달 AI 분야의 최신 연구 동향과 기술적 혁신을 반영하고 있습니다. 특히 비전 인코더의 설계, 멀티모달 디코더의 구현, 그리고 다양한 벤치마크에서의 평가 방법론 등에 있어 이전 연구들의 성과를 기반으로 하면서도 새로운 접근법을 도입했습니다.

Pixtral 12B는 이러한 다양한 연구 성과들을 통합하고 확장하여, 텍스트와 이미지를 모두 효과적으로 처리할 수 있는 강력한 멀티모달 모델로 발전시켰습니다. 특히 다양한 이미지 크기와 종횡비를 처리할 수 있는 능력, 긴 컨텍스트 윈도우 지원, 그리고 뛰어난 지시 따르기 능력은 실제 응용 시나리오에서 큰 유용성을 제공합니다.

## 부록

### 프롬프트

이 섹션에서는 본 논문의 주요 평가에 사용된 프롬프트를 공개합니다. 앞선 절에서 논의된 바와 같이, 프롬프트는 GPT-4o와 Claude-3.5 Sonnet의 보고된 성능을 재현하기 위해 선택되었습니다.

#### A.1 MMMU와 Mathvista

```
이미지와 질문을 주의 깊게 분석하고, 단계별 추론을 사용하세요. 먼저, 제공된 이미지를 자세히 설명하세요. 그런 다음, 추론 과정을 제시하세요. 마지막으로 최종 답변을 다음 형식으로 제시하세요: Final Answer: <answer> 여기서 <answer>는:
- 선택지가 제공된 경우, 정확한 글자 선택지 A, B, C, D, E, F 등입니다. 글자만 포함하세요.
- 선택지가 제공되지 않은 경우, 직접적인 답변을 단일 구문이나 숫자로 제시하세요.
- 답변이 숫자인 경우, 단위 없이 숫자만 포함하세요.
- 답변이 단어나 구문인 경우, 이미지에서 보이는 텍스트를 바꿔 쓰거나 재구성하지 마세요.
- 질문에 답할 수 없다고 대답할 수 없습니다. 선택지를 고르거나 직접적인 답변을 제공해야 합니다.
중요: 답변을 Final Answer: <answer>로 끝내는 것을 잊지 마세요.
```

#### A.2 ChartQA

```
이미지와 질문을 주의 깊게 분석하고, 단계별 추론을 사용하세요. 먼저, 제공된 이미지를 자세히 설명하세요. 그런 다음, 추론 과정을 제시하세요. 마지막으로 최종 답변을 다음 형식으로 제시하세요: Final Answer: <answer> 여기서 <answer>는 다음 지침을 따릅니다.
- <answer>는 단일 구문이나 숫자여야 합니다.
- <answer>는 이미지의 텍스트를 바꿔 쓰거나 재구성하지 않아야 합니다.
- <answer>가 비율인 경우, 1:4 대신 0.25와 같은 소수값이어야 합니다.
- 질문이 예/아니오 질문인 경우, <answer>는 Yes/No여야 합니다.
- <answer>가 숫자인 경우, 단위를 포함하지 않아야 합니다.
- <answer>가 백분율인 경우, % 기호를 포함해야 합니다.
- <answer>가 개체인 경우, 그래프에서 전체 레이블을 포함해야 합니다.
중요: 답변을 Final Answer: <answer>로 끝내는 것을 잊지 마세요.
```

#### A.3 VQAv2

```
- 질문에 단일 단어, 숫자 또는 짧은 구문을 사용하여 답하세요. 가능한 한 적은 단어를 사용하세요.
- 답변이 숫자인 경우, Two가 아닌 2와 같이 숫자로 보고하고 단위 없이 숫자만 포함하세요.
- 질문이 예/아니오인 경우, 예/아니오로만 답하고 다른 것은 포함하지 마세요(가능성 있음, 알 수 없음 등 없이).
- 질문에 답할 수 없다고 대답할 수 없습니다. 반드시 답해야 합니다.
```

#### A.4 DocVQA

```
단일 단어나 구문을 사용하여 질문에 답하세요.
```

#### A.5 MM-MT-Bench 심사 프롬프트

```
SYSTEM: 공정한 심사관으로 행동하여 이전 대화를 맥락으로 고려하여 가장 최근 질문에 대한 AI 어시스턴트의 응답 품질을 평가해 주세요. 평가는 정확성과 유용성을 고려해야 합니다. 참조 답변과 어시스턴트의 답변이 제공됩니다. 어시스턴트의 답변을 참조 답변과 비교하여 평가를 시작하세요. 실수를 식별하고 수정하세요. 가능한 한 객관적이어야 합니다. 설명을 제공한 후, 1부터 10까지의 척도로 응답을 평가해야 합니다. 이 형식을 엄격히 따라야 합니다. "[[평가]]", 예를 들어: "평가: [[5]]".

<|사용자와의 대화 시작|>
### 사용자: <이미지>
이 이미지를 분석하세요.
### 참조 답변: 이 이미지는 ...
### 어시스턴트: 이것은 ...의 이미지입니다.
<|사용자와의 대화 종료|>
```

대화 기록은 참조 답변을 어시스턴트 답변으로 사용하여(teacher-forcing) 심사관에게 전달됩니다.

이러한 프롬프트들은 다양한 멀티모달 벤치마크에서 모델의 성능을 평가하기 위해 신중하게 설계되었습니다. 각 프롬프트는 해당 벤치마크의 특성과 요구사항에 맞게 조정되었으며, 특히 출력 형식에 대한 명확한 지침을 제공합니다. 이는 모델이 정확한 형식으로 답변을 생성할 수 있도록 하여 평가의 일관성과 정확성을 보장합니다.

MMMU와 Mathvista 프롬프트는 단계별 추론과 명확한 답변 형식을 강조하며, ChartQA 프롬프트는 차트와 그래프 분석에 특화된 지침을 제공합니다. VQAv2 프롬프트는 간결한 답변을 요구하고, DocVQA는 문서 기반 질문에 대한 간단한 응답을 요청합니다. MM-MT-Bench 심사 프롬프트는 모델 응답의 품질을 평가하기 위한 구조화된 방법을 제공합니다.

이러한 프롬프트들은 모델의 성능을 공정하고 일관되게 평가하기 위한 표준화된 방법을 제공하며, 다양한 멀티모달 작업에서 모델의 능력을 종합적으로 평가할 수 있게 합니다. 특히 출력 형식에 대한 명확한 지침은 모델이 정확한 형식으로 답변을 생성할 수 있도록 하여, 평가 과정에서 형식 불일치로 인한 불이익을 최소화합니다.

## RoPE-2D의 상대적 위치 인코딩 속성

부록 B에서는 RoPE-2D의 상대적 위치 인코딩 속성에 대해 수학적으로 증명합니다. 이 섹션의 목표는 다음 수식이 성립함을 보이는 것입니다.

$$\langle\text{RoPE-2D}(x^{(p,q)},\Theta),\text{RoPE-2D}(y^{(r,s)},\Theta)\rangle=\langle\text{RoPE-2D}(x^{(p-r,q-s)},\Theta),\text{RoPE-2D}(y^{(0,0)},\Theta)\rangle$$

이 수식은 모든 특징 벡터 $x, y \in \mathbb{R}^d$와 모든 위치 $p, r \in \{0 \dots H\}$ 및 $q, s \in \{0 \dots W\}$에 대해 성립해야 합니다. 논의를 단순화하기 위해, 차원 $d = 4$인 경우에 대해 이 속성을 증명하겠습니다. 더 높은 차원으로의 확장은 동일한 원리를 따릅니다.

먼저 RoPE-2D의 정의를 살펴보겠습니다. 위치 $(p, q)$에 있는 특징 벡터 $x^{(p,q)}$에 대한 RoPE-2D 변환은 다음과 같이 정의됩니다.

$$\text{RoPE-2D}\left(x^{(p,q)},\Theta\right) = \begin{pmatrix}\cos p\theta_{1}&-\sin p\theta_{1}&0&0\\ \sin p\theta_{1}&\cos p\theta_{1}&0&0\\ 0&0&\cos q\theta_{2}&-\sin q\theta_{2}\\ 0&0&\sin q\theta_{2}&\cos q\theta_{2}\\ \end{pmatrix}\cdot\begin{pmatrix}x_{1}\\ x_{2}\\ x_{3}\\ x_{4}\end{pmatrix}$$

마찬가지로, 위치 $(r, s)$에 있는 특징 벡터 $y^{(r,s)}$에 대한 RoPE-2D 변환은 다음과 같습니다.

$$\text{RoPE-2D}\left(y^{(r,s)},\Theta\right) = \begin{pmatrix}\cos r\theta_{1}&-\sin r\theta_{1}&0&0\\ \sin r\theta_{1}&\cos r\theta_{1}&0&0\\ 0&0&\cos s\theta_{2}&-\sin s\theta_{2}\\ 0&0&\sin s\theta_{2}&\cos s\theta_{2}\\ \end{pmatrix}\cdot\begin{pmatrix}y_{1}\\ y_{2}\\ y_{3}\\ y_{4}\end{pmatrix}$$

이제 두 변환된 벡터 간의 내적을 계산해 보겠습니다.

$$\langle\text{RoPE-2D}(x^{(p,q)},\Theta),\text{RoPE-2D}(y^{(r,s)},\Theta)\rangle$$

이 내적은 다음과 같이 분해할 수 있습니다.

$$\begin{pmatrix}x_{1}\ \ x_{2}\end{pmatrix}\cdot\begin{pmatrix}\cos p\theta_{1}&-\sin p\theta_{1}\\ \sin p\theta_{1}&\cos p\theta_{1}\\ \end{pmatrix}^{T}\begin{pmatrix}\cos r\theta_{1}&-\sin r\theta_{1}\\ \sin r\theta_{1}&\cos r\theta_{1}\\ \end{pmatrix}\cdot\begin{pmatrix}y_{1}\\ y_{2}\end{pmatrix}$$

$$+ \begin{pmatrix}x_{3}\ \ x_{4}\end{pmatrix}\cdot\begin{pmatrix}\cos q\theta_{2}&-\sin q\theta_{2}\\ \sin q\theta_{2}&\cos q\theta_{2}\end{pmatrix}^{T}\begin{pmatrix}\cos s\theta_{2}&-\sin s\theta_{2}\\ \sin s\theta_{2}&\cos s\theta_{2}\end{pmatrix}\cdot\begin{pmatrix}y_{3}\\ y_{4}\end{pmatrix}$$

행렬 곱셈을 수행하면 다음과 같은 결과를 얻습니다.

$$\begin{pmatrix}x_{1}\ \ x_{2}\end{pmatrix}\cdot\begin{pmatrix}\cos p\theta_{1}\cos r\theta_{1}+\sin p\theta_{1}\sin r\theta_{1}&-\sin r\theta_{1}\cos p\theta_{1}+\sin p\theta_{1}\cos r\theta_{1}\\ \sin r\theta_{1}\cos p\theta_{1}-\sin p\theta_{1}\cos r\theta_{1}&\cos p\theta_{1}\cos r\theta_{1}+\sin p\theta_{1}\sin r\theta_{1}\\ \end{pmatrix}^{T}\cdot\begin{pmatrix}y_{1}\\ y_{2}\end{pmatrix}$$

$$+ \begin{pmatrix}x_{3}\ \ x_{4}\end{pmatrix}\cdot\begin{pmatrix}\cos q\theta_{2}\cos s\theta_{2}+\sin q\theta_{2}\sin s\theta_{2}&-\sin q\theta_{2}\cos s\theta_{2}+\sin q\theta_{2}\cos s\theta_{2}\\ \sin q\theta_{2}\cos s\theta_{2}-\sin q\theta_{2}\cos s\theta_{2}&\cos q\theta_{2}\cos s\theta_{2}+\sin q\theta_{2}\sin s\theta_{2}\\ \end{pmatrix}\cdot\begin{pmatrix}y_{3}\\ y_{4}\end{pmatrix}$$

여기서 삼각함수의 덧셈 공식을 적용할 수 있습니다. $\cos(a)\cos(b) + \sin(a)\sin(b) = \cos(a-b)$와 $\sin(a)\cos(b) - \cos(a)\sin(b) = \sin(a-b)$를 사용하면 다음과 같이 간소화됩니다.

$$\begin{pmatrix}x_{1}\ \ x_{2}\end{pmatrix}\cdot\begin{pmatrix}\cos\left((p-r)\cdot\theta_{1}\right)&-\sin\left((p-r)\cdot\theta_{1}\right)\\ \sin\left((p-r)\cdot\theta_{1}\right)&\cos\left((p-r)\cdot\theta_{1}\right)\\ \end{pmatrix}^{T}\cdot\begin{pmatrix}y_{1}\\ y_{2}\end{pmatrix}$$

$$+ \begin{pmatrix}x_{3}\ \ x_{4}\end{pmatrix}\cdot\begin{pmatrix}\cos\left((q-s)\cdot\theta_{2}\right)&-\sin\left((q-s)\cdot\theta_{2}\right)\\ \sin\left((q-s)\cdot\theta_{2}\right)&\cos\left((q-s)\cdot\theta_{2}\right)\\ \end{pmatrix}^{T}\cdot\begin{pmatrix}y_{3}\\ y_{4}\end{pmatrix}$$

이 결과는 다음과 같이 해석할 수 있습니다.

$$\langle\text{RoPE-2D}(y^{(p-r,q-s)},\Theta),\text{RoPE-2D}(y^{(0,0)},\Theta)\rangle$$

따라서 RoPE-2D의 상대적 위치 인코딩 속성이 증명되었습니다. 이 속성은 두 벡터 간의 내적이 그들의 절대 위치가 아닌 상대적 위치 차이에만 의존한다는 것을 보여줍니다.

이 속성은 RoPE-2D가 위치 정보를 효과적으로 인코딩하면서도 상대적 위치 관계를 보존한다는 것을 의미합니다. 이는 비전 트랜스포머에서 특히 중요한데, 이미지의 다양한 위치에 있는 패치 간의 관계를 모델링할 때 일관된 방식으로 처리할 수 있기 때문입니다.

RoPE-2D의 이러한 수학적 속성은 Su와 연구진이 제안한 원래의 RoPE(Rotary Position Embedding)의 확장으로 볼 수 있습니다. 원래의 RoPE는 1차원 시퀀스에서 상대적 위치 정보를 인코딩하는 데 사용되었으나, RoPE-2D는 이를 2차원으로 확장하여 이미지와 같은 2차원 데이터에 적용할 수 있게 합니다.

이 상대적 위치 인코딩 속성은 Pixtral 모델의 비전 인코더가 다양한 크기와 종횡비의 이미지를 효과적으로 처리할 수 있게 하는 핵심 요소입니다. 특히 이미지의 패치 간 관계를 모델링할 때, 패치의 절대적 위치보다는 상대적 위치 관계가 중요하기 때문에 이 속성이 매우 유용합니다.

## 부록 C: 유연한 구문 분석 설정

앞선 절에서는 모델을 점진적으로 더 느슨한 제약 조건 하에서 평가하는 세 가지 '구문 분석 수준'을 소개합니다. 일반적인 평가 지표는 정답 형식이 정확히 참조 주석과 일치하는 경우에만 보상하지만, 이러한 요구 사항을 완화하고 모델 성능이 어떻게 달라지는지 조사하고자 합니다.

### 기준선

이 설정에서는 프롬프트 지침을 정확히 따르는 것이 요구되며, 모델 응답은 "Final Answer: <ANSWER>" 문자열로 끝나야 합니다. 이는 가장 엄격한 평가 방식으로, 모델이 지시사항을 정확히 따르는지 확인합니다.

### 유연한 구문 분석 레벨 1

이 설정에서는 기준선의 요구 사항 외에도 모델 응답이 "Answer: <ANSWER>"로 끝나는 경우도 정답으로 인정합니다. 이는 약간의 유연성을 제공하여 모델이 "Final"이라는 단어를 생략하더라도 정답으로 인정받을 수 있게 합니다.

### 유연한 구문 분석 레벨 2

이 설정에서는 추가적으로 모델이 마크다운 형식을 추가한 경우도 정답으로 인정합니다. 다음과 같은 마크다운 형식을 제거합니다.
- "**Answer**"
- "**Answer:**"
- "*Answer: <ANSWER>*"

이러한 마크다운 형식은 특히 Llama-3.2 모델에서 많이 나타나는 것으로 확인되었습니다. 이 레벨에서는 모델이 답변을 강조하기 위해 마크다운 형식을 사용하더라도 정답으로 인정합니다.

### 유연한 구문 분석 레벨 3

이는 가장 관대한 평가 설정입니다. 이 레벨에서는 참조 정답이 모델 응답 어디에든 나타나면 해당 응답을 정답으로 표시합니다. 단일 문자 답변의 경우, 응답에서 "is <A>", "are <A>", "<A>"와 같은 패턴을 검색합니다. 단일 숫자 응답의 경우, 쉼표가 있는 형태와 없는 형태 모두 검색합니다.

유연한 구문 분석 레벨 3은 상한선을 제공하기 위한 것임을 강조합니다. 이 방식은 참조 정답이 "6"일 때 "6000"과 같은 답변도 정답으로 표시할 수 있기 때문에 잘못된 답변을 정답으로 표시할 가능성이 있습니다.

### 유연한 구문 분석의 중요성

이러한 유연한 구문 분석 설정은 모델 평가에 있어 중요한 역할을 합니다. 특히 앞선 절에서 설명한 바와 같이, 일부 모델은 더 유연한 구문 분석 지표로 평가될 때 성능이 극적으로 향상됩니다. 예를 들어, Llama-3.2 11B 모델은 MathVista 벤치마크에서 기준선 설정에서 24.3%의 성능을 보이지만, 유연한 구문 분석 레벨 3에서는 47.9%로 성능이 크게 향상됩니다.

이는 모델이 실질적으로 정확한 답변을 생성하더라도 형식적인 요구 사항을 정확히 따르지 않아 불이익을 받을 수 있음을 시사합니다. 예를 들어, 모델이 "Final Answer: 6" 대신 "The answer is 6."과 같은 형식으로 답변을 제공하는 경우, 기준선 설정에서는 오답으로 처리되지만 유연한 구문 분석 레벨 3에서는 정답으로 인정됩니다.

이러한 유연한 구문 분석 설정은 모델의 실제 능력을 더 정확하게 평가하는 데 도움이 됩니다. 특히 모델이 정답을 알고 있지만 형식적인 요구 사항을 정확히 따르지 않는 경우, 이러한 유연한 평가 방식은 모델의 실제 성능을 더 잘 반영할 수 있습니다.

그러나 유연한 구문 분석 레벨 3과 같은 매우 관대한 평가 방식은 잘못된 답변을 정답으로 표시할 가능성이 있으므로, 이는 상한선을 제공하기 위한 목적으로만 사용되어야 합니다. 실제 응용 시나리오에서는 모델이 지시사항을 정확히 따르는 능력도 중요한 평가 기준이 됩니다.

이러한 유연한 구문 분석 설정은 특히 Llama-3.2 모델과 같이 마크다운 형식을 자주 사용하는 모델을 평가할 때 유용합니다. 이러한 모델들은 기준선 설정에서는 낮은 성능을 보이지만, 유연한 구문 분석 설정에서는 실제 능력에 더 가까운 성능을 보여줍니다.

결론적으로, 유연한 구문 분석 설정은 모델의 실제 능력을 더 정확하게 평가하는 데 도움이 되지만, 동시에 모델이 지시사항을 정확히 따르는 능력도 중요한 평가 기준임을 인식해야 합니다. 따라서 다양한 구문 분석 수준에서 모델을 평가하는 것이 모델의 전반적인 성능을 더 포괄적으로 이해하는 데 도움이 됩니다.

## 프롬프트 강건성에 대한 분석

### D.1 Llama 특화 프롬프트

앞의 절에서는 모든 모델을 공통 프롬프트로 평가했으며, 이를 통해 GPT-4o와 Claude-3.5 Sonnet의 보고된 수치를 재현할 수 있었습니다. 이 프롬프트는 모델이 응답을 "Final Answer: <ANSWER>"로 끝내도록 요구합니다(전체 프롬프트는 부록 A에 제공됨). 그러나 Llama-3.2 모델을 평가할 때, 이 모델 계열은 명시적인 지시에도 불구하고 기본적으로 "**Answer:** <ANSWER>"(즉, 마크다운 형식을 사용하고 'Final'을 생략함)로 응답하는 경향이 있음을 발견했습니다. 정규식 불일치로 인한 성능 저하는 유연한 구문 분석 전략을 통해 완화되었지만, Llama-3.2 모델은 프롬프트가 특별히 "**Answer:** <ANSWER>"(즉, 기본 출력 형식을 존중함)를 요청할 때 상당히 더 나은 성능을 보였습니다.

표 6에서는 부록 A의 기본 프롬프트와 Llama 특화 프롬프트를 모두 사용한 모델의 결과를 보여줍니다(모두 정확한 일치 지표로 평가됨). Llama 특화 프롬프트가 Llama-3.2 모델의 성능을 상당히 향상시키는 것을 볼 수 있으며, 특히 11B 변형의 경우 Mathvista와 MMMU 모두에서 15% 이상의 성능 향상이 있었습니다. 또한 Pixtral의 성능은 프롬프트 변화에도 안정적이며, 11B 변형보다 상당한 차이로 앞서고 있습니다.

| | Mathvista | MMMU | ChartQA |
| --- | --- | --- | --- |
| | 정확한 일치 | 정확한 일치 | 정확한 일치 |
| **Llama-3.2 11B** |  |  |  |
| 기본 프롬프트 | 24.3 | 23.0 | 14.8 |
| Llama 특화 프롬프트 | 41.6 | 41.9 | 33.7 |
| **Llama-3.2 90B** |  |  |  |
| 기본 프롬프트 | 49.1 | 53.7 | 33.8 |
| Llama 특화 프롬프트 | 57.6 | 58.6 | 34.8 |
| **Qwen2-VL 7B** |  |  |  |
| 기본 프롬프트 | 53.7 | 48.1 | 41.2 |
| Llama 특화 프롬프트 | 52.6 | 47.4 | 74.0 |
| **Pixtral 12B** |  |  |  |
| 기본 프롬프트 | 58.3 | 52.0 | 81.8 |
| Llama 특화 프롬프트 | 57.7 | 50.8 | 83.8 |

표 6: Llama 특화 프롬프트를 사용한 평가. Llama-3.2 모델 계열에 맞춘 프롬프트로 모델을 재평가했습니다. 이는 모델의 11B 변형의 성능을 상당히 향상시키는 것으로 나타났습니다. Pixtral 12B는 두 프롬프트 모두에서 안정적인 성능을 보이며, Llama-3.2 11B와 Qwen2-VL 7B보다 상당한 우위를 유지합니다.

### D.2 프롬프트 간 평균 성능

여기서는 여러 프롬프트에 걸친 평균 결과를 보고합니다. Mistral Large v2에게 본 논문에서 사용된 프롬프트(부록 A 참조)의 10가지 버전을 작성하도록 요청했으며, 지시사항을 명시적으로 유지하면서 표현을 다양화했습니다. 이전 연구들이 더 엄격한 구문 분석 제약 조건에서 성능이 저하되기 때문에, 이 실험에서는 모든 모델을 '유연한 구문 분석 레벨 3'에서 평가했습니다.

연구 결과, 본 논문의 주요 결과와 동일한 경향이 나타났습니다. Pixtral은 비슷한 크기의 모델보다 우수한 성능을 보이며, Mathvista와 ChartQA에서는 Llama-3.2 90B를 능가했습니다. 또한 Pixtral은 일반적으로 프롬프트 간 성능 변동성(회색으로 표시)이 더 낮았습니다.

| | Mathvista | MMMU | ChartQA |
| --- | --- | --- | --- |
| | 유연한 레벨 3 | 유연한 레벨 3 | 유연한 레벨 3 |
| Llama-3.2 11B | 42.1(±1.9) | 45.3(±1.0) | 77.2(±0.8) |
| Llama-3.2 90B | 56.0(±1.5) | 56.7(±0.5) | 80.1(±0.5) |
| Qwen2-VL 7B | 53.7(±2.1) | 46.9(±1.9) | 77.0(±0.8) |
| Pixtral 12B | 56.4(±1.0) | 49.5(±1.5) | 83.8(±0.4) |

표 7: 프롬프트 간 멀티모달 평균 성능. 10개의 서로 다른 프롬프트로 모델을 평가하여 평균 성능과 표준 편차(회색)를 보고합니다. 결과는 본 논문의 주요 결과와 동일한 경향을 보이며, Pixtral이 비슷한 크기의 오픈소스 모델보다 우수한 성능을 발휘합니다. 모든 모델은 '유연한 레벨 3' 구문 분석으로 평가되었습니다.

이러한 결과는 Pixtral 모델이 프롬프트 변화에 대해 강건하다는 것을 보여줍니다. 특히 주목할 만한 점은 다음과 같습니다.

1. **Llama-3.2 11B의 프롬프트 민감성**: Llama-3.2 11B 모델은 기본 프롬프트와 Llama 특화 프롬프트 간에 가장 큰 성능 차이를 보였습니다. 예를 들어, Mathvista에서는 24.3%에서 41.6%로, MMMU에서는 23.0%에서 41.9%로 성능이 크게 향상되었습니다. 이는 이 모델이 프롬프트 형식에 특히 민감하다는 것을 나타냅니다.

2. **Pixtral의 안정성**: 반면 Pixtral 12B는 프롬프트 변화에 가장 안정적인 성능을 보였습니다. 기본 프롬프트와 Llama 특화 프롬프트 간의 성능 차이가 미미했으며, 10개의 다양한 프롬프트에 걸친 표준 편차도 가장 낮았습니다(Mathvista에서 ±1.0, ChartQA에서 ±0.4).

3. **Qwen2-VL 7B의 ChartQA 성능 향상**: Qwen2-VL 7B는 ChartQA에서 Llama 특화 프롬프트를 사용했을 때 41.2%에서 74.0%로 큰 성능 향상을 보였습니다. 이는 특정 작업에서 프롬프트 엔지니어링의 중요성을 강조합니다.

4. **유연한 구문 분석의 영향**: '유연한 레벨 3' 구문 분석을 사용했을 때, 모든 모델의 성능이 향상되었지만 상대적인 순위는 대체로 유지되었습니다. 이는 모델의 실제 능력을 평가할 때 구문 분석 방법의 중요성을 보여줍니다.

이러한 분석은 멀티모달 언어 모델을 평가할 때 프롬프트 설계와 구문 분석 방법이 결과에 상당한 영향을 미칠 수 있음을 보여줍니다. 특히 Pixtral 모델은 다양한 프롬프트 조건에서도 일관되게 높은 성능을 유지하는 강건성을 보여주었으며, 이는 실제 응용 시나리오에서 중요한 특성입니다.

프롬프트 강건성에 대한 이러한 분석은 멀티모달 모델의 평가 방법론에 중요한 시사점을 제공합니다. 모델의 진정한 능력을 평가하기 위해서는 다양한 프롬프트와 평가 지표를 고려해야 하며, 단일 프롬프트나 엄격한 구문 분석 방법에만 의존하는 것은 모델의 실제 성능을 과소평가할 수 있습니다. Pixtral 모델은 이러한 다양한 조건에서도 일관되게 우수한 성능을 보여, 실제 응용 환경에서의 강건성을 입증했습니다.

## 보고된 수치 재현하기

부록 E에서는 앞선 절에서 언급된 공통 평가 프로토콜 하에서 모델들의 성능이 보고된 수치와 차이를 보이는 현상을 분석하고, 이러한 차이를 해소하기 위해 필요한 조정 사항들을 상세히 설명합니다.

### E.1 요약

분석 결과에 따르면, 프론티어 모델들과 소규모 비공개 모델들은 앞선 절에서 논의된 공통 프로토콜을 통해 보고된 성능 수치를 재현하거나 초과할 수 있었습니다. 이는 '명시적' 프롬프트(부록 A 참조)의 지시사항을 정확히 따르는 능력 덕분입니다. 반면, 소규모 오픈소스 모델들은 보고된 성능을 재현하기 위해 모델별 프롬프트 튜닝이나 평가 지표 조정이 필요했습니다. 이러한 개입을 통해 대부분의 모델에서 보고된 수치를 재현하거나 초과할 수 있었습니다.

주목할 점은 Pixtral 12B가 비공개 모델들(예: Gemini-1.5-Flash 8B와 Claude-3 Haiku)과 마찬가지로 특별한 개입 없이도 프롬프트 지시사항을 따라 우수한 성능을 보여준다는 것입니다. 이는 부록 D에서 다양한 프롬프트에 걸친 강건한 성능과 LMSys Vision Arena 및 MM-MT-Bench에서의 우수한 성능으로 입증됩니다.

### E.2 비공개 모델: Claude-3 Haiku와 Gemini-Flash-8B

표준화된 평가 프로토콜은 유연한 구문 분석을 통해 보고된 수치와 일치하거나 이를 초과하는 결과를 보여줍니다. 유일한 예외는 Claude Haiku가 ChartQA에서 보인 성능으로, 보고된 성능에 근접하기 위해서는 유연한 구문 분석 레벨 3이 필요했습니다.

### E.3 Qwen2-VL 7B

먼저 프롬프트를 ChartQA 훈련 세트와 유사한 한 줄 지시사항으로 단순화했습니다. 다음으로, 예상되는 답변 형식에 따라 다른 프롬프트를 제공했습니다. 예를 들어, 답변이 부동 소수점 숫자인 경우 "소수점 두 자리 부동 소수점으로 답하세요"와 같은 지시사항을 제공했으며, 정수나 객관식 질문에 대해서도 유사한 프롬프트를 사용했습니다. 모든 형식 지정을 포함한 단일 통합 프롬프트(부록 A의 프롬프트와 같은)를 제공하면 성능이 저하되는 것으로 나타났습니다.

### E.4 Llama-3.2

이 모델들은 기본적으로 "**Answer**", "**Answer:**", "*Answer: <ANSWER>*"와 같은 마크다운 형식의 응답을 생성합니다. '명시적' 프롬프트를 이러한 형식을 요청하도록 변경하면 상당한 성능 향상이 있었으며(부록 D 참조), 유연한 레벨 3 평가를 통해 보고된 성능을 재현할 수 있었습니다.

Llama-3.2 90B를 DocVQA에서 평가할 때, 많은 생성 결과가 'The answer is <ANSWER>'와 같은 형태로 나타났는데, 이는 ANLS 지표에 의해 불이익을 받습니다. 이러한 접두사를 제거하면 DocVQA 성능이 $+4.8$ 향상되었습니다.

### E.5 Llava-OneVision 72B

Qwen2-7B와 유사하게, 프롬프트를 한 줄 지시사항으로 단순화하고 예상되는 답변 형식에 따라 다른 프롬프트를 제공했습니다. 모든 형식 지정을 포함한 단일 통합 프롬프트를 제공하면 성능이 저하되는 것으로 나타났습니다.

### E.6 Molmo

Qwen2-7B와 Llava-Onevision 7B와 유사하게, 프롬프트를 한 줄 지시사항으로 단순화하고 예상되는 답변 형식에 따라 다른 프롬프트를 제공했습니다. 또한 Llama-3.2에 대한 개입과 유사하게, 프롬프트를 재구성하고 평가 지표를 완화했습니다.

Molmo 모델은 긴 응답을 <ANSWER>로 끝내는 경향이 있습니다. 긴 답변의 경우, 이를 포착하도록 평가 지표를 조정했습니다. VQAv2의 경우, 숫자 답변의 텍스트 출력을 정수 숫자로 변환하는 등의 사용자 지정 후처리 필터를 적용했습니다(예: "Two"를 "2"로 변환).

| | Mathvista | MMMU | ChartQA | DocVQA | VQAv2 | MM-MT-Bench | LMSys-Vision |
|---|---|---|---|---|---|---|---|
| | CoT | CoT | CoT | ANLS | VQA Match | GPT-4o Judge | (Oct '24) |
| Pixtral 12B | 58.3 | 52.0 | 81.8 | 90.7 | 78.6 | 6.05 | 1076 |
| Qwen-2-VL 7B | | | | | | | |
| 측정값 (정확한 일치) | 53.7 | 48.1 | 41.2 | 94.5 | 75.9 | 5.45 | 1040 |
| 측정값 (사용자 지정 평가, E.3절 참조) | 63.7 | 50.6 | 83.4 | 94.5 | 82.1 | - | - |
| 보고된 값 | 58.2 | 54.1 | 83.0 | 94.5 | - | - | - |
| Llama-3.2 11B | | | | | | | |
| 측정값 (정확한 일치) | 24.3 | 23.0 | 14.8 | 91.1 | 67.1 | 4.79 | 1032 |
| 측정값 (사용자 지정 평가, E.4절 참조) | 47.9 | 46.6 | 78.5 | 91.1 | 67.1 | - | - |
| 보고된 값 | 51.5 | 50.7 | 83.4 | 88.4 | 75.2 | - | - |
| Molmo-D 7B | | | | | | | |
| 측정값 (정확한 일치) | 12.3 | 24.3 | 27.0 | 72.2 | 57.1 | 3.72 | - |
| 측정값 (사용자 지정 평가, E.6절 참조) | 43.2 | 47.0 | 76.7 | 72.2 | 70.0 | - | - |
| 보고된 값 | 51.6 | 45.3 | 84.1 | 92.2 | 85.6 | - | - |
| LLaVA-OneVision 7B | | | | | | | |
| 측정값 (정확한 일치) | 36.1 | 45.1 | 67.2 | 90.5 | 78.4 | 4.12 | - |
| 측정값 (사용자 지정 평가, E.5절 참조) | 63.1 | 48.1 | 80.2 | 90.5 | 83.7 | - | - |
| 보고된 값 | 63.2 | 48.8 | 80.0 | 87.5 | - | - | - |
| Molmo 72B | | | | | | | |
| 측정값 (정확한 일치) | 52.2 | 52.7 | 75.6 | 86.5 | 75.2 | 3.51 | - |
| 측정값 (사용자 지정 평가, E.6절 참조) | 61.3 | 52.9 | 82.3 | 86.5 | 75.5 | - | - |
| 보고된 값 | 58.6 | 54.1 | 87.3 | 93.5 | 86.5 | - | - |
| Llama-3.2 90B | | | | | | | |
| 측정값 (정확한 일치) | 49.1 | 53.7 | 33.8 | 85.7 | 67.0 | 5.50 | 1071 |
| 측정값 (사용자 지정 평가, E.4절 참조) | 57.5 | 60.2 | 91.7 | 91.5 | 67.0 | - | - |
| 보고된 값 | 57.3 | 60.3 | 85.5 | 90.1 | 78.1 | - | - |
| Claude-3 Haiku | | | | | | | |
| 측정값 (정확한 일치) | 44.8 | 50.4 | 69.6 | 74.6 | 68.4 | 5.46 | 1000 |
| 측정값 (사용자 지정 평가, E.2절 참조) | 44.8 | 51.3 | 79.8 | 74.6 | 68.4 | - | - |
| 보고된 값 | 46.4 | 50.2 | 81.7 | 88.8 | - | - | - |
| Gemini-1.5-Flash 8B(0827) | | | | | | | |
| 측정값 (정확한 일치) | 56.9 | 50.7 | 78.0 | 79.5 | 65.5 | 5.93 | 1111 |
| 측정값 (사용자 지정 평가, E.2절 참조) | 57.1 | 50.7 | 78.2 | 79.5 | 69.2 | - | - |
| 보고된 값 | - | 50.3 | - | 73.6 | - | - | - |

표 8: 이전 모델들의 보고된 성능 재현. 표 2에서는 모든 모델을 동일한 평가 프레임워크, 동일한 프롬프트와 지표로 공정하게 재평가했습니다. 여기서는 개별 모델에 맞게 평가 설정을 조정하여 보고된 성능을 재현하고자 했습니다. Pixtral 12B가 강력한 비공개 모델들(예: Gemini-1.5-Flash 8B와 Claude-3 Haiku)과 마찬가지로 특별한 개입 없이도 우수한 성능을 보고할 수 있다는 점을 강조합니다.

이 부록에서 제시된 분석은 멀티모달 모델 평가에 있어 프롬프트 설계와 평가 지표의 중요성을 보여줍니다. 특히 소규모 오픈소스 모델들은 보고된 성능을 재현하기 위해 모델별 맞춤 평가가 필요한 반면, Pixtral 12B와 같은 일부 모델들은 비공개 모델들과 마찬가지로 특별한 개입 없이도 강력한 성능을 보여줍니다. 이는 Pixtral 12B가 지시사항을 정확히 따르는 능력과 다양한 평가 조건에서의 강건성을 갖추고 있음을 시사합니다.

또한 이 분석은 멀티모달 모델 평가를 위한 표준화된 프로토콜의 필요성을 강조합니다. 프롬프트 설계, 평가 지표, 그리고 구문 분석 방법의 작은 변화가 모델 성능에 상당한 영향을 미칠 수 있으므로, 공정하고 일관된 평가를 위해서는 이러한 요소들을 신중하게 고려해야 합니다.

- - -
### References
* [Pixtral 12B](http://arxiv.org/pdf/2410.07073v2)
