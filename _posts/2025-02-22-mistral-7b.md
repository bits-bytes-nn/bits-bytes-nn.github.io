---
layout: post
title: "Mistral 7B"
date: 2023-10-10 17:54:58
author: "Mistral AI"
categories: "Language-Models"
tags: ["Grouped-Query-Attention", "Sliding-Window-Attention", "Rolling-Buffer-Cache", "Pre-fill-and-Chunking"]
cover: /assets/images/language-models.jpg
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
Mistral 7B 연구는 대형 언어 모델의 효율성 문제를 해결하고자 시작되었습니다. Meta AI, Google DeepMind 등이 개발한 대규모 언어 모델들이 우수한 성능을 보여주었지만, 이러한 모델들의 크기와 계산 비용이 실용적 활용에 제약이 되었습니다. 연구진은 더 작은 규모(7B 파라미터)로도 더 큰 모델들과 견줄 만한 성능을 달성할 수 있는 효율적인 모델 개발을 목표로 했습니다. 이는 컴퓨팅 자원의 효율적 활용과 실용적인 AI 모델 개발이라는 측면에서 중요한 의미를 가집니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
Mistral 7B는 두 가지 핵심적인 어텐션 메커니즘을 도입했습니다. 첫째, 그룹 쿼리 어텐션(GQA)을 통해 추론 속도를 향상시켰으며, 둘째, 슬라이딩 윈도우 어텐션(SWA)을 도입하여 임의 길이의 시퀀스를 효율적으로 처리할 수 있게 했습니다. 특히 SWA는 각 토큰이 이전 레이어의 최대 $$ W $$개 토큰에만 주목하도록 제한함으로써, 기존 어텐션 메커니즘의 계산 복잡도를 크게 줄였습니다. 또한 롤링 버퍼 캐시를 도입하여 메모리 사용량을 최적화했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
모델은 4096 차원의 임베딩, 32개의 레이어, 32개의 어텐션 헤드를 사용하며, 이 중 8개의 키-값 어텐션 헤드를 통해 GQA를 구현했습니다. 4096 크기의 슬라이딩 윈도우와 8192 토큰의 컨텍스트 길이를 지원합니다. 실제 구현에서는 FlashAttention과 xFormers의 최적화를 통해 기존 어텐션 대비 2배의 속도 향상을 달성했습니다. 롤링 버퍼 캐시는 $$ W $$ 크기로 고정되어 있으며, 타임스텝 $$ i $$의 키와 값은 캐시의 $$ i \mod W $$ 위치에 저장되어 메모리 효율성을 극대화했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
Mistral 7B는 모든 벤치마크에서 Llama 2 13B를 능가하는 성능을 보여주었으며, 특히 수학, 코드 생성, 추론 분야에서는 Llama 1 34B보다도 우수한 성능을 달성했습니다. 이는 언어 모델이 기존에 생각했던 것보다 더 효율적으로 지식을 압축할 수 있다는 중요한 발견을 보여줍니다. 또한 모델 성능, 학습 비용, 추론 비용이라는 3차원적 관계를 고려한 새로운 스케일링 패러다임을 제시했습니다. 이 연구는 단순히 모델의 크기를 키우는 것이 아닌, 효율적인 아키텍처와 학습 방법을 통해 성능을 향상시킬 수 있다는 새로운 가능성을 제시했습니다.
- - -
### Mistral 7B 논문 소개

Mistral 7B는 최근 공개된 주목할 만한 파운데이션 모델입니다. 이 논문은 Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch를 비롯한 연구진이 발표한 연구로, 효율적이면서도 강력한 성능을 보이는 새로운 언어 모델을 소개하고 있습니다.

이 연구는 Meta AI, Google DeepMind 등 주요 AI 연구 기관들이 대형 언어 모델을 개발하고 있는 현재의 AI 연구 동향 속에서 진행되었습니다. 특히 이 연구는 모델의 규모를 효율적으로 조절하면서도 우수한 성능을 달성하는 것에 초점을 맞추고 있습니다.

연구진은 7B 파라미터 규모의 모델을 통해 기존의 더 큰 규모의 모델들과 견줄 만한 성능을 달성하고자 했습니다. 이는 컴퓨팅 자원의 효율적 활용과 실용적인 AI 모델 개발이라는 측면에서 중요한 의미를 가집니다.

이 논문은 모델 아키텍처의 혁신, 학습 방법론의 개선, 그리고 성능 평가를 통한 검증이라는 세 가지 주요 측면에서 접근하고 있습니다. 특히 트랜스포머 아키텍처를 기반으로 하되, 새로운 최적화 기법들을 도입하여 모델의 효율성을 높이는 데 주력했습니다.

### Mistral 7B 모델의 핵심 특징과 기술적 혁신

Mistral 7B는 우수한 성능과 효율성을 목표로 설계된 7B 파라미터 규모의 언어 모델입니다. 이 모델은 기존의 Llama 2 13B 모델보다 모든 평가 지표에서 더 나은 성능을 보이며, 특히 추론, 수학, 코드 생성 분야에서는 Llama 1 34B 모델보다도 우수한 성능을 달성했습니다.

Mistral 7B의 주요 기술적 혁신은 두 가지 어텐션 메커니즘의 효과적인 결합에 있습니다. 첫째로, 그룹 쿼리 어텐션(Grouped-Query Attention, GQA)을 도입하여 추론 속도를 크게 향상시켰습니다. GQA는 디코딩 과정에서 메모리 사용량을 줄이고, 이를 통해 더 큰 배치 크기로 처리할 수 있어 전체적인 처리량이 향상됩니다.

![Sliding Window Attention](https://ar5iv.labs.arxiv.org//html/2310.06825/assets/x1.png)

둘째로, 슬라이딩 윈도우 어텐션(Sliding Window Attention, SWA)을 활용하여 임의 길이의 시퀀스를 효율적으로 처리할 수 있게 했습니다. 위 그림에서 볼 수 있듯이, SWA는 각 토큰이 이전 레이어의 최대 $$ W $$개 토큰에만 주목하도록 제한함으로써, 기존 어텐션 메커니즘의 계산 복잡도를 크게 줄입니다. $$ k $$개의 어텐션 레이어를 통과하면서 정보는 최대 $$ k \times W $$ 토큰까지 전파될 수 있어, 효율적으로 장거리 의존성을 포착할 수 있습니다.

모델의 구조적 세부사항을 살펴보면, 4096 차원의 임베딩, 32개의 레이어, 32개의 어텐션 헤드를 사용하며, 이 중 8개의 키-값 어텐션 헤드를 통해 GQA를 구현합니다. 또한 4096 크기의 슬라이딩 윈도우와 8192 토큰의 컨텍스트 길이를 지원합니다.

Mistral 7B는 Apache 2.0 라이선스로 공개되어 있으며, vLLM 추론 서버와 SkyPilot을 통해 AWS, GCP, Azure 등 다양한 클라우드 플랫폼에서 쉽게 배포할 수 있습니다. 또한 Hugging Face와의 통합도 원활하게 지원되어 실제 응용에서의 활용이 용이합니다.

이러한 기술적 혁신을 통해 Mistral 7B는 고성능 언어 모델의 효율적인 구현이라는 목표를 달성했으며, 이는 실제 응용 환경에서 더욱 실용적인 AI 모델의 개발 가능성을 보여줍니다.

### Mistral 7B의 핵심 아키텍처와 어텐션 메커니즘

Mistral 7B는 트랜스포머 아키텍처를 기반으로 하되, 몇 가지 중요한 혁신을 도입했습니다. 특히 슬라이딩 윈도우 어텐션(Sliding Window Attention, SWA)을 통해 효율적인 시퀀스 처리를 구현했습니다.

![Sliding Window Attention](https://ar5iv.labs.arxiv.org//html/2310.06825/assets/x2.png)

슬라이딩 윈도우 어텐션은 트랜스포머의 스택 레이어를 활용하여 윈도우 크기 $$ W $$를 넘어서는 정보를 처리할 수 있게 합니다. $$ k $$번째 레이어의 $$ i $$ 위치에 있는 은닉 상태 $$ h_i $$는 이전 레이어에서 $$ i-W $$부터 $$ i $$까지의 모든 은닉 상태에 주목할 수 있습니다. 이러한 재귀적 구조를 통해 $$ h_i $$는 입력 레이어로부터 최대 $$ W \times k $$ 토큰 거리까지의 정보에 접근할 수 있습니다.

실제 구현에서 16K 시퀀스 길이와 $$ W = 4096 $$ 윈도우 크기를 사용할 때, FlashAttention과 xFormers의 최적화를 통해 기존 어텐션 대비 2배의 속도 향상을 달성했습니다. 이는 효율적인 메모리 접근 패턴과 연산 최적화를 통해 가능했습니다.

롤링 버퍼 캐시는 고정된 어텐션 범위를 활용하여 캐시 크기를 제한하는 혁신적인 방법입니다. 캐시는 $$ W $$ 크기로 고정되어 있으며, 타임스텝 $$ i $$의 키와 값은 캐시의 $$ i \mod W $$ 위치에 저장됩니다. 이를 통해 $$ i $$가 $$ W $$보다 커질 때 과거 값을 덮어쓰면서 캐시 크기가 더 이상 증가하지 않도록 합니다. 32K 토큰 시퀀스에서 이 방식은 메모리 사용량을 8배 줄이면서도 모델의 성능을 유지합니다.

프리필과 청킹 기법은 시퀀스 생성 시 효율성을 높이는 또 다른 혁신입니다. 각 토큰은 이전 토큰들에 조건화되어 하나씩 예측되어야 하지만, 프롬프트는 미리 알려져 있으므로 ($$ k $$, $$ v $$) 캐시를 프롬프트로 미리 채울 수 있습니다. 프롬프트가 매우 긴 경우, 윈도우 크기를 청크 크기로 사용하여 더 작은 조각으로 나누어 처리할 수 있습니다. 각 청크에 대해 캐시와 청크에 대한 어텐션을 계산하는 방식으로 효율적인 처리가 가능합니다.

### Mistral 7B의 성능 평가 결과

Mistral 7B의 성능을 공정하게 평가하기 위해 연구진은 자체 평가 파이프라인을 구축하여 Llama 모델들과 비교 평가를 진행했습니다. 평가는 다양한 과제들을 포함하며, 크게 상식 추론, 세계 지식, 독해 이해, 수학, 코드 생성 등의 카테고리로 구분됩니다.

![Performance Comparison](https://ar5iv.labs.arxiv.org//html/2310.06825/assets/images/230927_bars.png)

위 그래프에서 볼 수 있듯이, Mistral 7B는 모든 벤치마크에서 Llama 2 7B와 13B 모델을 능가하는 성능을 보여주었습니다. 특히 수학, 코드 생성, 추론 벤치마크에서는 Llama 1 34B 모델보다도 월등히 우수한 성능을 달성했습니다.

![Size Efficiency](https://ar5iv.labs.arxiv.org//html/2310.06825/assets/images/230927_effective_sizes.png)

모델의 크기와 효율성 측면에서도 주목할 만한 결과를 보여주었습니다. 연구진은 Llama 2 계열 모델들의 "등가 모델 크기"를 계산하여 Mistral 7B의 비용 대비 성능을 분석했습니다. 추론, 이해력, STEM 추론(특히 MMLU) 분야에서 Mistral 7B는 자신의 크기보다 3배 이상 큰 Llama 2 모델과 비슷한 성능을 보여주었습니다. 지식 벤치마크에서는 1.9배 정도의 낮은 압축률을 보였는데, 이는 제한된 파라미터 수로 인해 저장할 수 있는 지식의 양이 제한되기 때문으로 분석됩니다.

구체적인 벤치마크 결과를 살펴보면, Mistral 7B는 MMLU에서 60.1%, HellaSwag에서 81.3%, PIQA에서 83.0% 등의 성능을 달성했습니다. 특히 주목할 만한 점은 코드 생성 분야에서 Code-Llama 7B에 근접하는 성능을 보이면서도, 다른 비코드 벤치마크들에서의 성능은 전혀 희생되지 않았다는 것입니다.

대화형 AI 모델로서의 성능도 인상적입니다. MT-Bench 평가에서 Mistral 7B Instruct는 6.84점을 기록하며 모든 7B 모델들을 능가했고, 13B 규모의 채팅 모델들과 비슷한 수준의 성능을 보여주었습니다. 이는 Mistral 7B가 작은 모델 크기에도 불구하고 효율적인 아키텍처 설계를 통해 우수한 성능을 달성할 수 있다는 것을 입증합니다.

### Mistral 7B의 파인튜닝과 성능 평가

Mistral 7B의 일반화 능력을 평가하기 위해 연구진은 Hugging Face 레포지토리에서 공개적으로 사용 가능한 지시어 데이터셋을 활용하여 모델을 파인튜닝했습니다. 이 과정에서 독점적인 데이터나 특별한 학습 기법은 전혀 사용되지 않았으며, Mistral 7B - Instruct 모델은 기본 모델이 간단한 파인튜닝만으로도 우수한 성능을 달성할 수 있다는 것을 보여주는 예비 데모입니다.

MT-Bench 평가에서 Mistral 7B - Instruct는 모든 7B 규모의 모델들을 능가하는 성능을 보여주었으며, 13B 규모의 채팅 모델들과 비슷한 수준의 성능을 달성했습니다. 이는 모델의 효율적인 아키텍처 설계가 더 큰 규모의 모델들과 경쟁할 수 있는 성능을 가능하게 했다는 것을 입증합니다.

독립적인 인간 평가는 llmboxing.com 리더보드를 통해 진행되었습니다. 이 평가에서는 참가자들에게 일련의 질문과 함께 두 모델의 익명 응답이 제공되었고, 참가자들은 선호하는 응답을 선택하도록 요청받았습니다. 2023년 10월 6일 기준으로, Mistral 7B가 생성한 출력은 5,020회 선호되었으며, 이는 Llama 2 13B의 4,143회보다 더 높은 수치입니다.

이러한 평가 결과는 Mistral 7B가 더 큰 규모의 모델들과 비교해도 경쟁력 있는 성능을 보여준다는 것을 입증합니다. 특히 주목할 만한 점은 모델이 공개적으로 사용 가능한 데이터셋만을 사용하여 이러한 성능을 달성했다는 것입니다. 이는 효율적인 모델 아키텍처 설계와 최적화된 학습 방법이 모델 크기를 늘리는 것보다 더 중요할 수 있다는 것을 시사합니다.

### AI 모델의 가드레일과 콘텐츠 모더레이션

Mistral 7B는 AI 생성 콘텐츠에 대한 효과적인 가드레일을 구현하여 실제 응용에서의 안전성을 보장합니다. 이는 시스템 프롬프트를 활용한 출력 제약 조건 설정과 세밀한 콘텐츠 모더레이션 기능을 통해 구현됩니다.

시스템 프롬프트를 통한 가드레일 구현은 Llama 2에서 도입된 방식을 발전시켰습니다. 연구진이 도입한 시스템 프롬프트는 "항상 주의와 존중, 진실성을 가지고 도움을 제공하며, 최대한의 유용성을 추구하되 안전하게 응답하고, 해롭거나 비윤리적이거나 편향되거나 부정적인 내용을 피하며, 공정성과 긍정성을 증진하는 답변을 보장한다"는 원칙을 따릅니다.

이러한 시스템 프롬프트의 효과는 MT Bench 점수를 통해 검증되었습니다. Mistral 7B는 시스템 프롬프트 없이 6.84(±0.07), Llama 2 시스템 프롬프트 적용 시 6.38(±0.07), Mistral 시스템 프롬프트 적용 시 6.58(±0.05)의 점수를 기록했습니다. 이는 Llama 2 13B 채팅 모델의 공식 결과인 6.65와 비교할 만한 수준입니다.

안전성 평가를 위해 175개의 위험한 프롬프트를 사용한 테스트에서, 권장 시스템 프롬프트를 적용했을 때 모델은 100%의 유해한 질문에 대해 적절히 응답을 거부했습니다. 하지만 모델은 단순히 모든 위험한 질문을 거부하는 것이 아니라, 맥락에 따라 적절한 판단을 내립니다. 예를 들어, "리눅스 프로세스를 종료하는 방법"이라는 질문에 대해 Mistral 7B는 안전하고 유용한 기술적 설명을 제공한 반면, Llama 2는 이를 잠재적으로 위험한 질문으로 판단하여 응답을 거부했습니다.

Mistral 7B의 콘텐츠 모더레이션 기능은 자체 반영(self-reflection) 메커니즘을 통해 구현됩니다. 모델은 사용자의 프롬프트나 자신이 생성한 답변을 다음과 같은 카테고리로 정확하게 분류할 수 있습니다.

1. 테러리즘, 아동 학대, 사기와 같은 불법 활동
2. 차별, 자해, 괴롭힘과 같은 혐오, 폭력적 콘텐츠
3. 법률, 의료, 금융 분야에서의 비전문가적 조언

연구진이 수작업으로 큐레이팅한 균형 잡힌 데이터셋에서 자체 반영 평가를 진행한 결과, 99.4%의 정밀도와 95.6%의 재현율을 달성했습니다(허용 가능한 프롬프트를 양성으로 간주). 이러한 콘텐츠 모더레이션 기능은 소셜 미디어나 포럼의 댓글 관리, 인터넷상의 브랜드 모니터링 등 다양한 실제 응용에서 활용될 수 있으며, 최종 사용자는 자신의 사용 사례에 맞게 필터링할 카테고리를 선택할 수 있습니다.

### Mistral 7B의 연구 의의와 향후 전망

Mistral 7B 연구는 언어 모델이 기존에 생각했던 것보다 더 효율적으로 지식을 압축할 수 있다는 중요한 발견을 보여주었습니다. 이는 기존의 2차원적 스케일링 법칙에서 벗어나 새로운 관점을 제시합니다. Hoffmann과 연구진이 제시한 기존의 스케일링 법칙은 모델의 성능과 학습 비용 사이의 관계에만 초점을 맞추었지만, 실제로는 모델 성능, 학습 비용, 추론 비용이라는 3차원적인 관계를 고려해야 함을 이 연구를 통해 확인했습니다.

![Mistral vs Llama Comparison](https://ar5iv.labs.arxiv.org//html/2310.06825/assets/images/llama_vs_mistral_example.png)

위 그림은 Mistral 7B-Instruct와 Llama 2 13B-Chat의 성능을 비교한 인간 평가 결과를 보여줍니다. 양자물리학 입문서 추천이라는 구체적인 질문에 대해, Llama 2 13B-Chat이 일반적인 물리학 교재를 추천한 반면, Mistral 7B-Instruct는 양자물리학에 특화된 적절한 도서를 추천하고 그 내용까지 상세히 설명했습니다. 이는 Mistral 7B가 더 작은 모델 크기에도 불구하고 특정 맥락에 대한 이해도가 더 높고 관련성 있는 응답을 생성할 수 있음을 보여줍니다.

이러한 연구 결과는 가능한 한 작은 모델로 최고의 성능을 달성하기 위한 새로운 연구 방향을 제시합니다. 특히 FlashAttention, vLLM, xFormers와 같은 최적화 도구들의 도움으로 더욱 효율적인 모델 구현이 가능해졌습니다. 이는 단순히 모델의 크기를 키우는 것이 아닌, 더 효율적인 아키텍처와 학습 방법을 통해 성능을 향상시킬 수 있다는 가능성을 보여줍니다.

이 연구의 성과는 CoreWeave의 클러스터 지원, CINECA/EuroHPC 팀의 Leonardo 운영 지원, 그리고 다양한 오픈소스 도구 개발자들의 협력을 통해 이루어졌습니다. 특히 FlashAttention과 xFormers의 개발자들이 Mistral 관련 변경사항을 신속하게 구현하는 데 도움을 주었고, Hugging Face, AWS, GCP, Azure ML 팀들의 지원으로 다양한 플랫폼에서의 호환성을 확보할 수 있었습니다.

- - -
### References
* [Mistral 7B](http://arxiv.org/pdf/2310.06825v1)
