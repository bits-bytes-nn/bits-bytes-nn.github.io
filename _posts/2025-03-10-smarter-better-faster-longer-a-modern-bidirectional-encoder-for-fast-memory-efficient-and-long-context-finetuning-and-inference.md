---
layout: post
title: "ModernBERT - Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference"
date: 2024-12-18 09:39:44
author: "Answer.AI"
categories: "Language-Models"
tags: ["Alternating-Local-Global-Attention", "Unpadding-Transformer-Architecture", "RoPE-Positional-Embedding-Extension", "Hardware-Aware-Model-Design", "Efficient-Long-Context-Encoder", "Sequence-Packing-Optimization", "GeGLU-Activation-Improvement", "Flash-Attention-Integration", "Efficient-Encoder-Pretraining", "Context-Length-Scaling"]
cover: /assets/images/language-models.jpg
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

인코더 기반 트랜스포머 모델은 자연어 처리 분야에서 중요한 역할을 해왔지만, 최근 몇 년간 큰 발전 없이 정체되어 있었습니다. 특히 BERT 모델은 2019년 이후로 거의 개선되지 않았으며, 512 토큰으로 제한된 시퀀스 길이, 비효율적인 모델 설계, 제한된 어휘 크기 등의 한계를 가지고 있었습니다. 대규모 언어 모델(LLM)의 등장에도 불구하고, 인코더 모델은 여전히 검색, 분류, 개체명 인식 등 다양한 비생성적 작업에서 중요한 역할을 수행하고 있었습니다.

연구진은 기존 인코더 모델의 한계를 극복하고, 현대적인 하드웨어와 최신 기계학습 기술을 활용하여 더욱 효율적이고 성능이 뛰어난 모델을 개발하고자 했습니다. 특히 코드 데이터를 포함한 광범위한 데이터셋으로 학습하고, 긴 컨텍스트를 지원하며, 추론 속도와 메모리 효율성을 크게 개선하는 것을 목표로 삼았습니다. 이는 인코더 모델이 여전히 다양한 실무 애플리케이션에서 중요한 역할을 하고 있음을 인식하고, 그 성능을 획기적으로 향상시키고자 하는 동기에서 비롯되었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

ModernBERT는 여러 혁신적인 기술적 접근법을 통해 기존 인코더 모델의 한계를 극복하고자 했습니다. 첫째, 회전 위치 임베딩(RoPE)과 GeGLU 활성화 함수 등 최신 트랜스포머 아키텍처 개선 사항을 통합했습니다. 둘째, 전역 및 지역 어텐션을 교차 사용하는 새로운 어텐션 메커니즘을 도입하여 계산 효율성을 높였습니다. 특히 3개의 레이어마다 전역 어텐션을, 나머지 레이어에서는 128 토큰 슬라이딩 윈도우 지역 어텐션을 사용하는 방식을 채택했습니다.

또한 ModernBERT는 언패딩(unpadding) 기술을 최초로 전면적으로 적용하여 추론 효율성을 크게 향상시켰습니다. 이 방법은 패딩 토큰을 제거하고 모든 시퀀스를 하나의 시퀀스로 연결하여 처리함으로써 계산 자원을 더욱 효율적으로 활용할 수 있게 합니다. 더불어 코드 데이터를 포함한 2조 개의 토큰에 대해 학습하여, 프로그래밍 관련 작업에서의 성능도 크게 개선했습니다. 이러한 접근법은 ModernBERT가 기존 인코더 모델과 차별화되는 핵심 혁신점입니다.

#### 제안된 방법은 어떻게 구현되었습니까?

ModernBERT의 구현은 여러 단계의 신중한 설계와 실험을 거쳤습니다. 모델 아키텍처 설계에서는 하드웨어 효율성을 최대화하기 위해 Deep & Narrow 접근법을 채택했으며, base와 large 모델에 대해 각각 22개와 28개의 레이어를 구성했습니다. 학습 과정에서는 StableAdamW 옵티마이저와 수정된 사다리꼴 학습률 스케줄을 사용하여 학습 안정성을 높였습니다.

데이터 혼합에 있어서는 웹 문서, 코드, 과학 문헌 등 다양한 소스의 2조 개 토큰을 활용했으며, 최신 BPE 토크나이저를 도입하여 토큰 효율성을 높였습니다. 특히 컨텍스트 길이 확장을 위해 1,024 시퀀스 길이에서 시작해 8,192 토큰으로 점진적으로 확장하는 전략을 사용했습니다. 이 과정에서 RoPE theta 값을 160,000으로 증가시키고, 추가로 300십억 토큰을 학습하여 긴 컨텍스트 성능을 최적화했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?

ModernBERT는 인코더 전용 모델의 성능과 효율성에 있어 중요한 이정표를 제시했습니다. GLUE 벤치마크에서 ModernBERT-base는 DeBERTaV3-base를 능가하는 최초의 인코더로 자리 잡았으며, 코드 및 긴 컨텍스트 검색 벤치마크에서도 기존 모델들을 크게 앞섰습니다. 특히 8,192 토큰의 네이티브 시퀀스 길이를 지원하면서도, 다른 모델들에 비해 2-3배 빠른 추론 속도와 뛰어난 메모리 효율성을 달성했습니다.

이 연구는 인코더 모델이 여전히 다양한 자연어 처리 작업에서 중요한 역할을 할 수 있음을 보여주었습니다. 특히 검색, 분류, 그리고 코드 관련 작업에서 ModernBERT는 대규모 언어 모델에 필적하는 성능을 입증했습니다. 또한 하드웨어 효율성을 고려한 모델 설계와 최신 아키텍처 개선 기법의 통합은 향후 인코더 모델 연구의 새로운 방향을 제시했다는 점에서 큰 의의가 있습니다. 이는 계산 자원이 제한된 환경에서도 고성능 자연어 처리 모델을 개발할 수 있는 가능성을 보여주었습니다.
- - -
# ModernBERT: 빠르고, 메모리 효율적이며, 긴 컨텍스트를 위한 현대적 양방향 인코더

## 소개

인코더 전용 트랜스포머 모델인 BERT는 검색 및 분류 작업에서 더 큰 디코더 전용 모델에 비해 뛰어난 성능-크기 트레이드오프를 제공합니다. BERT는 [Devlin과 연구진](https://arxiv.org/pdf/1810.04805.pdf)이 2019년에 발표한 이후 수많은 프로덕션 파이프라인의 핵심 요소로 자리잡았지만, 그 이후로 BERT에 대한 파레토 개선은 제한적이었습니다. 트랜스포머 아키텍처는 [Vaswani와 연구진](https://arxiv.org/pdf/1706.03762.pdf)이 2017년에 소개한 이후 자연어 처리의 기반이 되었습니다.

대규모 언어 모델(LLM)의 인기가 높아지고 있음에도 불구하고, 인코더 전용 모델은 여전히 비생성적 다운스트림 애플리케이션에서 널리 사용되고 있습니다. GPT([Radford와 연구진, 2018](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), [2019](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf); [Brown과 연구진, 2020](https://arxiv.org/pdf/2005.14165.pdf)), Llama([Touvron과 연구진, 2023](https://arxiv.org/pdf/2302.13971.pdf); [Dubey와 연구진, 2024](https://arxiv.org/pdf/2402.17838.pdf)), Qwen([Bai와 연구진, 2023](https://arxiv.org/pdf/2309.16609.pdf); [Yang과 연구진, 2024](https://arxiv.org/pdf/2405.05811.pdf))과 같은 LLM이 주목을 받고 있지만, 인코더 전용 모델은 여전히 다양한 비생성적 다운스트림 애플리케이션에서 널리 사용되고 있습니다.

인코더의 인기는 주로 그들의 적은 추론 요구사항 덕분입니다. 이를 통해 검색을 위해 대규모 문서 코퍼스를 효율적으로 처리하고 분류 작업을 빠르게 수행할 수 있습니다. 인코더 모델은 품질과 크기 사이에서 매력적인 트레이드오프를 제공하여, 대량의 데이터를 처리할 때 인코더-디코더 및 디코더 전용 언어 모델에 비해 인기 있는 선택지가 됩니다([Penedo와 연구진, 2024](https://arxiv.org/pdf/2401.00368.pdf)).

인코더 모델은 특히 정보 검색(IR) 애플리케이션, 예를 들어 시맨틱 검색에서 인기가 있으며, 이 분야에서는 [Karpukhin과 연구진(2020)](https://arxiv.org/pdf/2004.04906.pdf)과 [Khattab과 Zaharia(2020)](https://arxiv.org/pdf/2004.14503.pdf)의 연구와 같이 인코더를 활용하는 데 상당한 진전이 있었습니다. LLM이 최근 몇 년간 주목을 받았지만, 이는 또한 IR을 위한 인코더 전용 모델에 대한 관심을 새롭게 불러일으켰습니다. 실제로, 인코더 기반 시맨틱 검색은 검색 증강 생성(RAG) 파이프라인의 핵심 구성 요소입니다([Lewis와 연구진, 2020](https://arxiv.org/pdf/2005.11401.pdf)). 여기서 인코더 모델은 사용자 쿼리와 관련된 컨텍스트를 검색하고 LLM에 제공하는 데 사용됩니다.

인코더 전용 모델은 또한 분류([Tunstall과 연구진, 2022](https://arxiv.org/pdf/2209.11055.pdf))나 개체명 인식(NER)([Zaratiana와 연구진, 2024](https://arxiv.org/pdf/2401.14822.pdf))과 같은 다양한 판별 작업에도 여전히 자주 사용되며, 여기서 그들은 종종 특수화된 LLM의 성능과 일치합니다. 여기서도 그들은 LLM과 함께 사용될 수 있습니다. 예를 들어, 독성 프롬프트를 감지하고([Ji와 연구진, 2023](https://arxiv.org/pdf/2307.15043.pdf); [Jiang과 연구진, 2024b](https://arxiv.org/pdf/2402.08301.pdf)) 응답을 방지하거나, 에이전트 프레임워크에서 쿼리를 라우팅하는 데 사용될 수 있습니다([Yao와 연구진, 2023](https://arxiv.org/pdf/2309.07864.pdf); [Schick과 연구진, 2023](https://arxiv.org/pdf/2307.03025.pdf)).

놀랍게도, 이러한 파이프라인은 현재 오래된 모델, 그리고 종종 원래의 BERT 자체를 백본으로 사용하고 있으며([Wang과 연구진, 2022](https://arxiv.org/pdf/2204.06031.pdf); [Xiao와 연구진, 2023](https://arxiv.org/pdf/2303.08128.pdf)), 최근 몇 년간 개발된 개선 사항을 활용하지 않고 있습니다. 실무자들은 많은 단점에 직면하고 있습니다. 512 토큰으로 제한된 시퀀스 길이, 최적화되지 않은 모델 설계([Anthony와 연구진, 2024](https://arxiv.org/pdf/2401.14019.pdf))와 어휘 크기([Karpathy, 2023](https://arxiv.org/pdf/2307.09288.pdf)), 그리고 일반적으로 다운스트림 성능이나 계산 효율성 측면에서 비효율적인 아키텍처입니다. 마지막으로, 훈련 데이터는 양이 제한되어 있고 좁은 도메인(특히 코드 데이터가 부족함)으로 제한되거나 최근 이벤트에 대한 지식이 부족합니다.

최근의 현대화 노력은 제한된 범위로 인해 인코더 전용 모델의 단점을 부분적으로만 해결했습니다. MosaicBERT([Portes와 연구진, 2023](https://arxiv.org/pdf/2307.02971.pdf)), CrammingBERT([Geiping과 Goldstein, 2023](https://arxiv.org/pdf/2212.14034.pdf)), 그리고 AcademicBERT([Izsak과 연구진, 2021](https://arxiv.org/pdf/2106.15718.pdf))는 더 나은 훈련 효율성으로 BERT 성능을 일치시키는 데 초점을 맞췄습니다. NomicBERT([Nussbaum과 연구진, 2024](https://arxiv.org/pdf/2402.01613.pdf))와 GTE-en-MLM([Zhang과 연구진, 2024](https://arxiv.org/pdf/2402.03216.pdf))(이 작업과 동시에 개발됨)은 검색 애플리케이션에 초점을 맞춘 더 긴 컨텍스트 인코더 모델을 소개했지만, 효율성이나 분류 성능을 최적화하지 않았으며, 특히 프로그래밍 관련 작업에서 두드러지게 나타나는 오래된 훈련 데이터 혼합을 재사용했습니다.

### 기여

본 논문에서는 ModernBERT를 소개합니다. 이는 다운스트림 성능과 효율성을 향상시키기 위해 설계된 개선된 아키텍처를 갖춘 현대화된 인코더 전용 트랜스포머 모델로, 특히 더 긴 시퀀스 길이에서 효과적입니다. 또한 코드 데이터를 포함한 데이터 혼합으로 2조 개의 토큰에 대해 훈련함으로써 인코더 전용 모델을 현대적이고 더 큰 데이터 규모로 가져옵니다.

저자들은 ModernBERT-base와 ModernBERT-large라는 두 가지 모델을 공개했으며, 이 모델들은 다양한 다운스트림 작업에서 기존의 모든 인코더 모델에 비해 최첨단 전체 성능을 달성합니다. 이러한 결과는 상당히 높은 추론 효율성으로 달성되며, 8192 토큰의 시퀀스를 이전 모델보다 거의 두 배 빠르게 처리합니다.

인코더 전용 모델에 대한 향후 연구를 지원하기 위해, 저자들은 쉬운 실험을 가능하게 하는 모듈식 아키텍처 프레임워크인 FlexBERT를 공개하고, [Pythia](https://arxiv.org/pdf/2304.01373.pdf)([Biderman과 연구진, 2023](https://arxiv.org/pdf/2304.01373.pdf))에서 영감을 받아 모든 중간 훈련 체크포인트를 제공합니다.

## 방법론

### 아키텍처 개선

ModernBERT 모델 아키텍처는 [Vaswani와 연구진](https://arxiv.org/pdf/1706.03762v7)이 제안한 표준 트랜스포머 아키텍처를 확장하여 광범위하게 검증된 최신 기술들을 통합했습니다. 이 섹션에서는 모델 아키텍처의 개선 사항을 세 가지 주요 영역으로 나누어 설명합니다. 현대적 트랜스포머 설계, 효율성 개선, 그리고 GPU에 최적화된 모델 설계입니다. 모든 아키텍처 결정은 부록 D에 자세히 설명된 실험 결과를 바탕으로 이루어졌습니다.

#### 현대적 트랜스포머

**바이어스 항**

[Dayma와 연구진](https://arxiv.org/pdf/2002.05202)의 연구를 따라, ModernBERT는 최종 디코더 선형 레이어를 제외한 모든 선형 레이어에서 바이어스 항을 비활성화했습니다. 디코더에서 바이어스 항을 유지한 이유는 [Gao와 연구진](https://arxiv.org/pdf/1906.04341)과 [Welch와 연구진](https://arxiv.org/pdf/2001.04246)이 지적한 가중치 공유의 부정적 영향을 완화하기 위함입니다. 또한 [Xu와 연구진](https://arxiv.org/pdf/1911.07013)의 연구에 따라 모든 레이어 정규화(Layer Norm)에서도 바이어스 항을 비활성화했습니다. 이러한 두 가지 변경 사항을 통해 선형 레이어에 더 많은 파라미터 예산을 할당할 수 있게 되었습니다.

**위치 임베딩**

ModernBERT는 절대 위치 임베딩 대신 회전 위치 임베딩(RoPE, Rotary Positional Embeddings)을 사용합니다. 이 선택은 [Su와 연구진](https://arxiv.org/pdf/2104.09864v5)이 제안한 방식으로, 짧은 컨텍스트와 긴 컨텍스트 언어 모델에서 입증된 성능([Black과 연구진](https://arxiv.org/pdf/2204.06745), [Dubey와 연구진](https://arxiv.org/pdf/2402.17838.pdf), [Gemma와 연구진](https://arxiv.org/pdf/2403.08295))과 대부분의 프레임워크에서 효율적인 구현, 그리고 컨텍스트 확장의 용이성 때문에 선택되었습니다.

**정규화**

ModernBERT는 [Xiong과 연구진](https://arxiv.org/pdf/2002.04745v2)이 제안한 사전 정규화(pre-normalization) 블록과 표준 레이어 정규화([Lei Ba와 연구진](https://arxiv.org/pdf/1607.06450v1))를 사용합니다. 이는 학습을 안정화하는 데 도움이 된다고 알려져 있습니다. CrammingBERT([Geiping과 Goldstein](https://arxiv.org/pdf/2212.14034.pdf))와 유사하게 임베딩 레이어 이후에 레이어 정규화를 추가했으며, 중복을 피하기 위해 첫 번째 어텐션 레이어에서 첫 번째 레이어 정규화를 제거했습니다.

**활성화 함수**

ModernBERT는 원래 BERT의 GeLU([Hendrycks와 Gimpel](https://arxiv.org/pdf/1606.08415)) 활성화 함수를 기반으로 한 GeGLU([Shazeer](https://arxiv.org/pdf/2002.05202)) 활성화 함수를 채택했습니다. 이는 [Dauphin과 연구진](https://arxiv.org/pdf/1612.08083)이 제안한 Gated-Linear Units(GLU) 기반 활성화 함수입니다. 이러한 선택은 GLU 변형을 사용할 때 일관된 경험적 개선을 보여준 최근 연구([Shazeer](https://arxiv.org/pdf/2002.05202), [Geiping과 Goldstein](https://arxiv.org/pdf/2212.14034.pdf))와 일치합니다.

#### 효율성 개선

**교차 어텐션**

긴 컨텍스트 모델에 관한 최근 연구([Gemma와 연구진](https://arxiv.org/pdf/2403.08295))를 따라, ModernBERT의 어텐션 레이어는 전역 어텐션(global attention)과 지역 어텐션(local attention)을 번갈아 사용합니다. 전역 어텐션은 시퀀스 내의 모든 토큰이 다른 모든 토큰에 어텐션을 적용하는 방식이고, 지역 어텐션은 토큰들이 작은 슬라이딩 윈도우 내에서만 서로 어텐션을 적용하는 방식입니다([Beltagy와 연구진](https://arxiv.org/pdf/2004.05150)).

ModernBERT에서는 세 번째 레이어마다 160,000의 RoPE theta 값을 가진 전역 어텐션을 사용하고, 나머지 레이어에서는 10,000의 RoPE theta 값을 가진 128 토큰 크기의 지역 슬라이딩 윈도우 어텐션을 사용합니다.

**언패딩(Unpadding)**

ModernBERT는 MosaicBERT([Portes와 연구진](https://arxiv.org/pdf/2307.02971.pdf))와 GTE([Zhang과 연구진](https://arxiv.org/pdf/2402.03216.pdf))를 따라 학습과 추론 모두에서 언패딩([Zeng과 연구진](https://arxiv.org/pdf/2208.08124))을 사용합니다. 인코더 전용 언어 모델은 일반적으로 배치 내에서 균일한 시퀀스 길이를 보장하기 위해 패딩 토큰을 사용하는데, 이는 의미적으로 비어있는 토큰에 계산 자원을 낭비하게 됩니다. 언패딩은 패딩 토큰을 제거하고 미니배치의 모든 시퀀스를 하나의 시퀀스로 연결하여 배치 크기가 1인 것처럼 처리함으로써 이러한 비효율성을 피합니다.

기존의 언패딩 구현은 다른 모델 레이어에 대해 내부적으로 시퀀스를 언패드하고 다시 패드하여 계산과 메모리 대역폭을 낭비했습니다. ModernBERT는 Flash Attention의 가변 길이 어텐션과 RoPE 구현을 사용하여 들쭉날쭉한(jagged) 어텐션 마스크와 하나의 언패드된 시퀀스에 대한 RoPE 적용을 가능하게 합니다. ModernBERT는 토큰 임베딩 레이어 이전에 입력을 언패드하고 선택적으로 모델 출력을 다시 패드하여 다른 언패딩 방법보다 10-20% 성능 향상을 제공합니다.

**Flash Attention**

Flash Attention([Dao와 연구진](https://arxiv.org/pdf/2205.14135v2))은 메모리와 계산 효율적인 어텐션 커널을 제공하는 현대 트랜스포머 기반 모델의 핵심 구성 요소입니다. 이 작업을 시작할 때, Nvidia H100 GPU를 위한 최신 버전인 Flash Attention 3([Shah와 연구진](https://arxiv.org/pdf/2307.08691v1))은 슬라이딩 윈도우 어텐션에 대한 지원을 포함하지 않았습니다. ModernBERT는 전역 어텐션 레이어에는 Flash Attention 3을, 지역 어텐션 레이어에는 Flash Attention 2([Dao](https://arxiv.org/pdf/2307.08691v1))를 사용합니다.

**torch.compile**

ModernBERT는 학습 효율성을 향상시키기 위해 PyTorch의 내장 컴파일([Ansel과 연구진](https://arxiv.org/pdf/2308.16739))을 활용하여 호환되는 모든 모듈을 컴파일합니다. 이는 무시할 수 있는 컴파일 오버헤드로 처리량을 10% 향상시킵니다.

#### 모델 설계

동일한 파라미터 수에서, 더 많은 좁은 레이어를 가진 모델(Deep & Narrow)은 더 적은 넓은 레이어를 가진 모델(Shallow & Wide)과 다른 학습 패턴을 보입니다([Nguyen과 연구진](https://arxiv.org/pdf/2107.11817)). [Tay와 연구진](https://arxiv.org/pdf/2203.00555)과 [Liu와 연구진](https://arxiv.org/pdf/2405.14659)은 Deep & Narrow 언어 모델이 더 느린 추론을 감수하면서도 더 얕은 모델보다 더 나은 다운스트림 성능을 보인다는 것을 보여주었습니다. [Anthony와 연구진](https://arxiv.org/pdf/2401.14019)은 하드웨어를 고려한 방식으로 모델을 설계함으로써 큰 런타임 이득을 얻을 수 있다는 점을 강조했으며, 이는 이전에 여러 실무자들에 의해 경험적으로 관찰된 바 있습니다([Shoeybi와 연구진](https://arxiv.org/pdf/1909.08053), [Karpathy](https://arxiv.org/pdf/2307.09288.pdf), [Black과 연구진](https://arxiv.org/pdf/2204.06745)).

ModernBERT는 일반적인 GPU 바스켓(서버 GPU: NVIDIA T4, A10, L4, A100, H100 및 소비자 GPU: NVIDIA RTX 3090, 4090)의 활용을 최대화하면서, 추론 속도의 큰 저하 없이 가능한 한 Deep & Narrow 설계를 목표로 하는 많은 소규모 실험을 통해 설계되었습니다. 추론 GPU(A100 및 H100 제외)에 우선순위가 주어졌습니다.

ModernBERT는 base와 large 모델에 대해 각각 22개와 28개의 레이어를 가지며, 총 파라미터 수는 각각 1억 4900만과 3억 9500만입니다. 이는 다운스트림 성능과 하드웨어 효율성 사이의 균형을 맞춘 결과입니다. ModernBERT base는 768의 은닉 크기와 2,304의 GLU 확장을 가지고, large는 1,024의 은닉 크기와 5,248의 GLU 확장을 가집니다. 이러한 비율은 텐서 코어에서 최적의 타일링과 대상 GPU 바스켓의 다양한 스트리밍 멀티프로세서 수에 걸쳐 가장 효율적인 타일링을 가능하게 합니다. 모델 설계에 대한 자세한 내용은 부록 B에서 제공됩니다.

### 학습

#### 데이터 혼합

ModernBERT 모델은 주로 영어로 된 2조 개의 토큰으로 구성된 다양한 데이터 소스에서 학습되었습니다. 이 데이터 소스에는 웹 문서, 코드, 과학 문헌 등이 포함되어 있으며, 최신 데이터 혼합 방식을 따랐습니다. 최종 데이터 혼합은 일련의 실험을 통해 선택되었습니다.

#### 토크나이저

최근의 대부분의 인코더 모델이 원래 BERT 토크나이저를 재사용하는 것과 달리([Nussbaum과 연구진, 2024](https://arxiv.org/pdf/2402.01613.pdf); [Portes와 연구진, 2023](https://arxiv.org/pdf/2307.02971.pdf); [Zhang과 연구진, 2024](https://arxiv.org/pdf/2402.03216.pdf)), ModernBERT는 현대적인 BPE(Byte Pair Encoding) 토크나이저를 사용하기로 결정했습니다. 이 토크나이저는 [Groeneveld와 연구진, 2024](https://arxiv.org/pdf/2402.00838)이 개발한 OLMo 토크나이저의 수정 버전으로, 토큰 효율성을 높이고 코드 관련 작업에서 더 나은 성능을 제공합니다.

ModernBERT 토크나이저는 원래 BERT 모델([Devlin과 연구진, 2019](https://arxiv.org/pdf/1810.04805.pdf))과 동일한 특수 토큰(예: [CLS]와 [SEP])과 템플릿을 사용하여 이전 버전과의 호환성을 유지합니다. GPU 활용도를 최적화하기 위해([Anthony와 연구진, 2024](https://arxiv.org/pdf/2401.14019.pdf); [Karpathy, 2023](https://arxiv.org/pdf/2307.09288.pdf)), 어휘 크기는 64의 배수인 50,368로 설정되었으며, 다운스트림 애플리케이션을 지원하기 위해 83개의 미사용 토큰을 포함합니다.

#### 시퀀스 패킹

학습 배치 내에서 언패딩으로 인한 미니배치 크기의 높은 분산을 피하기 위해, ModernBERT는 [Raffel과 연구진, 2020](https://arxiv.org/pdf/2005.11401.pdf)과 [Krell과 연구진, 2022](https://arxiv.org/pdf/2107.02027)가 제안한 시퀀스 패킹을 그리디 알고리즘과 함께 채택했습니다. 이 방식은 99% 이상의 시퀀스 패킹 효율성을 달성하여 배치 크기의 균일성을 보장했습니다.

시퀀스 패킹은 다양한 길이의 시퀀스를 효율적으로 처리하기 위한 기법으로, 여러 짧은 시퀀스를 하나의 긴 시퀀스로 결합하여 계산 자원을 최대한 활용합니다. 이 방법은 특히 배치 내 시퀀스 길이가 다양할 때 패딩으로 인한 계산 낭비를 줄이는 데 효과적입니다. 그리디 알고리즘은 가능한 한 많은 시퀀스를 각 배치에 채워 넣는 방식으로 작동하며, 이를 통해 높은 패킹 효율성을 달성할 수 있습니다.

### 학습 설정

#### MLM(Masked Language Modeling)

ModernBERT는 MosaicBERT([Portes와 연구진, 2023](https://arxiv.org/pdf/2307.02971.pdf))에서 사용된 마스크드 언어 모델링(MLM) 설정을 따릅니다. 성능 향상 없이 눈에 띄는 오버헤드를 발생시키는 Next-Sentence Prediction 목표를 제거했으며([Liu와 연구진, 2019a](https://arxiv.org/pdf/1907.11692); [Izsak과 연구진, 2021](https://arxiv.org/pdf/2106.15718.pdf)), 마스킹 비율은 30%를 사용했습니다. 원래 15%의 마스킹 비율이 최적이 아니라는 것이 [Wettig과 연구진, 2023](https://arxiv.org/pdf/2202.08005)의 연구에서 입증되었기 때문입니다.

마스크드 언어 모델링은 입력 텍스트의 일부 토큰을 마스킹하고 모델이 이를 예측하도록 하는 사전 학습 방법입니다. 이 방법은 양방향 컨텍스트를 활용하여 마스킹된 토큰을 예측함으로써 모델이 언어의 문맥적 이해를 개발하도록 돕습니다. 마스킹 비율을 30%로 높인 것은 모델이 더 어려운 예측 작업에 직면하게 되어 더 강력한 표현을 학습하게 된다는 연구 결과를 반영한 것입니다.

#### 옵티마이저

ModernBERT는 [Wortsman과 연구진, 2023](https://arxiv.org/pdf/2304.13013)이 제안한 StableAdamW 옵티마이저를 사용합니다. 이 옵티마이저는 AdamW([Loshchilov와 Hutter, 2019](https://arxiv.org/pdf/1711.05101))를 개선한 것으로, Adafactor 스타일([Shazeer와 Stern, 2018](https://arxiv.org/pdf/1804.04235))의 업데이트 클리핑을 파라미터별 학습률 조정으로 추가했습니다. StableAdamW의 학습률 클리핑은 다운스트림 작업에서 표준 그래디언트 클리핑보다 더 나은 성능을 보였으며 더 안정적인 학습을 가능하게 했습니다. 하이퍼파라미터 세부 사항은 부록 A에 제공되어 있습니다.

StableAdamW 옵티마이저는 다음과 같은 수식으로 표현될 수 있습니다.

$$
\begin{aligned}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \\
r_t &= \frac{|g_t|}{\sqrt{\hat{v}_t} + \epsilon} \\
\eta_t &= \eta \cdot \min(1, \frac{c}{r_t}) \\
\theta_t &= \theta_{t-1} - \eta_t \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{aligned}
$$

여기서 $g_t$는 현재 그래디언트, $m_t$와 $v_t$는 각각 그래디언트의 1차 및 2차 모멘트 추정치, $\beta_1$과 $\beta_2$는 모멘트 감쇠율, $\eta$는 기본 학습률, $c$는 클리핑 임계값, $\theta_t$는 모델 파라미터입니다. 이 방식은 그래디언트의 크기가 지나치게 클 때 학습률을 자동으로 조정하여 학습 안정성을 높입니다.

#### 학습률 스케줄

사전 학습 중에 ModernBERT는 수정된 사다리꼴 학습률(LR) 스케줄([Xing과 연구진, 2018](https://arxiv.org/pdf/1812.01187))을 사용합니다. 이는 Warmup-Stable-Decay(WSD)([Zhai과 연구진, 2022](https://arxiv.org/pdf/2203.15556); [Hu와 연구진, 2024](https://arxiv.org/pdf/2401.16505))라고도 알려져 있습니다. 짧은 LR 웜업 후, 사다리꼴 스케줄은 학습의 대부분 동안 LR을 일정하게 유지한 다음 짧은 LR 감소 단계를 거칩니다. 이 스케줄은 코사인 스케줄링([Hägele과 연구진, 2024](https://arxiv.org/pdf/2405.18392); [Hallström과 연구진, 2024](https://arxiv.org/pdf/2402.19173))과 비슷한 성능을 보이면서도 콜드 리스타트 문제 없이 어떤 체크포인트에서도 지속적인 학습을 가능하게 하는 이점이 있습니다([Ash와 Adams, 2019](https://arxiv.org/pdf/1910.08475)).

대부분의 사다리꼴 스케줄과 달리, ModernBERT는 $1-\sqrt{1-\text{sqrt}}$ LR 감소([Hägele과 연구진, 2024](https://arxiv.org/pdf/2405.18392))를 사용했는데, 이는 선형 및 코사인 감소보다 더 나은 성능을 보였습니다. ModernBERT-base는 3십억 토큰의 웜업 후 1.7조 토큰 동안 8e-4의 일정한 LR로 학습되었습니다. ModernBERT-large는 2십억 토큰의 웜업 후 900십억 토큰 동안 5e-4의 LR로 학습되었습니다. 5e-4에서 몇 백억 토큰 동안 손실이 정체된 후, 학습을 롤백하고 나머지 800십억 토큰에 대해 5e-5로 재시작했습니다.

사다리꼴 학습률 스케줄은 다음과 같이 수식화할 수 있습니다.

$$
\text{LR}(t) = 
\begin{cases}
\text{LR}_{\text{max}} \cdot \frac{t}{t_{\text{warmup}}}, & \text{if } t < t_{\text{warmup}} \\
\text{LR}_{\text{max}}, & \text{if } t_{\text{warmup}} \leq t < t_{\text{stable}} \\
\text{LR}_{\text{max}} \cdot (1 - \sqrt{1 - \frac{t - t_{\text{stable}}}{t_{\text{total}} - t_{\text{stable}}}}), & \text{if } t_{\text{stable}} \leq t \leq t_{\text{total}}
\end{cases}
$$

여기서 $t$는 현재 학습 단계, $t_{\text{warmup}}$은 웜업 단계 수, $t_{\text{stable}}$은 안정 단계의 시작, $t_{\text{total}}$은 총 학습 단계 수, $\text{LR}_{\text{max}}$는 최대 학습률입니다.

#### 배치 크기 스케줄

배치 크기 스케줄링은 작은 그래디언트 누적 배치로 시작하여 시간이 지남에 따라 전체 배치 크기로 증가합니다. 실험에서 이 스케줄은 학습 진행 속도를 가속화했습니다. ModernBERT-base의 경우 배치 크기를 50십억 토큰에 걸쳐 768에서 4,608로 증가시켰고, ModernBERT-large의 경우 10십억 토큰에 걸쳐 448에서 4,928로 증가시켰습니다. 이때 각 배치 크기가 동일한 수의 업데이트 단계를 갖도록 불균등한 토큰 스케줄을 사용했습니다. 자세한 내용은 부록 A.1에 제공되어 있습니다.

배치 크기 스케줄링은 학습 초기에 더 작은 배치 크기를 사용하여 더 빠른 수렴을 촉진하고, 점차 배치 크기를 늘려 더 안정적인 그래디언트 추정과 병렬 처리의 이점을 활용하는 전략입니다. 이 접근법은 학습 초기의 빠른 진행과 후기의 안정성 사이의 균형을 맞추는 데 도움이 됩니다.

#### 가중치 초기화 및 타일링

ModernBERT-base는 Megatron 초기화([Shoeybi와 연구진, 2019](https://arxiv.org/pdf/1909.08053))를 따라 무작위 가중치로 초기화되었습니다. ModernBERT-large의 경우, Phi 모델 계열([Li와 연구진, 2023](https://arxiv.org/pdf/2306.11644); [Javaheripi와 연구진, 2023](https://arxiv.org/pdf/2309.05463))의 방식을 따라 ModernBERT-base의 가중치로 초기화했습니다. 이 방식은 실험에서 일관되게 Phi의 개선된 학습 결과와 일치했으며, 모델 학습의 초기 손실 감소 속도를 크게 향상시켰습니다. 이 초기화 방법은 ModernBERT-large에 필요한 배치 크기와 LR 웜업의 양을 줄였습니다. 자세한 내용은 부록 A.2에 제공되어 있습니다.

Megatron 초기화는 트랜스포머 모델의 가중치를 초기화하는 방법으로, 특히 대규모 모델에서 학습 안정성을 향상시키기 위해 설계되었습니다. 이 방법은 각 레이어의 가중치를 적절하게 스케일링하여 신호가 네트워크를 통과할 때 분산이 유지되도록 합니다. 작은 모델의 가중치를 사용하여 더 큰 모델을 초기화하는 방식은 전이 학습의 한 형태로, 작은 모델에서 학습된 패턴을 더 큰 모델의 출발점으로 활용합니다.

#### 컨텍스트 길이 확장

1,024 시퀀스 길이와 10,000의 RoPE theta로 1.7조 토큰을 학습한 후, ModernBERT의 네이티브 컨텍스트 길이를 8,192 토큰으로 확장했습니다. 이를 위해 전역 어텐션 레이어의 RoPE theta를 160,000으로 증가시키고 추가로 300십억 토큰을 학습했습니다. 먼저 [Fu와 연구진, 2024](https://arxiv.org/pdf/2402.10171)의 방식을 따라 원래 사전 학습 데이터셋에서 샘플링한 8,192 토큰 혼합물에 대해 250십억 토큰 동안 3e-4의 일정한 낮은 학습률로 학습했습니다. 다음으로, [Gao와 연구진, 2024](https://arxiv.org/pdf/2410.02660)의 방식을 따라 고품질 소스를 업샘플링하고 50십억 토큰에 걸쳐 $1-\sqrt{1-\text{sqrt}}$ LR 스케줄로 감소 단계를 수행했습니다. 이 컨텍스트 확장 과정은 다운스트림 작업에서 가장 균형 잡힌 모델을 제공했습니다. 이러한 전략 중 하나만 사용한 대부분의 실험은 검색 또는 분류 작업 중 하나에서 성능 손실을 초래했습니다.

컨텍스트 길이 확장은 모델이 더 긴 시퀀스를 효과적으로 처리할 수 있도록 하는 중요한 기능입니다. RoPE(Rotary Position Embedding)의 theta 값을 증가시키는 것은 위치 인코딩의 주기를 늘려 모델이 더 긴 시퀀스에서도 위치 정보를 효과적으로 인코딩할 수 있게 합니다. 이 과정에서 고품질 데이터 소스를 업샘플링하고 적절한 학습률 스케줄을 사용하는 것은 모델이 긴 컨텍스트 능력을 개발하면서도 기존의 성능을 유지하는 데 중요합니다.

### 다운스트림 평가

ModernBERT는 다양한 작업에 걸쳐 광범위한 평가를 수행하여 일반적인 시나리오에서의 다양성을 입증하고자 했습니다. 모든 작업에서 ModernBERT는 유사한 크기의 기존 인코더 모델들과 비교 평가되었습니다. 베이스 크기(일반적으로 1억 5천만 개 미만의 파라미터로 정의됨)에는 BERT-base([Devlin과 연구진, 2019](https://huggingface.co/google-bert/bert-base-uncased)), DeBERTa-v3-base([He과 연구진, 2023](https://huggingface.co/microsoft/deberta-v3-base)), RoBERTa-base([Liu과 연구진, 2019a](https://huggingface.co/FacebookAI/roberta-base)), 그리고 최근의 8192 컨텍스트를 지원하는 NomicBERT([Nussbaum과 연구진, 2024](https://huggingface.co/nomic-ai/NomicBERT-2048))와 GTE-en-MLM-base([Zhang과 연구진, 2024](https://huggingface.co/Alibaba-NLP/GTE-en-MLM-base))가 포함됩니다. 라지 크기(일반적으로 3억 개 이상 5억 개 미만의 파라미터로 정의됨)에는 BERT-large-uncased([Devlin과 연구진, 2019](https://huggingface.co/google-bert/bert-large-uncased)), DeBERTa-v3-large([He과 연구진, 2023](https://huggingface.co/microsoft/deberta-v3-large)), RoBERTa-large([Liu과 연구진, 2019a](https://huggingface.co/FacebookAI/roberta-large)), GTE-en-MLM-large([Zhang과 연구진, 2024](https://huggingface.co/Alibaba-NLP/GTE-en-MLM-large))가 포함됩니다.

### 평가 설정

#### 자연어 이해

일반 언어 이해 평가(GLUE) 벤치마크([Wang과 연구진, 2018](https://arxiv.org/pdf/1804.07461v3))는 인코더 모델을 위한 표준 자연어 이해(NLU) 벤치마크로, 모델이 감정 탐지([Liu과 연구진, 2019b](https://arxiv.org/pdf/1905.00537))나 MNLI([Williams과 연구진, 2018](https://arxiv.org/pdf/1704.05426))와 같은 언어 함의 작업 등 다양한 문장 또는 문장 쌍 이해 작업에서 얼마나 잘 수행하는지 측정하는 것을 목표로 합니다. GLUE는 대규모 언어 모델([Zhao과 연구진, 2023](https://arxiv.org/pdf/2302.13971.pdf))과 같은 최고 성능의 모델들에 의해 포화된 것으로 간주되지만, 여전히 작은 인코더 기반 모델에 가장 일반적으로 사용되는 평가 도구 중 하나이며, 일반적인 분류 작업에서 모델의 성능에 대한 좋은 인상을 제공합니다([Portes과 연구진, 2023](https://arxiv.org/pdf/2307.02971.pdf); [Zhang과 연구진, 2024](https://arxiv.org/pdf/2402.03216.pdf); [He과 연구진, 2023](https://arxiv.org/pdf/2111.09543v4)).

연구진은 이전 연구들([Devlin과 연구진, 2019](https://arxiv.org/pdf/1810.04805v2); [Liu과 연구진, 2019a](https://arxiv.org/pdf/1907.11692); [He과 연구진, 2023](https://arxiv.org/pdf/2111.09543v4))의 관행을 따라 각 GLUE 하위 집합에 대해 하이퍼파라미터 검색을 수행하여(부록 E.1에 자세히 설명됨) 다른 모델과 비교 가능한 값을 제공했습니다. [Zhang과 연구진(2024)](https://arxiv.org/pdf/2402.03216.pdf)은 파라미터 스윕을 명시적으로 언급하지 않았기 때문에, 연구진은 처음에는 ModernBERT와 동일한 하이퍼파라미터 스윕을 실행했지만 결과에 일관성이 없음을 발견했습니다. GTE-en-MLM의 능력을 과소평가하지 않기 위해, 그들이 보고한 GLUE 결과를 사용하기로 결정했습니다.

#### 텍스트 검색

정보 검색(IR)은 인코더 전용 모델의 가장 일반적인 응용 분야 중 하나입니다. 실제로, 허깅페이스 모델 허브에서 가장 많이 다운로드된 100개 모델 중 절반 이상이 인코더 기반 검색 모델입니다. 이 분야는 최근 대규모 언어 모델(LLM)의 확산에 따라 상당한 성장과 관심을 보였으며, 경량 모델에 의해 구동되는 시맨틱 검색은 검색 증강 생성(RAG) 파이프라인의 일부로 LLM에 관련 컨텍스트를 제공하는 데 사용됩니다.

연구진은 단일 벡터 밀집 패시지 검색(DPR)([Karpukhin과 연구진, 2020](https://arxiv.org/pdf/2004.04906v3)) 설정과 다중 벡터 ColBERT([Khattab과 Zaharia, 2020](https://arxiv.org/pdf/2004.12832v2)) 설정 모두에서 모델을 평가했습니다. 인기 있는 BEIR 평가 스위트([Thakur과 연구진, 2021](https://arxiv.org/pdf/2104.08663))에서 nDCG@10 메트릭을 사용하여 검색 결과를 보고했습니다. 이는 다양한 작업과 도메인에 걸쳐 검색 성능을 평가하는 일반적인 표준입니다. 아래에 설명된 각 설정에 대해, BEIR 벤치마크의 하위 집합에 대한 결과를 기반으로 최종 모델을 선택하기 위한 학습률 스윕을 수행했으며, 자세한 내용은 부록 E.2에 제공되어 있습니다.

##### 단일 벡터 검색

인코더를 사용한 신경망 검색의 가장 일반적인 접근법 중 하나는 DPR([Karpukhin과 연구진, 2020](https://arxiv.org/pdf/2004.04906v3))로, 단일 벡터가 전체 문서를 표현하는 데 사용됩니다. 쿼리와 문서 간의 유사성은 코사인 유사도와 같은 거리 연산을 통해 계산할 수 있습니다. 모델은 문서가 쿼리와 관련이 있으면 표현이 가깝고, 그렇지 않으면 멀어지도록 대조 학습을 사용하여 미세 조정됩니다([van den Oord과 연구진, 2018](https://arxiv.org/pdf/1807.03748)).

연구진은 모든 베이스 모델을 MS-MARCO([Bajaj과 연구진, 2016](https://arxiv.org/pdf/1611.09268)) 데이터셋과 마이닝된 하드 네거티브([Xuan과 연구진, 2020](https://arxiv.org/pdf/2010.02666))를 사용하여 125만 개의 샘플에 대해 배치 크기 16과 5%의 학습 웜업을 적용하여 sentence-transformers([Reimers와 Gurevych, 2019](https://arxiv.org/pdf/1908.10084))를 사용하여 훈련했습니다.

##### 다중 벡터 검색

ColBERT([Khattab과 Zaharia, 2020](https://arxiv.org/pdf/2004.12832v2))가 주도하는 다중 벡터 검색은 전체 시퀀스를 단일 벡터로 압축할 때 손실되는 정보를 완화하고자 합니다. 다중 벡터 검색에서는 각 문서가 개별 토큰 벡터 모두로 표현되며, 쿼리와 문서 간의 유사성은 MaxSim 연산자를 사용하여 계산됩니다. 이는 모든 쿼리 토큰에 대해 가장 유사한 문서 토큰과의 유사성 합계를 계산합니다.

연구진은 JaColBERTv2.5([Clavié, 2024](https://arxiv.org/pdf/2312.08871))의 훈련 설정을 채택했습니다. 이는 ColBERTv2([Santhanam과 연구진, 2022](https://arxiv.org/pdf/2112.01488)) 훈련 절차의 업데이트 버전으로, 배치 크기 16과 5%의 학습률 웜업을 사용합니다. 모든 모델은 정규화된 교사와 학생 점수 사이의 KL-다이버전스를 사용하여 교사 모델의 지식을 증류함으로써 훈련되었습니다. 모델들은 MS-Marco([Bajaj과 연구진, 2016](https://arxiv.org/pdf/1611.09268))에서 81만 개의 샘플과 BGE-M3([Chen과 연구진, 2024](https://arxiv.org/pdf/2402.03216))의 교사 점수를 사용하여 PyLate([Chaffin과 Sourty, 2024](https://arxiv.org/pdf/2312.08871)) 라이브러리를 통해 훈련되었습니다.

| 모델 | IR (DPR) | IR (ColBERT) | NLU | 코드 |
|:-----|:--------:|:------------:|:---:|:----:|
|      | BEIR | MLDR OOD | MLDR ID | BEIR | MLDR OOD | GLUE | CSN | SQA |
| **베이스** |
| BERT | 38.9 | 23.9 | 32.2 | 49.0 | 28.1 | 84.7 | 41.2 | 59.5 |
| RoBERTa | 37.7 | 22.9 | 32.8 | 48.7 | 28.2 | 86.4 | 44.3 | 59.6 |
| DeBERTaV3 | 20.2 | 5.4 | 13.4 | 47.1 | 21.9 | 88.1 | 17.5 | 18.6 |
| NomicBERT | 41.0 | 26.7 | 30.3 | 49.9 | 61.3 | 84.0 | 41.6 | 61.4 |
| GTE-en-MLM | 41.4 | 34.3 | 44.4 | 48.2 | 69.3 | 85.6 | 44.9 | 71.4 |
| ModernBERT | 41.6 | 27.4 | 44.0 | 51.3 | 80.2 | 88.4 | 56.4 | 73.6 |
| **라지** |
| BERT | 38.9 | 23.3 | 31.7 | 49.5 | 28.5 | 85.2 | 41.6 | 60.8 |
| RoBERTa | 41.4 | 22.6 | 36.1 | 49.8 | 28.8 | 88.9 | 47.3 | 68.1 |
| DeBERTaV3 | 25.6 | 7.1 | 19.2 | 46.7 | 23.0 | 91.4 | 21.2 | 19.7 |
| GTE-en-MLM | 42.5 | 36.4 | 48.9 | 50.7 | 71.3 | 87.6 | 40.5 | 66.9 |
| ModernBERT | 44.0 | 34.3 | 48.6 | 52.4 | 80.4 | 90.4 | 59.5 | 83.9 |

표 1: 모든 작업에 걸친 모든 모델의 결과 개요. CSN은 CodeSearchNet을 나타내고 SQA는 StackQA를 나타냅니다. MLDR ID는 도메인 내(훈련 세트에서 미세 조정됨) 평가를, MLDR OOD는 도메인 외 평가를 나타냅니다.

#### 긴 컨텍스트 텍스트 검색

8192 컨텍스트 길이를 네이티브로 지원하는 ModernBERT는 대부분의 기존 인코더보다 긴 컨텍스트 성능을 향상시킵니다. 그러나 인코더 전용 모델을 위한 표준화된 긴 컨텍스트 벤치마크는 상대적으로 적으며, Needle-in-a-haystack([Kamradt, 2023](https://arxiv.org/pdf/2307.03172))과 RULER([Hsieh과 연구진, 2024](https://arxiv.org/pdf/2401.00368))와 같은 대부분의 벤치마크는 생성적 작업에 맞춰져 있습니다. 이러한 제한을 고려하여, 연구진은 MLDR([Chen과 연구진, 2024](https://arxiv.org/pdf/2402.03216))의 영어 하위 집합에서 향상된 긴 컨텍스트 성능을 입증했습니다. 이는 20만 개 이상의 긴 문서로 구성된 긴 컨텍스트 검색 벤치마크입니다. 세 가지 설정을 평가했습니다.

##### 단일 벡터 - 도메인 외(Out-Of-Domain)
모델은 위에서 설명한 대로 짧은 컨텍스트 MS-MARCO에서 훈련되고, 추가 미세 조정 없이 긴 컨텍스트 MLDR에서 평가됩니다.

##### 단일 벡터 - 도메인 내(In Domain)
MS-MARCO에서 훈련된 모델은 평가 전에 긴 컨텍스트 MLDR 훈련 세트에서 추가로 미세 조정됩니다.

##### 다중 벡터 - 도메인 외(Out-Of-Domain)
토큰 수준 MaxSim 메커니즘 덕분에, ColBERT 모델은 특별한 훈련 없이도 긴 컨텍스트로 일반화할 수 있습니다([Bergum, 2024](https://arxiv.org/pdf/2307.10982)). 연구진은 섹션 3.1.2에서 최상의 체크포인트를 추가 미세 조정 없이 MLDR에서 직접 평가했습니다.

#### 코드 검색

코드 완성 모델의 성능이 점점 좋아짐에 따라([Jiang과 연구진, 2024a](https://arxiv.org/pdf/2401.14196)), 코드 어시스턴트의 등장 이후 다운스트림 애플리케이션이 빠르게 성장했습니다. 인코더 전용 모델은 리소스 제약 하에서 코드 관련 정보의 대량을 처리하고 검색하는 데 사용되어, 인코더 모델의 코드 기능을 측정하고 개선하는 것이 중요해졌습니다([Li과 연구진, 2024](https://arxiv.org/pdf/2402.05319)).

대부분의 이전 인코더가 주로 텍스트 데이터에만 훈련된 것과 달리([Devlin과 연구진, 2019](https://arxiv.org/pdf/1810.04805v2); [Liu과 연구진, 2019a](https://arxiv.org/pdf/1907.11692); [Portes과 연구진, 2023](https://arxiv.org/pdf/2307.02971.pdf); [Zhang과 연구진, 2024](https://arxiv.org/pdf/2402.03216.pdf); [Nussbaum과 연구진, 2024](https://arxiv.org/pdf/2402.01613v2)), ModernBERT는 코드에 대해 사전 훈련되었으며 코드 인식 토크나이저를 사용합니다. 이는 T5([Raffel과 연구진, 2020](https://arxiv.org/pdf/2005.11401.pdf))에서 볼 수 있는 것과 같은 문제, 즉 어휘에 중괄호가 포함되지 않는 문제를 피합니다.

프로그래밍 관련 성능을 측정하기 위해, 연구진은 모든 모델을 CodeSearchNet([Husain과 연구진, 2019](https://arxiv.org/pdf/1909.09436))에서 평가했습니다. 이는 모델이 코드 블록에 대한 관련 독스트링이나 주석을 식별해야 하는 코드-텍스트 벤치마크입니다. 또한 StackOverflow-QA([Li과 연구진, 2024](https://arxiv.org/pdf/2402.05319))에서도 평가했는데, 여기서 모델은 StackOverflow 질문에 대한 관련 응답을 식별해야 하며, 문서에는 텍스트와 코드가 모두 포함된 "하이브리드" 설정입니다. 후자의 벤치마크는 또한 긴 컨텍스트 기능을 활용하는데, 쿼리와 문서는 각각 평균 1,400단어와 1,200단어를 포함하여 평균 토큰 수가 2000개 이상입니다.

이러한 벤치마크는 CoIR(CodeIR) 프레임워크([Li과 연구진, 2024](https://arxiv.org/pdf/2402.05319))를 사용하여 단일 벡터 검색 작업으로 평가되었습니다. 모든 모델은 섹션 3.1.2에서 식별된 최상의 하이퍼파라미터를 재사용하여 훈련되었습니다.

| 모델 | 짧은 컨텍스트 | 긴 컨텍스트 |
|:-----|:------------:|:----------:|
|      | BS | 고정 | 가변 | BS | 고정 | 가변 |
| **베이스** |
| BERT | 1096 | 180.4 | 90.2 | - | - | - |
| RoBERTa | 664 | 179.9 | 89.9 | - | - | - |
| DeBERTaV3 | 236 | 70.2 | 35.1 | - | - | - |
| NomicBERT | 588 | 117.1 | 58.5 | 36 | 46.1 | 23.1 |
| GTE-en-MLM | 640 | 123.7 | 61.8 | 38 | 46.8 | 23.4 |
| GTE-en-MLMxformers | 640 | 122.5 | 128.6 | 38 | 47.5 | 67.3 |
| ModernBERT | 1604 | 148.1 | 147.3 | 98 | 123.7 | 133.8 |
| **라지** |
| BERT | 792 | 54.4 | 27.2 | - | - | - |
| RoBERTa | 460 | 42.0 | 21.0 | - | - | - |
| DeBERTaV3 | 134 | 24.6 | 12.3 | - | - | - |
| GTE-en-MLM | 472 | 38.7 | 19.3 | 28 | 16.2 | 8.1 |
| GTE-en-MLMxformers | 472 | 38.5 | 40.4 | 28 | 16.5 | 22.8 |
| ModernBERT | 770 | 52.3 | 52.9 | 48 | 46.8 | 49.8 |

표 2: NVIDIA RTX 4090에서 10회 실행 평균한 메모리(최대 배치 크기, BS)와 추론(초당 수천 개의 토큰) 효율성 결과. 대시는 지원되지 않는 구성을 나타냅니다.

### 다운스트림 결과 및 논의

모든 평가에 대한 종합 결과는 표 1에 제시되어 있습니다. BEIR과 GLUE, 두 가지 일반적인 평가 스위트의 경우, 연구진은 기존 관행에 따라 평균 결과를 보고했습니다. 자세한 결과는 부록 E에 제공되어 있습니다.

다운스트림 성능 측면에서, ModernBERT는 베이스와 라지 모델 크기 모두에서 전반적으로 가장 강력한 모델입니다. ModernBERT는 모든 평가 카테고리에서 더 나은 성능을 보이며 원래의 BERT와 RoBERTA 모델에 비해 파레토 개선을 나타냅니다.

#### 짧은 컨텍스트 검색

BEIR에서, ModernBERT의 두 변형 모두 DPR과 ColBERT 설정 모두에서 검색을 위한 더 나은 백본으로 설계된 최근의 GTE-en-MLM과 NomicBERT를 포함한 기존 인코더보다 우수한 성능을 보였습니다([Zhang과 연구진, 2024](https://arxiv.org/pdf/2402.03216.pdf); [Nussbaum과 연구진, 2024](https://arxiv.org/pdf/2402.01613v2)). ModernBERT-base는 DPR 평가에서 GTE-en-MLM-base보다 근소하게 앞서지만, ModernBERT-large는 GTE-en-MLM-large의 4억 3500만 개에 비해 상대적으로 적은 3억 9500만 개의 파라미터를 가짐에도 불구하고 그 격차를 더욱 벌립니다.

#### 긴 컨텍스트 검색 - 단일 벡터

DPR 설정에서, ModernBERT는 긴 컨텍스트 텍스트 검색 작업인 MLDR에서 인상적인 성능을 달성했습니다. 그러나 이러한 결과는 또한 흥미로운 현상을 강조합니다. 긴 컨텍스트 미세 조정 없이 ModernBERT는 짧은 컨텍스트 모델과 긴 컨텍스트 NomicBERT보다 우수한 성능을 보이지만 GTE-en-MLM보다는 눈에 띄게 성능이 떨어집니다. 도메인 내에서 평가할 때 성능 격차가 상당히 좁아지며, 두 모델 모두 유사한 성능을 보입니다. 이는 ModernBERT가 밀집 인코더로서 긴 컨텍스트 시퀀스를 효과적으로 처리할 수 있지만 더 적응된 튜닝이 필요할 수 있음을 시사합니다. 연구진은 향후 연구에서 이 현상에 대한 여러 가능한 설명, 예를 들어 로컬 어텐션의 영향이나 GTE-en-MLM이 더 긴 시퀀스 길이에 사전 훈련 계산 예산의 더 큰 부분을 할당했을 가능성([Zhang과 연구진, 2024](https://arxiv.org/pdf/2402.03216.pdf)) 등을 탐구할 계획입니다.

#### 긴 컨텍스트 검색 - 다중 벡터

ColBERT 설정에서, 긴 컨텍스트 모델(GTE-en-MLM, NomicBERT, ModernBERT)은 모두 특별한 미세 조정 없이도 짧은 컨텍스트 모델보다 최소 40 NDCG@10 포인트 이상 우수한 성능을 보였습니다. 이러한 결과는 [Bergum(2024)](https://arxiv.org/pdf/2307.10982)의 연구 결과를 확인하는데, ColBERT 모델이 긴 컨텍스트 검색 작업에 특히 적합하다는 것을 보여주었습니다. 긴 컨텍스트 모델 중에서, ModernBERT는 두 모델 크기 모두에서 다른 긴 컨텍스트 모델보다 최소 9 NDCG@10 포인트 이상의 우위를 보이며 우수한 성능을 보였습니다. 연구진은 이러한 상당한 이득이 긴 사전 훈련을 통해 토큰이 거의 또는 전혀 과소 훈련되지 않도록 보장하는 것과, ColBERT 스타일 검색과 로컬 어텐션 사이의 잠재적인 시너지 효과로 설명될 수 있다고 추측하지만, 이 현상에 대한 추가 탐구는 향후 연구로 남겨둡니다.

#### 자연어 이해

두 ModernBERT 모델 모두 GLUE로 측정된 뛰어난 NLU 결과를 보여주었습니다. ModernBERT-base는 DeBERTaV3-base를 포함한 모든 기존 베이스 모델을 능가하여, MLM으로 훈련된 모델 중 최초로 이를 달성했습니다. 이는 놀라운 결과인데, DeBERTaV3는 이전에 더 강력한 다운스트림 NLU 성능을 제공하는 것으로 생각되었던 Replaced-Token-Detection 목표로 훈련되었기 때문입니다([Clark과 연구진, 2020](https://arxiv.org/pdf/2003.10555); [He과 연구진, 2023](https://arxiv.org/pdf/2111.09543v4)). ModernBERT-large는 GLUE에서 두 번째로 우수한 라지 인코더로, 10분의 1 적은 파라미터로 DeBERTaV3-large와 거의 일치하면서도 토큰을 두 배 빠르게 처리합니다(섹션 4 참조).

#### 코드

프로그래밍 작업에서, 코드-텍스트(CodeSearchNet)와 더 긴 컨텍스트 하이브리드 설정(StackQA) 모두에서 ModernBERT는 다른 모든 모델보다 우수한 성능을 보였습니다. 이 결과는 예상된 것이었는데, 프로그래밍 데이터를 포함하는 데이터 혼합에서 훈련된 유일한 평가된 인코더이기 때문입니다. 이러한 결과와 다른 작업에서 ModernBERT의 강력한 성과를 결합하면, ModernBERT가 자연 텍스트를 처리하는 능력을 손상시키지 않으면서 코드에 대한 이해가 향상되었음을 나타냅니다.

### 효율성

#### 평가 설정

다양한 시퀀스 길이에서의 추론 효율성을 측정하기 위해, 연구진은 8,192개의 문서로 구성된 4개의 합성 데이터셋을 생성했습니다. 많은 일반적인 벤치마크는 짧고 균일한 시퀀스 길이에 편향되어 있어 실제 상황을 제대로 반영하지 못하는 경우가 많습니다.

첫 두 문서 세트는 고정 길이로 구성되었습니다. '고정 짧은 컨텍스트(fixed short-context)' 세트에서는 모든 문서가 512개의 토큰을 포함하고, '고정 긴 컨텍스트(fixed long-context)' 세트에서는 모든 문서가 8,192개의 토큰을 포함합니다. 512는 대부분의 기존 인코더의 최대 길이이며, 8,192는 모든 긴 컨텍스트 인코더의 최대 길이입니다.

언패딩(unpadding)의 영향을 고려하기 위해, 연구진은 또한 두 가지 가변 길이 문서 세트를 생성했습니다. 이 세트들에서 각 문서의 토큰 수는 최대 시퀀스 길이의 절반인 256과 4,096 토큰을 중심으로 하는 정규 분포에 따라 정의되었습니다. 데이터 통계에 대한 자세한 내용은 부록 F에 제공되어 있습니다.

연구진은 모든 모델을 초당 처리할 수 있는 토큰 수를 기준으로 평가했으며, 이는 10회 실행의 평균값입니다. 모든 효율성 평가는 섹션 2.1.3에서 설명한 ModernBERT의 대상 GPU 중 하나인 NVIDIA RTX 4090 한 대에서 실행되었습니다.

GTE-en-MLM 모델은 두 가지 설정에서 평가되었습니다. 기본 설정(out-of-the-box)과 언패딩과 같은 효율성 향상 기능을 제공하는 xformers 라이브러리([Lefaudeux와 연구진, 2022](https://arxiv.org/pdf/2107.02027))를 사용한 설정입니다.

#### 결과

모든 초당 토큰 처리량 효율성 결과는 표 2에 제시되어 있으며, 절대적인 실행 시간은 부록 F에 제공되어 있습니다.

ModernBERT는 전반적으로 가장 효율적인 모델로 두드러집니다. 짧은 컨텍스트에서는 고정 길이 512 토큰 입력을 다른 모든 최신 인코더보다 빠르게 처리하지만, 원래의 BERT와 RoBERTa 모델보다는 느립니다. 이는 부분적으로 BERT와 RoBERTa가 최근 인코더들에 비해 상대적으로 적은 파라미터 수를 가지고 있기 때문입니다.

긴 컨텍스트에서 ModernBERT는 모든 경쟁 인코더보다 빠르며, 베이스 및 라지 크기에서 각각 다음으로 빠른 인코더보다 2.65배 및 3배 더 빠르게 문서를 처리합니다. 8,192 길이에서 ModernBERT-large의 처리 속도(초당 46,801 토큰)는 GTE-en-MLM-large(초당 16,532 토큰)보다 GTE-en-MLM-base(초당 47,507 토큰)에 더 가깝습니다.

가변 길이 입력에서는 GTE-en-MLM과 ModernBERT 모델 모두 주로 언패딩 덕분에 다른 모든 모델보다 상당히 빠릅니다. 그러나 ModernBERT는 지역 어텐션(local attention)을 사용하기 때문에 GTE-en-MLM보다 눈에 띄게 더 효율적이며, 짧은 컨텍스트 길이에서는 14.5-30.9% 더 많은 토큰을, 긴 컨텍스트 길이에서는 98.8-118.8% 더 많은 토큰을 초당 처리합니다.

ModernBERT는 두 모델 크기 모두에서 전반적으로 가장 메모리 효율적인 모델입니다. ModernBERT-base는 두 입력 길이 모두에서 다른 모든 모델보다 두 배 큰 배치 크기를 처리할 수 있습니다. ModernBERT-large는 짧은 컨텍스트 입력에서 원래의 BERT-large보다 약간 덜 메모리 효율적이지만, 다른 모든 라지 모델보다 최소 60% 더 큰 배치를 처리할 수 있습니다.

효율성 평가를 위해 연구진은 다양한 시퀀스 길이에서 모델의 성능을 측정했습니다. 이를 위해 고정 길이와 가변 길이의 합성 데이터셋을 생성하여 실제 상황을 더 잘 반영하고자 했습니다. 특히 가변 길이 데이터셋은 언패딩의 효과를 평가하는 데 중요했습니다.

언패딩은 배치 내에서 패딩 토큰을 제거하고 모든 시퀀스를 하나의 시퀀스로 연결하여 처리함으로써 계산 효율성을 높이는 기술입니다. ModernBERT는 Flash Attention의 가변 길이 어텐션과 RoPE 구현을 활용하여 들쭉날쭉한 어텐션 마스크와 하나의 언패드된 시퀀스에 대한 RoPE 적용을 가능하게 했습니다. 이러한 접근 방식은 다른 언패딩 방법보다 10-20% 성능 향상을 제공했습니다.

표 2에서 볼 수 있듯이, ModernBERT는 특히 긴 컨텍스트 처리에서 뛰어난 효율성을 보여줍니다. 8,192 토큰 길이에서 ModernBERT-large는 GTE-en-MLM-large보다 거의 3배 빠르게 처리할 수 있으며, 이는 지역 어텐션의 사용과 효율적인 언패딩 구현 덕분입니다. 또한 ModernBERT의 메모리 효율성은 더 큰 배치 크기를 처리할 수 있게 하여 실제 애플리케이션에서 처리량을 더욱 향상시킬 수 있습니다.

효율성 평가에서 사용된 NVIDIA RTX 4090은 소비자용 GPU로, ModernBERT가 고가의 데이터센터 GPU뿐만 아니라 더 접근하기 쉬운 하드웨어에서도 효율적으로 작동할 수 있음을 보여줍니다. 이는 ModernBERT가 다양한 하드웨어 환경에서 실용적으로 사용될 수 있음을 시사합니다.

가변 길이 입력에서의 성능 향상은 특히 주목할 만합니다. 실제 애플리케이션에서는 입력 길이가 크게 다양할 수 있으므로, 언패딩과 같은 기술을 통한 효율성 향상은 실제 사용 시나리오에서 상당한 이점을 제공할 수 있습니다. ModernBERT의 지역 어텐션과 언패딩의 조합은 이러한 시나리오에서 특히 효과적임이 입증되었습니다.

## 결론

ModernBERT는 기존 인코더 모델에 비해 다양한 분류 및 검색 작업에서 새로운 최첨단 성능을 달성하는 오픈 인코더 전용 모델 제품군을 제시합니다. 이 연구는 인코더 모델이 최근의 대규모 사전 학습 데이터와 자기회귀 대규모 언어 모델(LLM)에서 발전된 아키텍처 개선 사항 모두에서 이점을 얻는다는 것을 보여줍니다. ModernBERT는 8,192 토큰의 네이티브 시퀀스 길이를 가지며, GeGLU 레이어, RoPE 위치 임베딩, 그리고 교차 로컬-글로벌 어텐션과 같은 최신 아키텍처 개선 사항을 통합했습니다. ModernBERT는 전체 모델 언패딩을 특징으로 하는 최초의 오픈 모델이며, 추론 효율성을 극대화하기 위해 하드웨어를 고려하여 설계된 최초의 인코더입니다.

ModernBERT는 광범위한 벤치마크에서 인코더의 최신 기술 수준을 향상시킵니다. GLUE에서 ModernBERT-base는 2021년 출시 이후 처음으로 DeBERTaV3-base를 능가하는 최초의 인코더입니다. ModernBERT는 코드 및 ColBERT 스타일의 긴 컨텍스트 검색 벤치마크에서 가장 가까운 모델보다 각각 최소 6.85 및 9.1 퍼센트 포인트 높은 점수를 기록하면서도, 단일 및 다중 벡터 설정 모두에서 짧은 컨텍스트 검색에서도 최고 성능을 유지합니다. 동시에 ModernBERT는 DeBERTaV3보다 짧은 컨텍스트 입력을 두 배 빠르게 처리하고 긴 컨텍스트 입력을 다음으로 빠른 모델보다 두 배 빠르게 처리하면서도 최고 수준의 메모리 효율성을 제공합니다.

ModernBERT는 원래의 인코더 모델에 비해 세대적 도약을 이루었으며, 분류 및 검색 작업 모두에서 BERT와 RoBERTa에 비해 주목할 만한 성능 향상을 보여줍니다. ModernBERT는 긴 컨텍스트 및 프로그래밍 애플리케이션을 지원하는 몇 안 되는 인코더 중 하나이면서도, 동시에 인코더 추론 효율성에서 새로운 기록을 세웠습니다.

## 한계점

### 언어
이 연구는 영어 언어에만 독점적으로 초점을 맞추고 매우 많은 수의 토큰에 대해 학습합니다. 따라서 이 연구의 주요 한계점은 다른 언어에 직접 적용할 수 없으며, 잠재적으로 자원이 적은 언어에는 더욱 적용하기 어렵다는 점입니다.

### 편향
이 모델은 주로 웹 데이터에서 학습되었으므로, 모든 표현은 그러한 데이터에 존재하는 편향의 영향을 받습니다.

### 유해 콘텐츠 생성
MLM(Masked Language Model) 목표는 모델에 [MASK] 토큰을 대체할 특정 토큰을 제안함으로써 텍스트를 생성하는 일부 능력을 부여합니다([Samuel(2024)](https://arxiv.org/abs/2406.04823)). 이로 인해 유해한 콘텐츠가 생성될 수 있습니다. 그러나 ModernBERT는 주로 생성 모델이 아니며, 따라서 더 긴 텍스트 시퀀스를 생성하도록 학습되지 않았고 생성할 수도 없습니다. 결과적으로, 어떤 종류의 유해한 콘텐츠를 생성할 위험이 상당히 낮습니다.

### MLM 전용 목표
분류 작업에서는 DeBERTav3의 강력한 결과와 검색에서의 약한 결과를 고려할 때, MLM과 RTD(Replaced Token Detection)를 모두 활용하는 학습이 분류에서 최상의 결과를 달성하는 데 더 적합할 수 있습니다. 따라서 RTD로 연구를 확장하는 것은 유망한 연구 방향입니다.

### 스케일링
아키텍처 수정 외에도, 이 연구의 핵심 측면은 데이터 스케일링입니다. 그러나 모델 매개변수 측면에서의 다른 스케일링 축은 아직 탐구되지 않았습니다.

### 부록 A 학습 설정

부록 A에서는 ModernBERT 모델의 상세한 학습 설정에 대해 설명합니다. 표 3(#A1.T3)에서 자세한 학습 설정을 확인할 수 있습니다. 학습 과정에서 MNLI를 실시간 평가로 사용했으며, 소스 데이터셋에서 무작위로 샘플링한 5억 개의 시퀀스에 대한 검증 손실과 토큰 정확도 메트릭을 함께 활용했습니다. 학습 프레임워크로는 Composer([Mosaic ML Team, 2021](https://github.com/composer/composer))를 사용했으며, 옵티마이저 구현에는 optimī([Warner, 2023](https://github.com/search?q=optimi&type=repositories))를 활용했습니다.

표 3은 ModernBERT의 학습 설정을 상세히 보여줍니다. 사전 학습 단계는 컨텍스트 확장 1단계와 컨텍스트 확장 2단계로 구분됩니다. 각 단계별로 Base와 Large 모델에 대한 설정이 제시되어 있습니다.

**표 3: ModernBERT 학습 설정**

| 사전 학습 단계 | 컨텍스트 확장: 1단계 | 컨텍스트 확장: 2단계 |
|:--------------|:------------------:|:------------------:|
|               | Base | Large | Base | Large | Base | Large |
| 학습 토큰 수   | 1.719조 | | 2500억 | | 500억 | |
| 최대 시퀀스 길이 | 1,024 | | 8,192 | | 8,192 | |
| 배치 크기      | 4,608 | 4,928 | 72 | 77 | 72 | 78 |
| 웜업 (토큰)    | 500억 | 100억 | - | - | - | - |
| 마이크로배치 크기 | 96 | 56 | 12 | 7 | 12 | 6 |
| 학습률         | 8e-4 | 5e-4, 5e-5 | 3e-4 | 5e-5 | 3e-4 | 5e-5 |
| 스케줄         | 사다리꼴 | | - | | 1-sqrt | |
| 웜업 (토큰)    | 30억 | 20억 | - | - | - | - |
| 감소 (토큰)    | - | | - | | 500억 | |
| 가중치 감소    | 1e-5 | 1e-5, 1e-6 | 1e-5 | 1e-6 | 1e-5 | 1e-6 |
| 총 시간 (시간)  | 194.2 | 425.3 | 39.9 | 80.7 | 11.5 | 21.7 |
| 학습 시간 (시간) | 191.1 | 420.4 | 36.3 | 75.1 | 7.5 | 15.3 |
| 모델 초기화     | Megatron | Base에서 | - | - | - | - |
| 드롭아웃 (어텐션 출력) | 0.1 |
| 드롭아웃 (기타 모든 레이어) | 0.0 |
| 옵티마이저      | StableAdamW |
| 베타           | (0.90, 0.98) |
| 엡실론         | 1e-06 |
| 학습 하드웨어   | 8x H100 |
| 학습 전략      | Distributed DataParallel |
| 소프트웨어 라이브러리 | PyTorch 2.4.0, Cuda 12.4.0, Composer 0.24.1, Flash Attention 2.6.3, FA3 commit 32792d3 |

표 3의 드롭아웃 및 그 아래 항목들은 모든 단계에서 공통적으로 적용됩니다.

### A.1 배치 크기 스케줄

배치 크기 웜업은 중간에서 큰 배치 크기로 작업할 때 모델 학습 속도를 높이는 일반적인 기법입니다. 최적하지 않은 초기 가중치 분포를 업데이트하는 데 전체 배치를 "낭비"하는 대신, 점진적으로 증가하는 배치 크기로 모델 가중치를 업데이트합니다. 배치 크기 웜업은 일반적으로 학습률 웜업보다 길며, 정의된 학습률 스케줄에 대한 미니 학습률 감소와 함께 더 높은 초기 학습률을 제공하는 것으로 생각할 수 있습니다.

ModernBERT-base의 경우 배치 크기를 500억 토큰에 걸쳐 768에서 4,608로 증가시켰고, ModernBERT-large의 경우 100억 토큰에 걸쳐 448에서 4,928로 증가시켰습니다. 이때 각 배치 크기가 동일한 수의 업데이트 단계를 갖도록 불균등한 토큰 스케줄을 사용했습니다.

### A.2 가중치 타일링

Phi 모델 계열([Li 등, 2023](https://arxiv.org/pdf/2306.11644); [Javaheripi 등, 2023](https://arxiv.org/pdf/2309.05463))을 따라, ModernBERT-large를 중앙 타일링과 Gopher 레이어 스케일링([Rae 등, 2022](https://arxiv.org/pdf/2112.11446))을 사용하여 ModernBERT-base의 사전 학습 가중치에서 직접 초기화했습니다. Base의 가중치 행렬이 Large의 가중치 행렬보다 작기 때문에, 각 토큰 임베딩과 어텐션 헤드를 고려하여 Base의 가중치를 중앙에 배치한 다음, 나머지 가중치를 순환(wraparound) 방식으로 채웠습니다. Phi와 마찬가지로, 무작위 가장자리 값을 가진 중앙 초기화와 가장자리에서부터의 타일링을 테스트했지만, 이 두 방법 모두 순환을 사용한 중앙 타일링보다 성능이 떨어졌습니다. 이러한 가중치 초기화 전략은 ModernBERT-large의 초기 학습을 크게 가속화했습니다.

### A.3 가중치 감소

바이어스 항이나 정규화 레이어에는 가중치 감소를 적용하지 않았습니다. PyTorch 스타일의 분리된 가중치 감소 대신, [Loshchilov와 Hutter(2019)](https://arxiv.org/pdf/1711.05101)를 따라 완전히 분리된 가중치 감소를 적용했습니다.

### A.4 최종 체크포인트

체크포인트 평균화가 더 강력한 최종 모델을 산출한다는 최근 연구([Dubey 등, 2024](https://arxiv.org/pdf/2402.17838.pdf); [Clavié, 2024](https://arxiv.org/pdf/2312.08871))에서 영감을 받아, 다양한 평균화 방법을 실험하고 평가 작업의 하위 집합에서 이를 평가하여 최종 체크포인트를 선택했습니다. [Dubey 등(2024)](https://arxiv.org/pdf/2402.17838.pdf)이 사용한 어닐링 중 지수 이동 평균(Exponential Moving Average)은 어떤 경우에도 더 강력한 성능을 보이지 않았습니다. ModernBERT-base는 가장 성능이 좋은 3개의 어닐링 체크포인트와 최종 체크포인트를 평균한 결과입니다. 평균화는 large 크기에서는 성공적인 결과를 내지 못했으며, ModernBERT-Large 모델은 가장 성능이 좋은 어닐링 체크포인트입니다.

## 부록 B 모델 설계

[Anthony와 연구진(2024)](https://arxiv.org/pdf/2401.14019)의 연구에 따르면, 어텐션 헤드를 64의 배수로 설정하고 임베딩 행렬을 2의 거듭제곱 또는 64의 배수로 설정하는 것 외에도, float16 또는 bfloat16 연산을 가정할 때 성능을 최대화하기 위한 세 가지 모델 설계 선택이 있습니다.

| | Base | Large |
|:-----|:------------:|:----------:|
| 어휘 크기 | 50,368 | 50,368 |
| 미사용 토큰 | 83 | 83 |
| 레이어 | 22 | 28 |
| 은닉 크기 | 768 | 1024 |
| 트랜스포머 블록 | Pre-Norm | Pre-Norm |
| 활성화 함수 | GeLU | GeLU |
| 선형 바이어스 | False | False |
| 어텐션 | Multi-head | Multi-head |
| 어텐션 헤드 | 12 | 16 |
| 전역 어텐션 | 3레이어마다 | 3레이어마다 |
| 지역 어텐션 윈도우 | 128 | 128 |
| GLU 확장 | 2,304 | 5,248 |
| 정규화 | LayerNorm | LayerNorm |
| Norm Epsilon | 1e-5 | 1e-5 |
| Norm 바이어스 | False | False |
| RoPE theta | 160,000 | 160,000 |
| 지역 어텐션 RoPE theta | 10,000 | 10,000 |

표 4: ModernBERT 모델 설계

• **텐서 코어 요구사항**: 가중치 행렬 차원은 64로 나누어 떨어져야 함
• **타일 양자화**: 가중치 행렬은 128 × 256 블록으로 나누어 떨어져야 함
• **웨이브 양자화**: 블록 수는 스트리밍 멀티프로세서(SM) 수로 나누어 떨어져야 함

다양한 SM 수를 가진 여러 GPU에서 좋은 성능을 목표로 했기 때문에, 웨이브 양자화는 불가능한 요구사항이었습니다. 따라서 연구진은 GPU 바스켓(NVIDIA T4, A10, L4, RTX 3090, RTX 4090, A100, H100)을 선택하고 각각에 대해 블록 모듈러스를 SM 수로 나누어 대략적인 SM 활용도를 계산했습니다. 이는 연구진의 현장 확인에서 괜찮은 성능 휴리스틱으로 나타났습니다. 그런 다음 GPU 바스켓에서 성능을 최대화하도록 모델을 설계했으며, 추론 GPU에 더 많은 가중치를 두었습니다.

## 부록 C 학습 로그

### C.1 샘플링 이슈

ModernBERT-base의 첫 번째 사전 학습 실행은 손실이 느린 시소 패턴을 보이다가 서서히 발산하는 재앙으로 끝났습니다. PyTorch의 분산 랜덤 샘플러를 사용했음에도 불구하고, 학습 메트릭은 모델이 데이터셋을 비무작위 순서로 학습하고 있음을 시사했습니다. Olmo 저자들처럼(우리가 이 문제를 스스로 해결한 후 Olmo 코드베이스에서 이에 대한 코멘트와 GitHub 이슈를 발견했습니다), 우리는 PyTorch 랜덤 샘플러가 샘플 수가 5억에서 10억 사이일 때 순차적으로 편향된 샘플을 반환한다고 판단했습니다(우리는 이것이 정확히 언제 발생하는지 엄격한 통계 분석을 수행하지 않았습니다). 이 문제는 PyTorch 샘플러를 NumPy의 PCG64DXSM 랜덤 샘플러로 대체하여 해결했습니다.

### C.2 대규모 롤백

ModernBERT-large 학습을 롤백하고 마지막 8000억 토큰에 대해 더 낮은 학습률 5e-5와 더 낮은 가중치 감소 1e-6으로 재시작했습니다. 학습을 재시작하기 전에, large 모델의 학습 손실, 검증 메트릭, MNLI에 대한 실시간 평가는 더 높은 5e-4 학습률에서 몇 백억 토큰 동안 정체되어 있었습니다. 대조적으로, ModernBERT-base는 1.719조 토큰의 전체 학습 단계 동안 학습 손실, 검증 메트릭, 실시간 평가에서 지속적이지만 감소하는 개선을 보여주었습니다. 이는 일정한 학습률로 학습할 때의 위험 중 하나를 강조합니다. 다른 학습률 스케줄은 학습률을 너무 높게 설정하거나(또는 배치 크기가 너무 작게 설정된 경우) 학습 과정에서 학습률을 낮춤으로써 이러한 문제를 완화할 수 있습니다.

## 부록 D 아키텍처 어블레이션

ModernBERT 아키텍처에 추가할 업데이트를 선택하기 위해 다양한 어블레이션을 수행했습니다. 특별히 언급된 경우를 제외하고, 대부분의 어블레이션은 80-200억 토큰 규모에서 실행되었습니다.

• GeGLU와 SwiGLU, 두 가지 GLU 레이어를 비교했습니다. 두 레이어 간에 거의 차이가 없어 GeGLU 레이어를 사용하기로 결정했습니다.

• RoPE 차원에 대해 헤드 차원의 다른 비율(50%, 75%, 100%)을 사용했습니다. 낮은 비율이 약간 더 나은 결과를 보였습니다. 그러나 관찰된 차이는 미미했습니다. 어블레이션이 최종 학습보다 상당히 작은 규모에서 수행되었기 때문에, 완전히 학습된 모델의 능력을 잠재적으로 저해하지 않도록 차원을 100%로 유지하는 쪽으로 신중하게 결정했습니다.

• LayerNorm과 RMSNorm 모두 매우 유사한 결과를 보였습니다. RMSNorm이 이론적으로 더 빠르지만, 이 작업이 수행된 시점에서 PyTorch는 네이티브 RMSNorm 구현을 갖고 있지 않았으며, 이로 인해 많은 사용자들에게 eager-mode RMSNorm이 기본 구현으로 사용되었습니다. ModernBERT가 가능한 한 높은 즉시 사용 가능한 효율성을 갖도록 하기 위해, 최종 모델에서는 LayerNorm을 사용하기로 결정했습니다.

• [Chowdhery와 연구진(2023)](https://arxiv.org/pdf/2204.02311)이 더 큰 모델 크기에서 처리 속도를 증가시키는 것으로 보여준 MLP와 어텐션 행렬을 동시에 계산하는 병렬 어텐션 사용을 조사했습니다. 그러나 우리의 목표 크기와 사전 학습 시퀀스 길이 내에서는 관찰된 속도 향상이 미미했으며, 다운스트림 성능에 상당한 저하가 있었습니다. 따라서 병렬 어텐션을 사용하지 않습니다. 그러나 더 큰 인코더 및/또는 더 긴 시퀀스 길이에서는 다른 트레이드오프가 있을 수 있습니다.

• 3레이어마다 전역 어텐션과 그 외에는 128 토큰 슬라이딩 윈도우에 걸친 지역 어텐션을 사용하는 교차 전역/지역 어텐션을 탐색했습니다. 이 설정은 1000억 토큰에서도 모든 레이어에서 전역 어텐션을 사용하는 것과 비교하여 동일한 다운스트림 성능을 보이면서도 상당한 속도 향상을 가져왔습니다.

• 최종 토크나이저를 선택하기 전에 여러 토크나이저를 실험했으며, 평가된 최근 토크나이저 중에서 가장 좋은 성능을 보인 수정된 OLMo([Groeneveld와 연구진(2024)](https://arxiv.org/pdf/2402.00838)) 토크나이저를 기반으로 한 토크나이저를 선택했습니다. BERT와 RoBERTa 세대의 인코더 모델의 토크나이저는 MNLI에서 경쟁력 있는 다운스트림 성능을 보였지만, 최근 학습 데이터와 코드 지원 부족이 다운스트림 애플리케이션을 방해할 것이라고 판단했습니다. 흥미롭게도, Llama 2([Touvron와 연구진(2023)](https://arxiv.org/pdf/2307.09288)) 토크나이저를 사용했을 때 다운스트림 성능이 크게 저하되는 것을 관찰했습니다.

## 부록 E 확장 결과

### E.1 GLUE 전체 결과

표 5에는 각 GLUE 하위 집합에 대한 모든 모델의 결과가 제시되어 있습니다. 이전 모델들의 값은 기존 문헌에서 추출되었습니다. 3.1.1절에서 언급한 바와 같이, 저자들은 [Liu와 연구진(2019a)](https://arxiv.org/pdf/1907.11692), [Portes와 연구진(2023)](https://arxiv.org/pdf/2307.02971.pdf), [He와 연구진(2023)](https://arxiv.org/pdf/2111.09543v4)의 표준 관행을 따라 각 하위 집합에 대해 하이퍼파라미터 검색을 수행했습니다. 구체적으로, 학습률은 $[1e{-5}, 3e{-5}, 5e{-5}, 8e{-5}]$ 범위에서, 가중치 감소는 $[1e{-6}, 5e{-6}, 8e{-6}, 1e{-5}]$ 범위에서, 에폭 수는 SST-2, MNLI, RTE 작업에 대해 $[1, 2, 3]$ 범위에서, QNLI, QQP, CoLA, MRPC, STS-B 작업에 대해 $[2, 5, 10]$ 범위에서 탐색했습니다. 최종 값은 표 6에 자세히 나와 있습니다. 모든 미세 조정 실행에는 조기 중단(early stopping)이 사용되어 전체 미세 조정 시간이 상당히 단축되었습니다. RTE, MRPC, STS-B 체크포인트는 MNLI 체크포인트에서 시작하여 훈련되었습니다.

표 5: GLUE 개발 세트 점수. α는 Liu와 연구진(2019a)의 표 8에서, β는 Portes와 연구진(2023)의 표 S3에서, γ는 Nussbaum와 연구진(2024)의 표 2에서, δ는 Zhang와 연구진(2024)의 표 21에서, ϵ는 Qiang와 연구진(2024)의 표 2에서, ζ는 He와 연구진(2023)의 표 3에서 가져왔습니다.

| 모델 | 파라미터 | 시퀀스 | 단일 문장 | 패러프레이즈 및 유사성 | 자연어 추론 |
|:-----|:--------:|:-----:|:--------:|:------------------:|:----------:|
|      |          |       | CoLA | SST-2 | MRPC | STS-B | QQP | MNLI | QNLI | RTE |
| **Base** |
| BERT^β | 110M | 512 | 59.0 | 93.1 | 89.5 | 89.4 | 91.4 | 85.4 | 91.6 | 78.2 |
| RoBERTa^α | 125M | 512 | 63.6 | 94.8 | 90.2 | 91.2 | 91.9 | 87.6 | 92.8 | 78.7 |
| DeBERTav3^ϵ | 183M | 512 | 69.2 | 95.6 | 89.5 | 91.6 | 92.4 | 90.0 | 94.0 | 83.8 |
| MosaicBERT-128^β | 137M | 128 | 58.2 | 93.5 | 89.0 | 90.3 | 92.0 | 85.6 | 91.4 | 83.0 |
| NomicBERT-2048^γ | 137M | 2048 | 50.0 | 93.0 | 88.0 | 90.0 | 92.0 | 86.0 | 92.0 | 82.0 |
| GTE-en-MLM^δ | 137M | 8192 | 57.0 | 93.4 | 92.1 | 90.2 | 88.8 | 86.7 | 91.9 | 84.8 |
| ModernBERT | 149M | 8192 | 65.1 | 96.0 | 92.2 | 91.8 | 92.1 | 89.1 | 93.9 | 87.4 |
| **Large** |
| BERT^β | 330M | 512 | 56.2 | 93.3 | 87.8 | 90.6 | 90.9 | 86.3 | 92.8 | 83.8 |
| RoBERTa^α | 355M | 512 | 68.0 | 96.4 | 90.9 | 92.4 | 92.2 | 90.2 | 94.7 | 86.6 |
| DeBERTav3^ζ | 434M | 512 | 75.3 | 96.9 | 92.2 | 93.0 | 93.3 | 91.8 | 96.0 | 92.7 |
| GTE-en-MLM^δ | 434M | 8192 | 60.4 | 95.1 | 93.5 | 91.4 | 89.2 | 89.2 | 93.9 | 88.1 |
| ModernBERT | 395M | 8192 | 71.4 | 97.1 | 91.7 | 92.8 | 92.7 | 90.8 | 95.2 | 92.1 |

표 6: GLUE 작업에 대한 ModernBERT의 미세 조정 하이퍼파라미터. LR: 학습률, WD: 가중치 감소, Ep: 에폭.

| 작업 | Base | Large |
|:-----|:----:|:-----:|
|      | LR | WD | Ep | LR | WD | Ep |
| CoLA | $8e{-5}$ | $1e{-6}$ | 5 | $3e{-5}$ | $8e{-6}$ | 5 |
| MNLI | $5e{-5}$ | $5e{-6}$ | 1 | $3e{-5}$ | $1e{-5}$ | 1 |
| MRPC | $5e{-5}$ | $5e{-6}$ | 10 | $8e{-5}$ | $5e{-6}$ | 2 |
| QNLI | $8e{-5}$ | $5e{-6}$ | 2 | $3e{-5}$ | $5e{-6}$ | 2 |
| QQP | $5e{-5}$ | $5e{-6}$ | 10 | $5e{-5}$ | $8e{-6}$ | 2 |
| RTE | $5e{-5}$ | $1e{-5}$ | 3 | $5e{-5}$ | $8e{-6}$ | 3 |
| SST-2 | $8e{-5}$ | $1e{-5}$ | 2 | $1e{-5}$ | $1e{-6}$ | 3 |
| STSB | $8e{-5}$ | $5e{-6}$ | 10 | $8e{-5}$ | $1e{-5}$ | 10 |

### E.2 BEIR 전체 결과

본문에서는 BEIR의 15개 다양한 데이터셋에 대한 평균 점수만 보고했습니다. 표 7과 표 8에서는 단일 벡터 및 다중 벡터 검색 모두에 대해 모든 하위 집합의 결과를 보고합니다. 두 설정 모두에서 모든 모델에 대해, 저자들은 $[1e{-5}, 2e{-5}, 3e{-5}, 5e{-5}, 8e{-5}, 1e{-4}]$ 범위의 학습률에 대한 탐색을 수행하고, NFCorpus, SciFact, TREC-Covid, FiQA로 구성된 데이터셋 하위 집합에서 최상의 평균 결과를 얻는 모델을 최종 모델로 선택했습니다. 각 설정에 대한 최적 학습률은 표 9에 보고되어 있습니다.

ModernBERT가 전반적으로 강력한 결과를 보여주지만, TREC-COVID([Voorhees와 연구진, 2021](https://arxiv.org/pdf/2104.09134))에서의 성능이 중요한 요소라는 점에 주목해야 합니다. 이는 ModernBERT가 대부분의 기존 인코더보다 더 최근의 지식 컷오프로 훈련되었기 때문에 얻는 이점을 보여줍니다. 그러나 NomicBERT와 GTE도 업데이트된 데이터로 훈련되었으므로, 컷오프만이 성능에 영향을 미치는 유일한 요소는 아닙니다.

표 7: 단일 벡터 검색 모델에 대한 BEIR nDCG@10 점수.

| 모델 | NFCorpus | SciFact | TREC-Covid | FiQA | ArguAna | Climate-FEVER | DBPedia | FEVER | HotpotQA | MSMARCO | NQ | Quora | SciDocs | Touche2020 | CQADupstack | 평균 |
|:-----|:--------:|:-------:|:----------:|:----:|:-------:|:-------------:|:-------:|:-----:|:--------:|:-------:|:--:|:-----:|:-------:|:----------:|:-----------:|:----:|
| **Base** |
| BERT | 24.3 | 51.3 | 49.5 | 22.8 | 31.6 | 21.9 | 28.2 | 64.1 | 47.9 | 58.5 | 37.9 | 83.1 | 12.9 | 20.4 | 28.5 | 38.9 |
| RoBERTa | 20.4 | 45.6 | 52.2 | 26.1 | 35.2 | 22.3 | 23.1 | 60.2 | 45.0 | 56.0 | 34.7 | 84.0 | 11.4 | 21.1 | 28.8 | 37.7 |
| DeBERTaV3 | 8.0 | 22.6 | 48.4 | 11.5 | 26.1 | 9.7 | 5.3 | 17.3 | 8.0 | 25.2 | 12.5 | 74.7 | 5.4 | 14.2 | 14.2 | 20.2 |
| NomicBERT | 25.7 | 52.0 | 63.0 | 23.5 | 35.5 | 22.9 | 30.3 | 65.0 | 48.0 | 60.6 | 42.6 | 84.5 | 12.6 | 19.0 | 29.2 | 41.0 |
| GTE-en-MLM | 26.3 | 54.1 | 49.7 | 30.1 | 35.7 | 24.5 | 28.9 | 66.5 | 49.9 | 63.1 | 41.7 | 85.2 | 14.1 | 19.1 | 32.5 | 41.4 |
| ModernBERT | 23.7 | 57.0 | 72.1 | 28.8 | 35.7 | 23.6 | 23.8 | 59.9 | 46.1 | 61.6 | 39.5 | 85.9 | 12.5 | 20.8 | 33.1 | 41.6 |
| **Large** |
| BERT | 23.3 | 50.7 | 48.9 | 24.0 | 35.2 | 22.1 | 27.2 | 61.7 | 45.9 | 59.8 | 39.5 | 83.6 | 13.0 | 19.5 | 28.9 | 38.9 |
| RoBERTa | 23.9 | 53.4 | 55.0 | 33.4 | 37.6 | 23.5 | 25.4 | 65.2 | 47.1 | 60.4 | 43.3 | 85.8 | 13.7 | 21.1 | 33.0 | 41.4 |
| DeBERTaV3 | 9.6 | 31.2 | 56.6 | 15.8 | 26.3 | 14.4 | 6.8 | 29.4 | 15.3 | 32.4 | 21.5 | 79.1 | 7.0 | 18.8 | 19.9 | 25.6 |
| GTE-en-MLM | 27.7 | 57.6 | 48.4 | 34.0 | 35.3 | 24.0 | 27.0 | 65.4 | 50.8 | 64.1 | 44.9 | 85.3 | 15.6 | 21.4 | 35.5 | 42.5 |
| ModernBERT | 26.2 | 60.4 | 74.1 | 33.1 | 38.2 | 20.5 | 25.1 | 62.7 | 49.2 | 64.9 | 45.5 | 86.5 | 13.8 | 23.1 | 36.5 | 44.0 |

표 8: 다중 벡터 검색 모델에 대한 BEIR nDCG@10 점수.

| 모델 | NFCorpus | SciFact | TREC-Covid | FiQA | ArguAna | Climate-FEVER | DBPedia | FEVER | HotpotQA | MSMARCO | NQ | Quora | SciDocs | Touche2020 | CQADupstack | 평균 |
|:-----|:--------:|:-------:|:----------:|:----:|:-------:|:-------------:|:-------:|:-----:|:--------:|:-------:|:--:|:-----:|:-------:|:----------:|:-----------:|:----:|
| **Base** |
| BERT | 34.2 | 71.5 | 69.9 | 35.0 | 49.9 | 19.2 | 42.4 | 83.1 | 69.8 | 45.4 | 55.4 | 84.1 | 14.7 | 27.0 | 34.2 | 49.0 |
| RoBERTa | 33.7 | 70.8 | 69.8 | 37.4 | 48.9 | 18.9 | 39.3 | 81.2 | 66.1 | 43.7 | 56.3 | 83.6 | 14.8 | 31.7 | 34.4 | 48.7 |
| DeBERTaV3 | 31.9 | 68.5 | 75.5 | 35.5 | 46.5 | 18.3 | 35.6 | 78.1 | 65.3 | 39.5 | 50.4 | 83.7 | 14.6 | 31.1 | 32.3 | 47.1 |
| NomicBERT | 35.5 | 72.2 | 73.5 | 35.9 | 44.8 | 19.0 | 43.6 | 83.9 | 71.1 | 46.3 | 58.5 | 84.0 | 15.1 | 31.3 | 33.9 | 49.9 |
| GTE-en-MLM | 35.1 | 71.5 | 69.4 | 36.0 | 48.5 | 17.4 | 41.2 | 79.9 | 67.0 | 44.4 | 52.8 | 85.2 | 15.0 | 25.4 | 34.6 | 48.2 |
| ModernBERT | 35.2 | 73.0 | 80.5 | 38.0 | 49.1 | 22.2 | 42.0 | 85.8 | 70.4 | 45.4 | 57.1 | 86.3 | 16.0 | 33.9 | 35.1 | 51.3 |
| **Large** |
| BERT | 34.6 | 72.9 | 68.8 | 35.5 | 48.3 | 19.7 | 42.4 | 83.6 | 70.7 | 45.9 | 57.2 | 84.8 | 15.2 | 28.9 | 34.9 | 49.5 |
| RoBERTa | 35.0 | 72.3 | 74.4 | 38.7 | 50.0 | 19.6 | 41.0 | 82.0 | 66.2 | 44.7 | 57.5 | 85.9 | 15.3 | 27.9 | 36.0 | 49.8 |
| DeBERTaV3 | 31.7 | 70.2 | 73.3 | 35.0 | 46.2 | 18.0 | 36.5 | 79.0 | 63.2 | 39.4 | 51.6 | 81.1 | 14.1 | 28.6 | 33.1 | 46.7 |
| GTE-en-MLM | 35.2 | 72.4 | 67.2 | 39.6 | 50.3 | 20.8 | 44.4 | 82.5 | 72.0 | 47.0 | 60.1 | 86.4 | 15.9 | 30.9 | 35.4 | 50.7 |
| ModernBERT | 36.0 | 73.2 | 81.3 | 40.3 | 50.3 | 22.3 | 44.1 | 85.8 | 72.5 | 46.0 | 59.9 | 86.1 | 16.9 | 34.6 | 35.9 | 52.4 |

표 9: BEIR에 대한 단일 벡터 및 다중 벡터 검색에 사용된 학습률.

| 모델 | 단일 벡터 (DPR) | 다중 벡터 (ColBERT) |
|:-----|:---------------:|:------------------:|
| **Base** |
| BERT | $5 \times 10^{-5}$ | $8 \times 10^{-5}$ |
| RoBERTa | $3 \times 10^{-5}$ | $8 \times 10^{-5}$ |
| DeBERTaV3 | $8 \times 10^{-5}$ | $5 \times 10^{-5}$ |
| NomicBERT | $5 \times 10^{-5}$ | $1 \times 10^{-4}$ |
| GTE-en-MLM | $5 \times 10^{-5}$ | $8 \times 10^{-5}$ |
| ModernBERT | $8 \times 10^{-5}$ | $1 \times 10^{-4}$ |
| **Large** |
| BERT | $3 \times 10^{-5}$ | $1 \times 10^{-4}$ |
| RoBERTa | $3 \times 10^{-5}$ | $1 \times 10^{-5}$ |
| DeBERTaV3 | $8 \times 10^{-5}$ | $1 \times 10^{-5}$ |
| GTE-en-MLM | $3 \times 10^{-5}$ | $3 \times 10^{-5}$ |
| ModernBERT | $1 \times 10^{-4}$ | $3 \times 10^{-5}$ |

## 부록 F 효율성

이전 절에서 모델의 효율성을 평가하는 데 사용된 합성 데이터셋의 전체 통계는 표 10에 제시되어 있습니다. 각 모델의 상세한 실행 시간과 최대 배치 크기는 표 11에 자세히 나와 있습니다. ModernBERT 모델이 두 크기 모두에서 달성한 높은 최대 배치 크기는 다른 모델보다 상당히 높아, 모델의 강력한 메모리 효율성을 강조합니다. 반대로, DeBERTaV3는 GLUE 성능이 경쟁력 있지만, 메모리 사용량과 처리 속도 모두에서 특히 비효율적인 것으로 나타났습니다. 실제로, 두 모델 크기 모두에서 DeBERTaV3의 메모리 사용량은 ModernBERT보다 5-7배 높으며, 모든 시퀀스가 최대 가능 길이인 가장 유리한 시나리오에서도 언패딩의 이점을 상쇄하면서 입력을 두 배 느리게 처리합니다.

표 10: 효율성 평가에 사용된 합성 데이터셋의 토큰 통계.

| | 짧은 컨텍스트 | 긴 컨텍스트 |
|:-----|:------------:|:----------:|
| | 고정 | 가변 | 고정 | 가변 |
| 총 토큰 수 | 4,194,304 | 2,096,510 | 67,108,864 | 33,604,913 |
| 표준 편차 | 0 | 64 | 0 | 1,024 |
| 평균 길이 | 512 | 256 | 8,192 | 4,102 |
| 가장 긴 시퀀스 | 512 | 476 | 8,192 | 7,624 |
| 가장 짧은 시퀀스 | 512 | 32 | 8,192 | 171 |
| 시퀀스 수 | 8,192 | 8,192 | 8,192 | 8,192 |

표 11: 모든 모델의 추론 실행 시간. 굵은 글씨는 열에서 가장 좋은 결과(2 표준 편차 이내)를 나타냅니다.

| 모델 | 짧은 컨텍스트 | 긴 컨텍스트 |
|:-----|:------------:|:----------:|
| | BS | 고정 | 가변 | BS | 고정 | 가변 |
| **Base** |
| BERT | 1096 | 23.3 ± 0.02 | - | - | - | - |
| RoBERTa | 664 | 23.3 ± 0.19 | - | - | - | - |
| DeBERTaV3 | 236 | 59.7 ± 0.11 | - | - | - | - |
| NomicBERT | 588 | 35.8 ± 0.01 | - | 36 | 1455.5 ± 0.31 | - |
| GTE-en-MLM | 640 | 33.9 ± 1.21 | - | 38 | 1434.7 ± 3.69 | - |
| GTE-en-MLMxformers | 640 | 34.2 ± 0.10 | 16.3 ± 0.04 | 38 | 1412.6 ± 3.19 | 499.2 ± 0.11 |
| ModernBERT | 1604 | **28.3 ± 0.55** | **14.2 ± 0.01** | 98 | **542.4 ± 0.20** | **251.2 ± 0.32** |
| **Large** |
| BERT | 792 | 77.1 ± 1.50 | - | - | - | - |
| RoBERTa | 460 | 99.8 ± 1.79 | - | - | - | - |
| DeBERTaV3 | 134 | 170.8 ± 0.06 | - | - | - | - |
| GTE-en-MLM | 472 | 108.4 ± 0.07 | - | 28 | 1444.7 ± 0.05 | - |
| GTE-en-MLMxformers | 472 | 109.0 ± 0.14 | 51.9 ± 0.02 | 28 | 1459.1 ± 4.55 | 1476.3 ± 0.94 |
| ModernBERT | 770 | **80.1 ± 1.65** | **39.6 ± 0.02** | 48 | **433.9 ± 0.99** | **674.9 ± 0.15** |

- - -
### References
* [Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](http://arxiv.org/pdf/2412.13663v2)
