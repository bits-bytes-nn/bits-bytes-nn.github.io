---
layout: post
title: "Mixtral of Experts"
date: 2024-01-08 18:47:34
author: "Mistral AI"
categories: "Language-Models"
tags: ["Sparse-Mixture-of-Experts", "Efficient-Transformer-Architecture", "Economical-Training", "Efficient-Inference", "Long-Context-Adaptation", "Instruction-Tuning", "Direct-Preference-Optimization"]
cover: /assets/images/language-models.jpg
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
대규모 언어 모델의 성능 향상을 위해서는 모델의 크기를 증가시켜야 하지만, 이는 계산 비용과 메모리 사용량의 급격한 증가를 수반합니다. 기존의 밀집형(dense) 트랜스포머 모델들은 모든 입력 토큰에 대해 전체 파라미터를 활성화해야 하는 비효율성을 가지고 있었습니다. 이러한 한계를 극복하고 더 효율적인 모델 아키텍처를 개발하기 위해 Meta AI와 Hugging Face 연구진은 희소 전문가 혼합(Sparse Mixture of Experts, SMoE) 접근 방식을 탐구하게 되었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
Mixtral 8x7B는 혁신적인 희소 전문가 혼합 아키텍처를 도입했습니다. 이 모델은 각 레이어에 8개의 전문가 네트워크를 배치하고, 각 토큰 처리 시 동적 라우팅을 통해 가장 적합한 2개의 전문가만을 선택적으로 활성화합니다. 이러한 접근 방식은 전체 47B 개의 파라미터 중 실제 추론 시에는 13B 개만을 사용하여, 계산 효율성과 모델 성능 사이의 최적의 균형을 달성했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
모델은 32k 토큰의 컨텍스트 길이를 지원하는 트랜스포머 아키텍처를 기반으로 구현되었습니다. 각 레이어는 32개의 어텐션 헤드와 4096 차원의 임베딩을 사용하며, 8개의 전문가 네트워크가 SwiGLU 아키텍처를 통해 구현되었습니다. 라우팅 메커니즘은 Top-2 게이팅을 사용하여 각 토큰에 대해 가장 적합한 두 전문가를 선택합니다. 모델의 학습은 다국어 데이터셋을 사용했으며, 지시어 수행 능력 향상을 위해 지도 학습 미세조정(SFT)과 직접 선호도 최적화(DPO)가 적용되었습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
Mixtral 8x7B는 대부분의 벤치마크에서 Llama 2 70B와 GPT-3.5를 능가하는 성능을 달성했으며, 특히 수학과 코드 생성 분야에서 두드러진 성과를 보여주었습니다. 이는 희소 전문가 혼합 아키텍처가 대규모 언어 모델의 효율성을 크게 개선할 수 있음을 입증합니다. Apache 2.0 라이선스로 공개된 이 모델은 학계와 산업계에서 널리 활용될 수 있으며, 향후 언어 모델 발전의 새로운 방향을 제시했다는 점에서 중요한 의미를 가집니다. 특히 계산 효율성과 성능의 균형을 달성한 이 연구는 지속 가능한 AI 발전을 위한 중요한 이정표가 될 것으로 기대됩니다.
- - -
### Mixtral 8x7B 모델 소개

Mixtral 8x7B는 희소 전문가 혼합(Sparse Mixture of Experts, SMoE) 아키텍처를 기반으로 하는 혁신적인 언어 모델입니다. 이 모델은 Mistral 7B의 기본 아키텍처를 확장하여, 각 레이어에 8개의 피드포워드 블록(전문가)을 도입했습니다. 모델의 가장 큰 특징은 토큰 처리 과정에서 라우터 네트워크가 각 레이어마다 8개의 전문가 중 2개만을 선택적으로 활용한다는 점입니다.

이러한 선택적 활성화 메커니즘을 통해 Mixtral은 매우 효율적인 파라미터 활용을 실현했습니다. 전체 47B 개의 파라미터를 보유하고 있지만, 실제 추론 과정에서는 단 13B 개의 파라미터만을 활성화하여 사용합니다. 각 토큰은 서로 다른 타임스텝에서 서로 다른 전문가 조합을 활용할 수 있어, 상황에 따라 최적의 전문가를 동적으로 선택할 수 있습니다.

Mixtral은 32k 토큰의 컨텍스트 크기로 학습되었으며, 다양한 벤치마크에서 Llama 2 70B와 GPT-3.5를 능가하거나 대등한 성능을 보여주었습니다. 특히 수학, 코드 생성, 다국어 처리 분야에서 Llama 2 70B를 크게 앞서는 성능을 달성했습니다. 연구진은 또한 지시사항 수행에 최적화된 Mixtral 8x7B - Instruct 모델을 제공하여, GPT-3.5 Turbo, Claude-2.1, Gemini Pro, Llama 2 70B 채팅 모델을 인간 평가 기준에서 능가하는 성과를 보여주었습니다.

이 모델의 모든 버전은 Apache 2.0 라이선스로 공개되어 있어, 연구 및 상업적 활용이 가능합니다. 소스 코드는 Mistral AI의 공식 GitHub 저장소를 통해 접근할 수 있으며, 자세한 기술 문서와 성능 분석은 Mistral AI의 웹사이트에서 확인할 수 있습니다.

### Mixtral 8x7B 모델의 혁신적 아키텍처와 성능

Mixtral 8x7B는 Apache 2.0 라이선스로 공개된 희소 전문가 혼합(Sparse Mixture of Experts, SMoE) 모델로, 대부분의 벤치마크에서 Llama 2 70B와 GPT-3.5를 능가하는 성능을 보여주는 혁신적인 언어 모델입니다. 이 모델의 핵심은 디코더 전용 아키텍처에 있는 피드포워드 블록으로, 8개의 서로 다른 파라미터 그룹을 활용합니다. 각 레이어에서 라우터 네트워크는 토큰마다 이 8개의 전문가 그룹 중 2개를 선택하여 처리하고, 그 출력을 가산적으로 결합합니다.

이러한 희소 라우팅 메커니즘은 모델의 총 파라미터 수를 증가시키면서도 비용과 지연 시간을 효과적으로 제어할 수 있게 합니다. 각 토큰 처리 시 전체 파라미터의 일부만을 사용하기 때문에, 작은 배치 크기에서는 더 빠른 추론 속도를, 큰 배치 크기에서는 더 높은 처리량을 달성할 수 있습니다.

Mixtral은 32k 토큰의 컨텍스트 크기로 다국어 데이터를 사용하여 사전 학습되었습니다. 실험 결과에 따르면, 시퀀스 길이나 정보의 위치와 관계없이 32k 토큰 컨텍스트 윈도우에서 정보를 성공적으로 검색할 수 있음이 입증되었습니다. 특히 수학, 코드 생성, 다국어 이해가 필요한 작업에서 Llama 2 70B를 크게 앞서는 성능을 보여주었습니다.

연구진은 또한 지도 학습 미세 조정과 직접 선호도 최적화(Direct Preference Optimization)를 통해 Mixtral 8x7B - Instruct 모델을 개발했습니다. 이 모델은 인간 평가 벤치마크에서 GPT-3.5 Turbo, Claude-2.1, Gemini Pro, Llama 2 70B 채팅 모델을 능가하는 성능을 보여주었습니다. 특히 BBQ와 BOLD와 같은 벤치마크에서 편향이 감소하고 더 균형 잡힌 감성 프로파일을 보여주었습니다.

Mixtral 8x7B와 Mixtral 8x7B - Instruct 모두 Apache 2.0 라이선스로 공개되어 있어 학술적, 상업적 용도로 자유롭게 사용할 수 있습니다. 커뮤니티의 효율적인 모델 활용을 위해 연구진은 vLLM 프로젝트에 Megablocks CUDA 커널을 통합하는 변경사항을 제출했으며, Skypilot을 통해 클라우드의 모든 인스턴스에서 vLLM 엔드포인트를 배포할 수 있도록 지원했습니다.

### Mixtral 8x7B의 아키텍처 세부사항

Mixtral 8x7B는 트랜스포머 아키텍처를 기반으로 하며, Vaswani와 연구진이 제안한 기본 구조에 몇 가지 중요한 수정사항을 적용했습니다. 모델의 가장 큰 특징은 32k 토큰의 완전 밀집 컨텍스트 길이를 지원하고, 피드포워드 블록을 전문가 혼합(Mixture-of-Expert) 레이어로 대체했다는 점입니다.

모델의 주요 아키텍처 매개변수를 살펴보면, 임베딩 차원(dim)은 4096으로 설정되었으며, 총 32개의 레이어로 구성되어 있습니다. 각 레이어는 32개의 어텐션 헤드를 가지고 있으며, 각 헤드의 차원은 128로 설정되었습니다. 히든 차원은 14336으로 설계되어 있어 풍부한 특징 표현이 가능합니다.

Mixtral의 MoE 구조에서는 8개의 전문가 네트워크를 사용하며, 각 토큰 처리 시 이 중 2개의 전문가만을 선택적으로 활용합니다. 이는 Shazeer와 연구진이 제안한 희소 게이팅 메커니즘을 효과적으로 구현한 것입니다. 키-값 어텐션에서는 8개의 헤드를 사용하여 계산 효율성을 높였습니다.

모델의 어휘 크기는 32,000 토큰으로 설정되어 있어 다양한 언어와 도메인을 효과적으로 처리할 수 있습니다. 특히 32,768 토큰의 컨텍스트 길이는 긴 문서나 복잡한 대화를 처리하는 데 충분한 용량을 제공합니다. 이러한 아키텍처 설계는 모델의 성능과 효율성을 최적의 상태로 유지하면서도 계산 자원을 효과적으로 활용할 수 있도록 합니다.

### 희소 전문가 혼합 레이어의 구조와 작동 원리

![Mixture of Experts Layer](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/images/smoe.png)

희소 전문가 혼합(Sparse Mixture of Experts, SMoE) 레이어는 입력 벡터를 8개의 전문가 블록 중 2개에 동적으로 할당하여 처리하는 혁신적인 신경망 구조입니다. 위 그림은 이러한 SMoE 레이어의 핵심 구성 요소와 작동 방식을 보여줍니다. 각 입력 토큰은 라우터 네트워크를 통해 가장 적합한 2개의 전문가로 전달되며, 최종 출력은 선택된 전문가들의 출력값을 가중 합산하여 계산됩니다.

SMoE 레이어의 수학적 정의를 살펴보면, $$ n $$개의 전문가 네트워크 \\(E_0, E_i, ..., E_{n-1}\\)가 있을 때, 입력 벡터 $$ x $$에 대한 레이어의 출력은 다음과 같이 계산됩니다.

$$ \sum_{i=0}^{n-1} G(x)_i \cdot E_i(x) $$

여기서 $$ G(x)_i $$는 게이팅 네트워크가 i번째 전문가에 할당한 가중치를 나타내며, $$ E_i(x) $$는 $$ i $$번째 전문가 네트워크의 출력을 의미합니다. 게이팅 네트워크는 선형 레이어의 $$ Top-K $$ 로짓에 소프트맥스를 적용하여 구현됩니다.

$$ G(x) := \text{Softmax}(\text{TopK}(x \cdot W_g)) $$

TopK 연산자는 로짓 벡터 ℓ ∈ ℝ^n의 상위 K개 좌표는 원래 값을 유지하고 나머지는 -∞로 설정합니다. 이러한 희소 게이팅 메커니즘을 통해 전체 파라미터 수를 늘리면서도 토큰당 계산 비용을 일정하게 유지할 수 있습니다.

Mixtral 모델에서는 K=2로 설정하여 각 토큰이 8개의 전문가 중 2개만을 활용하도록 구현했습니다. 전문가 함수로는 SwiGLU 아키텍처를 사용하며, 최종 출력 y는 다음과 같이 계산됩니다.

$$ y = \sum_{i=0}^{n-1} \text{Softmax}(\text{Top2}(x \cdot W_g))_i \cdot \text{SwiGLU}_i(x) $$

이러한 구조는 GShard 아키텍처와 유사하지만, Mixtral은 모든 피드포워드 블록을 MoE 레이어로 대체했다는 점에서 차이가 있습니다. 또한 GShard가 두 번째 전문가 선택에 더 복잡한 게이팅 전략을 사용하는 반면, Mixtral은 더 단순하고 효율적인 접근 방식을 채택했습니다.
### SMoE 레이어의 효율적인 구현과 병렬화 전략

SMoE 레이어는 단일 GPU에서도 특수화된 커널을 통해 높은 성능으로 실행될 수 있습니다. Megablocks는 MoE 레이어의 피드포워드 네트워크(FFN) 연산을 대규모 희소 행렬 곱셈으로 변환하여 실행 속도를 크게 향상시킵니다. 이 방식은 서로 다른 전문가들에게 할당되는 토큰 수가 불균형한 상황도 자연스럽게 처리할 수 있습니다.

MoE 레이어의 분산 처리는 일반적인 모델 병렬화 기법과 함께 전문가 병렬화(Expert Parallelism, EP)라는 특별한 분할 전략을 통해 구현됩니다. EP에서는 특정 전문가가 처리해야 할 토큰들이 해당 전문가가 위치한 GPU로 라우팅되어 처리된 후, 원래 토큰 위치로 결과가 반환됩니다. 이러한 EP 구현에서 가장 중요한 과제는 GPU 간의 부하 균형을 맞추는 것입니다. 개별 GPU에 과도한 부하가 집중되거나 계산 병목이 발생하는 것을 방지하기 위해 작업량을 균등하게 분배해야 합니다.

트랜스포머 모델에서 MoE 레이어는 토큰별로 독립적으로 적용되며, 트랜스포머 블록의 피드포워드(FFN) 서브블록을 대체합니다. Mixtral은 전문가 함수 $$ E_i(x) $$로 SwiGLU 아키텍처를 사용하고 $$ K=2 $$로 설정하여, 각 토큰이 서로 다른 가중치 집합을 가진 두 개의 SwiGLU 서브블록으로 라우팅됩니다.

이러한 구현은 GShard 아키텍처와 유사한 접근 방식을 취하지만, 두 가지 주요한 차이점이 있습니다. 첫째, Mixtral은 모든 FFN 서브블록을 MoE 레이어로 대체하는 반면, GShard는 두 블록마다 하나씩만 대체합니다. 둘째, GShard는 각 토큰에 할당되는 두 번째 전문가를 위해 더 복잡한 게이팅 전략을 사용하는데 비해, Mixtral은 더 단순하고 효율적인 접근 방식을 채택했습니다.

이러한 설계 선택은 모델의 성능과 효율성을 최적화하는 데 중요한 역할을 합니다. 특히 Mixtral의 단순화된 게이팅 메커니즘은 계산 효율성을 높이면서도 모델의 표현력을 유지할 수 있게 해줍니다. 또한 전문가 병렬화 전략을 통해 대규모 분산 환경에서도 효율적인 학습과 추론이 가능해집니다.

### Mixtral의 성능 평가 결과

Mixtral 모델의 성능을 공정하게 비교하기 위해 연구진은 자체 평가 파이프라인을 구축하여 Llama 모델들과 함께 광범위한 벤치마크 테스트를 수행했습니다. 평가는 상식 추론, 세계 지식, 독해력, 수학, 코드 생성 등 다양한 영역에서 이루어졌습니다.

![벤치마크 성능 비교](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/images/231209_bench_combined.png)

위 그래프는 Mixtral과 다양한 Llama 모델들의 성능을 MMLU, 지식, 추론, 이해력, AGI 평가, 수학, BBH, 코드 생성 등의 벤치마크에서 비교한 결과를 보여줍니다. Mixtral은 모든 벤치마크에서 Llama 2 70B 모델과 대등하거나 더 우수한 성능을 보여주었으며, 특히 수학과 코드 생성 분야에서 월등한 성과를 달성했습니다.

상식 추론 분야에서는 HellaSwag, Winogrande, PIQA 등의 제로샷 평가를 수행했으며, 세계 지식 평가에는 NaturalQuestions와 TriviaQA를 5-샷 설정으로 테스트했습니다. 독해력 평가는 BoolQ와 QuAC를 제로샷으로 진행했고, 수학 능력은 GSM8K(8-샷)과 MATH(4-샷) 데이터셋으로 측정했습니다. 코드 생성 능력은 Humaneval(제로샷)과 MBPP(3-샷)를 통해 평가했습니다.

![스케일링 성능](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/images/231209_scaling.png)

위 그래프는 Mistral과 LLaMA 2 모델들의 활성 파라미터 수에 따른 성능을 비교한 결과입니다. Mixtral 8x7B는 독해력을 제외한 모든 벤치마크에서 Llama 2 70B를 크게 앞서는 성능을 보여주었으며, 특히 수학과 코드 생성 분야에서 두드러진 우위를 보였습니다.

크기와 효율성 측면에서, Mixtral은 희소 전문가 혼합 모델로서 각 토큰 처리에 13B 개의 활성 파라미터만을 사용합니다. 이는 Llama 2 70B에 비해 5배 적은 수치임에도 불구하고 대부분의 평가에서 더 우수한 성능을 달성했습니다. 다만, 전체 파라미터 수는 47B로 메모리 사용량이 이에 비례하며, 라우팅 메커니즘으로 인한 추가적인 오버헤드가 발생합니다.

GPT-3.5와의 비교에서도 Mixtral은 MMLU, HellaSwag, ARC Challenge 등 대부분의 벤치마크에서 대등하거나 더 나은 성능을 보여주었습니다. 특히 MBPP와 GSM-8K와 같은 코드 생성과 수학 문제 해결에서 각각 60.7%와 58.4%의 높은 정확도를 달성했습니다.

### Mixtral의 다국어 성능 평가

Mixtral 모델은 사전 학습 과정에서 다국어 데이터의 비중을 크게 늘려 학습을 진행했습니다. 이러한 학습 전략을 통해 영어에서의 높은 정확도를 유지하면서도 다국어 벤치마크에서 우수한 성능을 달성할 수 있었습니다. 특히 프랑스어, 독일어, 스페인어, 이탈리아어에서 Llama 2 70B 모델을 크게 앞서는 결과를 보여주었습니다.

다국어 성능 평가는 ARC Challenge, HellaSwag, MMLU 세 가지 주요 벤치마크를 통해 이루어졌습니다. ARC Challenge에서 Mixtral은 프랑스어 58.2%, 독일어 54.3%, 스페인어 55.4%, 이탈리아어 52.8%의 정확도를 달성했습니다. 이는 Llama 2 70B와 비교했을 때 각각 8.3-13.5 퍼센트 포인트 향상된 결과입니다.

HellaSwag 벤치마크에서는 프랑스어 77.4%, 독일어 73.0%, 스페인어 77.6%, 이탈리아어 75.1%의 성능을 보여주었으며, 이는 Llama 2 70B 대비 4.9-9.1 퍼센트 포인트 개선된 수치입니다. MMLU 평가에서도 프랑스어 70.9%, 독일어 71.5%, 스페인어 72.5%, 이탈리아어 70.9%를 기록하며 Llama 2 70B보다 6.6-8.2 퍼센트 포인트 높은 성능을 달성했습니다.

주목할 만한 점은 Mixtral이 13B의 활성 파라미터만을 사용하면서도 70B 규모의 Llama 2 모델보다 우수한 다국어 성능을 보여주었다는 것입니다. 이는 사전 학습 과정에서 다국어 데이터의 비중을 전략적으로 증가시킨 접근 방식이 효과적이었음을 입증합니다. 특히 전문가 혼합 아키텍처를 통해 언어별 특화된 전문가들이 효율적으로 활용된 것으로 분석됩니다.

### Mixtral의 장거리 성능 평가

Mixtral 모델의 장거리 문맥 처리 능력을 평가하기 위해 연구진은 Mohtashami와 연구진이 제안한 패스키 검색 작업을 수행했습니다. 이 작업은 긴 입력 시퀀스 내에 무작위로 삽입된 패스키를 모델이 얼마나 정확하게 검색할 수 있는지 측정하는 합성 태스크입니다.

![장거리 성능](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/images/passkey.png)

위 그래프의 왼쪽 부분은 Mixtral이 입력 시퀀스의 길이나 패스키의 위치와 관계없이 100%의 검색 정확도를 달성했음을 보여줍니다. 이는 모델이 긴 문맥에서도 중요한 정보를 효과적으로 검색하고 활용할 수 있는 능력을 갖추고 있음을 입증합니다.

오른쪽 그래프는 Gao와 연구진이 구축한 proof-pile 데이터셋의 일부에서 측정한 Mixtral의 퍼플렉시티를 보여줍니다. 문맥 길이가 증가함에 따라 퍼플렉시티가 단조 감소하는 것을 확인할 수 있습니다. 이는 모델이 더 긴 문맥을 활용할수록 더 정확한 예측을 할 수 있다는 것을 의미합니다.

이러한 결과는 Mixtral이 32k 토큰의 긴 문맥을 효과적으로 처리할 수 있는 능력을 갖추고 있음을 보여줍니다. 특히 패스키 검색 작업에서의 완벽한 성능은 모델이 긴 시퀀스 내에서 특정 정보를 정확하게 식별하고 추출할 수 있는 강력한 능력을 가지고 있음을 시사합니다.

### Mixtral의 편향성 평가

![편향성 벤치마크](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/images/perplexity.png)

Mixtral의 편향성을 평가하기 위해 BBQ(Bias Benchmark for Question Answering)와 BOLD 감성 점수를 사용한 분석이 수행되었습니다. BBQ 정확도에서 Mixtral은 56.0%를 기록하여 Llama 2 70B의 51.5%보다 우수한 성능을 보여주었습니다.

BOLD 감성 점수 분석에서는 성별, 직업, 종교적 이념, 정치적 이념, 인종 등 다양한 속성에 대한 편향성이 평가되었습니다. Mixtral은 대부분의 카테고리에서 Llama 2 70B보다 낮은 표준편차를 보여주었으며, 이는 더 일관된 응답을 생성한다는 것을 의미합니다. 특히 종교적 이념(0.144 ± 0.089)과 인종(0.232 ± 0.052) 관련 응답에서 안정적인 성능을 보여주었습니다.

이러한 결과는 Mixtral이 사회적 편향을 줄이는 데 성공적이었음을 보여줍니다. 특히 BBQ에서의 높은 정확도와 BOLD 평가에서의 낮은 표준편차는 모델이 더 공정하고 균형 잡힌 응답을 생성할 수 있다는 것을 시사합니다.

### Mixtral의 편향성 평가 결과

Mixtral 모델의 편향성을 평가하기 위해 연구진은 BBQ(Bias Benchmark for Question Answering)와 BOLD(Bias in Open-Ended Language Generation Dataset) 데이터셋을 활용했습니다. BBQ는 연령, 장애 상태, 성 정체성, 국적, 외모, 인종/민족, 종교, 사회경제적 지위, 성적 지향과 같은 9가지 사회적으로 중요한 범주에 대한 편향을 측정하기 위해 수작업으로 작성된 질문 세트로 구성되어 있습니다.

BOLD는 23,679개의 영어 텍스트 생성 프롬프트로 구성된 대규모 데이터셋으로, 5개 도메인에 걸쳐 편향성을 벤치마킹할 수 있도록 설계되었습니다. 연구진은 자체 평가 프레임워크를 통해 Llama 2와 Mixtral을 BBQ와 BOLD에서 평가했습니다. BBQ 벤치마크에서 Mixtral은 Llama 2보다 낮은 편향성을 보여주었습니다(56.0% 대 51.5%).

BOLD의 각 그룹에 대해서는 평균 감성 점수와 표준편차를 측정했는데, 높은 평균 감성 점수는 더 긍정적인 감정을, 낮은 표준편차는 그룹 내 편향이 적음을 의미합니다. 전반적으로 Mixtral은 Llama 2와 비교했을 때 더 긍정적인 감성을 보여주었으며, 각 그룹 내에서 유사한 수준의 분산을 나타냈습니다.

이러한 결과는 Mixtral이 사회적 편향성 측면에서 Llama 2보다 개선된 성능을 보여주고 있음을 시사합니다. 특히 BBQ에서의 향상된 성능은 모델이 다양한 사회적 범주에 대해 더 공정한 응답을 생성할 수 있다는 것을 보여줍니다. BOLD 평가에서 나타난 긍정적인 감성 경향과 일관된 분산은 모델이 다양한 그룹에 대해 균형 잡힌 표현을 생성하고 있음을 나타냅니다.

### Mixtral의 지시어 미세조정 과정

Mixtral 연구진은 모델의 지시어 수행 능력을 향상시키기 위해 두 단계의 미세조정 과정을 진행했습니다. 첫 번째 단계에서는 지시어 데이터셋을 사용한 지도 학습 미세조정(Supervised Fine-tuning, SFT)을 수행했으며, 두 번째 단계에서는 직접 선호도 최적화(Direct Preference Optimization, DPO)를 적용했습니다.

DPO는 Rafailov와 연구진이 제안한 기법으로, 기존의 강화학습 기반 인간 피드백(RLHF) 방식과 달리 보상 모델 없이도 인간의 선호도를 직접 학습할 수 있는 방법입니다. DPO는 언어 모델의 정책을 인간 선호도 데이터를 만족시키도록 직접 최적화하며, 이는 수학적으로 다음과 같은 KL 제약 보상 최대화 목적함수와 동일합니다.

$$ r(x, y) = \beta \log \frac{\pi(y \vert x)}{\pi_{\text{ref}}(y \vert x)} $$

여기서 $$ \pi(y \vert x) $$ 는 최적화 대상인 언어 모델 정책이며, $$ {\pi}_{ref}(y \vert x) $$는 참조 정책입니다.

이러한 미세조정 과정을 거친 Mixtral-Instruct는 MT-Bench에서 8.30점을 기록하며 2023년 12월 기준 공개 가중치 모델 중 최고 성능을 달성했습니다. LMSys가 수행한 독립적인 인간 평가에서도 Mixtral-Instruct는 GPT-3.5-Turbo, Gemini Pro, Claude-2.1, Llama 2 70B 채팅 모델을 능가하는 성과를 보여주었습니다.

![LMSys 리더보드](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/images/lmsys_231222.png)

LMSys 리더보드에서 Mixtral 8x7B Instruct v0.1은 1121의 Arena Elo 점수를 기록했습니다. 이는 Claude-2.1(1117), GPT-3.5-Turbo의 모든 버전(최고 1117), Gemini Pro(1111), Llama-2-70b-chat(1077)을 크게 앞서는 수치입니다. 특히 공개 가중치 모델 중에서는 가장 높은 성능을 보여주었다는 점에서 큰 의미가 있습니다.

이러한 성과는 Mixtral의 희소 전문가 혼합 아키텍처가 지시어 수행 과제에서도 효과적으로 작동함을 입증합니다. 각 토큰마다 8개의 전문가 중 2개를 동적으로 선택하는 방식이 다양한 유형의 지시어를 효과적으로 처리할 수 있게 해주며, 이는 기존의 대규모 언어 모델들과 비교했을 때도 경쟁력 있는 성능을 달성할 수 있게 했습니다.

### 라우팅 분석 결과

Mixtral 모델의 전문가 선택 메커니즘에 대한 심층 분석을 수행한 결과, 흥미로운 패턴들이 발견되었습니다. 연구진은 The Pile 검증 데이터셋의 다양한 하위 집합에서 전문가 선택의 분포를 측정했으며, 특히 모델의 첫 번째 층(0), 중간 층(15), 마지막 층(31)에서의 패턴을 중점적으로 분석했습니다.

![전문가 선택 분포](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/x1.png)

위 그래프는 각 도메인별로 토큰이 전문가들에게 할당되는 비율을 보여줍니다. 놀랍게도 ArXiv 논문(LaTeX로 작성), PubMed 초록(생물학), PhilPapers(철학) 문서에서 전문가 할당 분포가 매우 유사한 패턴을 보였습니다. 유일하게 DM Mathematics에서만 약간 다른 분포가 관찰되었는데, 이는 이 데이터셋의 합성적 특성과 제한된 자연어 스펙트럼 때문인 것으로 분석됩니다.

![텍스트 샘플의 전문가 할당](https://ar5iv.labs.arxiv.org//html/2401.04088/assets/images/routing-sample.png)

위 시각화는 파이썬 코드, 수학, 영어 등 다양한 도메인의 텍스트에서 각 토큰이 어떤 전문가에게 할당되는지를 보여줍니다. 파이썬의 'self'나 영어의 'Question'과 같은 단어들은 여러 토큰으로 구성되어 있음에도 동일한 전문가를 통해 처리되는 경향을 보였습니다. 특히 코드에서는 들여쓰기 토큰들이 항상 같은 전문가들에게 할당되었으며, 이러한 패턴은 히든 스테이트가 입출력과 더 밀접하게 연관된 첫 번째와 마지막 층에서 더욱 두드러졌습니다.

연속된 토큰들의 전문가 할당 패턴을 분석한 결과, The Pile 데이터셋에서 상당한 수준의 위치적 지역성이 관찰되었습니다. 특히 상위 층에서는 연속된 토큰들이 동일한 전문가에게 할당되는 비율이 무작위 할당 대비 현저히 높았습니다. 이러한 발견은 모델의 학습과 추론 최적화에 중요한 시사점을 제공합니다. 예를 들어, 높은 지역성을 보이는 경우 전문가 병렬화 과정에서 특정 전문가에 과도한 부하가 집중될 수 있습니다. 반면, 이러한 지역성은 캐싱을 통한 성능 최적화에 활용될 수 있습니다.

### Mixtral 8x7B의 결론과 의의

Mixtral 8x7B는 공개 소스 모델 중 최초로 최고 수준의 성능을 달성한 희소 전문가 혼합 네트워크입니다. 특히 Mixtral 8x7B Instruct 모델은 Claude-2.1, Gemini Pro, GPT-3.5 Turbo와 같은 최신 상용 모델들을 인간 평가 벤치마크에서 능가하는 성과를 보여주었습니다.

이 모델의 가장 주목할 만한 특징은 각 타임스텝에서 단 2개의 전문가만을 활용하는 효율적인 아키텍처입니다. 이를 통해 토큰당 13B 개의 활성 파라미터만으로도 토큰당 70B 개의 파라미터를 사용하는 이전 최고 모델인 Llama 2 70B보다 우수한 성능을 달성했습니다. 이는 희소 전문가 혼합 아키텍처의 효율성과 확장성을 입증하는 중요한 성과입니다.

연구진은 사전 학습된 모델과 미세조정된 모델 모두를 Apache 2.0 라이선스로 공개했습니다. 이러한 공개는 새로운 기술과 응용 프로그램의 개발을 촉진하고, 다양한 산업과 도메인에서 활용될 수 있는 기회를 제공할 것으로 기대됩니다.

모델 개발 과정에서 CoreWeave와 Scaleway 팀은 모델 학습을 위한 기술적 지원을 제공했으며, NVIDIA는 TensorRT-LLM과 Triton의 통합을 지원하고 희소 전문가 혼합 아키텍처를 TensorRT-LLM과 호환되도록 만드는 데 협력했습니다. 이러한 협력은 대규모 언어 모델의 효율적인 학습과 배포를 가능하게 하는 중요한 기술적 진보를 이끌어냈습니다.

Mixtral 8x7B의 성공은 희소 전문가 혼합 아키텍처가 대규모 언어 모델의 미래를 이끌어갈 수 있는 유망한 접근 방식임을 보여줍니다. 특히 계산 효율성과 성능 사이의 최적의 균형을 달성했다는 점에서, 향후 언어 모델 발전의 새로운 방향을 제시했다고 할 수 있습니다.

- - -
### References
* [Mixtral of Experts](http://arxiv.org/pdf/2401.04088v1)
