---
layout: post
title: "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"
date: 2024-01-11 17:31:42
author: "DeepSeek-AI"
categories: "Language-Models"
tags: ["Ultimate-Expert-Specialization", "Fine-Grained-Expert-Segmentation", "Shared-Expert-Isolation", "Mixture-of-Experts-Architecture", "Parameter-Efficient-Language-Model-Scaling", "Sparse-Mixture-of-Experts", "Efficient-Transformer-Architecture", "Economical-Training", "Multi-Level-Load-Balancing", "Efficient-Inference"]
cover: /assets/images/language-models.jpg
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
대규모 언어 모델의 발전은 파라미터 수와 계산 자원의 증가에 크게 의존해왔으나, 이는 막대한 계산 비용을 수반합니다. 혼합 전문가(Mixture-of-Experts, MoE) 아키텍처는 이러한 문제를 해결할 수 있는 유망한 접근법으로 주목받았지만, 기존 MoE 모델들은 지식 혼재성과 지식 중복성이라는 두 가지 주요 한계에 직면해 있었습니다. 특히 제한된 수의 전문가로 인해 각 전문가가 다양한 종류의 지식을 동시에 처리해야 하는 문제와, 여러 전문가들이 동일한 공통 지식을 중복 저장하는 비효율성이 존재했습니다. 이러한 한계를 극복하고 MoE 모델의 잠재력을 최대한 끌어올리기 위한 새로운 접근법이 필요했습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
DeepSeekMoE는 두 가지 핵심 전략을 통해 이러한 문제들을 해결합니다. 첫째, 세밀한 전문가 분할(Fine-Grained Expert Segmentation) 전략을 도입하여 전문가들을 더 작은 단위로 세분화하고, 이를 통해 더욱 유연한 전문가 조합을 가능하게 했습니다. 둘째, 공유 전문가 분리(Shared Expert Isolation) 전략을 통해 특정 전문가들을 공유 전문가로 지정하여 공통 지식을 효율적으로 처리하도록 했습니다. 이러한 혁신적인 접근법은 각 전문가가 더 특화된 지식을 학습하고 활용할 수 있게 하면서도 파라미터 효율성을 높일 수 있게 합니다.

#### 제안된 방법은 어떻게 구현되었습니까?
DeepSeekMoE의 구현은 트랜스포머 아키텍처를 기반으로 하되, FFN을 MoE 층으로 대체하는 방식을 채택했습니다. 16B 모델의 경우, 28개의 트랜스포머 층과 2048의 은닉 차원을 사용했으며, 각 MoE 층은 2개의 공유 전문가와 64개의 라우팅된 전문가로 구성됩니다. 모델은 2T 토큰 규모의 다국어 말뭉치에서 학습되었으며, AdamW 옵티마이저와 웜업-스텝-감소 학습률 스케줄링을 사용했습니다. 특히 부하 균형을 위해 전문가 수준과 디바이스 수준의 균형 손실을 도입하여 효율적인 학습이 가능하도록 했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
DeepSeekMoE는 기존 MoE 모델들의 한계를 극복하고 새로운 성능 기준을 제시했습니다. 특히 16B 모델은 LLaMA2 7B와 비교하여 약 40%의 계산량만으로도 대등한 성능을 달성했으며, 145B 규모로의 확장 실험에서도 우수한 확장성을 입증했습니다. 이는 대규모 언어 모델의 효율적인 학습과 배포가 가능함을 보여주는 중요한 성과입니다. 또한 모델 체크포인트를 공개함으로써 연구 커뮤니티에 기여했으며, 이는 향후 MoE 기반 언어 모델 연구의 새로운 지평을 열 것으로 기대됩니다.
- - -
## DeepSeekMoE: 혼합 전문가 언어 모델에서의 궁극적 전문가 특화를 향하여

DeepSeek-AI에서 개발된 DeepSeekMoE는 대규모 언어 모델의 새로운 지평을 여는 혁신적인 혼합 전문가(Mixture-of-Experts, MoE) 시스템입니다. 이 연구는 DeepSeek-AI의 연구진들과 중국의 주요 연구기관들이 협력하여 진행한 결과물로, 특히 북경대학교의 멀티미디어 정보처리 국가중점연구소, 칭화대학교의 학제간 정보과학 연구소, 그리고 난징대학교의 소프트웨어 기술 국가중점연구소의 전문가들이 참여했습니다.

이 연구는 언어 모델에서 전문가 특화(expert specialization)의 개념을 한 단계 발전시키는 것을 목표로 합니다. MoE 아키텍처는 여러 전문가 모듈들이 각각 특정 입력 패턴을 처리하도록 설계되어 있는데, DeepSeekMoE는 이러한 전문가 특화를 더욱 효과적으로 구현하는 방법을 제시합니다.

연구진은 DeepSeek-AI의 인턴십 프로그램을 통해 이 혁신적인 모델을 개발했으며, 모델의 소스 코드는 GitHub를 통해 공개되어 있습니다. 이는 연구의 투명성을 보장하고 다른 연구자들이 이 기술을 활용할 수 있도록 하는 중요한 기여입니다.

이 논문은 MoE 언어 모델의 발전에 있어 중요한 이정표가 될 것으로 기대됩니다. 특히 전문가 모듈의 특화도를 극대화하는 새로운 접근 방식을 통해, 언어 모델의 성능과 효율성을 동시에 향상시키는 방법을 제시합니다.

### 대규모 언어 모델의 새로운 지평: DeepSeekMoE의 혁신적 접근

대규모 언어 모델의 시대에서 혼합 전문가(Mixture-of-Experts, MoE) 아키텍처는 모델 파라미터를 확장하면서도 계산 비용을 효율적으로 관리할 수 있는 유망한 접근 방식으로 주목받고 있습니다. 그러나 GShard와 같은 기존의 MoE 아키텍처는 전체 $N$개의 전문가 중 상위 $K$개를 활성화하는 방식을 사용하면서, 전문가 특화(expert specialization)를 달성하는 데 어려움을 겪어왔습니다. 여기서 전문가 특화란 각 전문가가 서로 중복되지 않는 집중된 지식을 획득하는 것을 의미합니다.

![DeepSeekMoE와 오픈소스 모델 비교](https://ar5iv.labs.arxiv.org//html/2401.06066/assets/x1.png)

이러한 문제를 해결하기 위해 연구진은 궁극적인 전문가 특화를 목표로 하는 DeepSeekMoE 아키텍처를 제안했습니다. 이 아키텍처는 두 가지 핵심 전략을 도입합니다. 첫째, 전문가들을 $mN$개로 세분화하고 이 중 $mK$개를 활성화함으로써 활성화된 전문가들의 더욱 유연한 조합을 가능하게 합니다. 둘째, $K_s$개의 전문가를 공유 전문가로 분리하여 공통 지식을 포착하고 라우팅된 전문가들 간의 중복성을 줄이는 것을 목표로 합니다.

2B 파라미터 규모의 초기 실험에서 DeepSeekMoE 2B는 1.5배 많은 전문가 파라미터와 계산량을 가진 GShard 2.9B와 대등한 성능을 달성했습니다. 더욱 주목할 만한 점은 DeepSeekMoE 2B가 동일한 파라미터 수를 가진 밀집 모델의 성능에 거의 근접했다는 것입니다. 이는 MoE 모델이 달성할 수 있는 상한선을 보여줍니다.

이후 연구진은 DeepSeekMoE를 16B 파라미터로 확장했고, 약 40%의 계산량만으로도 LLaMA2 7B와 대등한 성능을 달성했습니다. 더 나아가 145B 파라미터로의 예비 확장 실험에서도 GShard 아키텍처에 비해 상당한 이점을 보였으며, DeepSeek 67B와 비교했을 때 단 28.5%(혹은 18.2%)의 계산량으로 대등한 성능을 보여주었습니다.
### DeepSeekMoE의 혁신적 기술 구조와 연구 배경

대규모 언어 모델의 발전은 파라미터 수와 계산 자원의 증가에 크게 의존해왔습니다. Brown과 연구진, OpenAI, Touvron과 연구진, 그리고 Hoffmann과 연구진의 연구들은 충분한 학습 데이터가 주어졌을 때 모델의 파라미터와 계산 자원을 늘리면 현저히 강력한 모델을 만들 수 있다는 것을 실증적으로 보여주었습니다. 하지만 이러한 확장은 엄청난 계산 비용을 수반합니다.

이러한 맥락에서 혼합 전문가(Mixture-of-Experts, MoE) 아키텍처가 주목받게 되었습니다. Jacobs와 연구진이 1991년에 처음 제안한 이 접근법은 Jordan과 Jacobs, 그리고 Shazeer와 연구진에 의해 발전되어왔습니다. MoE는 파라미터 수를 늘리면서도 계산 비용을 적정 수준으로 유지할 수 있는 해결책을 제시합니다. 특히 트랜스포머 아키텍처에 MoE를 적용한 최근의 연구들은 대규모 언어 모델의 성공적인 확장 가능성을 보여주었습니다.

그러나 기존 MoE 아키텍처들은 지식 혼재성(Knowledge Hybridity)과 지식 중복성(Knowledge Redundancy)이라는 두 가지 주요 문제에 직면해 있습니다. 기존 MoE 아키텍처는 트랜스포머의 피드포워드 네트워크(FFN)를 MoE 층으로 대체하며, 각 MoE 층은 여러 전문가로 구성됩니다. 각 전문가는 구조적으로 표준 FFN과 동일하며, 각 토큰은 하나 또는 두 개의 전문가에 할당됩니다.

지식 혼재성 문제는 기존 MoE가 제한된 수의 전문가(예: 8개 또는 16개)를 사용하기 때문에 발생합니다. 특정 전문가에 할당된 토큰들이 다양한 종류의 지식을 포함하게 되어, 해당 전문가는 매우 다른 유형의 지식을 동시에 처리해야 하는 어려움에 직면합니다. 지식 중복성 문제는 서로 다른 전문가에 할당된 토큰들이 공통 지식을 필요로 할 때 발생합니다. 이로 인해 여러 전문가들이 동일한 공통 지식을 각자의 파라미터에 중복해서 저장하게 됩니다.

이러한 문제들은 MoE 모델이 이론적으로 달성할 수 있는 최대 성능에 도달하는 것을 방해합니다. DeepSeekMoE는 이러한 한계를 극복하기 위해 세밀한 전문가 분할과 공유 전문가 분리라는 혁신적인 전략을 도입했습니다. 이를 통해 각 전문가가 더욱 특화된 지식을 효율적으로 학습하고 활용할 수 있게 되었습니다.
### DeepSeekMoE의 주요 기술적 혁신과 기여

DeepSeekMoE의 핵심 기술적 혁신은 세밀한 전문가 분할(Fine-Grained Expert Segmentation)과 공유 전문가 분리(Shared Expert Isolation)에 있습니다. 세밀한 전문가 분할은 FFN의 중간 은닉 차원을 분할하여 전문가들을 더 작은 단위로 세분화합니다. 이를 통해 다양한 지식을 더욱 정밀하게 분해하고 각각의 전문가가 더 높은 수준의 특화된 지식을 학습할 수 있게 됩니다. 또한, 활성화되는 전문가들의 조합을 더욱 유연하게 만들어 더 정확하고 목표 지향적인 지식 획득이 가능해집니다.

공유 전문가 분리 전략은 특정 전문가들을 항상 활성화되는 공유 전문가로 지정하여 다양한 맥락에서 공통적으로 필요한 지식을 통합적으로 처리하도록 합니다. 이러한 공통 지식을 공유 전문가들에게 압축함으로써 다른 라우팅된 전문가들 간의 중복성이 줄어들게 됩니다. 이는 파라미터 효율성을 높이고 각 라우팅된 전문가가 고유한 특성에 집중할 수 있도록 합니다.

이러한 아키텍처 혁신을 통해 DeepSeekMoE는 각 전문가가 고도로 특화된 파라미터 효율적인 MoE 언어 모델을 학습할 수 있는 기회를 제공합니다. 연구진은 12개의 제로샷 또는 퓨샷 벤치마크에서 광범위한 평가를 수행했으며, 실험 결과는 DeepSeekMoE 2B가 GShard 2B를 상당한 차이로 능가하고 1.5배 더 많은 전문가 파라미터와 계산량을 가진 GShard 2.9B와 대등한 성능을 보여주었습니다.

특히 주목할 만한 점은 DeepSeekMoE 2B가 동일한 파라미터 수를 가진 밀집 모델의 성능에 거의 근접했다는 것입니다. 이는 MoE 언어 모델이 달성할 수 있는 엄격한 상한선을 설정합니다. 연구진은 전문가 특화에 대한 상세한 분석과 실험을 통해 세밀한 전문가 분할과 공유 전문가 분리의 효과성을 입증했으며, DeepSeekMoE가 높은 수준의 전문가 특화를 달성할 수 있다는 실증적 증거를 제시했습니다.
### DeepSeekMoE의 확장성과 실용적 적용

DeepSeekMoE의 성공적인 초기 실험을 바탕으로 연구진은 모델을 16B 파라미터 규모로 확장하고 2T 토큰 규모의 대규모 말뭉치에서 학습을 진행했습니다. 평가 결과에 따르면 DeepSeekMoE 16B는 약 40%의 계산량만으로도 동일한 2T 말뭉치에서 학습된 밀집 모델인 DeepSeek 7B와 대등한 성능을 달성했습니다.

오픈 소스 모델들과의 비교 평가에서 DeepSeekMoE 16B는 유사한 수준의 활성화된 파라미터를 가진 다른 모델들을 큰 차이로 능가했으며, 약 2.5배 많은 활성화된 파라미터를 가진 LLaMA2 7B와 대등한 성능을 보여주었습니다. 이러한 결과는 DeepSeekMoE의 아키텍처가 대규모 언어 모델 개발에 있어 매우 효율적이고 성능이 우수하다는 것을 입증합니다.

연구진은 더 나아가 지도 학습 미세조정(supervised fine-tuning)을 통해 모델을 대화형 모델로 변환하는 데도 성공했습니다. 평가 결과에 따르면 DeepSeekMoE Chat 16B는 대화 환경에서도 DeepSeek Chat 7B와 LLaMA2 SFT 7B와 대등한 성능을 보여주었습니다.

이러한 성공적인 결과에 고무된 연구진은 DeepSeekMoE를 145B 파라미터 규모로 확장하는 예비 실험을 진행했습니다. 실험 결과는 GShard 아키텍처에 비해 지속적으로 상당한 이점이 있음을 보여주었으며, DeepSeek 67B와 비교했을 때 단 28.5%(혹은 18.2%)의 계산량으로도 대등한 성능을 달성할 수 있음을 입증했습니다.

연구진은 이러한 연구 성과를 오픈 리서치의 정신에 입각하여 DeepSeekMoE 16B의 모델 체크포인트를 공개했습니다. 특히 주목할 만한 점은 이 모델이 양자화 없이도 40GB 메모리를 가진 단일 GPU에서 배포될 수 있다는 것입니다. 이는 DeepSeekMoE가 실용적인 환경에서도 효율적으로 활용될 수 있음을 보여줍니다.

### 트랜스포머 기반 MoE 아키텍처의 수학적 기초

트랜스포머 언어 모델은 $$L$$ 개의 표준 트랜스포머 블록을 쌓아 구성됩니다. 각 블록은 셀프 어텐션 모듈과 피드포워드 네트워크로 이루어져 있으며, 다음과 같은 수식으로 표현됩니다.

$$\mathbf{u}_{1:T}^{l} = \operatorname{Self-Att}(\mathbf{h}_{1:T}^{l-1}) + \mathbf{h}_{1:T}^{l-1}$$

$$\mathbf{h}_{t}^{l} = \operatorname{FFN}(\mathbf{u}_{t}^{l}) + \mathbf{u}_{t}^{l}$$

여기서 $$T$$는 시퀀스 길이를 나타내며, $$\operatorname{Self-Att}(\cdot)$$은 셀프 어텐션 모듈을, $$\operatorname{FFN}(\cdot)$$은 피드포워드 네트워크를 의미합니다. $$\mathbf{u}_{1:T}^{l} \in \mathbb{R}^{T \times d}$$는 $$l$$번째 어텐션 모듈 이후의 모든 토큰에 대한 은닉 상태를 나타내고, $$\mathbf{h}_{t}^{l} \in \mathbb{R}^{d}$$는 $$l$$번째 트랜스포머 블록 이후 $$t$$번째 토큰의 출력 은닉 상태를 나타냅니다.

![DeepSeekMoE 아키텍처](https://ar5iv.labs.arxiv.org//html/2401.06066/assets/x2.png)

MoE 언어 모델을 구성할 때는 일반적으로 트랜스포머의 FFN을 특정 간격으로 MoE 층으로 대체합니다. MoE 층은 여러 전문가로 구성되며, 각 전문가는 구조적으로 표준 FFN과 동일합니다. 각 토큰은 하나 또는 두 개의 전문가에 할당되어 처리됩니다. $$l$$번째 FFN이 MoE 층으로 대체될 때, 출력 은닉 상태 $$\mathbf{h}_{t}^{l}$$는 다음과 같이 계산됩니다.

$$\mathbf{h}_{t}^{l} = \sum_{i=1}^{N}(g_{i,t}\operatorname{FFN}_{i}(\mathbf{u}_{t}^{l})) + \mathbf{u}_{t}^{l}$$

$$g_{i,t} = \begin{cases}
s_{i,t}, & s_{i,t} \in \operatorname{Topk}(\{s_{j,t}|1 \leq j \leq N\}, K),\\
0, & \text{otherwise},
\end{cases}$$

$$s_{i,t} = \operatorname{Softmax}_{i}({\mathbf{u}_{t}^{l}}^{T}\mathbf{e}_{i}^{l})$$

여기서 $$N$$은 전체 전문가 수, $$\operatorname{FFN}_{i}(\cdot)$$는 $$i$$번째 전문가 FFN, $$g_{i,t}$$는 $$i$$번째 전문가에 대한 게이트 값, $$s_{i,t}$$는 토큰-전문가 간 친화도, $$\operatorname{Topk}(\cdot,K)$$는 $$t$$번째 토큰과 모든 $$N$$개 전문가 간에 계산된 친화도 점수 중 상위 $$K$$개의 점수를 포함하는 집합, $$\mathbf{e}_{i}^{l}$$은 $$l$$번째 층의 $$i$$번째 전문가의 중심점을 나타냅니다.

### DeepSeekMoE의 혁신적 아키텍처 설계

DeepSeekMoE는 전문가 특화를 극대화하기 위해 설계된 혁신적인 MoE 아키텍처입니다. 이 아키텍처는 세밀한 전문가 분할과 공유 전문가 분리라는 두 가지 핵심 전략을 도입하여 전문가 특화의 수준을 한 단계 높였습니다.

#### 세밀한 전문가 분할 전략

전문가의 수가 제한적일 때, 특정 전문가에 할당된 토큰들은 다양한 유형의 지식을 포함하게 됩니다. 이로 인해 해당 전문가는 매우 다른 종류의 지식을 동시에 학습해야 하는 상황에 직면하게 되며, 이는 효율적인 파라미터 활용을 저해합니다. 하지만 각 토큰이 더 많은 전문가에게 라우팅될 수 있다면, 다양한 지식은 서로 다른 전문가들에게 분해되어 학습될 수 있는 잠재력을 갖게 됩니다.

이러한 목표를 달성하기 위해, DeepSeekMoE는 전체 전문가 파라미터 수와 계산 비용을 일정하게 유지하면서 전문가들을 더 세밀한 단위로 분할합니다. 구체적으로, 각 전문가 FFN의 중간 은닉 차원을 원래 크기의 $$\frac{1}{m}$$배로 줄이고, 대신 활성화되는 전문가의 수를 $$m$$배로 증가시킵니다. 이러한 세밀한 전문가 분할을 통해 MoE 층의 출력은 다음과 같이 표현됩니다.

$$\mathbf{h}_{t}^{l} = \sum_{i=1}^{mN}(g_{i,t}\operatorname{FFN}_{i}(\mathbf{u}_{t}^{l})) + \mathbf{u}_{t}^{l}$$

여기서 게이팅 가중치 $$g_{i,t}$$는 다음과 같이 계산됩니다.

$$g_{i,t} = \begin{cases}
s_{i,t}, & s_{i,t} \in \operatorname{Topk}(\{s_{j,t}|1 \leqslant j \leqslant mN\}, mK),\\
0, & \text{otherwise},
\end{cases}$$

그리고 소프트맥스 게이트 $$s_{i,t}$$는 다음과 같이 정의됩니다.

$$s_{i,t} = \operatorname{Softmax}_{i}({\mathbf{u}_{t}^{l}}^{T}\mathbf{e}_{i}^{l})$$

이러한 세밀한 전문가 분할 전략은 활성화되는 전문가들의 조합 가능성을 크게 향상시킵니다. 예를 들어, $$N=16$$인 경우를 고려해보면, 일반적인 top-2 라우팅 전략은 $$\binom{16}{2}=120$$개의 조합만을 생성할 수 있습니다. 반면, 각 전문가를 4개의 더 작은 전문가로 분할하면, 세밀한 라우팅 전략은 $$\binom{64}{8}=4,426,165,368$$개의 잠재적 조합을 생성할 수 있습니다.
#### 공유 전문가 분리 전략

기존의 라우팅 전략에서는 서로 다른 전문가에 할당된 토큰들이 일부 공통 지식이나 정보를 필요로 할 수 있습니다. 이로 인해 여러 전문가들이 각자의 파라미터에 동일한 공통 지식을 중복해서 획득하게 되어 파라미터 중복성이 발생합니다. 하지만 다양한 맥락에서 공통 지식을 포착하고 통합하는 데 전념하는 공유 전문가들이 있다면, 다른 라우팅된 전문가들 간의 파라미터 중복성이 완화될 것입니다.

이러한 목표를 달성하기 위해 DeepSeekMoE는 세밀한 전문가 분할 전략에 더해 $$K_s$$개의 전문가를 공유 전문가로 분리합니다. 라우터 모듈과 관계없이 모든 토큰은 이러한 공유 전문가들에 결정론적으로 할당됩니다. 일정한 계산 비용을 유지하기 위해 다른 라우팅된 전문가들 중에서 활성화되는 전문가의 수는 $$K_s$$만큼 감소됩니다.

공유 전문가 분리 전략이 통합된 완전한 DeepSeekMoE 아키텍처에서 MoE 층은 다음과 같이 수식화됩니다.

$$\mathbf{h}_{t}^{l} = \sum_{i=1}^{K_s}{\operatorname{FFN}_{i}(\mathbf{u}_{t}^{l})} + \sum_{i=K_s+1}^{mN}(g_{i,t}\operatorname{FFN}_{i}(\mathbf{u}_{t}^{l})) + \mathbf{u}_{t}^{l}$$

여기서 게이팅 가중치 $$g_{i,t}$$는 다음과 같이 계산됩니다.

$$g_{i,t} = \begin{cases}
s_{i,t}, & s_{i,t} \in \operatorname{Topk}(\{s_{j,t}|K_{s}+1 \leqslant j \leqslant mN\}, mK-K_{s}),\\
0, & \text{otherwise},
\end{cases}$$

그리고 소프트맥스 게이트 $$s_{i,t}$$는 다음과 같이 정의됩니다.

$$s_{i,t} = \operatorname{Softmax}_{i}({\mathbf{u}_{t}^{l}}^{T}\mathbf{e}_{i}^{l})$$

최종적으로 DeepSeekMoE에서는 공유 전문가의 수가 $$K_s$$이고, 라우팅된 전문가의 총 수는 $$mN-K_s$$이며, 0이 아닌 게이트의 수는 $$mK-K_s$$입니다. 공유 전문가 분리의 원형은 Rajbhandari와 연구진의 연구에서 찾을 수 있습니다. 주요한 차이점은 그들이 엔지니어링 관점에서 이 전략을 도출한 반면, DeepSeekMoE는 알고리즘적 관점에서 접근했다는 점입니다.
#### 부하 균형 고려사항

자동으로 학습되는 라우팅 전략은 부하 불균형 문제에 직면할 수 있으며, 이는 두 가지 주목할 만한 결함을 초래합니다. 첫째, 라우팅 붕괴(routing collapse)의 위험이 있습니다. 이는 모델이 항상 소수의 전문가만을 선택하여 다른 전문가들이 충분한 학습을 받지 못하는 현상을 의미합니다. 둘째, 전문가들이 여러 디바이스에 분산되어 있을 경우, 부하 불균형은 계산 병목 현상을 악화시킬 수 있습니다.

#### 전문가 수준 균형 손실

라우팅 붕괴의 위험을 완화하기 위해 DeepSeekMoE는 전문가 수준 균형 손실을 도입합니다. 균형 손실의 계산은 다음과 같습니다.

$$\mathcal{L}_{\mathrm{ExpBal}} = \alpha_{1}\sum_{i=1}^{N^{\prime}}{f_{i}P_{i}}$$

여기서 $$f_i$$는 다음과 같이 계산됩니다.

$$f_{i} = \frac{N^{\prime}}{K^{\prime}T}\sum_{t=1}^{T}{\mathbb{1}(\text{Token $t$ selects Expert $i$})}$$

그리고 $$P_i$$는 다음과 같이 정의됩니다.

$$P_{i} = \frac{1}{T}\sum_{t=1}^{T}{s_{i,t}}$$

여기서 $$\alpha_1$$은 전문가 수준 균형 인자라고 불리는 하이퍼파라미터이며, $$N^{\prime}$$은 $$(mN-K_{s})$$와 같고 $$K^{\prime}$$은 $$(mK-K_{s})$$와 같습니다. $$\mathbb{1}(\cdot)$$은 지시 함수를 나타냅니다.

#### 디바이스 수준 균형 손실

전문가 수준 균형 손실에 더해 DeepSeekMoE는 디바이스 수준 균형 손실을 도입합니다. 계산 병목 현상을 완화하는 것이 목표일 때는 전문가 수준에서 엄격한 균형 제약을 강제할 필요가 없습니다. 과도한 균형 제약은 모델 성능을 저하시킬 수 있기 때문입니다. 대신 주요 목표는 디바이스 간의 균형 잡힌 계산을 보장하는 것입니다.

모든 라우팅된 전문가들을 $$D$$개의 그룹 $$\{\mathcal{E}_{1},\mathcal{E}_{2},...,\mathcal{E}_{D}\}$$로 분할하고 각 그룹을 하나의 디바이스에 배치한다면, 디바이스 수준 균형 손실은 다음과 같이 계산됩니다.

$$\mathcal{L}_{\mathrm{DevBal}} = \alpha_{2}\sum_{i=1}^{D}{f_{i}^{\prime}P_{i}^{\prime}}$$

여기서 $$f_i^{\prime}$$와 $$P_i^{\prime}$$는 각각 다음과 같이 계산됩니다.

$$f_{i}^{\prime} = \frac{1}{|\mathcal{E}_{i}|}\sum_{j\in\mathcal{E}_{i}}{f_{j}}$$

$$P_{i}^{\prime} = \sum_{j\in\mathcal{E}_{i}}{P_{j}}$$

여기서 $$\alpha_2$$는 디바이스 수준 균형 인자라고 불리는 하이퍼파라미터입니다. 실제 구현에서는 라우팅 붕괴의 위험을 완화하기 위해 작은 전문가 수준 균형 인자를 설정하고, 동시에 디바이스 간의 균형 잡힌 계산을 촉진하기 위해 더 큰 디바이스 수준 균형 인자를 설정합니다.
### 검증 실험 설정과 구현

#### 학습 데이터와 토크나이제이션

DeepSeekMoE의 학습 데이터는 DeepSeek-AI가 구축한 대규모 다국어 말뭉치에서 추출되었습니다. 이 말뭉치는 영어와 중국어를 중심으로 하되 다른 언어들도 포함하며, 웹 텍스트, 수학 자료, 코딩 스크립트, 출판된 문헌 등 다양한 소스로 구성되어 있습니다. 검증 실험을 위해 연구진은 이 말뭉치에서 100B 토큰을 샘플링하여 모델 학습에 사용했습니다.

토크나이제이션을 위해 연구진은 HuggingFace Tokenizer 도구를 사용하여 학습 말뭉치의 부분집합에서 바이트 페어 인코딩(BPE) 토크나이저를 학습했습니다. 검증 실험에서는 8K 크기의 어휘를 사용했으며, 더 큰 모델을 학습할 때는 어휘 크기를 확장했습니다.

#### 인프라스트럭처

실험은 HAI-LLM이라는 효율적이고 경량화된 학습 프레임워크를 기반으로 수행되었습니다. 이 프레임워크는 텐서 병렬화, ZeRO 데이터 병렬화, PipeDream 파이프라인 병렬화, 그리고 특히 데이터 및 텐서 병렬화를 결합한 전문가 병렬화 등 다양한 병렬화 전략을 통합합니다.

성능 최적화를 위해 연구진은 CUDA와 Triton을 사용하여 게이팅 알고리즘용 GPU 커널을 개발하고 서로 다른 전문가의 선형 층 간 계산을 융합했습니다. 모든 실험은 NVIDIA A100 또는 H800 GPU가 장착된 클러스터에서 수행되었습니다. A100 클러스터의 각 노드는 NVLink 브릿지로 페어와이즈 연결된 8개의 GPU를 포함합니다. H800 클러스터 역시 노드당 8개의 GPU를 갖추고 있으며, 노드 내에서는 NVLink와 NVSwitch로 상호 연결되어 있습니다. 두 클러스터 모두 노드 간 통신을 위해 InfiniBand 인터커넥트를 사용합니다.

#### 하이퍼파라미터 설정

모델 설정에서는 트랜스포머 층의 수를 9개로, 은닉 차원을 1280으로 설정했습니다. 멀티헤드 어텐션 메커니즘은 총 10개의 어텐션 헤드를 사용하며, 각 헤드의 차원은 128입니다. 초기화를 위해 모든 학습 가능한 파라미터는 표준편차 0.006으로 무작위 초기화되었습니다. 모든 FFN을 MoE 층으로 대체했으며, 전체 전문가 파라미터의 수가 표준 FFN의 16배가 되도록 했습니다. 또한 공유 전문가 파라미터와 활성화된 라우팅된 전문가 파라미터를 포함한 활성화된 전문가 파라미터가 표준 FFN의 2배가 되도록 유지했습니다. 이 구성에서 각 MoE 모델은 약 2B개의 전체 파라미터를 가지며, 활성화된 파라미터의 수는 약 0.3B개입니다.

학습 설정에서는 AdamW 옵티마이저를 사용했으며, 하이퍼파라미터는 $$\beta_{1}=0.9$$, $$\beta_{2}=0.95$$, $$\text{weight decay}=0.1$$로 설정했습니다. 학습률은 웜업-스텝-감소 전략으로 스케줄링되었습니다. 처음 2K 스텝 동안 학습률은 0에서 최대값까지 선형적으로 증가하고, 이후 학습 스텝의 80%에서 0.316을 곱하고, 다시 90%에서 0.316을 곱합니다. 검증 실험의 최대 학습률은 $$1.08 \times 10^{-3}$$으로 설정되었으며, 그래디언트 클리핑 노름은 1.0으로 설정되었습니다.
### 검증 실험 설정과 평가

#### 배치 크기와 학습 구성

배치 크기는 2K로 설정되었으며, 최대 시퀀스 길이 2K와 함께 각 학습 배치는 4M 토큰을 포함합니다. 이에 따라 100B 학습 토큰을 달성하기 위한 전체 학습 스텝 수는 25,000으로 설정되었습니다. 학습 데이터가 풍부하기 때문에 학습 중에는 드롭아웃을 사용하지 않았습니다. 상대적으로 작은 모델 크기를 고려하여 전문가 파라미터를 포함한 모든 파라미터는 계산 불균형을 피하기 위해 단일 GPU 디바이스에 배치되었습니다. 이에 따라 학습 중에는 토큰을 드롭하지 않았으며 디바이스 수준 균형 손실도 사용하지 않았습니다. 라우팅 붕괴를 방지하기 위해 전문가 수준 균형 인자는 0.01로 설정되었습니다.

#### 평가 벤치마크

DeepSeekMoE의 성능 평가는 다양한 유형의 작업을 포괄하는 광범위한 벤치마크에서 수행되었습니다. 언어 모델링 평가를 위해 Pile 테스트 세트를 사용했으며, 평가 지표로는 교차 엔트로피 손실을 사용했습니다. 언어 이해와 추론 능력 평가를 위해서는 HellaSwag, PIQA, ARC-challenge, ARC-easy를 고려했으며, 평가 지표는 정확도를 사용했습니다.

독해 능력 평가를 위해 RACE-high와 RACE-middle을 사용했으며, 코드 생성 능력 평가를 위해서는 HumanEval과 MBPP를 사용했습니다. 코드 생성의 평가 지표로는 단일 생성 시도에서의 통과율을 나타내는 Pass@1을 사용했습니다. 클로즈 북 질의응답 평가를 위해서는 TriviaQA와 NaturalQuestions를 고려했으며, 평가 지표로는 정확 일치(Exactly Matching) 비율을 사용했습니다.

#### 실험 결과 분석

실험 결과는 DeepSeekMoE가 기존의 MoE 아키텍처들과 비교하여 상당한 성능 우위를 보여주었습니다. 동일한 수의 전체 파라미터(2.0B)와 활성화된 파라미터(0.3B)를 가진 GShard와 비교했을 때, DeepSeekMoE는 모든 평가 지표에서 더 우수한 성능을 달성했습니다. 특히 Pile 손실에서 1.808을 기록하여 GShard의 1.867보다 낮은 손실을 보여주었으며, HellaSwag 정확도는 54.8%로 GShard의 50.5%를 크게 상회했습니다.

더욱 주목할 만한 점은 DeepSeekMoE가 1.5배 더 많은 전문가 파라미터를 가진 GShard 모델과 비교 가능한 성능을 달성했다는 것입니다. 또한 16배의 FFN 파라미터를 가진 밀집 모델의 성능에 근접했는데, 이는 MoE 모델이 모델 용량 측면에서 달성할 수 있는 상한선을 설정합니다.

### DeepSeekMoE의 전문가 특화 분석

DeepSeekMoE 2B 모델의 전문가 특화 현상을 실증적으로 분석한 결과를 살펴보겠습니다. 여기서 언급하는 DeepSeekMoE 2B는 총 2.0B 파라미터를 가지며, 1개의 공유 전문가와 63개의 라우팅된 전문가 중 7개가 활성화되는 모델을 의미합니다.

![전문가 비활성화 실험 결과](https://ar5iv.labs.arxiv.org//html/2401.06066/assets/x4.png)

상위 라우팅된 전문가들을 비활성화했을 때의 영향을 분석한 결과, DeepSeekMoE는 GShard×1.5 모델과 비교하여 전문가 비활성화에 더 민감한 반응을 보였습니다. 이는 DeepSeekMoE의 라우팅된 전문가들 간의 중복성이 더 낮다는 것을 의미합니다. 구체적으로, 각 토큰에 대해 라우팅 확률이 가장 높은 전문가들의 특정 비율을 마스킹하고, 남은 전문가들 중에서 상위 K개를 선택하는 방식으로 실험을 진행했습니다.

![활성화된 전문가 수에 따른 성능](https://ar5iv.labs.arxiv.org//html/2401.06066/assets/x5.png)

공유 전문가의 역할을 조사하기 위해, 공유 전문가를 비활성화하고 대신 하나의 라우팅된 전문가를 추가로 활성화하는 실험을 수행했습니다. 그 결과 Pile 손실이 1.808에서 2.414로 크게 증가했는데, 이는 동일한 계산 비용을 유지했음에도 불구하고 성능이 현저히 저하되었음을 보여줍니다. 이러한 결과는 공유 전문가가 라우팅된 전문가들과 공유되지 않는 기본적이고 필수적인 지식을 포착하고 있어 라우팅된 전문가로 대체될 수 없다는 것을 입증합니다.

![GShard와 DeepSeekMoE 비교](https://ar5iv.labs.arxiv.org//html/2401.06066/assets/x6.png)

DeepSeekMoE가 더 정확하고 효율적으로 지식을 획득한다는 주장을 검증하기 위해, 활성화되는 라우팅된 전문가의 수를 3개에서 7개까지 변화시키면서 Pile 손실을 평가했습니다. 실험 결과, 단 4개의 라우팅된 전문가만으로도 GShard와 비슷한 수준의 Pile 손실을 달성할 수 있었습니다. 이는 DeepSeekMoE가 필요한 지식을 더 정확하고 효율적으로 획득할 수 있다는 것을 보여줍니다.

이러한 발견을 바탕으로, DeepSeekMoE의 전문가 특화와 정확한 지식 획득 능력을 더욱 엄밀하게 검증하기 위해 새로운 모델을 처음부터 학습시켰습니다. 이 모델은 1개의 공유 전문가와 63개의 라우팅된 전문가로 구성되며, 단 3개의 라우팅된 전문가만 활성화됩니다. 평가 결과, 동일한 전체 전문가 파라미터를 가지고 활성화된 전문가 파라미터는 절반에 불과함에도 불구하고 GShard보다 우수한 성능을 보여주었습니다. 이는 DeepSeekMoE가 전문가 파라미터를 더 효율적으로 활용할 수 있다는 것을 의미하며, 활성화된 전문가들의 유효 파라미터 비율이 GShard보다 훨씬 높다는 것을 시사합니다.
### DeepSeekMoE 16B 모델의 확장과 성능 평가

DeepSeekMoE 아키텍처의 성공적인 검증을 바탕으로 연구진은 모델을 16B 파라미터 규모로 확장하고 2T 토큰 규모의 말뭉치에서 학습을 진행했습니다. 평가 결과에 따르면 DeepSeekMoE 16B는 LLaMA2 7B와 비교하여 약 40%의 계산량만으로도 우수한 성능을 달성했습니다.

#### 실험 환경 구성

학습 데이터와 토크나이제이션을 위해 연구진은 앞서 검증 실험에서 사용한 것과 동일한 말뭉치에서 데이터를 샘플링했으나, 이번에는 LLaMA2 7B와 동일한 2T 토큰 규모의 데이터를 사용했습니다. 토크나이제이션을 위해 HuggingFace Tokenizer 도구를 사용하여 BPE 토크나이저를 학습했으며, DeepSeekMoE 16B를 위해 어휘 크기를 100K로 설정했습니다.

#### 하이퍼파라미터 설정

DeepSeekMoE 16B의 모델 구성에서는 트랜스포머 층의 수를 28개로, 은닉 차원을 2048로 설정했습니다. 멀티헤드 어텐션 메커니즘은 총 16개의 어텐션 헤드를 사용하며, 각 헤드의 차원은 128입니다. 모든 학습 가능한 파라미터는 표준편차 0.006으로 무작위 초기화되었습니다.

첫 번째 층을 제외한 모든 FFN을 MoE 층으로 대체했는데, 이는 첫 번째 층에서 부하 균형 상태가 특히 더 늦게 수렴하는 것을 관찰했기 때문입니다. 각 MoE 층은 2개의 공유 전문가와 64개의 라우팅된 전문가로 구성되며, 각 전문가는 표준 FFN의 0.25배 크기를 가집니다. 각 토큰은 2개의 공유 전문가와 64개의 라우팅된 전문가 중 6개로 라우팅됩니다.

이러한 구성에서 DeepSeekMoE 16B는 약 16.4B의 전체 파라미터를 가지며, 활성화된 파라미터의 수는 약 2.8B입니다. 더 작은 전문가 크기를 사용하지 않은 이유는 과도하게 작은 전문가 크기가 계산 효율성을 저하시킬 수 있기 때문입니다. 16B 이상의 더 큰 규모에서는 더 세밀한 전문가 분할도 가능합니다.

#### 학습 설정

옵티마이저로는 AdamW를 사용했으며, 하이퍼파라미터는 $$\beta_{1}=0.9$$, $$\beta_{2}=0.95$$, $$\text{weight decay}=0.1$$로 설정했습니다. 학습률은 웜업-스텝-감소 전략으로 스케줄링되었습니다. 처음 2K 스텝 동안 학습률은 0에서 최대값까지 선형적으로 증가하고, 이후 학습 스텝의 80%에서 0.316을 곱하고, 다시 90%에서 0.316을 곱합니다. DeepSeekMoE 16B의 최대 학습률은 $$4.2 \times 10^{-4}$$로 설정되었으며, 그래디언트 클리핑 노름은 1.0으로 설정되었습니다.

배치 크기는 4.5K로 설정되었으며, 최대 시퀀스 길이 4K와 함께 각 학습 배치는 18M 토큰을 포함합니다. 이에 따라 2T 학습 토큰을 달성하기 위한 전체 학습 스텝 수는 106,449로 설정되었습니다. 학습 데이터가 풍부하기 때문에 학습 중에는 드롭아웃을 사용하지 않았습니다.
### 평가 벤치마크와 실험 결과

DeepSeekMoE의 성능 평가는 검증 실험에서 사용된 벤치마크에 추가적인 평가 지표를 포함하여 더욱 포괄적으로 진행되었습니다. 언어 모델링 평가를 위해 Pile 테스트 세트를 사용했으며, DeepSeekMoE 16B와 LLaMA2 7B가 서로 다른 토크나이저를 사용하기 때문에 공정한 비교를 위해 바이트당 비트(BPB)를 평가 지표로 활용했습니다.

독해 능력 평가를 위해 DROP을 추가로 고려했으며, 정확 일치(Exactly Matching) 비율을 평가 지표로 사용했습니다. 수학적 추론 능력 평가를 위해서는 GSM8K와 MATH를 추가했고, 마찬가지로 정확 일치 비율을 평가 지표로 활용했습니다. 다중 주제 객관식 평가를 위해서는 MMLU를 추가했으며, 정확도를 평가 지표로 사용했습니다.

중의성 해소 능력 평가를 위해 WinoGrande를 추가로 고려했으며, 정확도를 평가 지표로 사용했습니다. DeepSeekMoE 16B가 이중 언어 말뭉치에서 사전 학습되었기 때문에 중국어 벤치마크에서도 평가를 진행했습니다. CLUEWSC는 중국어 중의성 해소 벤치마크이며, CEval과 CMMLU는 MMLU와 유사한 형태의 중국어 다중 주제 객관식 벤치마크입니다. CHID는 중국어 관용구 완성 벤치마크로, 중국 문화에 대한 이해도를 평가합니다.

모든 벤치마크는 연구진의 내부 평가 프레임워크를 기반으로 평가되었습니다. 오픈 소스 모델들과의 공정하고 편리한 비교를 위해 DeepSeekMoE 16B는 Open LLM Leaderboard에서도 추가로 평가되었습니다. 이 리더보드는 HuggingFace가 지원하는 공개 리더보드로, ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, GSM8K 등 6개의 과제로 구성되어 있습니다.

실험 결과, DeepSeekMoE 16B는 DeepSeek 7B와 비교했을 때 약 40.5%의 계산량만으로도 대등한 성능을 달성했습니다. 특히 언어 모델링과 지식 집약적 과제인 Pile, HellaSwag, TriviaQA, NaturalQuestions에서 강점을 보였습니다. 이는 MoE 모델에서 FFN 파라미터가 어텐션 파라미터보다 훨씬 많다는 점을 고려할 때, 트랜스포머의 FFN이 지식 기억 능력을 가진다는 기존 연구 결과와 일치합니다.

다만 다중 선택 과제에서는 상대적으로 제한된 성능을 보였는데, 이는 DeepSeekMoE 16B의 제한된 어텐션 파라미터(약 0.5B)가 DeepSeek 7B(2.5B)에 비해 현저히 적기 때문입니다. DeepSeek 7B에 대한 이전 연구에서 어텐션 용량과 다중 선택 과제 성능 간의 양의 상관관계가 확인된 바 있습니다.
### DeepSeekMoE 16B와 오픈 소스 모델 비교 분석

LLaMA2 7B와의 내부 비교 평가에서 DeepSeekMoE 16B는 총 파라미터 수는 245% 많지만 계산량은 39.6%에 불과한 상태에서 대부분의 벤치마크에서 우수한 성능을 보여주었습니다. 특히 수학적 추론과 코드 생성 능력에서 LLaMA2 7B를 크게 앞섰는데, 이는 사전 학습 말뭉치에 수학 및 코드 관련 텍스트가 풍부하게 포함되어 있기 때문입니다.

중국어 벤치마크에서도 DeepSeekMoE 16B는 LLaMA2 7B에 비해 큰 성능 우위를 보였습니다. 특히 주목할 만한 점은 영어 텍스트의 비중이 상대적으로 적음에도 불구하고 영어 이해력과 지식 집약적 벤치마크에서 LLaMA2 7B와 대등하거나 더 나은 성능을 보여주었다는 것입니다. 이는 DeepSeekMoE 16B의 뛰어난 지식 획득 및 활용 능력을 입증합니다.

Open LLM Leaderboard에서의 평가에서는 LLaMA2 7B 외에도 LLaMA 7B, Falcon 7B, GPT-J 6B, RedPajama-INCITE 7B와 3B, Open LLaMA 7B와 3B, OPT 2.7B, Pythia 2.8B, GPT-neo 2.7B, BLOOM 3B 등 다양한 오픈 소스 모델들과 비교 평가가 이루어졌습니다. 평가 결과, DeepSeekMoE 16B는 유사한 수준의 활성화된 파라미터를 가진 모델들을 큰 차이로 능가했으며, 약 2.5배의 활성화된 파라미터를 가진 LLaMA2 7B와 대등한 성능을 달성했습니다.

이러한 결과는 DeepSeekMoE의 혁신적인 아키텍처가 대규모 언어 모델의 효율성을 크게 향상시킬 수 있음을 보여줍니다. 특히 공유 전문가와 라우팅된 전문가의 조합, 세밀한 전문가 분할 전략, 그리고 효율적인 파라미터 활용이 이러한 성능 향상의 핵심 요인으로 작용했습니다.

### DeepSeekMoE 16B의 지도 학습 미세조정 실험

DeepSeekMoE 16B 모델의 미세조정 가능성을 평가하기 위해 연구진은 광범위한 지도 학습 미세조정(Supervised Fine-tuning, SFT) 실험을 수행했습니다. 이전 연구에서는 MoE 모델이 미세조정을 통해 큰 성능 향상을 보이지 않는다는 결과가 있었으나, Shen과 연구진의 최근 연구는 MoE 모델이 지시어 튜닝(instruction tuning)을 통해 의미 있는 성능 향상을 달성할 수 있음을 보여주었습니다.

#### 실험 설정과 데이터

연구진은 1.4M개의 학습 예제로 구성된 자체 큐레이션 데이터셋을 사용했습니다. 이 데이터셋은 수학, 코딩, 작문, 질의응답, 추론, 요약 등 다양한 범주의 과제를 포함하며, 대부분의 데이터는 영어와 중국어로 구성되어 있어 이중 언어 시나리오에서의 활용이 가능합니다.

학습 과정에서는 AdamW 옵티마이저를 사용했으며, 배치 크기는 1024로 설정하고 8 에포크 동안 학습을 진행했습니다. 최대 시퀀스 길이는 4K로 설정했으며, 시퀀스 길이 제한에 도달할 때까지 학습 예제를 최대한 조밀하게 패킹했습니다. 학습률은 $$10^{-5}$$로 고정했으며, 별도의 학습률 스케줄링 전략은 사용하지 않았습니다.

#### 평가 벤치마크와 실험 결과

평가는 이전 섹션에서 사용된 벤치마크를 기반으로 하되, 몇 가지 중요한 조정을 가했습니다. 순수 언어 모델링 과제인 Pile은 제외했으며, 결과의 불안정성으로 인해 CHID도 제외했습니다. 대신 채팅 모델의 추론 능력을 더욱 포괄적으로 평가하기 위해 BBH를 추가했습니다.

실험 결과, DeepSeekMoE Chat 16B는 LLaMA2 SFT 7B 및 DeepSeek Chat 7B와 비교하여 약 40%의 계산량만으로도 대등한 성능을 달성했습니다. 특히 언어 이해와 추론(PIQA, ARC, BBH), 기계 독해(RACE), 수학(GSM8K, MATH), 지식 집약적 과제(TriviaQA, NaturalQuestions) 등 다양한 벤치마크에서 우수한 성능을 보여주었습니다.

코드 생성 과제에서는 DeepSeekMoE Chat 16B가 LLaMA2 SFT 7B를 크게 앞섰으며, HumanEval과 MBPP에서 현저한 성능 향상을 보였습니다. 또한 DeepSeek Chat 7B보다도 우수한 성능을 달성했습니다.

다중 선택형 질의응답 벤치마크인 MMLU, CEval, CMMLU에서는 여전히 DeepSeek Chat 7B에 비해 다소 낮은 성능을 보였으나, 기본 모델과 비교했을 때 성능 격차가 줄어든 것을 확인할 수 있었습니다. 이중 언어 말뭉치에서의 사전 학습 덕분에 DeepSeekMoE Chat 16B는 모든 중국어 벤치마크에서 LLaMA2 SFT 7B를 크게 앞섰습니다.

### DeepSeekMoE 145B 모델의 확장과 성능 평가

DeepSeekMoE 16B의 성공적인 결과에 고무되어 연구진은 모델을 145B 규모로 확장하는 시도를 진행했습니다. 이 예비 연구에서 DeepSeekMoE 145B는 245B 토큰으로 학습되었으며, GShard 아키텍처와 비교하여 일관된 성능 우위를 보여주었고, DeepSeek 67B(Dense) 모델의 성능에 근접하거나 이를 능가하는 잠재력을 보여주었습니다.

#### 실험 설정과 모델 구조

DeepSeekMoE 145B는 DeepSeekMoE 16B와 동일한 학습 데이터와 토크나이저를 사용했으며, 트랜스포머 층의 수는 62개, 은닉 차원은 4096으로 설정되었습니다. 멀티헤드 어텐션 메커니즘은 총 32개의 어텐션 헤드를 사용하며, 각 헤드의 차원은 128입니다. 모든 학습 가능한 파라미터는 표준편차 0.006으로 무작위 초기화되었습니다.

DeepSeekMoE 16B와 마찬가지로, 첫 번째 층을 제외한 모든 FFN을 MoE 층으로 대체했습니다. 각 MoE 층은 4개의 공유 전문가와 128개의 라우팅된 전문가로 구성되며, 각 전문가는 표준 FFN의 0.125배 크기를 가집니다. 각 토큰은 4개의 공유 전문가와 128개의 라우팅된 전문가 중 12개로 라우팅됩니다. 이러한 구성에서 DeepSeekMoE 145B는 약 144.6B의 전체 파라미터를 가지며, 활성화된 파라미터의 수는 약 22.2B입니다.

#### 학습 설정과 최적화

학습을 위해 AdamW 옵티마이저를 사용했으며, 하이퍼파라미터는 $$\beta_{1}=0.9$$, $$\beta_{2}=0.95$$, $$\text{weight decay}=0.1$$로 설정했습니다. DeepSeekMoE 145B의 예비 연구에서는 웜업-상수 학습률 스케줄러를 사용했습니다. 처음 2K 스텝 동안 학습률은 0에서 최대값까지 선형적으로 증가하고, 이후 학습 과정에서는 일정하게 유지됩니다. 최대 학습률은 $$3.0 \times 10^{-4}$$로 설정되었으며, 그래디언트 클리핑 노름은 1.0으로 설정되었습니다.

배치 크기는 4.5K로 설정되었으며, 최대 시퀀스 길이 4K와 함께 각 학습 배치는 18M 토큰을 포함합니다. DeepSeekMoE 145B는 13,000 스텝 동안 학습되어 총 245B 토큰의 학습을 달성했습니다. 학습 중에는 드롭아웃을 사용하지 않았습니다.

파이프라인 병렬화를 활용하여 모델의 서로 다른 층을 다른 디바이스에 배치했으며, 각 층에서 모든 라우팅된 전문가는 4개의 디바이스에 균일하게 배치되었습니다(전문가 병렬화와 데이터 병렬화 결합). DeepSeekMoE 145B에서는 전문가 병렬화를 사용하기 때문에 계산 병목 현상을 줄이기 위해 디바이스 수준의 부하 균형을 고려해야 했습니다. 이를 위해 디바이스 수준 균형 인자를 0.05로 설정하여 디바이스 간의 균형 잡힌 계산을 촉진했습니다. 또한 라우팅 붕괴를 방지하기 위해 작은 전문가 수준 균형 인자 0.003을 설정했습니다.

### 혼합 전문가 모델의 발전과 진화

혼합 전문가(Mixture of Experts, MoE) 기법은 1991년 Jacobs와 연구진이 처음 제안했으며, 서로 다른 샘플들을 독립적인 전문가 모듈로 처리하는 혁신적인 방법을 소개했습니다. 이후 Jordan과 Jacobs가 1994년에 이 개념을 더욱 발전시켰고, 2017년 Shazeer와 연구진이 LSTM 기반의 대규모 MoE 모델을 구축하며 언어 모델 학습에 MoE를 도입했습니다.

트랜스포머가 자연어 처리 분야의 주류 아키텍처로 자리잡으면서, 많은 연구자들이 트랜스포머의 피드포워드 네트워크(FFN)를 MoE 층으로 확장하는 시도를 했습니다. GShard와 Switch Transformer는 학습 가능한 top-2 또는 top-1 라우팅 전략을 도입하여 MoE 언어 모델을 극도로 큰 규모로 확장하는 선구적인 연구를 수행했습니다.

Hash Layer와 StableMoE는 더욱 안정적인 라우팅과 학습을 위해 고정된 라우팅 전략을 사용했습니다. Zhou와 연구진은 각 토큰이 서로 다른 수의 전문가에 할당될 수 있는 전문가 선택 라우팅 전략을 제안했으며, Zoph와 연구진은 MoE 모델의 학습 불안정성과 미세조정의 어려움에 초점을 맞추어 이러한 문제들을 해결하기 위한 ST-MoE를 제안했습니다.

MoE 아키텍처와 학습 전략에 대한 연구 외에도, 최근에는 기존 MoE 아키텍처를 기반으로 한 다수의 대규모 언어 모델이나 멀티모달 모델이 등장했습니다. 그러나 대부분의 기존 MoE 모델들은 전통적인 top-1 또는 top-2 라우팅 전략에 기반하고 있어, 전문가 특화를 개선할 여지가 많이 남아있었습니다. 이러한 배경에서 DeepSeekMoE 아키텍처는 전문가 특화를 최대한으로 끌어올리는 것을 목표로 개발되었습니다.

## DeepSeekMoE: 혼합 전문가 언어 모델에서의 궁극적 전문가 특화를 향하여

본 논문에서는 궁극적인 전문가 특화를 목표로 하는 DeepSeekMoE 아키텍처를 소개합니다. DeepSeekMoE는 세밀한 전문가 분할과 공유 전문가 분리를 통해 기존 MoE 아키텍처들과 비교하여 현저히 높은 전문가 특화도와 성능을 달성했습니다.

2B 파라미터 규모의 초기 실험에서 DeepSeekMoE의 장점을 검증했으며, MoE 모델이 달성할 수 있는 성능의 상한선에 근접하는 결과를 보여주었습니다. 또한 GShard와 비교하여 더 높은 전문가 특화도를 달성했다는 실증적 증거를 제시했습니다.

16B 파라미터 규모로 확장한 DeepSeekMoE는 2T 토큰으로 학습을 진행했으며, DeepSeek 7B와 LLaMA2 7B와 비교하여 약 40%의 계산량만으로도 대등한 성능을 보여주었습니다. 추가로 지도 학습 미세조정을 통해 DeepSeekMoE 16B를 기반으로 한 대화형 모델을 구축했으며, 이를 통해 모델의 적응성과 범용성을 입증했습니다.

더 나아가 145B 파라미터 규모로의 예비 확장 실험을 진행했습니다. DeepSeekMoE 145B는 GShard 아키텍처와 비교하여 지속적인 우위를 보였으며, DeepSeek 67B와 비교했을 때 단 28.5%(또는 18.2%)의 계산량으로도 대등한 성능을 달성했습니다.

연구 목적으로 DeepSeekMoE 16B의 모델 체크포인트를 공개했으며, 이는 40GB 메모리를 가진 단일 GPU에서도 배포가 가능합니다. 본 연구가 학계와 산업계 모두에 가치 있는 통찰을 제공하고, 대규모 언어 모델의 발전을 가속화하는 데 기여하기를 기대합니다.

- - -
### References
* [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/pdf/2401.06066v1)
