---
layout: post
title: "Tulu 3: Pushing Frontiers in Open Language Model Post-Training"
date: 2024-11-22 18:44:04
author: "Allen Institute for AI"
categories: "Language-Models"
tags: ["Direct-Preference-Optimization", "Reinforcement-Learning-with-Verifiable-Rewards", "Prompt-Decontamination", "Supervised-Finetuning", "Evaluation-Framework"]
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
최근 언어 모델의 후처리 학습(post-training) 분야에서 비공개 모델들이 공개 모델들보다 우수한 성능을 보이고 있습니다. 특히 후처리 학습에 사용되는 데이터와 학습 방법은 언어 모델 발전의 핵심 요소임에도 불구하고, 이에 대한 투명성이 매우 부족한 상황이었습니다. 이러한 공개/비공개 모델 간의 성능 격차를 해소하고, 후처리 학습 분야의 발전을 위해 완전히 공개된 최첨단 후처리 학습 방법론의 필요성이 대두되었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
TÜLU 3는 세 가지 주요 학습 알고리즘을 통합한 포괄적인 후처리 학습 프레임워크를 제시합니다. 먼저 지도 학습 미세조정(SFT)을 통해 기본적인 학습을 수행하고, 직접 선호도 최적화(DPO)를 적용하여 모델의 출력을 인간의 선호도에 맞게 조정합니다. 여기에 새롭게 개발된 검증 가능한 보상을 통한 강화학습(RLVR) 방법을 도입하여 모델의 성능을 한층 더 개선했습니다. 특히 RLVR은 수학 문제 풀이나 코드 생성과 같이 객관적으로 검증 가능한 작업에서 탁월한 성능 향상을 보여주었습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
TÜLU 3의 구현은 데이터 구축, 학습 방법론, 평가 체계라는 세 가지 핵심 축을 중심으로 이루어졌습니다. 데이터 구축 과정에서는 공개 데이터셋의 활용과 목표 기술 합성이라는 두 가지 접근 방식을 결합했으며, ConTAM이라는 새로운 방법론을 통해 데이터 오염을 철저히 제거했습니다. 학습 과정에서는 SFT, DPO, RLVR을 순차적으로 적용하며, 각 단계별로 최적화된 하이퍼파라미터와 학습 전략을 사용했습니다. 평가 체계는 개발용 평가와 미공개 평가를 포함하며, 표준화된 벤치마크 구현과 함께 철저한 데이터 오염 제거 작업을 수행했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
TÜLU 3는 공개된 후처리 학습 방법론으로는 처음으로 비공개 모델들과 대등한 수준의 성능을 달성했다는 점에서 큰 의미를 가집니다. 특히 모든 구성 요소(데이터, 코드, 평가 방법론)를 완전히 공개함으로써, 후처리 학습 분야의 발전을 위한 새로운 기준을 제시했습니다. 또한 RLVR이라는 새로운 학습 방법론을 통해 객관적으로 검증 가능한 작업에서의 성능 향상 가능성을 보여주었으며, 향후 긴 문맥 처리, 다국어 능력, 도구 사용 등으로의 확장 가능성을 제시했습니다. 이는 언어 모델 발전의 민주화와 투명성 향상에 크게 기여할 것으로 기대됩니다.
- - -
## TÜLU 3: 언어 모델 후처리 학습의 새로운 지평을 열다

TÜLU 3는 언어 모델의 후처리 학습(post-training) 분야에서 획기적인 발전을 이룬 연구입니다. 최근 다양한 언어 모델들이 후처리 학습을 통해 새로운 능력을 획득하고 행동을 개선하고 있지만, 공개된 학습 방법들은 비공개 모델들에 비해 뒤처져 있는 실정입니다. 특히 후처리 학습에 사용되는 데이터와 학습 방법은 언어 모델 발전에 가장 중요한 요소임에도 불구하고, 이에 대한 투명성이 매우 부족했습니다.

이러한 간극을 해소하기 위해 연구진은 TÜLU 3를 개발했습니다. TÜLU 3는 최첨단 성능을 달성한 완전히 공개된 후처리 학습 모델군으로, 데이터셋과 코드, 그리고 상세한 학습 방법까지 모두 공개되었습니다. Llama 3.1 기반 모델을 토대로 개발된 TÜLU 3는 Llama 3.1의 instruction 버전, Qwen 2.5, Mistral은 물론 GPT-4o-mini와 Claude 3.5-Haiku와 같은 비공개 모델들의 성능까지 뛰어넘는 결과를 보여주었습니다.

TÜLU 3의 학습 과정에는 세 가지 주요 알고리즘이 사용되었습니다. 먼저 지도 학습 미세조정(Supervised Finetuning, SFT)을 통해 기본적인 학습을 진행했으며, 직접 선호도 최적화(Direct Preference Optimization, DPO)를 적용해 모델의 출력을 인간의 선호도에 맞게 조정했습니다. 여기에 연구진이 새롭게 개발한 검증 가능한 보상을 통한 강화학습(Reinforcement Learning with Verifiable Rewards, RLVR) 방법을 도입하여 모델의 성능을 한층 더 개선했습니다.

또한 연구진은 후처리 학습을 위한 다중 작업 평가 체계를 구축했습니다. 이 평가 체계는 개발용 평가와 미공개 평가를 포함하며, 표준화된 벤치마크 구현과 함께 기존 공개 데이터셋의 오염을 제거하는 작업도 수행했습니다. 연구진은 성능 향상에 실패한 학습 방법들에 대한 분석과 논의도 함께 제공하여, 후처리 학습 분야의 발전에 기여했습니다.

TÜLU 3의 공개 자료에는 모델 가중치와 데모뿐만 아니라, 다양한 핵심 기술을 위한 데이터셋, 데이터 큐레이션과 평가를 위한 도구, 학습 코드와 인프라까지 포함된 완전한 레시피가 포함되어 있습니다. 특히 상세한 보고서를 통해 TÜLU 3 접근 방식을 재현하고 다른 도메인에 적용할 수 있도록 했습니다.

### TÜLU 3 개요

TÜLU 3는 대규모 언어 모델의 후처리 학습을 위한 포괄적인 프레임워크를 제시합니다. 이 프레임워크는 크게 세 가지 핵심 구성 요소로 이루어져 있습니다. TÜLU 3 데이터셋, 평가 방법론, 그리고 학습 레시피입니다.

TÜLU 3 데이터셋은 공개 데이터셋에서 수집한 프롬프트와 특정 기술을 목표로 하는 합성 프롬프트를 포함합니다. 이 데이터셋의 가장 큰 특징은 철저한 데이터 오염 제거(decontamination) 과정을 거쳤다는 점입니다. 연구진은 학습 데이터와 평가 데이터 간의 중복을 방지하기 위해 엄격한 필터링 과정을 적용했습니다.

프롬프트 큐레이션 과정은 두 가지 주요 접근 방식을 따릅니다. 첫째, 공개 데이터셋에서 고품질 프롬프트를 선별하여 수집합니다. 이는 기존의 검증된 데이터를 활용하면서도, 데이터의 품질을 보장하기 위한 전략입니다. 둘째, 특정 기술이나 능력을 목표로 하는 프롬프트를 직접 합성합니다. 이를 통해 모델이 특정 작업에서 더 나은 성능을 보일 수 있도록 유도합니다.

TÜLU 3의 평가 방법론은 모델의 성능을 다각도로 측정하기 위한 체계적인 접근을 제시합니다. 이는 단순한 정확도나 성능 지표를 넘어서, 모델의 실제 활용 가능성과 신뢰성을 종합적으로 평가하는 것을 목표로 합니다. 특히, 데이터 오염으로 인한 성능 왜곡을 방지하기 위해 엄격한 평가 기준을 적용합니다.

TÜLU 3 레시피는 모델 학습을 위한 상세한 지침을 제공합니다. 이는 데이터 전처리부터 모델 학습, 평가에 이르기까지의 전체 과정을 포함하며, 다른 연구자들이 이를 재현하거나 자신들의 연구에 적용할 수 있도록 상세히 문서화되어 있습니다. 이러한 투명한 공개는 언어 모델 학습 분야의 발전에 크게 기여할 것으로 기대됩니다.
### TÜLU 3 데이터

TÜLU 3의 데이터 구축 과정은 프롬프트 큐레이션과 데이터 오염 제거라는 두 가지 핵심 단계로 구성됩니다. 프롬프트 큐레이션은 공개 데이터셋 활용과 목표 기술 합성이라는 두 가지 방법론을 통해 이루어집니다.

공개 데이터셋 활용 과정에서는 FLAN-v2, CoT(Chain-of-Thought), OASST1, Dolly, GPT4-Alpaca, Code-Alpaca, ShareGPT 등 다양한 고품질 데이터셋을 활용합니다. 각 데이터셋은 고유한 특성과 장점을 가지고 있어, 이들을 적절히 조합함으로써 모델의 다양한 능력을 향상시킬 수 있습니다. 예를 들어, FLAN-v2는 다중 작업 학습을, CoT는 추론 능력을, Code-Alpaca는 코딩 능력을 강화하는 데 기여합니다.

목표 기술 합성 과정에서는 특정 능력을 향상시키기 위한 프롬프트를 직접 생성합니다. 이는 수학적 추론, 코드 생성, 윤리적 판단 등 특정 도메인에서 모델의 성능을 개선하기 위한 전략입니다. 연구진은 각 목표 기술에 대해 체계적인 프롬프트 템플릿을 설계하고, 이를 바탕으로 다양한 예시를 생성했습니다.

데이터 오염 제거는 TÜLU 3의 평가 신뢰성을 보장하기 위한 핵심 과정입니다. 이는 ConTAM(Contamination Threshold Analysis Method)이라는 새로운 방법론을 통해 수행됩니다. ConTAM은 평가 데이터와 학습 데이터 간의 중복을 탐지하고, 이를 통해 발생할 수 있는 성능 왜곡을 방지합니다. 구체적으로, n-gram 기반의 문자열 매칭을 사용하여 중복을 탐지하고, 발견된 중복 데이터는 평가 세트에서 제거됩니다.

이러한 데이터 구축 과정은 Python 기반의 자동화된 파이프라인을 통해 구현됩니다. 데이터 처리 파이프라인은 Hugging Face의 datasets 라이브러리를 활용하여 효율적인 데이터 로딩과 전처리를 수행하며, 필요한 경우 병렬 처리를 통해 처리 속도를 최적화합니다. 특히, 데이터 오염 제거 과정에서는 정교한 필터링 알고리즘을 적용하여 데이터의 품질을 보장합니다.

### 지도 학습 미세조정 (Supervised Fine-tuning)

TÜLU 3의 지도 학습 미세조정(SFT) 과정은 모델의 기본적인 능력을 향상시키는 핵심 단계입니다. 이 과정은 크게 SFT 데이터 구성, 주요 데이터 실험, 그리고 SFT 레시피와 분석의 세 가지 주요 구성 요소로 이루어져 있습니다.

SFT 데이터는 프롬프트를 SFT 데이터로 변환하는 과정과 TÜLU 3 SFT 믹스라는 두 가지 중요한 측면을 포함합니다. 프롬프트를 SFT 데이터로 변환하는 과정에서는 Longpre와 연구진이 제안한 방법론을 기반으로, 다양한 형식의 프롬프트를 일관된 형태의 학습 데이터로 변환합니다. 이 과정에서는 제로샷, 퓨샷, 그리고 사고 연쇄(Chain-of-Thought) 프롬프트를 균형 있게 조합하여 모델의 다양한 추론 능력을 향상시킵니다.

TÜLU 3 SFT 믹스는 여러 고품질 데이터셋을 효과적으로 조합한 것으로, 각 데이터셋의 고유한 특성을 활용하여 모델의 전반적인 성능을 향상시킵니다. 예를 들어, FLAN-v2에서는 다중 작업 학습 능력을, CoT 데이터셋에서는 복잡한 추론 능력을, Code-Alpaca에서는 프로그래밍 관련 능력을 향상시키는 데 중점을 둡니다.

주요 데이터 실험에서는 데이터 품질과 다양성이 모델 성능에 미치는 영향을 체계적으로 분석합니다. 이 과정에서는 데이터 오염을 방지하기 위한 ConTAM(Contamination Threshold Analysis Method) 방법론을 적용하여, 학습 데이터와 평가 데이터 간의 중복을 철저히 제거합니다. 이는 n-gram 기반의 문자열 매칭을 통해 수행되며, 발견된 중복 데이터는 평가 세트에서 제거됩니다.

SFT 레시피와 분석 부분에서는 주요 학습 실험과 배치 집계(Batch Aggregation)에 대해 다룹니다. 학습 과정에서는 AdamW 옵티마이저를 사용하며, 학습률은 선형적으로 감소하는 스케줄러를 적용합니다. 수학적으로 이는 다음과 같이 표현됩니다.

$$ \text{learning rate}_t = \text{initial lr} \cdot (1 - \frac{t}{\text{total steps}}) $$

여기서 \\(t\\)는 현재 학습 단계를, \\(\text{total\_steps}\\)는 전체 학습 단계 수를 나타냅니다.

배치 집계 과정에서는 효율적인 학습을 위해 그래디언트 누적(Gradient Accumulation)을 사용합니다. 이는 다음과 같은 수식으로 표현됩니다.

$$ \text{effective batch size} = \text{batch size} \cdot \text{gradient accumulation steps} $$

이러한 접근 방식을 통해 메모리 제약 없이 큰 배치 크기로 학습할 수 있으며, 이는 모델의 안정적인 학습에 도움이 됩니다.
SFT 레시피의 핵심 구성 요소 중 하나는 학습 과정에서의 하이퍼파라미터 최적화입니다. 연구진은 다양한 실험을 통해 최적의 하이퍼파라미터 조합을 찾아냈습니다. 특히 배치 크기와 학습률의 관계에 주목했는데, 이는 다음과 같은 스케일링 법칙을 따릅니다.

$$ \text{learning rate} = \text{base lr} \cdot \sqrt{\frac{\text{batch size}}{\text{base batch size}}} $$

여기서 \\(\text{base\_lr}\\)은 기준 학습률을, \\(\text{base\_batch\_size}\\)는 기준 배치 크기를 나타냅니다. 이러한 스케일링은 Ivison과 연구진이 제안한 방법을 기반으로 하며, 배치 크기가 증가할 때 학습의 안정성을 유지하는 데 중요한 역할을 합니다.

학습 과정에서는 웜업(warmup) 단계를 도입하여 초기 학습의 안정성을 확보했습니다. 웜업 기간 동안의 학습률은 다음 수식에 따라 조정됩니다.

$$ \text{lr}_t = \text{target lr} \cdot \min\left(1, \frac{t}{\text{warmup steps}}\right) $$

여기서 \\(t\\)는 현재 스텝을, \\(\text{warmup\_steps}\\)는 웜업 기간의 총 스텝 수를 나타냅니다.

배치 집계 과정에서는 메모리 효율성을 높이기 위해 그래디언트 체크포인팅(gradient checkpointing)을 활용했습니다. 이 기법은 중간 활성화(activation) 값을 저장하는 대신 필요할 때 재계산하는 방식으로, 메모리 사용량을 크게 줄일 수 있습니다. 수학적으로 이는 다음과 같은 트레이드오프를 가집니다.

$$ \text{memory saved} = O(L) \text{ vs. } \text{compute increased} = O(\log L) $$

여기서 \\(L\\)은 모델의 레이어 수를 나타냅니다.

연구진은 또한 학습 과정에서 발생할 수 있는 그래디언트 폭주(gradient explosion)를 방지하기 위해 그래디언트 클리핑(gradient clipping)을 적용했습니다. 구체적으로, 그래디언트의 L2 노름이 임계값을 초과할 경우 다음과 같이 스케일링됩니다.

$$ \mathbf{g}_{\text{clipped}} = \min\left(1, \frac{\text{clip threshold}}{\|\mathbf{g}\|_2}\right) \cdot \mathbf{g} $$

여기서 \\(\mathbf{g}\\)는 원래의 그래디언트를, \\(\text{clip threshold}\\)는 설정된 임계값을 나타냅니다.

### 선호도 기반 미세조정 (Preference Finetuning)

선호도 기반 미세조정은 TÜLU 3의 핵심 학습 방법론 중 하나로, 언어 모델이 사용자의 선호도와 의도에 더 잘 부합하는 응답을 생성하도록 학습하는 과정입니다. 이 방법은 크게 배경 설명, 선호도 데이터 구성, 데이터 실험 분석, 그리고 구체적인 학습 방법과 인프라 구축의 네 가지 주요 부분으로 구성됩니다.

#### 선호도 학습의 배경

선호도 학습의 핵심은 정책 최적화(Policy Optimization)에 있습니다. 이는 강화학습의 한 형태로, 모델이 생성하는 응답(정책)을 사용자의 선호도에 따라 최적화하는 과정입니다. 기본적인 설정에서는 각 프롬프트에 대해 두 가지 응답이 주어지며, 이 중 하나는 '선호되는' 응답이고 다른 하나는 '선호되지 않는' 응답입니다. 이러한 쌍별 비교 데이터를 통해 모델은 어떤 종류의 응답이 더 바람직한지 학습하게 됩니다.

정책 최적화 과정은 수학적으로 다음과 같이 표현됩니다.

$$ J(\theta) = \mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} [\log \sigma(r_\theta(x,y_w) - r_\theta(x,y_l))] $$

여기서:
- \\(\\theta\\)는 모델의 파라미터
- \\(x\\)는 입력 프롬프트
- \\(y_w\\)는 선호되는 응답
- \\(y_l\\)는 선호되지 않는 응답
- \\(r_\\theta\\)는 모델이 예측하는 선호도 점수
- \\(\\sigma\\)는 시그모이드 함수

이 목적 함수를 최적화함으로써 모델은 선호되는 응답에 더 높은 점수를 부여하고, 선호되지 않는 응답에는 더 낮은 점수를 부여하도록 학습됩니다. 이는 직접 선호도 최적화(Direct Preference Optimization, DPO)라고 불리는 방법의 기본 원리입니다.

TÜLU 3의 선호도 학습에서는 이러한 기본 원리를 바탕으로, 더욱 복잡하고 다양한 선호도 신호를 활용합니다. 예를 들어, 응답의 유용성(helpfulness), 정직성(honesty), 안전성(safety) 등 여러 측면에서의 선호도를 종합적으로 고려하여 학습이 이루어집니다. 이를 위해 연구진은 각 측면에 대한 선호도 데이터를 체계적으로 수집하고, 이를 효과적으로 활용할 수 있는 학습 방법을 개발했습니다.
#### TÜLU 3의 선호도 데이터

TÜLU 3의 선호도 데이터는 프롬프트를 선호도 데이터로 변환하는 과정과 TÜLU 3 선호도 믹스라는 두 가지 핵심 요소로 구성됩니다. 이 과정에서 연구진은 다양한 소스의 데이터를 체계적으로 통합하고 가공하여 효과적인 학습 데이터셋을 구축했습니다.

프롬프트를 선호도 데이터로 변환하는 과정은 다음과 같은 수식을 기반으로 합니다.

$$ P(y_w \succ y_l|x) = \sigma(\beta(r_\theta(x,y_w) - r_\theta(x,y_l))) $$

여기서:
- \\(y_w \succ y_l\\)은 응답 \\(y_w\\)가 \\(y_l\\)보다 선호됨을 의미
- \\(\beta\\)는 선호도 강도를 조절하는 온도 파라미터
- \\(r_\theta\\)는 모델의 선호도 점수 함수

이 변환 과정에서는 각 프롬프트에 대해 여러 응답을 생성하고, 이들 간의 상대적 선호도를 평가합니다. 평가는 다음과 같은 측면들을 고려합니다.

1. 응답의 유용성: 사용자의 질문이나 요구사항을 얼마나 잘 해결하는지
2. 정직성: 응답이 얼마나 진실되고 신뢰할 수 있는지
3. 지시사항 준수: 주어진 프롬프트의 요구사항을 얼마나 잘 따르는지
4. 사실성: 제공된 정보가 얼마나 정확하고 사실에 기반하는지

TÜLU 3 선호도 믹스는 이러한 평가를 바탕으로 구성된 종합적인 데이터셋입니다. 이 데이터셋의 품질을 보장하기 위해 연구진은 다음과 같은 손실 함수를 사용했습니다.

$$ \mathcal{L}_\text{DPO}(\theta) = -\mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} [\log \sigma(\beta(r_\theta(x,y_w) - r_\theta(x,y_l)))] $$

이 손실 함수는 선호되는 응답과 선호되지 않는 응답 간의 점수 차이를 최대화하도록 모델을 학습시킵니다. 특히, \\(\beta\\) 파라미터를 통해 선호도 차이의 강도를 조절할 수 있어, 모델이 더 명확한 선호도 구분을 학습하도록 유도할 수 있습니다.

데이터 실험 분석 결과, 연구진은 다양한 소스에서 수집된 선호도 데이터를 균형 있게 조합하는 것이 모델의 성능 향상에 핵심적임을 발견했습니다. 특히, 인간 평가자들의 직접적인 선호도 평가와 자동화된 평가 메트릭을 결합하여 더욱 견고한 선호도 학습 데이터셋을 구축했습니다.
#### 데이터 실험의 주요 발견

TÜLU 3의 선호도 학습에서 데이터 실험을 통한 주요 발견은 모델의 성능 향상에 결정적인 역할을 했습니다. 연구진은 다양한 데이터 구성과 학습 방법을 체계적으로 분석하여 최적의 접근 방식을 도출했습니다.

데이터 실험에서 가장 중요한 발견은 선호도 데이터의 다양성과 품질 간의 균형입니다. 이는 다음과 같은 수식으로 표현되는 품질 메트릭을 통해 평가되었습니다.

$$ Q(D) = \frac{1}{|D|} \sum_{(x,y_w,y_l) \in D} \alpha \cdot q(y_w, y_l) + (1-\alpha) \cdot d(y_w, y_l) $$

여기서:
- \\(q(y_w, y_l)\\)은 선호도 쌍의 품질 점수
- \\(d(y_w, y_l)\\)은 응답 쌍 간의 다양성 측정
- \\(\alpha\\)는 품질과 다양성의 상대적 중요도를 조절하는 가중치

연구진은 이러한 메트릭을 바탕으로 선호도 데이터를 필터링하고 보강하는 과정을 거쳤습니다. 특히, 응답의 품질을 평가하기 위해 다음과 같은 다중 측면 평가 함수를 도입했습니다.

$$ r_\text{multi}(x,y) = \sum_{i=1}^k w_i \cdot r_i(x,y) $$

여기서 \\(r_i(x,y)\\)는 각각의 평가 측면(유용성, 정직성, 안전성 등)에 대한 개별 평가 함수이며, \\(w_i\\)는 각 측면의 상대적 중요도를 나타내는 가중치입니다.

실험 결과, 다음과 같은 핵심적인 발견들이 있었습니다.

1. 선호도 데이터의 균형: 단순히 데이터의 양을 늘리는 것보다 다양한 유형의 선호도 데이터를 균형있게 포함하는 것이 더 효과적이었습니다. 이는 다음과 같은 엔트로피 기반 메트릭으로 측정되었습니다.

$$ H(D) = -\sum_{t \in T} p(t) \log p(t) $$

여기서 \\(T\\)는 선호도 데이터의 유형 집합이고, \\(p(t)\\)는 각 유형의 비율입니다.

2. 품질 기반 필터링: 연구진은 선호도 데이터의 품질을 자동으로 평가하고 필터링하는 방법을 개발했습니다. 이는 다음과 같은 품질 임계값 함수를 사용합니다.

$$ f_\text{filter}(x,y_w,y_l) = \mathbb{1}[q(y_w,y_l) > \tau] $$

여기서 \\(\tau\\)는 품질 임계값이며, \\(\mathbb{1}\\)은 지시 함수입니다.

#### 선호도 학습 레시피와 분석

선호도 학습의 효과적인 구현을 위해 연구진은 하이퍼파라미터 설정과 알고리즘 설계에 특별한 주의를 기울였습니다. 특히, DPO 알고리즘의 확장성을 위한 인프라 구축에 중점을 두었습니다.
### 선호도 학습 레시피와 분석

선호도 학습의 하이퍼파라미터와 알고리즘 설계는 TÜLU 3의 성능을 결정짓는 핵심 요소입니다. 연구진은 DPO(Direct Preference Optimization) 알고리즘의 효과적인 구현을 위해 다음과 같은 수학적 프레임워크를 개발했습니다.

$$ \mathcal{L}_{\text{DPO}}(\theta) = \mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} [\lambda \cdot \text{KL}(\pi_\theta \| \pi_{\text{ref}}) - \log \sigma(r_\theta(x,y_w) - r_\theta(x,y_l))] $$

여기서:
- \\(\lambda\\)는 KL 발산 항의 가중치
- \\(\pi_\theta\\)는 현재 모델의 정책
- \\(\pi_{\text{ref}}\\)는 참조 모델의 정책
- \\(r_\theta\\)는 선호도 점수 함수

이 손실 함수는 모델이 선호도 학습을 수행하면서도 기존 모델의 유용한 특성들을 보존할 수 있도록 설계되었습니다. 특히, KL 발산 항은 모델이 참조 모델로부터 너무 멀리 벗어나지 않도록 제한하는 역할을 합니다.

학습 과정에서는 적응적 학습률 스케줄링을 사용하여 최적의 수렴을 달성했습니다.

$$ \eta_t = \eta_0 \cdot \min(1, \sqrt{\frac{t_{\text{warmup}}}{t}}) $$

여기서:
- \\(\eta_t\\)는 스텝 \\(t\\)에서의 학습률
- \\(\eta_0\\)는 초기 학습률
- \\(t_{\text{warmup}}\\)은 웜업 스텝 수

DPO의 확장성을 위해 연구진은 분산 학습 인프라를 구축했습니다. 이는 다음과 같은 배치 처리 전략을 포함합니다.

$$ B_{\text{effective}} = B_{\text{local}} \cdot N_{\text{gpu}} \cdot G_{\text{acc}} $$

여기서:
- \\(B_{\text{effective}}\\)는 효과적인 배치 크기
- \\(B_{\text{local}}\\)은 각 GPU에서의 로컬 배치 크기
- \\(N_{\text{gpu}}\\)는 사용된 GPU 수
- \\(G_{\text{acc}}\\)는 그래디언트 누적 스텝 수

이러한 분산 학습 설정에서 그래디언트 업데이트는 다음과 같이 수행됩니다.

$$ \theta_{t+1} = \theta_t - \eta_t \cdot \frac{1}{G_{\text{acc}}} \sum_{i=1}^{G_{\text{acc}}} \nabla_\theta \mathcal{L}_{\text{DPO}}(\theta_t, B_i) $$

여기서 \\(B_i\\)는 \\(i\\)번째 마이크로 배치를 나타냅니다.

연구진은 이러한 학습 프레임워크를 통해 대규모 언어 모델에서도 효율적인 선호도 학습이 가능함을 입증했습니다. 특히, 그래디언트 체크포인팅과 혼합 정밀도 학습을 통해 메모리 효율성을 크게 개선했으며, 이는 더 큰 배치 크기와 더 긴 시퀀스 길이에서의 학습을 가능하게 했습니다.

### 검증 가능한 보상을 통한 강화학습 (RLVR)

TÜLU 3의 핵심 혁신 중 하나는 검증 가능한 보상을 통한 강화학습(Reinforcement Learning with Verifiable Rewards, RLVR) 방법론입니다. RLVR은 언어 모델의 출력을 객관적으로 평가할 수 있는 명확한 기준이 있는 작업들에 특화된 학습 방법입니다.

RLVR의 데이터셋은 수학 문제 풀이, 코드 생성, 논리적 추론과 같이 정답이 명확하게 정의될 수 있는 작업들로 구성됩니다. 이러한 작업들은 모델의 출력이 정답과 일치하는지를 자동으로 검증할 수 있어, 인간의 주관적인 평가 없이도 모델의 성능을 정확하게 측정할 수 있습니다.

RLVR의 학습 과정은 다음과 같은 수학적 프레임워크를 따릅니다.

$$ R(y|x) = \begin{cases} 
r_{\text{verify}} & \text{if } V(y, y^*) = 1 \\
0 & \text{otherwise}
\end{cases} $$

여기서:
- \\(x\\)는 입력 프롬프트
- \\(y\\)는 모델의 출력
- \\(y^*\\)는 정답
- \\(V(y, y^*)\\)는 출력이 정답과 일치하는지 검증하는 함수
- \\(r_{\text{verify}}\\)는 정답일 경우 주어지는 보상값

이 보상 함수를 기반으로, RLVR은 다음과 같은 정책 최적화 목적 함수를 사용합니다.

$$ J(\theta) = \mathbb{E}_{(x,y^*) \sim \mathcal{D}} [\mathbb{E}_{y \sim \pi_\theta(\cdot|x)} [R(y|x)]] $$

여기서 \\(\pi_\theta\\)는 모델의 정책을 나타내며, \\(\mathcal{D}\\)는 학습 데이터셋입니다.

RLVR의 주요 분석 결과는 다음과 같습니다.

1. 검증 가능한 보상의 효과성: RLVR은 GSM8K와 같은 수학 문제 해결 작업에서 기존의 지도 학습이나 선호도 기반 학습 방법들보다 우수한 성능을 보였습니다.

2. 학습 안정성: 객관적인 검증 기준을 사용함으로써, 인간 평가자들의 주관적인 판단에 의존하는 기존 방법들보다 더 안정적인 학습이 가능했습니다.

3. 확장성: RLVR은 코드 실행 결과나 논리적 추론의 정확성과 같이 자동화된 검증이 가능한 다양한 작업들로 쉽게 확장될 수 있습니다.

RLVR의 인프라는 효율적인 분산 학습을 위해 설계되었습니다. 특히 Ray를 활용한 분산 컴퓨팅 프레임워크를 도입하여, 다음과 같은 이점을 얻었습니다.

$$ \text{throughput} = \frac{N_{\text{gpu}} \cdot B_{\text{local}}}{T_{\text{step}}} $$

여기서:
- \\(N_{\text{gpu}}\\)는 사용된 GPU 수
- \\(B_{\text{local}}\\)은 각 GPU에서의 로컬 배치 크기
- \\(T_{\text{step}}\\)은 각 학습 스텝에 소요되는 시간

이러한 분산 학습 설정을 통해 RLVR은 대규모 언어 모델의 효율적인 학습이 가능했으며, 최종적으로 GPT-4 수준의 성능을 달성할 수 있었습니다.
### 검증 가능한 보상을 통한 강화학습 (RLVR)

RLVR의 최종 실험 결과는 이 방법론의 효과성을 명확하게 입증했습니다. 연구진은 GSM8K 수학 문제 해결 데이터셋에서 RLVR을 적용한 결과, 기존의 지도 학습 방식보다 약 15% 높은 정확도를 달성했습니다. 이는 다음과 같은 수식으로 표현되는 정책 그래디언트 업데이트를 통해 가능했습니다.

$$ \nabla_\theta J(\theta) = \mathbb{E}_{(x,y^*) \sim \mathcal{D}} [\mathbb{E}_{y \sim \pi_\theta(\cdot|x)} [R(y|x) \nabla_\theta \log \pi_\theta(y|x)]] $$

여기서 정책 그래디언트는 PPO(Proximal Policy Optimization) 알고리즘을 통해 최적화되며, 다음과 같은 클리핑된 목적 함수를 사용합니다.

$$ L^{\text{CLIP}}(\theta) = \mathbb{E}_t [\min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t)] $$

여기서:
- \\(r_t(\theta)\\)는 새로운 정책과 이전 정책의 확률 비율
- \\(A_t\\)는 이점(advantage) 추정값
- \\(\epsilon\\)은 클리핑 파라미터

RLVR의 인프라는 효율적인 배치 처리를 위해 그래디언트 누적을 활용합니다. 실제 배치 크기는 다음과 같이 계산됩니다.

$$ B_{\text{effective}} = B_{\text{local}} \cdot N_{\text{gpu}} \cdot G_{\text{acc}} $$

여기서:
- \\(B_{\text{effective}}\\)는 효과적인 배치 크기
- \\(B_{\text{local}}\\)은 각 GPU에서의 로컬 배치 크기
- \\(N_{\text{gpu}}\\)는 사용된 GPU 수
- \\(G_{\text{acc}}\\)는 그래디언트 누적 스텝 수

실험 결과, RLVR은 특히 복잡한 추론이 필요한 작업에서 탁월한 성능을 보였습니다. 예를 들어, 다단계 수학 문제에서 RLVR로 학습된 모델은 중간 계산 과정을 명확하게 보여주며 최종 답안에 도달하는 능력을 보여주었습니다. 이는 검증 가능한 보상 체계가 모델의 추론 과정 전반을 개선하는 데 효과적임을 입증합니다.

또한, RLVR은 학습 과정에서 발생하는 그래디언트 폭주 문제를 방지하기 위해 그래디언트 클리핑을 적용합니다.

$$ \mathbf{g}_{\text{clipped}} = \min\left(1, \frac{\text{clip threshold}}{\|\mathbf{g}\|_2}\right) \cdot \mathbf{g} $$

여기서 \\(\mathbf{g}\\)는 원래의 그래디언트를, \\(\text{clip threshold}\\)는 설정된 임계값을 나타냅니다.

### TÜLU 3의 평가 프레임워크

TÜLU 3의 평가 프레임워크는 세 가지 핵심 목표를 바탕으로 설계되었습니다. 첫째, 모든 평가가 재현 가능해야 하며, 둘째, 개발 과정에서 사용된 벤치마크뿐만 아니라 미공개 태스크에서도 모델의 일반화 능력을 평가할 수 있어야 하고, 셋째, 평가 설정(프롬프트 템플릿과 전략)이 다양한 모델들에 대해 공정해야 합니다.

이러한 목표를 달성하기 위해 연구진은 OLMES(Open Language Model Evaluation System)라는 재현 가능한 평가를 위한 오픈소스 툴킷을 개발했습니다. OLMES는 개발용 평가 세트와 미공개 평가 세트로 구성된 포괄적인 평가 스위트를 제공하며, 다양한 모델들과의 실험을 통해 도출된 권장 평가 설정을 포함합니다.

평가 스위트는 지식 회상(Knowledge Recall), 추론(Reasoning), 수학(Math), 코딩(Coding), 지시사항 준수(Instruction Following), 안전성(Safety) 등 핵심 기술들을 평가하도록 설계되었습니다. 각 평가는 다음과 같은 세부 설정을 포함합니다.

1. Chain-of-Thought(CoT) 프롬프팅 사용 여부
2. Few-shot 학습을 위한 예시 수
3. 채팅 형식 사용 여부
4. 멀티턴 In-Context Learning(ICL) 적용 여부
5. 평가 메트릭 (예: Exact Match, Pass@k, Winrate 등)

특히 MMLU와 같은 지식 평가에서는 다음과 같은 프롬프트 형식을 사용합니다.

```
질문: [문제]
생각해봅시다.
[추론 과정]
따라서, 답은 [답안]입니다.
```

이러한 형식은 모델이 단순히 답을 맞히는 것을 넘어 추론 과정을 명시적으로 보여줄 수 있도록 유도합니다.

TÜLU 3의 평가 프레임워크는 또한 데이터 오염을 방지하기 위한 엄격한 기준을 적용합니다. 연구진은 n-gram 매칭을 사용하여 학습 데이터와 평가 데이터 간의 중복을 탐지하고, 중복이 발견된 데이터는 평가 세트에서 제거합니다. 이는 다음과 같은 수식으로 표현됩니다.

$$ \text{overlap}(x, y) = \frac{|\text{ngrams}(x) \cap \text{ngrams}(y)|}{|\text{ngrams}(x)|} $$

여기서 \\(\text{ngrams}(x)\\)는 텍스트 \\(x\\)의 n-gram 집합을 나타냅니다. 두 텍스트 간의 중복도가 임계값을 초과하면 해당 데이터는 제거됩니다.

$$ \text{is contaminated}(x, y) = \mathbb{1}[\text{overlap}(x, y) > \tau] $$

여기서 \\(\tau\\)는 중복 판단 임계값을 나타냅니다.
TÜLU 3의 평가 프레임워크는 개발용 평가와 미공개 평가라는 두 가지 주요 구성 요소로 나뉩니다. 개발용 평가 세트는 모델 개발 과정에서 성능을 측정하고 개선하는 데 사용되며, 미공개 평가 세트는 최종 모델의 일반화 능력을 평가하는 데 활용됩니다.

개발용 평가 세트에서는 다양한 평가 방법을 적용합니다. 예를 들어, MMLU 평가에서는 제로샷 CoT 설정을 도입하여 모델이 추론 과정을 "요약"하도록 유도합니다. 이는 다음과 같은 수식으로 표현되는 성능 메트릭을 사용합니다.

$$ \text{accuracy}_{\text{macro}} = \frac{1}{|S|} \sum_{s \in S} \frac{1}{|Q_s|} \sum_{q \in Q_s} \mathbb{1}[f(q) = y_q] $$

여기서 \\(S\\)는 평가 대상 과목들의 집합, \\(Q_s\\)는 과목 \\(s\\)의 문제 집합, \\(f(q)\\)는 모델의 예측, \\(y_q\\)는 정답을 나타냅니다.

GSM8K와 MATH 평가에서는 멀티턴 ICL 설정을 사용하며, 답안 추출을 위해 'flex' 방식을 도입했습니다. 이는 다음과 같은 검증 함수를 통해 구현됩니다.

$$ v(y, y^*) = \begin{cases} 
1 & \text{if } \exists e \in E: \text{extract}(y, e) = y^* \\
0 & \text{otherwise}
\end{cases} $$

여기서 \\(E\\)는 다양한 답안 추출 방식의 집합이며, \\(\text{extract}(y, e)\\)는 추출 방식 \\(e\\)를 사용하여 모델의 출력 \\(y\\)에서 답안을 추출하는 함수입니다.

안전성 평가는 HarmBench, XSTest, WildGuard 등 다양한 벤치마크를 통합하여 수행됩니다. 각 벤치마크는 모델이 안전하지 않은 요청을 거부하는지, 그리고 XSTest와 WildJailbreak의 경우 정상적인 요청에는 적절히 응답하는지를 평가합니다. 안전성 점수는 다음과 같이 계산됩니다.

$$ \text{safety score} = \frac{1}{|B|} \sum_{b \in B} \text{accuracy}_b $$

여기서 \\(B\\)는 안전성 벤치마크의 집합이며, \\(\text{accuracy}_b\\)는 각 벤치마크에서의 정확도를 나타냅니다.

이러한 평가 프레임워크는 [Github](http://github.com/allenai/olmes)를 통해 공개되어 있으며, 연구 커뮤니티가 쉽게 재현하고 활용할 수 있도록 설계되었습니다. 예를 들어, MMLU-Pro 평가를 수행하기 위해서는 `olmes --task mmlu_pro::tulu3 --model llama3.1-8b-instruct`와 같은 간단한 명령어를 사용할 수 있습니다.
### TÜLU 3의 평가 프레임워크 - 미공개 평가

TÜLU 3의 미공개 평가 스위트는 모델의 실제 활용 시나리오에 더 가깝게 설계되었습니다. 연구진은 미공개 평가를 위한 태스크 설계 과정에서 개발용 평가와는 독립적인 설계 프로세스를 적용했으며, 다음과 같은 일반 원칙을 따랐습니다.

첫째, 실제 사용자들이 모델과 상호작용하는 방식에 가깝게 태스크를 구성했습니다. 예를 들어, 대화형 형식으로 제시되는 퓨샷 예제나 정해진 형식의 사고 연쇄(Chain-of-Thought) 예제를 피하고, 대신 자연스러운 지시사항 형태로 태스크를 제시합니다.

둘째, 명확한 지시사항을 통해 맥락을 설정하고 간결한 추론을 유도하며 답안 형식을 명시합니다. 이는 다음과 같은 수식으로 표현되는 평가 메트릭을 사용합니다.

$$ \text{score}(y, y^*) = \text{metric}(\text{normalize}(y), y^*) $$

여기서 \\(\text{normalize}(y)\\)는 모델의 출력을 표준화된 형식으로 변환하는 함수이며, \\(\text{metric}\\)은 태스크별 평가 기준을 나타냅니다.

연구진은 또한 IFEval-OOD(IFEval Out-of-Distribution)라는 새로운 평가 벤치마크를 도입했습니다. 이는 기존 IFEval의 25가지 제약 조건을 넘어서는 52가지의 제약 조건을 포함하며, 다음과 같은 여섯 가지 범주로 구성됩니다.

1. 인명 언급 관련 제약
2. 형식 관련 제약
3. 비율 관련 제약
4. 문장 관련 제약
5. 단어 관련 제약
6. 사용자 정의 제약

각 제약 조건은 WildChat에서 추출한 미공개 프롬프트와 결합되어 최종 평가 데이터셋을 구성합니다. 제약 조건 준수 여부는 다음과 같은 검증 함수로 평가됩니다.

$$ v_c(y) = \begin{cases} 
1 & \text{if constraint } c \text{ is satisfied by } y \\
0 & \text{otherwise}
\end{cases} $$

또한 연구진은 HREF(Human Reference-guided Evaluation of instruction Following)라는 새로운 자동 평가 방법을 개발했습니다. HREF는 브레인스토밍, 개방형/폐쇄형 질의응답, 추출, 생성, 재작성, 요약, 분류, 수치 추론, 다중 문서 합성, 사실 확인 등 11가지 지시사항 수행 태스크를 평가합니다. 평가는 다음과 같은 승률 계산을 통해 이루어집니다.

$$ \text{winrate}(f, f_{\text{base}}) = \frac{1}{|D|} \sum_{x \in D} \mathbb{1}[\text{judge}(f(x), f_{\text{base}}(x)) = 1] $$

여기서 \\(f\\)는 평가 대상 모델, \\(f_{\text{base}}\\)는 기준 모델, \\(\text{judge}\\)는 출력의 품질을 비교하는 판단 함수입니다.
### TÜLU 3의 평가 프레임워크 - 미공개 평가 결과

TÜLU 3의 미공개 평가 스위트에서의 실험 결과는 모델의 일반화 능력에 대한 중요한 통찰을 제공합니다. 연구진은 TÜLU 3 모델들을 Llama 3.1 Instruct 모델들, Hermes 3 Llama 3.1 모델들과 8B 및 70B 크기에서 비교 평가했습니다. 이때 주목할 점은 TÜLU 3 모델들의 경우 이러한 평가들이 모두 미공개였던 반면, 다른 모델들의 경우 GPQA, MMLU-Pro, AGIEval, DeepMind Math, BigCodeBench 등의 평가에 노출되었을 가능성을 배제할 수 없다는 것입니다.

평가 결과는 다음과 같은 주요 발견을 보여줍니다. 첫째, TÜLU 3는 미공개 평가에서도 우수한 일반화 능력을 보여주었습니다. 이는 다음과 같은 수식으로 표현되는 평균 성능 지표에서 확인할 수 있습니다.

$$ \text{avg performance} = \frac{1}{|T|} \sum_{t \in T} \text{score}_t $$

여기서 \\(T\\)는 미공개 평가 태스크의 집합이며, \\(\text{score}_t\\)는 각 태스크에서의 성능 점수입니다.

둘째, MATH 평가에 최적화된 학습이 DeepMind Math로의 일반화에는 제한적인 효과를 보였습니다. 이는 다음과 같은 형식 불일치 문제에 기인합니다.

$$ \text{format mismatch}(y) = \begin{cases}
1 & \text{if LaTeX formatting in } y \\
0 & \text{otherwise}
\end{cases} $$

셋째, 모든 모델들이 IFEval과 IFEval-OOD 간에 상당한 성능 차이를 보였습니다. 이는 검증 가능한 제약 조건을 따르는 것이 모델들에게 여전히 도전적인 과제임을 시사합니다. 성능 차이는 다음과 같이 정량화됩니다.

$$ \text{performance gap} = \text{score}_{\text{IFEval}} - \text{score}_{\text{IFEval-OOD}} $$

넷째, 지식 회상 능력의 일반화는 후처리 학습 방식에 따라 다른 양상을 보였습니다. 특히 GPQA에서의 성능은 기본 모델이 동일하더라도 후처리 학습 방식에 따라 상이한 결과를 보였습니다.

마지막으로, 지시사항 수행 능력은 태스크의 종류에 따라 다양한 성능을 보였습니다. HREF의 11가지 하위 태스크에서 TÜLU 3 70B는 Llama 3.1 70B Instruct보다 5개 태스크에서 우수한 성능을 보였으며, 이는 지시사항 수행 능력이 태스크의 특성에 따라 다르게 일반화됨을 시사합니다.

이러한 결과들은 대규모 언어 모델의 일반화 능력을 평가하고 개선하는 데 있어 중요한 시사점을 제공합니다. 특히 특정 벤치마크에 대한 과적합을 피하고 실제 활용 시나리오에서의 성능을 향상시키기 위해서는 더욱 다양하고 체계적인 평가 방법론이 필요함을 보여줍니다.

### 논의 및 향후 연구 방향

TÜLU 3 연구진은 최종 모델에 포함되지 않은 여러 방법론과 접근 방식들에 대한 실험 결과를 공유함으로써, 후속 연구에 중요한 통찰을 제공합니다. 특히 온라인 DPO(Direct Preference Optimization)와 거부 샘플링(Rejection Sampling)에 대한 시도와 그 한계점을 상세히 분석했습니다.

온라인 DPO는 기존 DPO 방법의 한계를 극복하기 위해 제안되었습니다. 일반적인 DPO는 사전에 수집된 선호도 데이터셋을 사용하는 오프라인 방식인 반면, 온라인 DPO는 학습 중인 정책 \\(\pi_\theta\\)로부터 직접 피드백을 받아 학습하는 방식입니다. 이 방법은 다음과 같은 세 단계로 구성됩니다.

1. 현재 정책에서 프롬프트당 2개의 응답을 샘플링
2. 응답 쌍에 대한 온라인 피드백을 통해 선호도 데이터 생성
3. 생성된 선호도 데이터를 사용하여 표준 DPO 손실 함수로 정책 \\(\pi_\theta\\) 업데이트

연구진은 실험의 확장성을 위해 보상 모델을 사용하여 피드백을 얻었습니다. Skywork 데이터셋의 82K 선호도 데이터 포인트를 사용하여 보상 모델을 1 에포크 동안 학습시켰으며, 수학적 추론 능력 향상을 위해 합성 온폴리시(on-policy) 수학 관련 선호도 데이터로 추가 학습을 진행했습니다.

그러나 TÜLU 3 DPO 체크포인트에서 시작하여 수학 문제에 대해 200K 에피소드 동안 온라인 DPO를 적용한 결과, GSM8K에서는 거의 개선이 없었고 MATH 성능은 오히려 저하되었습니다. 연구진은 다양한 샘플링 온도와 KL 페널티 계수를 실험했지만, 일반적인 영역과 특정 목표 영역 모두에서 제한적인 성과를 보여 이 접근 방식을 더 이상 탐구하지 않았습니다.

거부 샘플링은 최근 대규모 언어 모델의 학습 후 성능을 향상시키는 데 널리 사용되는 방법입니다. 이 방법은 초기 SFT 모델과 선호도 데이터를 사용하여 각 SFT 프롬프트에 대해 n개의 응답을 생성하고, 보상 모델이나 LLM을 판단자로 사용하여 최상의 응답을 선택합니다. 나머지 응답들은 선호/거부 쌍을 만드는 데 활용됩니다.

연구진의 실험 결과, 거부 샘플링은 필요한 계산 비용에 비해 성능 향상이 미미했습니다. 특히 공개된 모델들은 후보 응답들 중 최상의 응답을 선택하는 데 어려움을 겪었습니다. 또한 원본 응답을 판단 대상에 포함시키는 것이 새로 생성된 응답들만을 고려하는 것보다 더 나은 결과를 보였습니다.
TÜLU 3 연구진은 향후 연구 방향으로 세 가지 주요 영역을 제시했습니다. 첫째는 긴 문맥과 다중 턴 대화 능력의 향상입니다. 현재 TÜLU 3의 데이터는 상대적으로 짧은 길이와 제한된 다중 턴 대화(평균 2.4턴, 대부분 2,048 토큰 미만)로 구성되어 있습니다. 

[Pawar와 연구진](https://arxiv.org/abs/2401.07872)이 지적한 바와 같이, 언어 모델의 문맥 윈도우를 확장하는 것은 새로운 활용 사례를 가능하게 하고 더 많은 인컨텍스트 예제를 활용할 수 있게 함으로써 성능 향상을 이끌어낼 수 있습니다. [Gemini Team](https://arxiv.org/abs/2403.05530)의 연구에서도 볼 수 있듯이, 긴 문맥 처리 능력은 실제 응용에서 매우 중요한 역할을 합니다.

둘째는 다국어 능력의 향상입니다. TÜLU 3는 현재 [Üstün과 연구진](https://arxiv.org/abs/2402.06619)의 Aya 데이터셋을 포함하고는 있지만, 주로 영어 데이터와 평가에 초점을 맞추고 있습니다. 이는 전 세계의 다양한 언어 사용자들의 필요를 충분히 반영하지 못하는 한계가 있습니다. 다국어 후처리 학습은 단일어 후처리 학습과는 다른 기술이 필요할 수 있습니다. 예를 들어, 교차 언어 정렬이나 신중한 데이터 균형 전략이 요구될 수 있습니다.

셋째는 도구 사용과 에이전트 프레임워크의 통합입니다. [Qu와 연구진](https://arxiv.org/abs/2405.17935)의 연구에서 볼 수 있듯이, 언어 모델들은 점점 더 큰 시스템의 일부로 배치되어 도구를 사용하거나 에이전트 프레임워크의 일부로 작동하고 있습니다. 특히 도구 사용 학습은 모델의 추론과 수학적 능력을 극적으로 향상시킬 수 있는 자연스러운 방법입니다. 모든 것을 '가중치에 담는' 대신, 외부 도구를 활용하여 모델의 능력을 확장하는 것이 더 효과적일 수 있습니다.

이러한 향후 연구 방향은 TÜLU 3의 현재 한계를 명확히 인식하고, 이를 개선하기 위한 구체적인 방향을 제시한다는 점에서 의미가 있습니다. 특히 도구 사용과 에이전트 프레임워크의 통합은 언어 모델의 능력을 획기적으로 향상시킬 수 있는 유망한 연구 방향으로 보입니다.

## 관련 연구와 결론

### 후처리 학습 방법론의 진화

현대적인 후처리 학습의 기원은 다중 작업 언어 모델 학습, 특히 지시사항 학습(instruction tuning)에서 찾을 수 있습니다. Mishra와 연구진, Wei와 연구진, Sanh와 연구진 등은 언어 모델이 작업 지시사항과 그에 대한 응답을 포함하는 샘플들로 학습되어 새로운 작업에 대해 '제로샷' 일반화가 가능하도록 하는 연구를 진행했습니다. 초기의 지시사항 학습 데이터셋들은 자연어 추론과 같은 전통적인 NLP 작업에 초점을 맞추었으나, 이후 ChatGPT와 같은 대화형 언어 모델들의 등장으로 후처리 학습 기법은 지시사항 학습을 넘어 선호도 학습(preference tuning) 단계를 포함하도록 발전했습니다.

선호도 학습의 초기 연구는 제어를 위한 심층 강화학습 실험에서 시작되었습니다. Christiano와 연구진, Ibarz와 연구진, Leike와 연구진의 연구는 인간의 선호도로부터 보상 모델을 학습하고, 이를 통해 언어 모델을 강화학습 프레임워크로 최적화하는 방법을 제시했습니다. 최근에는 Rafailov와 연구진, Zhao와 연구진이 언어 모델을 선호도 데이터에 직접 학습시키는 방법을 개발하여 선호도 학습의 복잡성을 크게 줄였습니다.

초기의 선호도 학습 접근법들은 수만에서 수십만 개의 인간 작성 지시사항과 선호도 레이블을 사용하는 등 매우 인간 중심적이었습니다. 하지만 최근의 연구들은 인간과 합성 선호도 데이터의 혼합, 다중 라운드 학습, 다양한 학습 알고리즘 등을 활용하는 방향으로 발전했습니다. 

공개된 후처리 학습 방법론들은 비공개 연구실에서 발전한 RLHF에 비해 다소 뒤처져 있었습니다. 초기의 '공개 후처리 학습 방법론'은 지시사항 학습 단계에 초점을 맞추어, 공개된 언어 모델들을 합성 생성되거나 인간이 작성한 데이터셋으로 미세조정하는 방식을 취했습니다. Wang과 연구진의 연구에서 볼 수 있듯이 이러한 데이터셋들을 결합하는 것으로도 강력한 성능을 달성할 수 있었지만, Ivison과 연구진의 연구는 인간 평가를 기반으로 할 때 비공개 모델들과의 격차를 줄이기 위해서는 선호도 학습 단계가 중요하다는 것을 보여주었습니다.

오늘날 대부분의 인기 있는 공개 모델들은 DPO(Direct Preference Optimization) 또는 그 변형을 사용하여 선호도 학습을 수행합니다. 예를 들어 TÜLU 2, Zephyr-β, Starling 등이 있습니다. 그러나 이러한 모델들은 데이터와 성능 측면에서 비공개 후처리 학습 방법론들에 비해 뒤처져 있습니다. LMSYS의 ChatBotArena 상위 50위 안에 후처리 학습 데이터를 공개한 모델이 없다는 점이 이를 잘 보여줍니다. 대부분의 공개 방법론들은 비공개 설정에 비해 상대적으로 적은 데이터와 적은 수의 학습 라운드를 사용합니다.

### 검증 가능한 보상을 통한 학습

RLVR(Reinforcement Learning with Verifiable Rewards) 접근법은 최근의 여러 연구들과 관련이 있습니다. 특히 STaR(Self-taught Reasoner)와 TRICE 연구는 기존의 정답을 신호로 사용하여 더 나은 모델의 추론 과정(또는 사고 연쇄)을 생성하는 방법을 연구했습니다. STaR는 정책 그래디언트 알고리즘의 근사로 볼 수 있으며, Quiet-STaR는 이 접근법을 확장하여 모델이 추가 생성을 통해 일반적인 언어 모델링을 개선하도록 학습시켰습니다.

TRICE도 여러 추론 경로에 걸쳐 학습을 수행하여 정답의 가능성을 높이고자 했으며, 이를 위해 맞춤형 MCMC 기반 EM 알고리즘을 사용했습니다. 최근의 VinePPO는 GSM8k와 MATH의 정확성에서 얻은 이진 보상을 사용하여 새로운 PPO 기반 알고리즘을 테스트했으며, 다른 최근 연구들은 코드 피드백을 학습 신호로 사용하는 방법을 탐구했습니다.

이에 비해 TÜLU 3가 제안하는 RLVR은 기존의 RL 프레임워크(PPO)를 사용하여 이진 보상으로 완전히 온라인 학습을 수행한다는 점에서 차이가 있습니다. 또한 수학 영역을 넘어 정확한 지시사항 준수 능력 향상에도 이 방법이 효과적임을 보여주었습니다. 연구진은 가치 모델 초기화와 검증 가능한 보상이 있는 일반 보상 모델 사용 등 RLVR의 핵심 구성 요소들에 대한 체계적인 분석도 수행했습니다.

### 결론

TÜLU 3는 완전히 공개된 최첨단 언어 모델군으로, 현대적인 후처리 프레임워크와 함께 TÜLU 3 DATA, TÜLU 3 EVAL, TÜLU 3 CODE, TÜLU 3 RECIPE 등 모든 구성 요소를 공개했습니다. Llama 3.1 기반 버전의 최종 모델들과 함께 중간 체크포인트, 학습 데이터, 학습 코드, 평가 코드를 공개함으로써 후속 연구를 위한 기반을 마련했습니다. TÜLU 3는 공개와 비공개 후처리 방법론 간의 격차를 해소하여 공개 후처리 연구의 새로운 이정표를 세웠으며, 이를 통해 다른 연구자들이 공개 기반 모델들을 활용하여 다양한 작업에서 높은 성능을 달성할 수 있게 되었습니다.

- - -
### References
* [Tulu 3: Pushing Frontiers in Open Language Model Post-Training](http://arxiv.org/pdf/2411.15124v2)