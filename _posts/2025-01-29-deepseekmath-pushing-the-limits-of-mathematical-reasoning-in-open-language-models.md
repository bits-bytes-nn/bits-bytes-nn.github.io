---
layout: post
title: "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
date: 2024-02-05 18:55:32
author: "DeepSeek-AI"
categories: "Language-Models"
tags: ["DeepSeekMath-Corpus", "Group-Relative-Policy-Optimization", "Iterative-Reinforcement-Learning", "Large-Scale-Reinforcement-Learning-on-Base-Model", "Reasoning-Oriented-Reinforcement-Learning"]
use_math: true
cover: /assets/images/language-models.webp
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
수학적 추론은 복잡한 구조와 엄격한 논리적 사고를 요구하는 특성으로 인해 언어 모델에게 큰 도전 과제를 제시합니다. GPT-4나 Gemini-Ultra와 같은 최신 비공개 모델들이 수학적 추론에서 인상적인 성능을 보여주고 있지만, 공개적으로 사용 가능한 오픈소스 모델들은 이들과 상당한 성능 차이를 보이고 있었습니다. 이러한 격차를 해소하고 수학적 추론 능력이 뛰어난 오픈소스 모델을 개발하는 것이 이 연구의 주요 동기가 되었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
연구진은 두 가지 핵심적인 혁신을 제시했습니다. 첫째, Common Crawl에서 1,200억 개의 고품질 수학 관련 토큰을 추출하는 정교한 데이터 선택 파이프라인을 개발했습니다. 이는 fastText 기반 분류기와 반복적인 데이터 정제 과정을 통해 구현되었습니다. 둘째, 기존 PPO의 한계를 극복하기 위해 그룹 상대 정책 최적화(GRPO)라는 새로운 강화학습 알고리즘을 도입했습니다. GRPO는 비평자 모델을 제거하고 그룹 점수에서 직접 기준선을 추정함으로써 메모리 효율성을 크게 개선했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
구현은 크게 사전 학습과 강화학습 두 단계로 진행되었습니다. 사전 학습 단계에서는 DeepSeek-Coder-Base-v1.5 7B를 기반 모델로 사용하여 5,000억 개의 토큰으로 학습을 진행했습니다. 이 중 56%는 DeepSeekMath Corpus에서, 나머지는 코드, arXiv, Common Crawl 데이터에서 추출되었습니다. 강화학습 단계에서는 GRPO를 적용하여 모델을 더욱 개선했습니다. GRPO는 각 입력에 대해 여러 응답을 생성하고 이들의 상대적 선호도를 활용하여 정책을 최적화하는 방식으로 구현되었습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
이 연구의 가장 주목할 만한 성과는 DeepSeekMath가 MATH 벤치마크에서 51.7%의 정확도를 달성하여 오픈소스 모델 최초로 50%를 넘어섰다는 점입니다. 이는 웹 데이터에서 고품질의 수학 콘텐츠를 추출하고 효율적인 강화학습을 적용함으로써 비공개 모델들과의 격차를 크게 줄일 수 있다는 것을 보여줍니다. 또한 코드 학습이 수학적 추론 능력 향상에 도움이 된다는 것과 arXiv 논문이 예상보다 제한적인 효과를 보인다는 새로운 통찰을 제공했습니다. 이러한 발견들은 향후 더 강력한 수학적 추론 모델 개발을 위한 중요한 지침이 될 것입니다.
- - -
## DeepSeekMath: 오픈 언어 모델의 수학적 추론 한계에 도전하다

### 서론

수학적 추론은 복잡하고 구조화된 특성으로 인해 언어 모델에게 상당한 도전 과제를 제시합니다. DeepSeekMath 7B는 이러한 도전에 대응하기 위해 개발된 모델로, DeepSeek-Coder-Base-v1.5 7B를 기반으로 Common Crawl에서 추출한 1,200억 개의 수학 관련 토큰과 함께 자연어 및 코드 데이터를 활용하여 사전 학습을 진행했습니다.

![MATH 벤치마크 성능 비교](https://ar5iv.org//html/2402.03300/assets/figures/Math.png)

위 그래프는 외부 도구나 투표 기법을 사용하지 않고도 DeepSeekMath 7B가 경쟁 수준의 MATH 벤치마크에서 51.7%의 인상적인 점수를 달성했음을 보여줍니다. 이는 Gemini-Ultra와 GPT-4의 성능에 근접한 수준입니다. 특히 64개의 샘플에 대한 자기 일관성(self-consistency) 평가에서는 60.9%의 성능을 달성했습니다.

DeepSeekMath의 수학적 추론 능력은 두 가지 핵심 요소에 기인합니다. 첫째, 정교하게 설계된 데이터 선택 파이프라인을 통해 공개적으로 사용 가능한 웹 데이터의 잠재력을 효과적으로 활용했습니다. 둘째, Proximal Policy Optimization (PPO)의 변형인 Group Relative Policy Optimization (GRPO)를 도입하여 PPO의 메모리 사용을 최적화하면서 동시에 수학적 추론 능력을 향상시켰습니다.

이러한 혁신적인 접근 방식을 통해 DeepSeekMath는 오픈소스 언어 모델 중에서 가장 높은 수준의 수학적 추론 능력을 보여주고 있으며, 이는 인공지능의 수학적 추론 능력 향상에 있어 중요한 이정표가 되고 있습니다.

대규모 언어 모델(Large Language Models, LLM)은 수학적 추론 분야에서 혁신적인 발전을 이루어냈습니다. 이러한 모델들은 정량적 추론 벤치마크와 기하학적 추론 벤치마크에서 괄목할 만한 성과를 보여주었으며, 복잡한 수학 문제 해결에서 인간을 효과적으로 지원하고 있습니다. 하지만 GPT-4나 Gemini-Ultra와 같은 최신 모델들은 공개적으로 사용할 수 없으며, 현재 접근 가능한 오픈소스 모델들은 이들과 상당한 성능 차이를 보이고 있습니다.

이러한 한계를 극복하기 위해 본 연구에서는 DeepSeekMath를 소개합니다. DeepSeekMath는 오픈소스 모델들의 수학적 능력을 크게 향상시키고 학술적 벤치마크에서 GPT-4의 성능에 근접하는 도메인 특화 언어 모델입니다. 이 모델의 개발을 위해 연구진은 DeepSeekMath Corpus를 구축했습니다. 이 데이터셋은 Common Crawl에서 fastText 기반 분류기를 사용하여 추출한 1,200억 개의 고품질 수학 관련 토큰으로 구성되어 있습니다.

데이터 구축 과정은 다음과 같이 진행되었습니다. 첫 번째 단계에서는 OpenWebMath의 데이터를 긍정 예시로, 다양한 웹 페이지를 부정 예시로 활용하여 분류기를 학습시켰습니다. 이후 이 분류기를 사용하여 Common Crawl에서 추가적인 긍정 예시를 발굴하고, 이를 인간 주석자를 통해 정제했습니다. 마지막으로, 이렇게 향상된 데이터셋으로 분류기를 업데이트하여 성능을 개선했습니다.

평가 결과는 이 대규모 코퍼스의 우수한 품질을 입증합니다. DeepSeekMath-Base 7B 모델은 GSM8K에서 64.2%, MATH 데이터셋에서 36.2%의 성능을 달성하여 Minerva 540B를 능가했습니다. 또한 DeepSeekMath Corpus가 다국어를 지원하기 때문에 중국어 수학 벤치마크에서도 성능 향상이 관찰되었습니다.

DeepSeekMath-Base는 DeepSeek-Coder-Base-v1.5 7B를 초기 모델로 사용했는데, 이는 일반 언어 모델보다 코드 학습 모델에서 시작하는 것이 더 효과적이라는 발견에 기반합니다. 흥미롭게도 수학 학습은 MMLU와 BBH 벤치마크에서도 모델의 성능을 향상시켰는데, 이는 수학적 능력뿐만 아니라 일반적인 추론 능력도 함께 강화됨을 시사합니다.

사전 학습 이후에는 체인오브소트, 프로그램오브소트, 도구 통합 추론 데이터를 활용하여 수학적 명령어 튜닝을 적용했습니다. 그 결과로 얻어진 DeepSeekMath-Instruct 7B는 모든 7B 규모의 경쟁 모델들을 능가하며, 700억 개의 파라미터를 가진 오픈소스 명령어 튜닝 모델들과 비견할 만한 성능을 보여주었습니다.

연구진은 또한 근접 정책 최적화(Proximal Policy Optimization, PPO)의 변형인 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)를 도입했습니다. GRPO는 기존 PPO의 비평자 모델을 제거하고 대신 그룹 점수에서 기준선을 추정함으로써 학습에 필요한 자원을 크게 절감했습니다. 영어 명령어 튜닝 데이터의 일부만을 사용했음에도 불구하고, GRPO는 이미 강력한 성능을 보여준 DeepSeekMath-Instruct의 성능을 더욱 향상시켰습니다. 도메인 내 과제에서는 GSM8K의 경우 82.9%에서 88.2%로, MATH의 경우 46.8%에서 51.7%로 성능이 향상되었으며, 도메인 외 수학 과제(예: CMATH)에서도 84.6%에서 88.8%로 성능이 개선되었습니다.

연구진은 거부 샘플링 미세 조정(Rejection Sampling Fine-Tuning, RFT), 직접 선호도 최적화(Direct Preference Optimization, DPO), PPO, GRPO와 같은 다양한 방법들을 이해하기 위한 통합 패러다임을 제시했습니다. 이 통합 패러다임을 통해 이러한 방법들이 직접적이거나 단순화된 강화학습 기법으로 개념화될 수 있음을 발견했습니다. 또한 온라인 대 오프라인 학습, 결과 대 과정 감독, 단일 턴 대 반복적 강화학습 등 광범위한 실험을 수행하여 이 패러다임의 핵심 요소들을 심도 있게 조사했습니다.

마지막으로 연구진은 강화학습이 명령어 튜닝 모델의 성능을 향상시키는 원리를 설명하고, 이 통합 패러다임을 기반으로 더 효과적인 강화학습을 달성하기 위한 잠재적 방향성을 제시했습니다. 이러한 연구 결과는 수학적 데이터 처리에 대한 연구진의 경험이 연구 커뮤니티에 중요한 시작점이 될 수 있으며, 향후 더 많은 개선의 여지가 있음을 시사합니다.

### 주요 기여

DeepSeekMath의 연구는 크게 두 가지 핵심 영역에서 중요한 기여를 했습니다. 첫째는 대규모 수학 사전 학습 분야이고, 둘째는 강화학습의 탐구와 분석입니다.

수학 사전 학습 측면에서, 연구진은 Common Crawl 데이터에 수학적 정보가 풍부하게 포함되어 있다는 사실을 입증했습니다. 정교하게 설계된 데이터 선택 파이프라인을 통해 수학적 내용이 포함된 웹 페이지에서 1,200억 개의 토큰으로 구성된 고품질 데이터셋인 DeepSeekMath Corpus를 구축했습니다. 이는 Lewkowycz와 연구진이 Minerva에서 사용한 수학 웹 페이지의 약 7배, 최근 공개된 OpenWebMath의 9배에 달하는 규모입니다.

연구진의 사전 학습 기반 모델인 DeepSeekMath-Base 7B는 Minerva 540B와 비견할 만한 성능을 달성했습니다. 이는 수학적 추론 능력에 있어 모델의 파라미터 수가 유일한 핵심 요소가 아니며, 고품질 데이터로 학습된 더 작은 모델도 강력한 성능을 달성할 수 있다는 점을 시사합니다. 또한 수학 학습 이전에 코드 학습을 진행하면 도구 사용 여부와 관계없이 수학 문제 해결 능력이 향상된다는 사실을 발견했습니다. 이는 코드 학습이 추론 능력을 향상시키는지에 대한 오랜 의문에 대한 부분적인 답을 제시합니다.

강화학습 분야에서는 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)라는 효율적이고 효과적인 강화학습 알고리즘을 도입했습니다. GRPO는 비평자 모델을 사용하지 않고 대신 그룹 점수에서 기준선을 추정함으로써 근접 정책 최적화(Proximal Policy Optimization, PPO)에 비해 학습 리소스를 크게 절감했습니다. 이 알고리즘은 명령어 튜닝 데이터만을 사용하여 DeepSeekMath-Instruct 모델의 성능을 크게 향상시켰으며, 강화학습 과정에서 도메인 외 성능도 향상되는 것이 관찰되었습니다.

연구진은 RFT, DPO, PPO, GRPO와 같은 다양한 방법들을 이해하기 위한 통합 패러다임을 제시했습니다. 온라인과 오프라인 학습, 결과와 과정 감독, 단일 턴과 반복적 강화학습 등 광범위한 실험을 통해 이 패러다임의 핵심 요소들을 심도 있게 조사했습니다. 이를 바탕으로 강화학습의 효과성 이면의 이유를 탐구하고, 대규모 언어 모델의 더 효과적인 강화학습을 달성하기 위한 잠재적 방향성을 제시했습니다.

### 평가 지표와 벤치마크 분석

DeepSeekMath의 성능 평가는 영어와 중국어 수학 추론 능력, 형식 수학, 자연어 이해력, 추론 능력, 코드 생성 능력 등 다양한 측면에서 이루어졌습니다. 영어 수학 추론 평가에는 GSM8K, MATH, SAT, OCW Courses, MMLU-STEM과 같은 벤치마크가 사용되었으며, 중국어 평가에는 MGSM-zh, CMATH, Gaokao-MathCloze, Gaokao-MathQA가 활용되었습니다.

영어 벤치마크에서 DeepSeekMath-Base는 비공개 모델인 Minerva 540B와 견줄 만한 성능을 보여주었으며, Mistral 7B나 Llemma-34B와 같은 기존의 오픈소스 기반 모델들을 큰 차이로 능가했습니다. 특히 주목할 만한 점은 중국어 벤치마크에서의 우수한 성능입니다. 이는 기존 연구들과 달리 영어 데이터에만 국한되지 않고 고품질의 다국어 수학 사전 학습 데이터를 포함했기 때문입니다.

수학적 명령어 튜닝과 강화학습을 적용한 DeepSeekMath-Instruct와 DeepSeekMath-RL은 더욱 향상된 성능을 보여주었습니다. 특히 경쟁 수준의 MATH 데이터셋에서 오픈소스 커뮤니티 최초로 50% 이상의 정확도를 달성했습니다.

형식 수학 분야에서는 Isabelle 증명 보조기를 사용하여 miniF2F 데이터셋에서 비형식적 증명을 형식적 증명으로 변환하는 과제를 수행했습니다. DeepSeekMath-Base는 이 과제에서도 뛰어난 퓨 샷 자동 형식화 성능을 보여주었습니다.

모델의 전반적인 이해력과 추론 능력을 평가하기 위해 57개의 다양한 주제를 다루는 MMLU 벤치마크와 23개의 복잡한 다단계 추론 과제로 구성된 BIG-Bench Hard(BBH)에서도 평가를 진행했습니다. 코드 생성 능력은 HumanEval과 MBPP 벤치마크를 통해 검증했습니다. 주목할 만한 점은 수학 사전 학습이 언어 이해력과 추론 능력 향상에도 긍정적인 영향을 미쳤다는 것입니다.

### 수학 사전 학습

DeepSeekMath의 수학 사전 학습은 대규모 언어 모델의 수학적 추론 능력을 향상시키기 위한 핵심 과정입니다. 이 과정은 DeepSeek-Coder-Base-v1.5 7B를 기반 모델로 사용하여 진행되었으며, Common Crawl에서 추출한 1,200억 개의 수학 관련 토큰으로 구성된 DeepSeekMath Corpus를 활용했습니다.

사전 학습 과정에서는 수학적 내용의 특성을 고려한 특별한 토큰화 전략을 적용했습니다. 수학 공식과 수식은 LaTeX 형식으로 표현되었으며, 이를 위해 기존 토크나이저를 확장하여 수학 기호와 표현을 효과적으로 처리할 수 있도록 했습니다. 특히 \\\\(, \\\\), \\$, \\$$와 같은 LaTeX 구분자들이 단일 토큰으로 처리되도록 하여 수식의 구조적 특성을 보존했습니다.

데이터 품질 관리를 위해 연구진은 3단계 필터링 파이프라인을 구축했습니다. 첫째, fastText 기반 분류기를 사용하여 Common Crawl에서 수학 관련 콘텐츠를 식별했습니다. 둘째, 정규 표현식과 휴리스틱 규칙을 적용하여 수식의 구문 오류를 검출하고 제거했습니다. 마지막으로, 인간 전문가의 검토를 통해 최종 데이터셋의 품질을 검증했습니다.

사전 학습 과정에서는 수학적 추론 능력 향상을 위해 특별히 설계된 학습 목표를 도입했습니다. 기존의 마스크 언어 모델링(Masked Language Modeling, MLM)에 더해, 수식 완성 과제와 수학적 추론 과제를 포함시켰습니다. 수식 완성 과제에서는 수식의 일부를 마스킹하고 이를 복원하도록 했으며, 수학적 추론 과제에서는 문제 해결 과정의 중간 단계를 예측하도록 했습니다.

모델 학습은 분산 학습 환경에서 진행되었으며, 그래디언트 누적과 혼합 정밀도 학습을 활용하여 학습 효율성을 최적화했습니다. 배치 크기는 2,048로 설정되었으며, AdamW 옵티마이저를 사용했습니다. 학습률은 선형 웜업 후 코사인 감소 스케줄을 따랐으며, 가중치 감쇠는 0.1로 설정되었습니다.

이러한 사전 학습 과정을 통해 DeepSeekMath는 수학적 추론에 필요한 기본적인 지식과 능력을 획득할 수 있었습니다. 특히 코드 기반 모델을 시작점으로 선택한 것이 효과적이었는데, 이는 코드 작성 능력이 구조화된 수학적 사고와 밀접한 관련이 있기 때문입니다.
### 수학 사전 학습의 기술적 세부사항

DeepSeekMath의 사전 학습 과정에서는 수학적 표현의 정확한 처리를 위해 특수한 토크나이저 확장 기법을 도입했습니다. 기존 토크나이저의 어휘 사전에 LaTeX 수식 구분자와 자주 사용되는 수학 기호들을 추가했으며, 이 과정에서 토큰 임베딩의 차원은 4,096으로 유지되었습니다. 수식 처리의 효율성을 높이기 위해 자주 등장하는 수학 표현들은 단일 토큰으로 처리되도록 설계했습니다.

데이터 필터링 파이프라인은 정밀한 품질 관리를 위해 구체적인 기준을 적용했습니다. fastText 분류기는 OpenWebMath 데이터셋을 긍정 예시로, 일반 웹 페이지를 부정 예시로 사용하여 학습되었습니다. 이 분류기는 95% 이상의 정확도를 달성했으며, 특히 수학적 표현이 풍부한 콘텐츠를 효과적으로 식별했습니다. 구문 오류 검출을 위한 정규 표현식은 LaTeX 수식의 괄호 균형, 수식 구분자의 올바른 사용, 특수 문자의 이스케이프 처리 등을 검증했습니다.

학습 과정의 최적화를 위해 그래디언트 누적 단계는 32로 설정되어 효과적인 배치 크기 16,384를 달성했습니다. 학습률은 초기 1e-4에서 시작하여 처음 2,000 스텝 동안 선형적으로 증가했고, 이후 코사인 감소 스케줄을 따라 감소했습니다. 가중치 감쇠는 전체 파라미터에 대해 0.1로 균일하게 적용되었으며, 드롭아웃은 사용하지 않았습니다.

수학적 추론 과제를 위한 특별한 학습 목표에서는 수식의 구조적 특성을 고려했습니다. 수식 완성 과제에서는 전체 수식의 15-25%를 마스킹했으며, 마스킹된 부분은 수식의 구조적 완결성을 해치지 않는 범위 내에서 선택되었습니다. 수학적 추론 과제에서는 문제 해결 과정의 중간 단계를 예측하도록 하여, 모델이 단순히 최종 답을 맞추는 것이 아니라 추론 과정 전체를 이해하도록 유도했습니다.

분산 학습 환경은 32개의 A100 GPU를 활용하여 구축되었으며, 데이터 병렬성과 모델 병렬성을 조합하여 학습 효율성을 최적화했습니다. 전체 학습 과정은 약 2주가 소요되었으며, 이 기간 동안 모델은 약 1,200억 개의 토큰을 처리했습니다. 검증 손실이 더 이상 감소하지 않는 시점에서 학습을 종료했으며, 이는 약 100,000 학습 스텝에 해당했습니다.

### 데이터 수집과 정제

DeepSeekMath 연구진은 Common Crawl에서 대규모 수학 코퍼스를 구축하기 위한 체계적인 반복 파이프라인을 개발했습니다. 이 파이프라인은 시드 코퍼스(예: 고품질 수학 관련 데이터셋의 작은 모음)에서 시작하여 Common Crawl에서 수학 관련 웹 페이지를 수집하고 정제하는 과정을 자동화합니다.

![데이터 수집 파이프라인](https://ar5iv.org//html/2402.03300/assets/x1.png)

위 그림은 Common Crawl 데이터셋에서 수학 관련 웹 페이지를 수집하고 처리하는 반복적인 파이프라인을 보여줍니다. 이 파이프라인의 주요 목적은 fastText 모델을 학습시켜 수학 관련 웹 페이지를 식별하고, Common Crawl에서 해당 페이지들을 추출한 뒤, URL 경로를 분석하여 수학 관련 도메인을 발견하는 것입니다.

파이프라인의 첫 단계에서는 Paster와 연구진이 개발한 OpenWebMath를 초기 시드 코퍼스로 선택했습니다. 이 코퍼스를 사용하여 fastText 모델을 학습시켰는데, 시드 코퍼스에서 50만 개의 데이터 포인트를 긍정 예시로, Common Crawl에서 무작위로 선택한 50만 개의 웹 페이지를 부정 예시로 사용했습니다.

fastText 모델의 학습 구성은 다음과 같습니다.
- 벡터 차원: 256
- 학습률: 0.1
- 단어 n-gram의 최대 길이: 3
- 단어 최소 출현 횟수: 3
- 학습 에포크: 3

원본 Common Crawl의 크기를 줄이기 위해 URL 기반 중복 제거와 근접 중복 제거 기법을 적용하여 400억 개의 HTML 웹 페이지로 축소했습니다. 학습된 fastText 모델을 사용하여 이 중복이 제거된 Common Crawl에서 수학 관련 웹 페이지를 추출했습니다. 저품질 콘텐츠를 필터링하기 위해 fastText 모델이 예측한 점수에 따라 페이지들의 순위를 매기고, 상위 랭킹의 페이지들만 보존했습니다. 보존할 데이터의 양은 상위 400억, 800억, 1,200억, 1,600억 토큰에 대한 사전 학습 실험을 통해 평가되었으며, 첫 번째 반복에서는 상위 400억 토큰을 유지하기로 결정했습니다.

첫 번째 데이터 수집 반복 이후에도 많은 수학 관련 웹 페이지가 수집되지 않은 상태로 남아있었는데, 이는 fastText 모델이 충분한 다양성을 갖지 못한 긍정 예시 세트로 학습되었기 때문입니다. 이를 해결하기 위해 연구진은 시드 코퍼스를 보강할 추가적인 수학 관련 소스를 식별했습니다.
### 데이터 수집과 정제

구체적으로, 연구진은 전체 Common Crawl을 동일한 기본 URL을 공유하는 웹 페이지들로 구성된 개별 도메인으로 구성했습니다. 각 도메인에 대해 첫 번째 반복에서 수집된 웹 페이지의 비율을 계산했으며, 10% 이상의 웹 페이지가 수집된 도메인을 수학 관련 도메인으로 분류했습니다. 예를 들어, mathoverflow.net과 같은 도메인이 이러한 방식으로 식별되었습니다.

이후 연구진은 이렇게 식별된 도메인 내에서 수학적 콘텐츠와 관련된 URL을 수동으로 주석 처리했습니다. 예를 들어, mathoverflow.net/questions와 같은 URL 패턴을 식별했습니다. 이전에 수집되지 않았지만 이러한 URL과 연결된 웹 페이지들은 시드 코퍼스에 추가되었습니다. 이러한 접근 방식을 통해 더 많은 긍정 예시를 수집할 수 있었고, 이를 바탕으로 개선된 fastText 모델을 학습하여 후속 반복에서 더 많은 수학 데이터를 추출할 수 있었습니다.

네 번의 데이터 수집 반복 후, 연구진은 3,550만 개의 수학 관련 웹 페이지를 수집했으며, 이는 총 1,200억 개의 토큰에 해당합니다. 네 번째 반복에서 연구진은 데이터의 약 98%가 이미 세 번째 반복에서 수집되었음을 발견했고, 이에 따라 데이터 수집을 종료하기로 결정했습니다.

벤치마크 오염을 방지하기 위해 연구진은 Guo와 연구진의 방법을 따라 GSM8K, MATH와 같은 영어 수학 벤치마크와 CMATH, AGIEval과 같은 중국어 벤치마크의 문제나 답변이 포함된 웹 페이지를 필터링했습니다. 필터링 기준은 다음과 같습니다.

평가 벤치마크의 어떤 하위 문자열과 정확히 일치하는 10-gram 문자열을 포함하는 텍스트 세그먼트는 수학 학습 코퍼스에서 제거되었습니다. 10-gram보다 짧지만 최소 3-gram 이상인 벤치마크 텍스트의 경우, 정확한 매칭을 사용하여 오염된 웹 페이지를 필터링했습니다.

이러한 반복적인 데이터 수집 및 정제 과정을 통해 연구진은 고품질의 수학 관련 콘텐츠를 대규모로 확보할 수 있었습니다. 특히 도메인 기반의 분석과 URL 패턴 식별을 통한 시드 코퍼스 확장은 데이터의 다양성과 품질을 크게 향상시켰습니다. 또한 엄격한 벤치마크 오염 방지 절차를 통해 평가의 신뢰성을 보장했습니다.

### DeepSeekMath 코퍼스의 품질 검증

DeepSeekMath 연구진은 최근 공개된 수학 학습 코퍼스들과의 비교 실험을 통해 DeepSeekMath 코퍼스의 품질을 검증했습니다. 비교 대상이 된 코퍼스는 MathPile, OpenWebMath, Proof-Pile-2입니다. MathPile은 교과서, 위키피디아, ProofWiki, CommonCrawl, StackExchange, arXiv 등에서 수집한 89억 개의 토큰으로 구성되어 있으며, 그 중 85% 이상이 arXiv에서 추출되었습니다. OpenWebMath는 수학적 내용을 포함하는 CommonCrawl 데이터를 필터링하여 만든 136억 개의 토큰으로 구성된 코퍼스입니다. Proof-Pile-2는 OpenWebMath, AlgebraicStack(103억 개의 수학 관련 코드 토큰), arXiv 논문(280억 개의 토큰)으로 구성되어 있으며, arXiv:Web:Code의 비율을 2:4:1로 설정했습니다.

#### 학습 설정

실험을 위해 연구진은 DeepSeek LLM과 동일한 프레임워크를 공유하는 13억 개의 파라미터를 가진 사전 학습된 언어 모델을 기반으로 사용했습니다. 각각의 수학 코퍼스에 대해 1,500억 개의 토큰을 학습했으며, 모든 실험은 효율적이고 가벼운 HAI-LLM 학습 프레임워크를 사용하여 진행되었습니다.

학습 과정에서는 DeepSeek LLM의 학습 방식을 따라 AdamW 옵티마이저를 사용했으며, \\(\beta_1=0.9\\), \\(\beta_2=0.95\\), \\(\text{weight\_decay}=0.1\\)로 설정했습니다. 학습률은 다단계 스케줄링을 통해 2,000번의 웜업 스텝 후 최대값에 도달하고, 학습 과정의 80% 지점에서 31.6%로 감소하며, 90% 지점에서 최대값의 10.0%로 더 감소하도록 설정했습니다. 최대 학습률은 5.3e-4로 설정했으며, 4K 컨텍스트 길이에서 4M 토큰의 배치 크기를 사용했습니다.

![코퍼스 비교](https://ar5iv.org//html/2402.03300/assets/figures/corpus_comparisons.png)

위 그래프는 다양한 수학 데이터셋에서 각 언어 모델의 성능을 보여줍니다. 특히 GSM8K, MATH, CMATH, BBH 등의 벤치마크에서 모델들의 정확도가 학습 토큰 수에 따라 어떻게 향상되는지를 나타냅니다. DeepSeekMath 코퍼스로 학습한 모델이 대부분의 벤치마크에서 다른 모델들을 능가하는 것을 확인할 수 있으며, 이는 대규모 텍스트 데이터로부터 수학적 추론을 학습하는 능력이 우수함을 보여줍니다.

#### 평가 결과

DeepSeekMath 코퍼스는 높은 품질, 다국어 지원, 대규모 크기라는 세 가지 주요 특징을 가지고 있습니다. 품질 측면에서는 8개의 수학 벤치마크에서 퓨 샷 체인오브소트 프롬프팅을 사용한 평가를 진행했습니다. 실험 결과는 DeepSeekMath 코퍼스로 학습한 모델이 다른 모델들보다 우수한 성능을 보여주었습니다. 특히 Proof-Pile-2의 전체 에포크(500억 토큰)에서도 DeepSeekMath 코퍼스로 학습한 모델이 더 나은 성능을 보여주었는데, 이는 DeepSeekMath 코퍼스의 평균적인 품질이 더 높다는 것을 시사합니다.

다국어 측면에서 DeepSeekMath 코퍼스는 영어와 중국어를 주요 언어로 포함하고 있습니다. 실험 결과는 이 코퍼스로 학습한 모델이 영어와 중국어 모두에서 수학적 추론 능력이 향상되었음을 보여줍니다. 반면 기존의 영어 중심 수학 코퍼스들은 중국어 수학적 추론에서 제한적인 개선을 보이거나 오히려 성능을 저하시키는 경우도 있었습니다.

마지막으로 규모 측면에서 DeepSeekMath 코퍼스는 기존 수학 코퍼스들보다 몇 배 더 큽니다. 이로 인해 DeepSeek-LLM 1.3B 모델은 DeepSeekMath 코퍼스로 학습했을 때 더 가파른 학습 곡선과 지속적인 성능 향상을 보여주었습니다. 반면 기준 코퍼스들은 크기가 작아 학습 과정에서 여러 번 반복되었고, 그 결과 모델 성능이 빠르게 정체되는 현상을 보였습니다.

### DeepSeekMath-Base 7B의 학습과 평가

DeepSeekMath-Base 7B는 강력한 수학적 추론 능력을 갖춘 기반 모델로, DeepSeek-Coder-Base-v1.5 7B를 초기 모델로 사용하여 5,000억 개의 토큰으로 학습되었습니다. 학습 데이터는 DeepSeekMath Corpus에서 56%, AlgebraicStack에서 4%, arXiv에서 10%, GitHub 코드에서 20%, 그리고 영어와 중국어로 된 Common Crawl 데이터에서 10%로 구성되었습니다. 학습 과정에서는 앞서 설명한 학습 설정을 따르되, 최대 학습률을 4.2e-4로 설정하고 1,000만 토큰의 배치 크기를 사용했습니다.

DeepSeekMath-Base 7B의 수학적 능력을 평가하기 위해 연구진은 외부 도구 없이 수학 문제를 해결하는 능력, 도구를 활용한 문제 해결 능력, 그리고 형식적 정리 증명 능력을 포괄적으로 평가했습니다. 또한 자연어 이해력, 추론 능력, 프로그래밍 능력 등 모델의 전반적인 성능도 함께 평가했습니다.

#### 단계별 추론을 통한 수학 문제 해결

연구진은 영어와 중국어로 된 8개의 벤치마크에서 퓨 샷 체인오브소트 프롬프팅을 사용하여 DeepSeekMath-Base의 성능을 평가했습니다. 이 벤치마크들은 GSM8K, MATH, CMATH와 같은 정량적 추론과 MMLU-STEM, Gaokao-MathQA와 같은 객관식 문제들을 포함하며, 초등학교부터 대학 수준까지 다양한 난이도의 수학 분야를 다룹니다.

DeepSeekMath-Base 7B는 모든 오픈소스 기반 모델들 중에서 8개 벤치마크 전체에서 가장 우수한 성능을 보여주었습니다. 특히 대회 수준의 MATH 데이터셋에서는 기존 오픈소스 기반 모델들보다 10% 이상 높은 정확도를 달성했으며, PaLM을 기반으로 수학 텍스트로 추가 학습된 77배 더 큰 비공개 기반 모델인 Minerva 540B의 성능도 능가했습니다.

#### 도구를 활용한 수학 문제 해결

연구진은 GSM8K와 MATH 데이터셋에서 퓨 샷 프로그램오브소트 프롬프팅을 사용하여 프로그램 기반 수학적 추론 능력을 평가했습니다. 모델은 각 문제를 math나 sympy와 같은 라이브러리를 활용할 수 있는 Python 프로그램을 작성하여 해결하도록 프롬프팅되었으며, 프로그램의 실행 결과가 답안으로 평가되었습니다. 평가 결과, DeepSeekMath-Base 7B는 이전 최고 성능을 보였던 Llemma 34B의 성능을 뛰어넘었습니다.
#### 형식적 정리 증명 능력

형식적 증명 자동화는 수학적 증명의 정확성과 신뢰성을 보장하고 효율성을 높이는 데 중요한 역할을 합니다. DeepSeekMath-Base 7B는 Jiang과 연구진이 제안한 비형식적-형식적 증명 변환 과제에서 평가되었습니다. 이 과제는 비형식적 진술문, 그에 대응하는 형식적 진술문, 그리고 비형식적 증명이 주어졌을 때 형식적 증명을 생성하는 것입니다.

연구진은 올림피아드 수준의 형식적 수학을 다루는 miniF2F 벤치마크에서 모델을 평가했으며, 퓨 샷 프롬프팅을 통해 각 문제에 대한 Isabelle 형식적 증명을 생성했습니다. Jiang과 연구진의 방법론을 따라, 모델이 증명 스케치를 생성하면 자동화된 증명기인 Sledgehammer를 활용하여 누락된 세부 사항을 채우는 방식으로 평가를 진행했습니다.

평가 결과에서 DeepSeekMath-Base 7B는 강력한 증명 자동화 성능을 보여주었습니다. miniF2F-valid에서 25.8%, miniF2F-test에서 24.6%의 정확도를 달성하여, Mistral 7B(18.9%, 18.0%), CodeLlama 7B(16.3%, 17.6%), Llemma 7B(20.6%, 22.1%) 등 다른 오픈소스 모델들의 성능을 크게 상회했습니다.

#### 자연어 이해, 추론, 코드 생성 능력

DeepSeekMath-Base 7B의 전반적인 능력을 평가하기 위해 자연어 이해력은 MMLU에서, 추론 능력은 BBH에서, 코딩 능력은 HumanEval과 MBPP에서 평가를 진행했습니다. MMLU와 BBH에서는 퓨 샷 체인오브소트 프롬프팅을 사용했으며, HumanEval에서는 제로 샷 설정을, MBPP에서는 퓨 샷 설정을 적용했습니다.

평가 결과, DeepSeekMath-Base 7B는 선행 모델인 DeepSeek-Coder-Base-v1.5와 비교했을 때 MMLU와 BBH에서 상당한 성능 향상을 보여주었습니다. 이는 수학 학습이 언어 이해력과 추론 능력 향상에도 긍정적인 영향을 미쳤음을 시사합니다. 또한 학습 과정에서 코드 토큰을 포함시킴으로써 두 코딩 벤치마크에서 DeepSeek-Coder-Base-v1.5의 성능을 효과적으로 유지할 수 있었습니다.

종합적으로 DeepSeekMath-Base 7B는 일반 모델인 Mistral 7B와 비교했을 때 세 가지 추론 및 코딩 벤치마크에서 모두 우수한 성능을 보여주었습니다. MMLU에서 54.9%, BBH에서 59.5%, HumanEval에서 40.9%, MBPP에서 52.6%의 정확도를 달성하여, Mistral 7B의 성능(각각 62.4%, 55.7%, 28.0%, 41.4%)을 대부분의 벤치마크에서 능가했습니다.

### 강화학습

DeepSeekMath의 강화학습 접근 방식은 기존 근접 정책 최적화(Proximal Policy Optimization, PPO)를 개선한 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)를 도입했습니다. GRPO는 PPO의 비평자 모델을 제거하고 그룹 점수에서 기준선을 추정함으로써 학습에 필요한 메모리 사용량을 크게 절감했습니다.

![GRPO 아키텍처](https://ar5iv.org/html/2402.03300/assets/figures/GRPO.png)

위 그림은 GRPO의 핵심 구조를 보여줍니다. GRPO는 각 입력에 대해 여러 응답을 생성하고, 이들을 선호도에 따라 순위를 매깁니다. 이 과정에서 그룹 내 상대적 선호도를 활용하여 정책을 최적화합니다. 수학적으로 GRPO의 목적 함수는 다음과 같이 정의됩니다.

\\[
\mathcal{L}_{\text{GRPO}} = \mathbb{E}_{(x,y^{1:n})} \left[ \sum_{k=1}^{n-1} \log \frac{\exp(r_\pi(x,y^k))}{\sum_{i=k}^n \exp(r_\pi(x,y^i))} \right]
\\]

여기서 \\(x\\)는 입력 프롬프트, \\(y^{1:n}\\)은 선호도 순으로 정렬된 \\(n\\)개의 응답, \\(r_\pi\\)는 정책 \\(\pi\\)의 보상 함수입니다. 이 목적 함수는 더 높은 선호도를 가진 응답이 더 낮은 선호도를 가진 응답보다 높은 확률을 갖도록 정책을 최적화합니다.

GRPO의 주요 혁신은 PPO의 비평자 모델을 제거하고 그룹 점수에서 직접 기준선을 추정하는 것입니다. 이는 다음과 같이 구현됩니다.

\\[
\hat{A}(x,y^k) = r_\pi(x,y^k) - \frac{1}{n-k} \sum_{i=k+1}^n r_\pi(x,y^i)
\\]

이 장점 추정치는 현재 응답의 보상과 그보다 낮은 순위의 응답들의 평균 보상 간의 차이를 계산합니다. 이를 통해 비평자 모델 없이도 효과적인 정책 최적화가 가능해졌습니다.

GRPO는 영어 명령어 튜닝 데이터의 일부만을 사용했음에도 불구하고, GSM8K에서 82.9%에서 88.2%로, MATH에서 46.8%에서 51.7%로 성능을 향상시켰습니다. 특히 주목할 만한 점은 도메인 외 수학 과제인 CMATH에서도 84.6%에서 88.8%로 성능이 개선되었다는 것입니다.

연구진은 또한 거부 샘플링 미세 조정(Rejection Sampling Fine-Tuning, RFT), 직접 선호도 최적화(Direct Preference Optimization, DPO), PPO, GRPO와 같은 다양한 방법들을 이해하기 위한 통합 패러다임을 제시했습니다. 이 통합 패러다임은 이러한 방법들이 직접적이거나 단순화된 강화학습 기법으로 개념화될 수 있음을 보여줍니다.
### 강화학습의 기술적 세부사항

GRPO의 학습 과정은 크게 세 가지 핵심 단계로 구성됩니다. 첫 번째는 응답 생성 단계로, 각 입력 프롬프트 \\(x\\)에 대해 다양한 샘플링 전략을 사용하여 여러 응답 \\(y^{1:n}\\)을 생성합니다. 두 번째는 선호도 평가 단계로, 생성된 응답들의 품질을 평가하여 순위를 매깁니다. 마지막은 정책 업데이트 단계로, GRPO의 목적 함수를 최적화하여 정책을 개선합니다.

GRPO의 구현에서는 동적 온도 조절 메커니즘을 도입하여 학습의 안정성을 향상시켰습니다.

\\[
\mathcal{T}^{i>k}_{k} = \frac{1}{r_\pi(x,y^k) - r_\pi(x,y^i)}
\\]

\\[
\mathcal{T}^{k}_{k} = \min_{i>k}\mathcal{T}^{i}_{k}
\\]

이 동적 온도 조절은 선호도 차이가 명확한 경우와 그렇지 않은 경우를 구분하여 처리합니다. 큰 선호도 차이가 있는 경우에는 더 강한 학습 신호를, 작은 차이가 있는 경우에는 더 약한 학습 신호를 제공합니다.

GRPO의 장점 추정 방식은 PPO의 일반화된 장점 추정(Generalized Advantage Estimation, GAE)과 비교할 때 다음과 같은 특징을 가집니다.

\\[
\hat{A}_{\text{GRPO}}(x,y^k) = r_\pi(x,y^k) - V_{\text{group}}(x,k)
\\]

여기서 \\(V_{\text{group}}(x,k)\\)는 그룹 기반 가치 추정치로, \\(k\\) 순위 이하의 응답들의 평균 보상을 나타냅니다. 이는 PPO의 학습된 가치 함수를 대체하며, 계산 효율성과 안정성을 모두 개선합니다.

GRPO의 정책 업데이트는 다음과 같은 클리핑된 목적 함수를 사용합니다.

\\[
\mathcal{L}_{\text{GRPO}}^{\text{clip}} = \mathbb{E} \left[ \min(\rho_t \hat{A}_{\text{GRPO}}, \text{clip}(\rho_t, 1-\epsilon, 1+\epsilon)\hat{A}_{\text{GRPO}}) \right]
\\]

여기서 \\(\rho_t\\)는 새로운 정책과 이전 정책의 확률 비율이며, \\(\epsilon\\)은 클리핑 파라미터입니다. 이 클리핑 메커니즘은 정책 업데이트의 크기를 제한하여 학습의 안정성을 보장합니다.

GRPO의 실제 구현에서는 배치 크기 1024, 학습률 1e-5, 클리핑 파라미터 0.2를 사용했습니다. 각 배치에서는 16개의 응답을 생성하고, 이들을 보상 함수에 따라 순위를 매깁니다. 보상 함수는 수학적 정확성, 추론 과정의 명확성, 답안의 완결성 등을 종합적으로 고려하여 설계되었습니다.
### 강화학습의 실험 결과와 분석

GRPO의 성능을 검증하기 위해 연구진은 다양한 실험을 수행했습니다. 특히 온라인 대 오프라인 학습, 결과 대 과정 감독, 단일 턴 대 반복적 강화학습의 세 가지 주요 축을 중심으로 실험을 진행했습니다.

![강화학습 성능 비교](https://ar5iv.org/html/2402.03300/assets/figures/RL.png)

위 그래프는 GRPO와 다른 강화학습 방법들의 성능을 비교한 결과를 보여줍니다. GRPO는 PPO와 비교하여 더 적은 계산 자원으로도 우수한 성능을 달성했습니다. 특히 메모리 사용량 측면에서 GRPO는 PPO 대비 약 40% 감소를 보였으며, 학습 속도도 25% 이상 향상되었습니다.

강화학습의 효과성을 분석하기 위해 연구진은 다음과 같은 수학적 프레임워크를 도입했습니다.

\\[
\mathcal{L}_{\text{unified}} = \mathbb{E}_{(x,y)} \left[ \alpha \mathcal{L}_{\text{direct}} + (1-\alpha) \mathcal{L}_{\text{relative}} \right]
\\]

여기서 \\(\mathcal{L}_{\text{direct}}\\)는 직접적인 보상 최적화를, \\(\mathcal{L}_{\text{relative}}\\)는 상대적 선호도 학습을 나타내며, \\(\alpha\\)는 두 목적 함수 간의 균형을 조절하는 파라미터입니다.

연구진은 강화학습이 명령어 튜닝 모델의 성능을 향상시키는 세 가지 주요 메커니즘을 식별했습니다.

1. 탐색 공간의 효율적 활용: GRPO는 다양한 응답을 생성하고 평가함으로써 더 넓은 해결책 공간을 탐색합니다.

2. 선호도 학습의 일반화: 모델은 특정 문제에 대한 선호도 학습을 통해 유사한 유형의 다른 문제에도 적용할 수 있는 일반적인 패턴을 학습합니다.

3. 점진적 능력 향상: 반복적인 강화학습을 통해 모델은 초기 능력을 기반으로 점진적으로 더 복잡한 문제 해결 전략을 개발합니다.

실험 결과는 GRPO가 특히 복잡한 수학적 추론이 필요한 과제에서 뛰어난 성능을 보여주었습니다. MATH 데이터셋의 대수학 문제에서는 54.3%, 기하학 문제에서는 49.8%의 정확도를 달성했으며, 이는 기존 방법들과 비교하여 각각 7.2%p와 5.9%p의 향상을 나타냅니다.
### 강화학습의 구현 최적화

GRPO의 실제 구현에서는 메모리 효율성을 극대화하기 위한 여러 최적화 기법이 도입되었습니다. 그룹 기반 장점 추정을 위한 메모리 효율적인 구현은 다음과 같은 수식을 사용합니다.

\\[
\nabla_\theta \mathcal{L}_{\text{GRPO}} = \mathbb{E}_{(x,y^{1:n})} \left[ \sum_{k=1}^{n-1} \nabla_\theta \log \pi_\theta(y^k|x) \hat{A}_{\text{GRPO}}(x,y^k) \right]
\\]

이 구현에서는 그래디언트 계산을 위한 계산 그래프를 효율적으로 구성하여 메모리 사용량을 최소화합니다. 특히 그룹 내 응답들의 보상값을 계산할 때, 중간 결과를 재사용하여 중복 계산을 방지합니다.

GRPO의 배치 처리 구현에서는 동적 배치 크기 조정 기법을 도입했습니다.

\\[
B_{\text{effective}} = \min(B_{\text{max}}, \max(B_{\text{min}}, \lceil N/M \rceil))
\\]

여기서 \\(B_{\text{effective}}\\)는 실제 사용되는 배치 크기, \\(N\\)은 전체 데이터 크기, \\(M\\)은 목표 업데이트 횟수입니다. 이를 통해 학습 과정에서 메모리 사용량과 학습 효율성 사이의 균형을 동적으로 조절할 수 있습니다.

GRPO의 구현에서는 또한 그래디언트 누적 기법을 도입하여 큰 배치 효과를 시뮬레이션했습니다.

\\[
\theta_{t+1} = \theta_t + \eta \sum_{i=1}^K \frac{1}{K} \nabla_\theta \mathcal{L}_{\text{GRPO}}^i
\\]

여기서 \\(K\\)는 그래디언트 누적 단계 수, \\(\eta\\)는 학습률입니다. 이 접근 방식은 메모리 제약 하에서도 큰 배치 학습의 이점을 얻을 수 있게 해줍니다.

연구진은 GRPO의 구현에서 발생할 수 있는 수치적 안정성 문제를 해결하기 위해 로그 공간에서의 계산을 도입했습니다.

\\[
\log V_{\text{group}}(x,k) = \text{logsumexp}(\{r_\pi(x,y^i)\}_{i=k+1}^n) - \log(n-k)
\\]

이 구현은 큰 보상값 차이로 인한 수치적 오버플로우나 언더플로우를 방지하며, 계산의 정확성을 향상시킵니다.

GRPO의 학습 과정에서는 적응적 클리핑 임계값을 사용하여 학습의 안정성을 개선했습니다.

\\[
\epsilon_t = \epsilon_0 \cdot \min(1, \sqrt{\frac{\sigma_{\text{target}}}{\sigma_t}})
\\]

여기서 \\(\epsilon_t\\)는 시간 \\(t\\)에서의 클리핑 임계값, \\(\sigma_t\\)는 현재 정책 업데이트의 표준 편차, \\(\sigma_{\text{target}}\\)은 목표 표준 편차입니다. 이를 통해 학습 과정에서 발생할 수 있는 급격한 정책 변화를 방지하고 안정적인 학습을 보장합니다.

### 그룹 상대 정책 최적화

강화학습은 지도 학습 미세조정(Supervised Fine-Tuning, SFT) 단계 이후 대규모 언어 모델의 수학적 추론 능력을 더욱 향상시키는 데 효과적임이 입증되었습니다. 이 섹션에서는 효율적이고 효과적인 강화학습 알고리즘인 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)를 소개합니다.

#### PPO에서 GRPO로의 발전

근접 정책 최적화(Proximal Policy Optimization, PPO)는 대규모 언어 모델의 강화학습 미세조정 단계에서 널리 사용되는 액터-크리틱 강화학습 알고리즘입니다. PPO는 다음과 같은 대리 목적 함수를 최적화하여 모델을 학습시킵니다.

\\[
\mathcal{J}_{PPO}(\theta)=\mathbb{E}{[q\sim P(Q),o\sim\pi_{\theta_{old}}(O|q)]}\frac{1}{|o|}\sum_{t=1}^{|o|}\min\left[\frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{\theta_{old}}(o_{t}|q,o_{<t})}A_{t},\text{clip}\left(\frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{\theta_{old}}(o_{t}|q,o_{<t})},1-\varepsilon,1+\varepsilon\right)A_{t}\right]
\\]

여기서 \\(\pi_{\theta}\\)와 \\(\pi_{\theta_{old}}\\)는 각각 현재와 이전 정책 모델을 나타내며, \\(q\\)와 \\(o\\)는 질문 데이터셋과 이전 정책 \\(\pi_{\theta_{old}}\\)에서 샘플링된 출력입니다. \\(\varepsilon\\)은 PPO에서 학습 안정성을 위해 도입된 클리핑 관련 하이퍼파라미터입니다. \\(A_t\\)는 일반화된 장점 추정(Generalized Advantage Estimation, GAE)을 적용하여 계산된 장점값으로, 보상 \\({r_{\geq t}}\\)와 학습된 가치 함수 \\(V_{\psi}\\)를 기반으로 합니다.

![PPO와 GRPO 비교](https://ar5iv.org//html/2402.03300/assets/x2.png)

PPO에서는 보상 모델의 과도한 최적화를 방지하기 위해 각 토큰에서 참조 모델로부터의 KL 페널티를 보상에 추가하는 것이 표준적인 접근 방식입니다.

\\[
r_{t}=r_{\varphi}(q,o_{\leq t})-\beta\log\frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{ref}(o_{t}|q,o_{<t})}
\\]

여기서 \\(r_{\varphi}\\)는 보상 모델, \\(\pi_{ref}\\)는 일반적으로 초기 SFT 모델인 참조 모델, \\(\beta\\)는 KL 페널티의 계수입니다.

PPO에서 사용되는 가치 함수는 일반적으로 정책 모델과 비슷한 크기의 또 다른 모델이므로 상당한 메모리와 계산 부담을 초래합니다. 또한 대규모 언어 모델의 맥락에서는 보통 마지막 토큰에만 보상 모델이 점수를 할당하므로, 각 토큰에서 정확한 가치 함수를 학습하는 것이 복잡해질 수 있습니다.

이러한 문제를 해결하기 위해 위 그림과 같이 그룹 상대 정책 최적화(GRPO)를 제안합니다. GRPO는 PPO에서와 같은 추가적인 가치 함수 근사를 사용하지 않고, 대신 동일한 질문에 대해 생성된 여러 출력의 평균 보상을 기준선으로 사용합니다.

### DeepSeekMath-RL의 학습과 평가

DeepSeekMath-RL은 DeepSeekMath-Instruct 7B를 기반으로 강화학습을 적용하여 개발되었습니다. 강화학습 데이터셋은 SFT 데이터에서 추출한 GSM8K와 MATH 관련 체인오브소트 형식의 문제 약 14만 4천 개로 구성되었습니다. 연구진은 강화학습 단계에서 데이터가 부족한 벤치마크에 대한 영향을 조사하기 위해 다른 SFT 문제들은 제외했습니다.

보상 모델의 학습 데이터셋은 Wang과 연구진의 방법론을 따라 구축되었습니다. 초기 보상 모델은 DeepSeekMath-Base 7B를 기반으로 2e-5의 학습률로 학습되었습니다. GRPO 학습에서는 정책 모델의 학습률을 1e-6으로 설정했으며, KL 계수는 0.04로 지정했습니다. 각 문제에 대해 64개의 출력을 샘플링했으며, 최대 길이는 1,024 토큰, 학습 배치 크기는 1,024로 설정했습니다. 정책 모델은 각 탐색 단계 이후 단 한 번의 업데이트만 수행했습니다.

![DeepSeekMath-RL 성능 비교](https://ar5iv.org/html/2402.03300/assets/figures/Math.png)

위 그래프는 DeepSeekMath-RL 7B의 평가 결과를 보여줍니다. 평가는 DeepSeekMath-Instruct 7B와 동일한 벤치마크에서 진행되었으며, GSM8K와 MATH는 체인오브소트 추론이 적용된 도메인 내 과제로, 다른 모든 벤치마크는 도메인 외 과제로 간주되었습니다. 

평가 결과에서 주목할 만한 점은 다음과 같습니다.

1. DeepSeekMath-RL 7B는 체인오브소트 추론을 사용하여 GSM8K에서 88.2%, MATH에서 51.7%의 정확도를 달성했습니다. 이는 7B에서 70B 범위의 모든 오픈소스 모델들과 대부분의 비공개 모델들의 성능을 능가하는 결과입니다.

2. DeepSeekMath-RL 7B는 DeepSeekMath-Instruct 7B에서 시작하여 GSM8K와 MATH의 체인오브소트 형식 명령어 튜닝 데이터만으로 학습되었음에도 불구하고, 모든 평가 지표에서 DeepSeekMath-Instruct 7B의 성능을 뛰어넘었습니다. 이는 강화학습의 효과성을 입증하는 결과입니다.

이러한 결과는 제한된 도메인의 데이터로 학습된 강화학습 모델이 도메인 내 과제뿐만 아니라 도메인 외 과제에서도 성능 향상을 이끌어낼 수 있음을 보여줍니다. 특히 MATH 데이터셋에서의 51.7% 정확도는 오픈소스 모델 중 최초로 50%를 넘어선 성과이며, 이는 GRPO의 효과적인 학습 능력을 입증합니다.

### 실험 결과 논의

DeepSeekMath의 사전 학습과 강화학습 실험을 통해 얻은 주요 연구 결과들을 살펴보겠습니다. 연구진은 수학적 추론 능력 향상을 위한 두 가지 핵심 접근 방식인 대규모 수학 사전 학습과 강화학습의 효과성을 검증했습니다.

사전 학습 측면에서는 Common Crawl에서 추출한 1,200억 개의 토큰으로 구성된 DeepSeekMath Corpus의 품질이 입증되었습니다. 이 코퍼스는 기존의 수학 데이터셋들과 비교했을 때 더 큰 규모와 더 높은 품질을 보여주었으며, 특히 영어와 중국어를 모두 포함하는 다국어 지원 능력이 돋보였습니다.

강화학습 측면에서는 그룹 상대 정책 최적화(GRPO)가 기존의 근접 정책 최적화(PPO)와 비교하여 두 가지 중요한 이점을 제공했습니다. 첫째, 비평자 모델을 제거하고 그룹 점수에서 기준선을 추정함으로써 메모리 사용량을 크게 절감했습니다. 둘째, 영어 명령어 튜닝 데이터의 일부만을 사용했음에도 불구하고 도메인 내외 과제 모두에서 성능 향상을 달성했습니다.

연구진은 또한 거부 샘플링 미세 조정(RFT), 직접 선호도 최적화(DPO), PPO, GRPO와 같은 다양한 방법들을 이해하기 위한 통합 패러다임을 제시했습니다. 이 패러다임을 통해 이러한 방법들이 직접적이거나 단순화된 강화학습 기법으로 개념화될 수 있음을 발견했으며, 이는 향후 더 효과적인 강화학습 방법론 개발의 기반이 될 것으로 기대됩니다.

### 사전 학습에서 얻은 교훈

연구진은 사전 학습 과정에서 얻은 중요한 통찰을 공유합니다. 특히 코드 학습이 수학적 추론 능력에 미치는 영향과 arXiv 논문의 효과성에 대해 심도 있는 분석을 진행했습니다.

#### 코드 학습이 수학적 추론 능력을 향상시키다

코드 학습이 추론 능력을 향상시킨다는 가설은 오랫동안 검증되지 않은 채로 남아있었습니다. 연구진은 이를 수학적 영역에서 검증하기 위해 DeepSeek-LLM 1.3B 모델을 사용하여 두 가지 학습 방식을 실험했습니다.

첫 번째는 2단계 학습 방식으로, 4,000억 개의 코드 토큰으로 학습한 후 1,500억 개의 수학 토큰으로 추가 학습을 진행했습니다. 대조군으로는 코드 토큰 대신 일반 텍스트 토큰을 사용한 실험도 함께 진행했습니다. 두 번째는 1단계 학습 방식으로, 1,500억 개의 수학 토큰만을 사용하거나 4,000억 개의 코드 토큰과 1,500억 개의 수학 토큰을 혼합하여 학습했습니다.

실험 결과, 코드 학습은 도구 사용 여부와 관계없이 수학적 추론 능력을 향상시키는 것으로 나타났습니다. 2단계 학습에서 코드 학습만으로도 Python을 활용한 GSM8K와 MATH 문제 해결 능력이 크게 향상되었으며, 이후 수학 학습을 통해 더욱 개선되었습니다. 특히 코드와 수학 토큰을 혼합한 1단계 학습은 2단계 학습에서 발생하는 재앙적 망각(catastrophic forgetting) 문제를 완화하고 코딩과 프로그램 기반 수학적 추론 능력을 시너지 있게 향상시켰습니다.

#### arXiv 논문의 제한적 효과

연구진은 수학 사전 학습 데이터로 자주 사용되는 arXiv 논문의 효과를 분석했습니다. DeepSeek-LLM 1.3B와 DeepSeek-Coder-Base-v1.5 7B 모델을 대상으로 서로 다른 처리 과정을 거친 두 가지 arXiv 코퍼스를 실험했습니다.

1. MathPile: 89억 개의 토큰으로 구성된 코퍼스로, 85% 이상이 정제된 arXiv 논문입니다.
2. ArXiv-RedPajama: 서문, 주석, 매크로, 참고문헌이 제거된 280억 개의 토큰으로 구성된 코퍼스입니다.

각 모델을 각각의 arXiv 코퍼스로 학습한 결과, 예상과 달리 수학적 추론 능력이 크게 향상되지 않았습니다. GSM8K, MATH와 같은 정량적 추론 과제, MMLU-STEM과 같은 객관식 문제, miniF2F와 같은 형식 수학 과제 모두에서 성능 향상이 미미하거나 오히려 저하되는 현상이 관찰되었습니다.

다만 연구진은 이러한 결론에 몇 가지 제한 사항이 있음을 지적합니다. 정리의 비형식화와 같이 이번 연구에서 다루지 않은 특정 수학 관련 과제에서의 효과, 다른 유형의 데이터와 결합했을 때의 시너지 효과, 그리고 더 큰 규모의 모델에서 나타날 수 있는 잠재적 이점 등은 추가 연구가 필요한 영역입니다.

### 강화학습의 통찰

#### 통합 패러다임을 향하여

강화학습의 다양한 학습 방법들을 분석하기 위해 통합된 패러다임을 제시합니다. SFT(Supervised Fine-Tuning), RFT(Rejection Fine-Tuning), DPO(Direct Preference Optimization), PPO(Proximal Policy Optimization), GRPO(Group Relative Policy Optimization) 등의 학습 방법들은 모두 파라미터 θ에 대한 그래디언트로 표현될 수 있습니다.

$$
\nabla_\theta \mathcal{J}_\mathcal{A}(\theta) = \mathbb{E}_{(q,o)\sim\mathcal{D}}\left[\frac{1}{|o|}\sum_{t=1}^{|o|}GC_\mathcal{A}(q,o,t,\pi_{rf})\nabla_\theta\log\pi_\theta(o_t|q,o_{<t})\right]
$$

이 통합 패러다임에는 세 가지 핵심 요소가 있습니다. 첫째, 데이터 소스 $\mathcal{D}$로 학습 데이터를 결정합니다. 둘째, 보상 함수 $\pi_{rf}$로 학습 보상 신호의 원천이 됩니다. 셋째, 알고리즘 $\mathcal{A}$로 학습 데이터와 보상 신호를 처리하여 그래디언트 계수 $GC$를 결정하며, 이는 데이터에 대한 페널티나 강화의 크기를 결정합니다.

![DeepSeekMath-Instruct 1.3B 모델의 성능](https://ar5iv.org//html/2402.03300/assets/x3.png)

위 그래프는 DeepSeekMath-Instruct 1.3B 모델에 다양한 학습 방법(RFT, Online RFT, GRPO+OS, GRPO+PS)을 적용했을 때의 성능을 보여줍니다. 특히 온라인 RFT가 오프라인 RFT보다 우수한 성능을 보이는 것을 확인할 수 있습니다.

![반복적 강화학습의 성능](https://ar5iv.org//html/2402.03300/assets/x4.png)

위 그래프는 DeepSeekMath-Instruct 7B 모델에 반복적 강화학습을 적용한 결과를 보여줍니다. Iteration-0부터 Iteration-2까지의 성능 향상을 확인할 수 있으며, 특히 첫 번째 반복에서 가장 큰 성능 향상이 있었음을 알 수 있습니다.

데이터 소스 관점에서 학습 방법들은 온라인 샘플링과 오프라인 샘플링으로 나눌 수 있습니다. 온라인 샘플링은 실시간 학습 정책 모델의 탐색 결과를 사용하고, 오프라인 샘플링은 초기 SFT 모델의 샘플링 결과를 사용합니다. RFT와 DPO는 오프라인 방식을, Online RFT와 GRPO는 온라인 방식을 따릅니다.

실험 결과, Online RFT가 RFT보다 두 벤치마크에서 모두 우수한 성능을 보였습니다. 특히 학습 초기에는 비슷한 성능을 보이다가 후반부에서 절대적인 우위를 보였는데, 이는 초기에는 액터와 SFT 모델이 유사하여 샘플링된 데이터의 차이가 작지만, 후반부에는 액터에서 샘플링된 데이터가 더 큰 차이를 보이고 실시간 데이터 샘플링이 더 큰 이점을 제공하기 때문입니다.
### 강화학습의 통찰

#### 그래디언트 계수의 역할

그래디언트 계수는 알고리즘이 보상 신호를 처리하여 모델 파라미터를 업데이트하는 방식을 결정합니다. 실험에서는 보상 함수를 'Rule'과 'Model' 두 가지로 구분했습니다. Rule은 응답의 정확성을 기반으로 품질을 판단하고, Model은 학습된 보상 모델을 사용하여 각 응답에 점수를 부여합니다. 보상 모델의 학습 데이터는 Rule 판단을 기반으로 구성됩니다.

GRPO와 Online RFT의 주요 차이점은 방정식 10과 21에서 확인할 수 있습니다. GRPO는 보상 모델이 제공하는 보상값에 따라 그래디언트 계수를 조정하여 응답의 차등적인 강화와 페널티를 가능하게 합니다. 반면 Online RFT는 이러한 기능이 없어 잘못된 응답에 대한 페널티를 부여하지 않고 정답인 응답을 모두 동일한 강도로 강화합니다.

![SFT와 RL DeepSeekMath 7B의 성능 비교](https://ar5iv.org//html/2402.03300/assets/x5.png)

위 그래프에서 볼 수 있듯이 GRPO가 Online RFT를 능가하는 성능을 보여주었으며, 이는 긍정적 및 부정적 그래디언트 계수를 조정하는 방식의 효율성을 입증합니다. 특히 GRPO+PS(Process Supervision)가 GRPO+OS(Outcome Supervision)보다 우수한 성능을 보여, 단계별 인식이 가능한 그래디언트 계수를 사용하는 것이 이점이 있음을 알 수 있습니다.

#### 강화학습의 효과성 분석

연구진은 명령어 튜닝 데이터의 일부만을 사용한 강화학습으로도 명령어 튜닝 모델의 성능이 크게 향상되는 현상을 관찰했습니다. 이러한 효과의 원인을 분석하기 위해 두 벤치마크에서 SFT와 RL 모델의 Pass@K와 Maj@K 정확도를 평가했습니다.

실험 결과는 강화학습이 Maj@K 성능은 향상시키지만 Pass@K 성능은 향상시키지 않는다는 것을 보여줍니다. 이는 강화학습이 TopK 응답에서 정답의 비율을 높임으로써 모델의 전반적인 성능을 향상시키는 것이지, 근본적인 능력을 향상시키는 것은 아님을 시사합니다. Wang과 연구진은 SFT 모델의 추론 과제에서 이러한 불일치 문제를 식별했으며, 다양한 선호도 정렬 전략을 통해 SFT 모델의 추론 성능을 개선할 수 있음을 보여주었습니다.
#### 효과적인 강화학습을 위한 방향성

강화학습이 수학적 추론 과제에서 효과적임을 입증한 연구진은 앞서 제시한 통합 패러다임을 기반으로 더 효과적인 강화학습을 달성하기 위한 세 가지 핵심 방향성을 제시합니다.

첫째, 데이터 소스의 개선입니다. 강화학습에서 데이터 소스는 정책 모델에서 샘플링된 출력이 있는 레이블이 없는 질문들을 의미합니다. 현재 연구에서는 명령어 튜닝 단계의 질문들과 단순한 핵 샘플링만을 사용했는데, 이것이 Maj@K 성능만 향상되고 Pass@K 성능이 향상되지 않은 잠재적 원인일 수 있습니다. 향후에는 분포 외 질문 프롬프트에 대한 강화학습과 트리 검색 방법 등 고급 샘플링 전략의 탐색이 필요합니다. 또한 Xia와 연구진이 제안한 효율적 추론 기법들은 정책 모델의 탐색 효율성을 결정하는 중요한 역할을 합니다.

둘째, 알고리즘의 개선입니다. 앞서 설명한 통합 패러다임의 방정식 5를 보면, 현재의 모든 방법들은 보상 함수의 신호를 완전히 신뢰하여 특정 토큰의 조건부 확률을 증가시키거나 감소시킵니다. 하지만 극도로 복잡한 과제에서는 보상 신호가 항상 신뢰할 수 있다고 보장할 수 없습니다. 예를 들어, 전문가들이 신중하게 주석을 단 PRM800K 데이터셋조차도 약 20%의 잘못된 주석을 포함하고 있습니다. 이러한 문제를 해결하기 위해 연구진은 노이즈가 있는 보상 신호에도 강건한 강화학습 알고리즘의 개발이 필요하다고 제안합니다. Burns와 연구진이 제시한 WEAK-TO-STRONG 정렬 방법은 학습 알고리즘에 근본적인 변화를 가져올 것으로 기대됩니다.

셋째, 보상 함수의 개선입니다. 강화학습에서 보상 함수는 일반적으로 신경망 보상 모델을 사용합니다. 연구진은 보상 모델의 세 가지 중요한 개선 방향을 제시합니다. 첫째, 보상 모델의 일반화 능력을 향상시키는 것입니다. 보상 모델은 분포 외 질문과 고급 디코딩 출력을 효과적으로 처리할 수 있어야 하며, 그렇지 않으면 강화학습이 단순히 언어 모델의 분포를 안정화시키는 데 그칠 수 있습니다. 둘째, 보상 모델의 불확실성을 반영하는 것입니다. 이러한 불확실성은 약한 보상 모델과 약-강 학습 알고리즘 사이의 연결 다리 역할을 할 수 있습니다. 셋째, Lightman과 Wang의 연구진이 제안한 것처럼 추론 과정에 대한 세밀한 학습 신호를 제공할 수 있는 고품질 프로세스 보상 모델을 효율적으로 구축하는 것입니다.

### 결론, 한계점 및 향후 연구 방향

DeepSeekMath는 경쟁 수준의 MATH 벤치마크에서 모든 오픈소스 모델의 성능을 뛰어넘고 비공개 모델들의 성능에 근접하는 결과를 달성했습니다. DeepSeek-Coder-v1.5 7B를 초기 모델로 사용하여 Common Crawl에서 추출한 1,200억 개의 수학 토큰을 포함한 5,000억 토큰으로 지속적인 학습을 진행했습니다. 광범위한 실험 연구를 통해 웹 페이지가 고품질 수학 데이터의 잠재력을 가지고 있음을 확인했으며, 예상과 달리 arXiv가 기대했던 만큼의 이점을 제공하지 않는다는 것을 발견했습니다.

연구진은 근접 정책 최적화(PPO)의 변형인 그룹 상대 정책 최적화(GRPO)를 도입했습니다. GRPO는 메모리 사용량을 줄이면서도 수학적 추론 능력을 크게 향상시킬 수 있었습니다. 실험 결과는 DeepSeekMath-Instruct 7B가 이미 높은 점수를 달성했음에도 불구하고 GRPO가 효과적임을 보여주었습니다. 또한 연구진은 일련의 방법들을 이해하기 위한 통합 패러다임을 제시하고 더 효과적인 강화학습을 위한 잠재적 방향성을 제시했습니다.

DeepSeekMath가 정량적 추론 벤치마크에서 인상적인 점수를 달성했지만, 기하학과 정리 증명 분야에서는 비공개 모델들보다 상대적으로 약한 성능을 보였습니다. 예를 들어, 연구진의 예비 실험에서 모델은 삼각형과 타원에 관련된 문제를 처리하는 데 어려움을 보였는데, 이는 사전 학습과 미세조정 과정에서 데이터 선택의 편향이 있었음을 시사합니다. 또한 모델 규모의 제약으로 인해 DeepSeekMath는 GPT-4보다 퓨 샷 능력이 떨어졌습니다. GPT-4는 퓨 샷 입력을 통해 성능을 향상시킬 수 있었지만, DeepSeekMath는 제로 샷과 퓨 샷 평가에서 비슷한 성능을 보였습니다.

향후 연구에서는 더 높은 품질의 사전 학습 코퍼스를 구축하기 위해 데이터 선택 파이프라인을 개선할 계획입니다. 또한 대규모 언어 모델의 더 효과적인 강화학습을 위해 앞서 제시한 잠재적 방향성을 탐구할 예정입니다.

- - -
### References
* [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](http://arxiv.org/pdf/2402.03300v3)