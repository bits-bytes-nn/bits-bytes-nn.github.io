---
layout: post
title: "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism"
date: 2024-01-05 18:59:13
author: "DeepSeek-AI"
categories: "Language-Models"
tags: ["Scaling-Laws", "Optimal-Model/Data-Scaling-Up-Allocation", "Multi-Step-Learning-Rate-Scheduler", "Grouped-Query-Attention", "DeepSeekMath-Corpus", "Reinforcement-Learning-with-Human-Feedback", "Direct-Preference-Optimization", "Safety-Alignment"]
cover: /assets/images/language-models.webp
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
오픈소스 대규모 언어 모델(LLMs)의 발전이 가속화되면서, 모델 스케일링에 대한 명확한 이해의 필요성이 대두되었습니다. 기존 연구들은 스케일링 법칙에 대해 서로 다른 결론을 제시했으며, 하이퍼파라미터 설정에 대한 상세한 설명이 부족했습니다. 특히 AGI 발전의 초기 단계에서 효율적인 모델 스케일링 전략의 수립이 중요한 과제로 대두되었습니다. 이러한 배경에서 DeepSeek LLM 연구는 장기적 관점에서 오픈소스 언어 모델의 체계적인 발전을 위한 포괄적인 프레임워크를 제시하고자 시작되었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
이 연구는 세 가지 주요 혁신을 제시합니다. 첫째, 비임베딩 FLOPs/토큰(M)이라는 새로운 모델 규모 표현 방식을 도입하여 계산 비용을 더 정확하게 추정할 수 있게 했습니다. 둘째, 컴퓨트 예산 증가에 따른 최적의 모델/데이터 스케일링 할당 전략을 도출했으며, 약 52.43%를 모델 스케일링에, 47.57%를 데이터 스케일링에 할당하는 것이 최적임을 발견했습니다. 셋째, 다단계 학습률 스케줄러를 도입하여 연속 학습에서의 재사용성을 향상시켰습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
DeepSeek LLM은 7B와 67B 두 가지 규모로 구현되었으며, LLaMA 아키텍처를 기반으로 하되 몇 가지 중요한 수정을 도입했습니다. 67B 모델의 경우 Grouped-Query Attention(GQA)을 채택하여 추론 효율성을 개선했으며, 네트워크 깊이를 확장하는 전략을 채택했습니다. 학습 데이터는 약 2조 개의 토큰으로 구성되었으며, 중복 제거, 필터링, 리믹싱의 세 단계로 처리되었습니다. 모델은 지도 학습 미세조정(SFT)과 직접 선호도 최적화(DPO)를 통해 정제되었습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
이 연구의 결과는 오픈소스 LLM 개발에 있어 몇 가지 중요한 시사점을 제공합니다. 첫째, DeepSeek LLM 67B는 코드, 수학, 추론 영역에서 LLaMA-2 70B를 능가했으며, GPT-3.5보다 우수한 성능을 보여 오픈소스 모델의 잠재력을 입증했습니다. 둘째, 제시된 스케일링 법칙은 데이터셋 품질이 최적의 모델/데이터 스케일링 전략에 미치는 영향을 밝혀냈습니다. 셋째, 홀드아웃 평가를 통해 모델의 실제 성능과 일반화 능력을 검증했으며, 이는 향후 LLM 개발에 있어 중요한 지표가 될 것입니다.
- - -
## DeepSeek LLM: 장기적 관점으로 오픈소스 언어 모델 확장하기

오픈소스 대규모 언어 모델(Large Language Models, LLMs)의 발전은 놀라운 속도로 이루어져 왔습니다. 그러나 기존 연구에서 제시된 스케일링 법칙들은 서로 다른 결론을 보여주며, 이는 LLM 스케일링에 대한 불확실성을 야기했습니다. 본 연구에서는 스케일링 법칙을 심도 있게 분석하여, 널리 사용되는 두 가지 오픈소스 모델 구성인 7B와 67B 파라미터 모델의 스케일링에 관한 독창적인 발견을 제시합니다.

이러한 스케일링 법칙을 기반으로, 저자들은 장기적 관점에서 오픈소스 언어 모델을 발전시키기 위한 DeepSeek LLM 프로젝트를 소개합니다. 사전 학습을 위해 현재 2조 개의 토큰으로 구성되어 있으며 지속적으로 확장되고 있는 데이터셋을 구축했습니다. DeepSeek LLM Base 모델에 지도 학습 미세조정(Supervised Fine-tuning, SFT)과 직접 선호도 최적화(Direct Preference Optimization, DPO)를 적용하여 DeepSeek Chat 모델을 개발했습니다.

평가 결과에 따르면, DeepSeek LLM 67B는 코드, 수학, 추론 영역에서 특히 우수한 성능을 보이며 LLaMA-2 70B를 다양한 벤치마크에서 능가했습니다. 더불어 개방형 평가에서 DeepSeek LLM 67B Chat은 GPT-3.5보다 우수한 성능을 보여주었습니다.

본 논문은 스케일링 법칙에 대한 체계적인 연구와 함께, 대규모 언어 모델의 효율적인 학습을 위한 방법론을 제시합니다. Kaplan과 연구진이 제시한 기존의 스케일링 법칙을 확장하고, Hoffmann과 연구진의 연구에서 발견된 컴퓨트 최적 스케일링 원칙을 심화 발전시켰습니다. 특히 모델 크기와 학습 데이터 규모 간의 최적 균형을 찾는데 중점을 두었으며, 이는 LLaMA와 같은 최신 오픈소스 모델들의 성공적인 스케일링 전략과도 맥을 같이 합니다.

### 대규모 언어 모델의 발전과 현황

최근 몇 년간 디코더 전용 트랜스포머를 기반으로 하는 대규모 언어 모델(Large Language Models, LLMs)은 인공일반지능(Artificial General Intelligence, AGI)을 달성하기 위한 핵심 기술로 자리잡았습니다. 이러한 모델들은 연속적인 텍스트에서 다음 단어를 예측하는 자기지도 학습(self-supervised pre-training)을 통해 창작, 요약, 코드 완성 등 다양한 능력을 갖추게 되었습니다.

지도 학습 미세조정(supervised fine-tuning)과 보상 모델링(reward modeling)의 발전으로 대규모 언어 모델들은 사용자의 의도와 지시를 더 잘 따르게 되었고, 이는 더욱 다재다능한 대화 능력으로 이어졌습니다. ChatGPT, Claude, Bard와 같은 상용 제품들은 막대한 컴퓨팅 자원과 주석 비용을 투자하여 개발되었으며, 이는 오픈소스 언어 모델에 대한 커뮤니티의 기대치를 크게 높였습니다.

이러한 흐름 속에서 LLaMA 시리즈 모델은 특히 주목할 만한 성과를 보여주었습니다. 효율적이고 안정적인 아키텍처를 통합하여 7B에서 70B 파라미터에 이르는 우수한 성능의 모델을 구축했으며, 이는 오픈소스 모델들의 아키텍처와 성능을 평가하는 사실상의 기준이 되었습니다.

LLaMA 이후 오픈소스 커뮤니티는 주로 고정된 크기(7B, 13B, 34B, 70B)의 고품질 모델 학습에 집중해왔으며, 언어 모델의 스케일링 법칙에 대한 연구는 상대적으로 소홀히 다루어졌습니다. 그러나 현재의 오픈소스 모델들이 AGI 발전의 초기 단계에 있다는 점을 고려할 때, 스케일링 법칙에 대한 연구는 매우 중요합니다. 특히 초기 연구들은 컴퓨팅 예산 증가에 따른 모델과 데이터의 스케일링에 대해 서로 다른 결론을 도출했으며, 하이퍼파라미터에 대한 논의도 충분히 이루어지지 않았습니다.

본 연구에서는 언어 모델의 스케일링 동작을 광범위하게 조사하고, 이를 7B와 67B라는 두 가지 널리 사용되는 대규모 모델 구성에 적용했습니다. 배치 크기와 학습률의 스케일링 법칙을 먼저 검토하여 모델 크기에 따른 경향을 파악했으며, 이를 바탕으로 데이터와 모델 규모의 스케일링 법칙에 대한 포괄적인 연구를 수행했습니다. 이를 통해 최적의 모델/데이터 스케일링 할당 전략을 도출하고 대규모 모델의 예상 성능을 예측할 수 있었습니다.

특히 주목할 만한 발견은 서로 다른 데이터셋에서 도출된 스케일링 법칙이 상당한 차이를 보인다는 점입니다. 이는 데이터셋 선택이 스케일링 동작에 큰 영향을 미친다는 것을 시사하며, 스케일링 법칙을 데이터셋 간에 일반화할 때 주의가 필요함을 보여줍니다.

### 데이터셋 구축 방법론

DeepSeek LLM의 핵심 목표는 데이터셋의 풍부성과 다양성을 포괄적으로 향상시키는 것입니다. 이를 위해 데이터 처리 과정을 중복 제거(deduplication), 필터링(filtering), 리믹싱(remixing)의 세 단계로 구성했습니다.

중복 제거 단계에서는 적극적인 전략을 채택했습니다. Common Crawl 전체 코퍼스를 대상으로 중복을 제거하는 방식을 택했는데, 이는 단일 덤프 내에서만 중복을 제거하는 것보다 더 효과적인 결과를 보여주었습니다. 실제로 91개의 덤프에 걸쳐 중복을 제거했을 때, 단일 덤프 방식에 비해 4배 더 많은 문서가 제거되었습니다. 아래 표는 Common Crawl 덤프의 수에 따른 중복 제거율을 보여줍니다.

| 사용된 덤프 수 | 1    | 2    | 6    | 12   | 16   | 22   | 41   | 91   |
| -------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 중복 제거율(%) | 22.2 | 46.7 | 55.7 | 69.9 | 75.7 | 76.3 | 81.6 | 89.8 |

필터링 단계에서는 문서 품질을 평가하기 위한 강건한 기준을 개발했습니다. 언어적 평가와 의미론적 평가를 모두 포함하는 상세한 분석을 수행하여, 개별 문서와 전체적인 관점에서 데이터 품질을 평가했습니다. 리믹싱 단계에서는 데이터 불균형 문제를 해결하기 위해 접근 방식을 조정했으며, 특히 과소 대표된 도메인의 비중을 높이는 데 중점을 두었습니다.

토크나이저 구현에 있어서는 Huggingface Team이 개발한 tokenizers 라이브러리를 기반으로 바이트 수준 바이트 페어 인코딩(BBPE) 알고리즘을 구현했습니다. GPT-2와 유사하게, 개행 문자, 문장 부호, 한중일(CJK) 기호와 같은 서로 다른 문자 범주의 토큰이 병합되는 것을 방지하기 위해 사전 토크나이징을 적용했습니다. 또한 LLaMA 모델과 마찬가지로 숫자를 개별 자릿수로 분할하는 방식을 채택했습니다.

어휘 크기는 기존 토큰 100,000개로 설정했으며, 여기에 15개의 특수 토큰을 추가하여 최종 어휘 크기는 100,015가 되었습니다. 학습 시 계산 효율성을 고려하고 향후 추가될 수 있는 특수 토큰을 위한 여유 공간을 확보하기 위해 모델의 어휘 크기는 102,400으로 설정했습니다. 토크나이저 학습에는 약 24GB 규모의 다국어 코퍼스가 사용되었습니다.

### DeepSeek LLM의 아키텍처 설계

DeepSeek LLM의 마이크로 아키텍처는 LLaMA 모델의 설계를 기반으로 하고 있습니다. 모델의 핵심 구조는 Pre-Norm 방식을 채택하여 RMSNorm 정규화 함수를 사용하며, Feed-Forward Network(FFN)의 활성화 함수로는 SwiGLU를 적용했습니다. FFN의 중간 계층 차원은 모델 차원의 8/3배로 설정되어 있으며, 이는 수식으로 표현하면 $$\frac{8}{3}d_{model}$$입니다.

위치 정보 인코딩을 위해서는 Rotary Embedding을 도입했는데, 이는 상대적 위치 정보를 효과적으로 표현할 수 있는 방식입니다. 특히 67B 모델의 경우, 추론 비용을 최적화하기 위해 전통적인 Multi-Head Attention(MHA) 대신 Grouped-Query Attention(GQA)을 채택했습니다. GQA는 키와 밸류 헤드를 그룹화하여 메모리 대역폭을 효율적으로 사용하면서도 MHA에 근접한 성능을 달성할 수 있게 해줍니다.

DeepSeek LLM의 매크로 아키텍처는 기존 모델들과 약간의 차이를 보입니다. 7B 모델은 30개의 레이어로 구성되어 있으며, 67B 모델은 95개의 레이어를 가지고 있습니다. 이러한 레이어 구성은 다른 오픈소스 모델들과 파라미터 수를 일치시키면서도 모델 파이프라인 분할을 용이하게 하여 학습과 추론 과정을 최적화할 수 있게 합니다.

특히 주목할 만한 점은 67B 모델의 설계 방식입니다. 일반적으로 GQA를 사용하는 다른 연구들이 FFN 레이어의 중간 너비를 늘리는 방식을 선택한 것과 달리, DeepSeek LLM은 네트워크의 깊이를 확장하는 전략을 채택했습니다. 이는 더 나은 성능을 달성하기 위한 전략적 선택이었습니다.

### DeepSeek LLM의 학습 세부 사항

DeepSeek LLM은 0.006의 표준편차로 초기화되었으며, AdamW 옵티마이저를 사용하여 학습되었습니다. AdamW의 주요 하이퍼파라미터는 $$\beta_1=0.9$$, $$\beta_2=0.95$$, 그리고 $$\text{weight decay}=0.1$$로 설정되었습니다.

사전 학습 과정에서는 일반적으로 사용되는 코사인 스케줄러 대신 다단계 학습률 스케줄러를 채택했습니다. 구체적으로, 모델의 학습률은 2000 웜업 스텝 이후 최대값에 도달하며, 전체 학습 토큰의 80%를 처리한 후에는 최대값의 31.6%로 감소합니다. 이어서 90%의 토큰을 처리한 후에는 최대값의 10%로 더욱 감소합니다. 학습 과정에서의 그래디언트 클리핑은 1.0으로 설정되었습니다.

![학습률 스케줄러 비교](https://ar5iv.org//html/2401.02954/assets/figures/loss_step_cosine.png)

위 그래프는 다단계 학습률 스케줄러와 코사인 스케줄러의 성능을 비교한 것입니다. 실험 결과에 따르면, 학습 중 손실 감소 추세에는 차이가 있지만, 최종 성능은 두 스케줄러가 실질적으로 동일한 수준을 보여주었습니다. 다단계 학습률 스케줄러를 선택한 주된 이유는 모델 크기를 고정한 상태에서 학습 규모를 조정할 때, 첫 번째 단계의 학습을 재사용할 수 있다는 독특한 이점 때문이었습니다.

![다단계 비율 비교](https://ar5iv.org//html/2401.02954/assets/figures/loss_diff_step.png)

위 그래프는 다단계 학습률 스케줄러의 각 단계 비율을 조정했을 때의 영향을 보여줍니다. 실험 결과, 각 단계의 비율을 조정하면 약간 더 나은 성능을 얻을 수 있었습니다. 그러나 연속 학습에서의 재사용 비율과 모델 성능 사이의 균형을 고려하여, 최종적으로 세 단계의 비율을 80%, 10%, 10%로 설정했습니다.

### 인프라스트럭처

DeepSeek LLM의 학습과 평가를 위해 HAI-LLM이라는 경량화된 효율적인 학습 프레임워크를 사용했습니다. 이 프레임워크는 Megatron에서 구현된 것과 같이 데이터 병렬화, 텐서 병렬화, 시퀀스 병렬화, 그리고 1F1B 파이프라인 병렬화를 통합했습니다. 하드웨어 활용도를 높이기 위해 플래시 어텐션 기법을 도입했으며, 데이터 병렬 랭크에 걸쳐 옵티마이저 상태를 분할하기 위해 ZeRO-1을 활용했습니다.

추가적인 대기 시간을 최소화하기 위해 연산과 통신을 중첩시키는 노력도 기울였습니다. 여기에는 마지막 마이크로 배치의 역전파 과정과 ZeRO-1의 reduce-scatter 연산, 그리고 시퀀스 병렬에서의 GEMM 연산과 all-gather/reduce-scatter가 포함됩니다. 학습 속도를 높이기 위해 LayerNorm, 가능한 경우의 GEMM, 그리고 Adam 업데이트와 같은 일부 레이어와 연산자들을 융합했습니다.

모델 학습의 안정성을 높이기 위해 bf16 정밀도로 학습을 진행하되, 그래디언트는 fp32 정밀도로 누적했습니다. GPU 메모리 사용량을 줄이기 위해 인플레이스 크로스 엔트로피를 수행했는데, 이는 크로스 엔트로피 CUDA 커널에서 bf16 로짓을 fp32 정밀도로 즉시 변환하고 해당하는 bf16 그래디언트를 계산한 후 로짓을 그래디언트로 덮어쓰는 방식입니다.

모델 가중치와 옵티마이저 상태는 5분마다 비동기적으로 저장되어, 하드웨어나 네트워크 장애가 발생하더라도 최대 5분의 학습 시간만 손실됩니다. 이러한 임시 모델 체크포인트는 저장 공간을 과도하게 소비하지 않도록 정기적으로 정리됩니다. 또한 컴퓨팅 클러스터 부하의 동적 변화에 대응하기 위해 다른 3D 병렬 구성에서도 학습을 재개할 수 있도록 지원합니다.

평가 단계에서는 생성 태스크에는 vLLM을, 비생성 태스크에는 연속 배칭을 사용하여 수동 배치 크기 조정을 피하고 토큰 패딩을 줄였습니다.
### 스케일링 법칙 연구

대규모 언어 모델이 등장하기 이전부터 스케일링 법칙에 대한 연구는 진행되어 왔습니다. 스케일링 법칙에 따르면 컴퓨트 예산 $$C$$, 모델 규모 $$N$$, 그리고 데이터 규모 $$D$$가 증가함에 따라 모델의 성능이 예측 가능한 방식으로 향상됩니다. 모델 규모 $$N$$을 모델 파라미터로, 데이터 규모 $$D$$를 토큰 수로 표현할 때, $$C$$는 $$C=6ND$$로 근사할 수 있습니다. 따라서 컴퓨트 예산을 증가시킬 때 모델과 데이터 규모 사이의 할당을 최적화하는 것도 스케일링 법칙 연구의 중요한 목표가 되었습니다.

대규모 언어 모델의 발전과 함께, 더 큰 모델이 예상치 못한 상당한 성능 향상을 보여주면서 스케일링 법칙 연구는 새로운 정점을 맞이했습니다. 스케일링 법칙의 연구 결과들은 컴퓨트 예산을 확장하는 것이 계속해서 상당한 이점을 가져온다는 것을 보여주었고, 이는 모델 규모의 증가를 더욱 촉진했습니다.

그러나 초기 연구들에서 제시된 최적의 모델/데이터 스케일링 할당 전략은 서로 다른 결론을 보여주었고, 이는 스케일링 법칙의 일반적인 적용 가능성에 대한 의문을 제기했습니다. 또한 이러한 연구들은 하이퍼파라미터 설정에 대한 완전한 설명이 부족했기 때문에, 서로 다른 컴퓨트 예산에서 모델들이 최적의 성능에 도달했는지 확실하지 않았습니다.

이러한 불확실성을 해소하고 효율적인 컴퓨트 스케일업을 위한 올바른 경로를 확인하기 위해, 본 연구에서는 스케일링 법칙을 재검토했습니다. 이는 장기적 관점을 반영하며, 지속적으로 향상되는 모델을 개발하는 데 핵심이 됩니다.

서로 다른 컴퓨트 예산에서 모델이 최적의 성능을 달성할 수 있도록 하기 위해, 먼저 하이퍼파라미터의 스케일링 법칙을 연구했습니다. 경험적으로 대부분의 파라미터의 최적값은 컴퓨트 예산이 변화해도 변하지 않는 것으로 관찰되었습니다. 따라서 이러한 파라미터들은 앞서 설명한 값들과 일치하며 서로 다른 컴퓨트 예산에서도 변하지 않습니다. 그러나 성능에 가장 큰 영향을 미치는 하이퍼파라미터인 배치 크기와 학습률은 재검토되었습니다.

초기 연구들은 배치 크기와 학습률 설정에 대한 경험적 관찰을 제공했지만, 예비 실험에서 이러한 관찰들의 적용 가능성이 제한적임을 발견했습니다. 광범위한 실험을 통해 컴퓨트 예산 $$C$$와 최적의 배치 크기 및 학습률 사이의 멱법칙 관계를 모델링했습니다. 이 관계를 하이퍼파라미터의 스케일링 법칙이라고 부르며, 이는 서로 다른 컴퓨트 예산에서 모델이 최적에 가까운 성능에 도달할 수 있도록 하는 경험적 프레임워크를 제공합니다.
이어서 모델과 데이터 규모의 스케일링 법칙을 연구했습니다. 실험 비용과 피팅의 어려움을 줄이기 위해 Chinchilla에서 사용된 IsoFLOP 프로파일 접근 방식을 채택하여 스케일링 곡선을 피팅했습니다. 모델 규모를 더 정확하게 표현하기 위해 이전에 사용되던 모델 파라미터 $$N$$ 대신 비임베딩 FLOPs/토큰 $$M$$이라는 새로운 모델 규모 표현을 도입했고, 근사적인 컴퓨트 예산 공식 $$C=6ND$$ 대신 더 정확한 $$C=MD$$를 사용했습니다.

실험 결과는 최적의 모델/데이터 스케일링 할당 전략과 성능 예측에 대한 통찰을 제공했으며, DeepSeek LLM 7B와 67B 모델의 예상 성능도 정확하게 예측했습니다. 또한 스케일링 법칙을 탐구하는 과정에서 사용된 데이터는 여러 차례 반복적으로 품질이 개선되었습니다. 다양한 데이터셋에서 스케일링 곡선을 피팅한 결과, 데이터 품질이 최적의 모델/데이터 스케일링 할당 전략에 상당한 영향을 미친다는 것을 발견했습니다. 데이터 품질이 높을수록 증가된 컴퓨트 예산을 모델 스케일링에 더 많이 할당해야 합니다. 이는 동일한 데이터 규모에서도 고품질 데이터가 더 큰 모델의 학습을 가능하게 한다는 것을 의미합니다.

최적의 모델/데이터 스케일링 할당 전략의 차이는 데이터 품질을 평가하는 간접적인 방법으로도 활용될 수 있습니다. 저자들은 데이터 품질의 변화와 이것이 스케일링 법칙에 미치는 영향에 대해 지속적으로 주목하고 있으며, 향후 연구에서 더 많은 분석을 제공할 예정입니다.

### 하이퍼파라미터의 스케일링 법칙

1e17의 컴퓨트 예산에서 작은 규모의 실험으로 배치 크기와 학습률에 대한 그리드 서치를 수행했으며, 특정 모델 크기(177M FLOPs/토큰)에 대한 결과는 아래 그래프와 같습니다.

![배치 크기와 학습률 관계](https://ar5iv.org//html/2401.02954/assets/figures/loss_bs_lr_1e17.png)

실험 결과는 일반화 오차가 배치 크기와 학습률의 선택에 있어 넓은 범위에서 안정적으로 유지된다는 것을 보여줍니다. 이는 상대적으로 넓은 파라미터 공간 내에서 최적에 가까운 성능을 달성할 수 있다는 것을 의미합니다.

![1e20 FLOPs에서의 최적 하이퍼파라미터](https://ar5iv.org//html/2401.02954/assets/figures/loss_bs_lr_1e20.png)

앞서 언급한 다단계 학습률 스케줄러를 활용하여 첫 번째 단계를 재사용함으로써 1e17에서 2e19까지의 다양한 컴퓨트 예산에서 서로 다른 배치 크기와 학습률로 여러 모델을 효과적으로 학습했습니다. 파라미터 공간의 중복성을 고려하여, 일반화 오차가 최소값보다 0.25% 이상 초과하지 않는 모델들의 파라미터를 최적에 가까운 하이퍼파라미터로 간주했습니다.
이어서 배치 크기 $$B$$와 학습률 $$\eta$$를 컴퓨트 예산 $$C$$에 대해 피팅했습니다. 피팅 결과는 아래 그래프와 같이 나타났습니다.

![배치 크기 스케일링 곡선](https://ar5iv.org//html/2401.02954/assets/figures/flops_bsz_fitting.png)

![학습률 스케일링 곡선](https://ar5iv.org//html/2401.02954/assets/figures/flops_lr_fitting.png)

피팅 결과에 따르면, 최적의 배치 크기 $$B$$는 컴퓨트 예산 $$C$$가 증가함에 따라 점진적으로 증가하는 반면, 최적의 학습률 $$\eta$$는 점진적으로 감소하는 것으로 나타났습니다. 이는 모델을 스케일업할 때 배치 크기와 학습률에 대한 직관적인 경험적 설정과 일치합니다. 또한 모든 최적에 가까운 하이퍼파라미터들이 넓은 밴드 범위 내에 위치하는 것으로 나타났는데, 이는 이 구간 내에서 최적에 가까운 파라미터를 선택하는 것이 상대적으로 용이하다는 것을 의미합니다.

배치 크기와 학습률에 대해 최종적으로 피팅된 공식은 다음과 같습니다.

$$\eta_{\mathrm{opt}} = 0.3118 \cdot C^{-0.1250}$$
$$B_{\mathrm{opt}} = 0.2920 \cdot C^{0.3271}$$

이러한 공식의 유효성을 검증하기 위해 1e20 컴퓨트 예산을 가진 일련의 모델들에 대해 테스트를 수행했으며, 특정 모델 크기(2.94B FLOPs/토큰)에 대한 결과는 피팅된 파라미터가 최적의 파라미터 공간의 중심에 위치한다는 것을 보여주었습니다. 이후 섹션에서도 DeepSeek LLM 7B와 67B 모델에 대해 피팅된 파라미터들이 유사하게 좋은 성능을 달성했음을 확인할 수 있습니다.

그러나 컴퓨트 예산 $$C$$ 이외의 요인들이 최적의 하이퍼파라미터에 미치는 영향은 아직 고려하지 않았다는 점에 주목해야 합니다. 이는 최적의 배치 크기가 일반화 오차 $$L$$과만 관련이 있다고 제안한 일부 초기 연구들과는 일치하지 않는 부분입니다. 또한 동일한 컴퓨트 예산을 가진 모델들 사이에서도 모델/데이터 할당이 다른 경우 최적의 파라미터 공간이 약간씩 달라지는 것을 관찰했습니다. 이는 하이퍼파라미터 선택과 학습 동역학을 이해하기 위해 추가적인 연구가 필요하다는 것을 시사하며, 저자들은 이러한 측면들을 향후 연구에서 탐구할 예정입니다.
### 최적의 모델과 데이터 스케일링 추정

최적의 하이퍼파라미터를 피팅하는 공식을 도출한 후, 스케일링 곡선을 피팅하고 최적의 모델/데이터 스케일링 할당 전략을 분석했습니다. 이 전략은 컴퓨트 예산 $$C$$에 대해 $$N_{\mathrm{opt}} \propto C^a$$와 $$D_{\mathrm{opt}} \propto C^b$$를 만족하는 모델 스케일링 지수 $$a$$와 데이터 스케일링 지수 $$b$$를 찾는 것입니다.

데이터 규모 $$D$$는 일관되게 데이터셋의 토큰 수로 표현할 수 있습니다. 이전 연구들에서는 모델 규모를 일반적으로 모델 파라미터로 표현했는데, 비임베딩 파라미터 $$N_1$$과 전체 파라미터 $$N_2$$ 두 가지 방식이 사용되었습니다. 컴퓨트 예산 $$C$$와 모델/데이터 규모의 관계는 $$C=6ND$$로 근사적으로 표현될 수 있었고, 이는 $$6N_1$$ 또는 $$6N_2$$를 모델 규모의 근사치로 사용할 수 있다는 것을 의미했습니다.

그러나 $$6N_1$$과 $$6N_2$$ 모두 어텐션 연산의 계산 오버헤드를 고려하지 않으며, $$6N_2$$는 모델의 용량에 덜 기여하는 어휘 계산까지 포함하기 때문에 특정 설정에서 상당한 근사 오차를 보입니다. 이러한 오차를 줄이기 위해 새로운 모델 규모 표현인 비임베딩 FLOPs/토큰 $$M$$을 도입했습니다. $$M$$은 어텐션 연산의 계산 오버헤드는 포함하지만 어휘 계산은 고려하지 않습니다. $$M$$으로 모델 규모를 표현할 때, 컴퓨트 예산 $$C$$는 간단히 $$C=MD$$로 표현될 수 있습니다.

$$6N_1$$, $$6N_2$$, $$M$$ 사이의 구체적인 차이는 다음 공식들로 나타낼 수 있습니다.

$$6N_1 = 72\,n_{\mathrm{layer}}\,d_{\mathrm{model}}^2$$
$$6N_2 = 72\,n_{\mathrm{layer}}\,d_{\mathrm{model}}^2+6\,n_{\mathrm{vocab}}\,d_{\mathrm{model}}$$
$$M = 72\,n_{\mathrm{layer}}\,d_{\mathrm{model}}^2+12\,n_{\mathrm{layer}}\,d_{\mathrm{model}}\,l_{\mathrm{seq}}$$

여기서 $$n_{\mathrm{layer}}$$는 레이어 수, $$d_{\mathrm{model}}$$은 모델 너비, $$n_{\mathrm{vocab}}$$은 어휘 크기, $$l_{\mathrm{seq}}$$는 시퀀스 길이를 나타냅니다.
### 모델 규모 표현 방식의 비교 분석

다양한 규모의 모델들에 대해 세 가지 표현 방식의 차이를 평가했으며, 그 결과는 $$6N_1$$과 $$6N_2$$ 모두 서로 다른 규모의 모델에서 계산 비용을 과대 또는 과소 추정한다는 것을 보여줍니다. 이러한 불일치는 특히 소규모 모델에서 두드러지며, 차이가 최대 50%까지 발생할 수 있습니다. 이러한 부정확성은 스케일링 곡선을 피팅할 때 상당한 통계적 오차를 초래할 수 있습니다.

### 최적 모델과 데이터 스케일링 문제 정의

모델 규모를 $$M$$으로 표현하면서, 우리의 목표는 다음과 같이 더 명확하게 정의될 수 있습니다. 주어진 컴퓨팅 예산 $$C=MD$$에 대해, 모델의 일반화 오차를 최소화하는 최적의 모델 규모 $$M_{\mathrm{opt}}$$와 데이터 규모 $$D_{\mathrm{opt}}$$를 찾는 것입니다. 이는 수학적으로 다음과 같이 형식화됩니다.

$$M_{\mathrm{opt}}(C),D_{\mathrm{opt}}(C)=\underset{M,D\,\mathrm{s.t.}\,C=MD}{\mathrm{argmin}}L(N,D)$$

실험 비용과 피팅의 어려움을 줄이기 위해 Chinchilla에서 사용된 IsoFLOP 프로파일 접근 방식을 채택했습니다. 1e17에서 3e20까지 8개의 서로 다른 컴퓨트 예산을 선택하고, 각 예산에 대해 약 10개의 서로 다른 모델/데이터 규모 할당을 설계했습니다. 각 예산에 대한 하이퍼파라미터는 앞서 도출한 공식을 통해 결정되었으며, 일반화 오차는 학습 세트와 유사하게 분포된 100M 토큰의 독립적인 검증 세트에서 계산되었습니다.

### 스케일링 곡선 분석

IsoFLOP 곡선과 모델/데이터 스케일링 곡선은 각 컴퓨트 예산에 대한 최적의 모델/데이터 할당을 사용하여 피팅되었습니다. 최적의 비임베딩 FLOPs/토큰 $$M_{\mathrm{opt}}$$와 최적의 토큰 수 $$D_{\mathrm{opt}}$$에 대한 구체적인 공식은 다음과 같습니다.

$$M_{\mathrm{opt}} = M_{\mathrm{base}} \cdot C^a, \quad M_{\mathrm{base}} = 0.1715, \quad a = 0.5243$$
$$D_{\mathrm{opt}} = D_{\mathrm{base}} \cdot C^b, \quad D_{\mathrm{base}} = 5.8316, \quad b = 0.4757$$

이러한 스케일링 법칙은 컴퓨트 예산이 증가할 때 모델과 데이터 규모를 어떻게 최적으로 조정해야 하는지에 대한 명확한 지침을 제공합니다. 특히 지수 $$a$$와 $$b$$의 값은 컴퓨트 예산 증가분의 약 52.43%를 모델 스케일링에, 47.57%를 데이터 스케일링에 할당해야 한다는 것을 시사합니다.

### 데이터셋 구축과 학습 과정

DeepSeek LLM의 학습을 위해 약 150만 개의 영어와 중국어 지시문 데이터를 수집했습니다. 이 데이터셋은 유용성과 안전성 관련 주제들을 폭넓게 다루고 있습니다. 유용성 데이터는 120만 개의 인스턴스로 구성되어 있으며, 일반 언어 과제가 31.2%, 수학 문제가 46.6%, 코딩 연습이 22.2%의 비율을 차지합니다. 안전성 데이터는 30만 개의 인스턴스로 구성되어 있으며, 다양한 민감한 주제들을 포함하고 있습니다.

모델의 정렬(alignment) 파이프라인은 두 단계로 구성됩니다. 첫 번째는 지도 학습 미세조정(Supervised Fine-Tuning, SFT) 단계입니다. 7B 모델은 4번의 에포크로 미세조정되었지만, 67B 모델은 과적합 문제가 심각하게 나타나 2번의 에포크만 진행했습니다. GSM8K와 HumanEval 벤치마크에서 7B 모델은 지속적인 성능 향상을 보였지만, 67B 모델은 빠르게 상한에 도달했습니다. 학습률은 7B 모델의 경우 1e-5, 67B 모델의 경우 5e-6로 설정되었습니다.

벤치마크 정확도 모니터링과 함께, 채팅 모델의 반복 비율도 미세조정 과정에서 평가되었습니다. 총 3,868개의 중국어와 영어 프롬프트를 수집하여 생성된 응답이 종료되지 않고 텍스트 시퀀스를 무한히 반복하는 비율을 측정했습니다. 수학 SFT 데이터의 양이 증가할수록 반복 비율이 높아지는 경향이 관찰되었는데, 이는 수학 SFT 데이터가 때때로 유사한 추론 패턴을 포함하기 때문입니다. 상대적으로 약한 모델들은 이러한 추론 패턴을 제대로 이해하지 못해 반복적인 응답을 생성하게 됩니다.

이 문제를 해결하기 위해 2단계 미세조정과 직접 선호도 최적화(Direct Preference Optimization, DPO)를 시도했으며, 두 방법 모두 벤치마크 점수를 거의 유지하면서 반복을 크게 줄일 수 있었습니다. DPO 단계에서는 유용성과 안전성 측면에서 선호도 데이터를 구성했습니다. 유용성 데이터를 위해 창작 글쓰기, 질문 답변, 지시 따르기 등의 카테고리를 포함하는 다국어 프롬프트를 수집했고, DeepSeek Chat 모델을 사용하여 응답 후보를 생성했습니다. 안전성 선호도 데이터 구성에도 유사한 방식이 적용되었습니다.

DPO 학습은 1 에포크 동안 진행되었으며, 학습률 5e-6와 배치 크기 512를 사용했습니다. 학습률 웜업과 코사인 학습률 스케줄러가 적용되었습니다. 실험 결과, DPO는 표준 벤치마크에서의 성능 차이는 거의 없으면서도 모델의 개방형 생성 능력을 강화할 수 있다는 것이 확인되었습니다.

### DeepSeek LLM의 평가 방법론

DeepSeek LLM의 성능을 평가하기 위해 영어와 중국어 공개 벤치마크에서 광범위한 평가를 수행했습니다. 평가는 내부 평가 프레임워크를 기반으로 진행되었으며, 다음과 같은 다양한 카테고리의 벤치마크가 사용되었습니다.

다중 주제 객관식 데이터셋으로는 MMLU, C-Eval, CMMLU가 사용되었습니다. 언어 이해와 추론 능력을 평가하기 위해 HellaSwag, PIQA, ARC, OpenBookQA, BigBench Hard(BBH)가 활용되었습니다. 외부 자료 없이 질문에 답변하는 능력을 평가하기 위해서는 TriviaQA와 NaturalQuestions가 사용되었으며, 독해력 평가를 위해 RACE, DROP, C3 데이터셋이 활용되었습니다.

참조 모호성 해소 능력을 평가하기 위해 WinoGrande와 CLUEWSC가 사용되었고, 언어 모델링 능력은 Pile 데이터셋으로 평가했습니다. 중국어 이해력과 문화적 지식을 평가하기 위해 CHID와 CCPM이 활용되었으며, 수학적 능력 평가를 위해 GSM8K, MATH, CMath가 사용되었습니다. 코딩 능력은 HumanEval과 MBPP로 평가했으며, 표준화된 시험 성능은 AGIEval을 통해 측정했습니다.

평가 방식은 크게 세 가지로 나뉩니다. 첫째, 퍼플렉시티 기반 평가는 여러 선택지 중에서 답을 고르는 형식의 데이터셋에 적용되었습니다. 각 선택지에 대한 퍼플렉시티를 계산하여 가장 낮은 값을 가진 선택지를 모델의 예측으로 선택했습니다. ARC와 OpenBookQA의 경우 무조건부 정규화를 적용했으며, 다른 데이터셋에서는 길이 정규화를 사용했습니다.

둘째, 생성 기반 평가는 TriviaQA, NaturalQuestions, DROP, MATH, GSM8K, HumanEval, MBPP, BBH, AGIEval, CLUEWSC, CMath에 적용되었습니다. 이 방식에서는 모델이 자유롭게 텍스트를 생성하고, 생성된 텍스트에서 결과를 추출하여 평가했습니다. 생성 시에는 그리디 디코딩 방식을 사용했습니다.

셋째, 언어 모델링 기반 평가는 Pile-test에 적용되었으며, 테스트 코퍼스에서 바이트당 비트(bits-per-byte)를 계산하는 방식으로 진행되었습니다. 평가 시 최대 시퀀스 길이는 벤치마크에 따라 2048 또는 4096을 사용했습니다.

![벤치마크 평가 결과](https://ar5iv.org//html/2401.02954/assets/figures/pretrain_metric.png)

위 그래프는 DeepSeek LLM Base 모델의 벤치마크 평가 결과를 보여줍니다. ChineseQA는 TriviaQA와 유사한 방식으로 구성된 내부 테스트 세트입니다. 학습이 진행됨에 따라 모든 벤치마크에서 지속적인 성능 향상이 관찰되었으며, 이는 학습이 더 진행된다면 성능이 더욱 개선될 수 있음을 시사합니다.
### DeepSeek LLM의 기본 모델 성능 분석

DeepSeek LLM의 기본 모델 평가 결과는 매우 인상적인 성능을 보여주었습니다. 영어 벤치마크에서 DeepSeek 모델은 2조 개의 이중 언어 코퍼스로 학습되었음에도 불구하고, 영어에 초점을 맞춘 LLaMA2 모델과 대등한 성능을 달성했습니다. 특히 DeepSeek 67B는 MATH, GSM8K, HumanEval, MBPP, BBH와 같은 수학 및 코딩 관련 벤치마크와 중국어 벤치마크에서 LLaMA2 70B를 크게 앞섰습니다.

모델 스케일링에 따른 성능 향상 패턴을 분석한 결과, GSM8K와 BBH와 같은 일부 태스크에서는 모델 크기가 증가함에 따라 성능이 크게 향상되는 것으로 나타났습니다. 7B와 67B 모델이 동일한 데이터셋으로 학습되었다는 점을 고려할 때, 이러한 성능 향상은 대규모 모델의 강력한 퓨 샷 학습 능력에 기인한 것으로 해석됩니다. 다만, 수학적 데이터의 비중이 증가하면 작은 모델과 큰 모델 간의 성능 차이가 줄어들 수 있다는 점도 주목할 만합니다.

특히 흥미로운 점은 DeepSeek 67B가 LLaMA2 70B에 비해 보이는 우위가 DeepSeek 7B와 LLaMA2 7B 간의 차이보다 더 크다는 것입니다. 이는 언어 충돌이 작은 모델에 더 큰 영향을 미친다는 것을 시사합니다. 또한 LLaMA2는 중국어 데이터로 특별히 학습되지 않았음에도 CMath와 같은 특정 중국어 태스크에서 인상적인 성능을 보여주었습니다. 이는 수학적 추론과 같은 기본적인 능력이 언어 간에 효과적으로 전이될 수 있다는 것을 보여줍니다. 그러나 CHID와 같이 중국어 관용구의 사용을 평가하는 태스크에서는 사전 학습 과정에서 상당한 양의 중국어 토큰을 학습해야 하므로, LLaMA2는 DeepSeek LLM에 비해 현저히 낮은 성능을 보였습니다.

### DeepSeek LLM의 채팅 모델 성능 분석

채팅 모델의 미세조정 후 성능 변화를 분석한 결과, 대부분의 태스크에서 전반적인 성능 향상이 관찰되었습니다. 지식 관련 태스크인 TriviaQA, MMLU, C-Eval에서는 기본 모델과 채팅 모델 간의 성능 변동이 관찰되었으나, 이러한 변동이 지도 학습 미세조정(SFT) 후 지식의 획득이나 손실을 의미하지는 않습니다. SFT의 진정한 가치는 기본 모델의 퓨 샷 설정에서 달성한 성능과 비슷한 수준을 채팅 모델이 제로 샷 설정에서도 달성할 수 있다는 점에 있습니다.
### DeepSeek LLM의 채팅 모델 성능 분석

추론 능력 측면에서는 체인 오브 소트 형식의 SFT 인스턴스가 상당 부분을 차지하면서, 채팅 모델이 BBH와 NaturalQuestions와 같은 추론 태스크에서 약간의 성능 향상을 보였습니다. 그러나 SFT 단계에서 새로운 추론 능력을 학습하는 것이 아니라, 올바른 추론 경로의 형식을 학습하는 것으로 해석됩니다.

모델 크기나 사전 학습된 체크포인트와 관계없이, 일부 태스크에서는 미세조정 후 일관되게 성능이 하락하는 현상이 관찰되었습니다. 이러한 태스크들은 주로 HellaSwag와 같은 빈칸 채우기나 문장 완성 태스크들입니다. 이는 순수한 언어 모델이 이러한 유형의 태스크를 더 잘 처리할 수 있다는 것을 시사합니다.

수학과 코딩 분야에서는 미세조정 후 상당한 성능 향상이 있었습니다. 예를 들어, HumanEval과 GSM8K 점수가 20점 이상 향상되었습니다. 이는 기본 모델이 이러한 태스크에 대해 초기에 과소적합 상태였으며, SFT 단계에서 광범위한 SFT 데이터를 통해 코딩과 수학 분야의 추가적인 지식을 학습했기 때문으로 설명됩니다. 그러나 모델의 능력이 주로 코드 완성과 대수학 문제에 집중되어 있다는 점에 주목해야 합니다. 수학과 코딩에 대한 포괄적인 이해를 위해서는 사전 학습 단계에서 다양한 데이터를 통합하는 것이 중요하며, 이는 향후 연구 과제로 남아있습니다.

7B 모델의 미세조정에서는 먼저 모든 데이터를 사용하여 모델을 미세조정한 후, 두 번째 단계에서는 수학과 코드 데이터를 제외하고 진행했습니다. 이러한 접근 방식을 채택한 이유는 1단계 모델에서 2.0%였던 반복 비율이 2단계 조정 후 1.4%로 감소하면서도 벤치마크 점수를 유지할 수 있었기 때문입니다. 67B 모델의 경우, 첫 번째 단계 미세조정 후 이미 반복 비율이 1% 미만이었고, 두 번째 단계는 오히려 벤치마크 점수를 저하시켰기 때문에 1단계 SFT만 진행했습니다.
### DeepSeek LLM의 오픈 벤치마크 평가 결과

AlignBench 리더보드에서 DeepSeek 67B Chat 모델은 ChatGPT와 다른 기준 모델들을 큰 차이로 앞섰으며, 이는 기본적인 중국어 언어 태스크와 고급 중국어 추론 태스크 모두에서 우수한 성능을 보여주었음을 의미합니다. 특히 DPO 과정이 거의 모든 분야에서 성능 향상을 가져왔다는 점이 주목할 만합니다.

채팅 모델의 경우, 표준 벤치마크의 메트릭을 관찰하는 것 외에도 개방형 도메인과 개방형 질문에서 생성된 결과의 품질이 실제 사용자 경험에 직접적인 영향을 미칩니다. 따라서 중국어와 영어 태스크 모두에서 채팅 모델의 개방형 생성 능력을 별도로 테스트했습니다.

중국어 개방형 평가를 위해 AlignBench라는 고품질 개방형 질문 테스트셋에서 다양한 도메인에 걸친 모델의 종합적인 능력을 테스트했습니다. AlignBench는 8개의 주요 카테고리와 36개의 하위 카테고리로 구성되어 있으며, 총 683개의 질문을 포함합니다. 각 질문에 대해 AlignBench는 전문가의 참조 답변과 GPT-4가 응답의 품질을 판단하기 위한 평가 템플릿을 제공합니다.

영어 개방형 평가에서는 MT-Bench 벤치마크를 사용했으며, 이는 8개의 서로 다른 카테고리의 다중 턴 질문들을 포함합니다. DeepSeek LLM 67B Chat은 LLaMA-2-Chat 70B, Xwin 70b v0.1, TÜLU 2+DPO 70B와 같은 다른 오픈소스 모델들을 능가했으며, GPT-3.5-turbo와 비슷한 8.35점을 달성했습니다. DPO 단계 이후에는 평균 점수가 8.76으로 더욱 향상되어 GPT-4에 이어 두 번째로 높은 성능을 보여주었습니다.

### 홀드아웃 평가

데이터 오염과 벤치마크 과적합은 LLM을 평가할 때 직면하는 두 가지 주요 과제입니다. 이를 해결하기 위한 일반적인 방법은 최근에 발표된 테스트셋을 홀드아웃 테스트셋으로 활용하여 모델을 평가하는 것입니다.

코딩 능력 평가를 위해 LeetCode 주간 콘테스트(Weekly Contest 351-372, Bi-Weekly Contest 108-117, 2023년 7월부터 11월까지)의 문제들을 활용했습니다. LeetCode에서 크롤링한 데이터는 126개의 문제로 구성되어 있으며, 각 문제마다 20개 이상의 테스트 케이스가 포함되어 있습니다. HumanEval과 유사한 평가 메트릭을 사용하여, 모델의 출력이 모든 테스트 케이스를 통과하면 해당 문제를 성공적으로 해결한 것으로 간주했습니다.

수학적 능력 평가를 위해서는 Grok-1과 마찬가지로 헝가리 국가 고등학교 시험을 활용했습니다. 이 시험은 33개의 문제로 구성되어 있으며, 모델의 점수는 인간 주석자의 평가를 통해 결정되었습니다. 평가는 공식 답안의 채점 기준을 따랐습니다.

지시사항 준수 능력 평가를 위해 2023년 11월 15일에 구글이 공개한 평가 데이터셋을 활용했습니다. 이 데이터셋은 25가지 유형의 검증 가능한 지시사항을 식별하고, 각 프롬프트가 하나 이상의 검증 가능한 지시사항을 포함하는 약 500개의 프롬프트로 구성되어 있습니다.
### 홀드아웃 평가 결과 분석

홀드아웃 평가에서 다양한 크기의 기준 모델들과 비교 분석을 수행했습니다. Qwen 72B Chat, ChatGLM3, Baichuan2, Yi-34B Chat 등의 모델들과 비교한 결과, 일반적인 벤치마크에서 우수한 성능을 보이는 일부 작은 모델들도 홀드아웃 데이터셋에서는 큰 모델들과 상당한 성능 차이를 보이는 것으로 나타났습니다.

예를 들어, ChatGLM3는 코드 테스트셋인 MBPP에서 52.4점을 달성하여 DeepSeek 67B와 비슷한 성능을 보였습니다. 그러나 새로운 벤치마크에서 평가했을 때는 DeepSeek 67B에 비해 현저히 낮은 성능을 보였습니다. 수학 데이터셋에서도 유사한 패턴이 관찰되었는데, ChatGLM3는 GSM8K에서 72.3점이라는 매우 강력한 성능을 보였지만, 헝가리 시험 점수에서는 큰 모델들에 비해 열등한 성능을 보였습니다.

지시사항 준수 능력에서도 총 컴퓨팅이 중요한 역할을 하는 것으로 나타났습니다. DeepSeek 7B와 67B 모델은 동일한 학습 파이프라인을 사용했지만, 성능에서 상당한 차이를 보였습니다. 주관적 평가를 통해 67B 모델로 스케일링했을 때 다양한 태스크에서 지능적 능력의 현저한 차이가 관찰되었습니다. DeepSeek 7B는 표준 벤치마크에서 다른 작은 언어 모델들보다 낮은 성능을 보였지만, 홀드아웃 태스크에서는 상대적으로 우수한 성능을 보여주었습니다.

### 안전성 평가 방법론

DeepSeek LLM의 안전성 평가를 위해 다양한 분야의 전문가 20명으로 구성된 팀을 구성하고, 인간의 가치와 부합하는 안전 콘텐츠 분류 시스템을 구축했습니다. 전문가 팀은 각 안전성 하위 카테고리에 대해 수십 개의 고품질 테스트 케이스를 수작업으로 구성했습니다.

안전성 콘텐츠 영역의 다양성뿐만 아니라 안전성 콘텐츠의 형식적 다양성에도 주목했습니다. 악명 높은 "할머니" 취약점은 모델이 쿼리의 표면적 형식에 속아 안전하지 않은 응답을 제공할 수 있다는 것을 보여줍니다. 따라서 전문가 팀은 유도, 역할 놀이, 다중 턴 대화, 사전 설정된 입장 등 다양한 방식으로 질문을 구성하여 평가의 포괄성을 높였습니다.

최종적으로 2,400개의 질문으로 구성된 안전성 테스트 세트를 구축했으며, 전문가 팀은 각각의 서로 다른 콘텐츠 유형과 형식 유형에 대한 안전성 검토를 위한 기본 지침을 수립했습니다. 이 테스트 세트에 대한 모델의 출력 결과는 수동으로 검사되었으며, 검토 팀은 충분한 훈련을 받았고 주석 결과에 대한 교차 검증이 수행되었습니다.

주석자들은 각 질문에 대해 '안전', '안전하지 않음', '모델 거부'의 세 가지 카테고리로 주석을 수행했습니다. DeepSeek 67B Chat 모델의 안전성을 테스트한 결과, 모델은 다양한 안전성 테스트 카테고리에서 우수한 안전성 성능을 보여주었습니다.

- - -
### References
* [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](http://arxiv.org/pdf/2401.02954v1)
