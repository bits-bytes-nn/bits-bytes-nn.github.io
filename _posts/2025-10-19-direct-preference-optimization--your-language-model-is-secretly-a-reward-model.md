---
layout: post
title: "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
date: 2023-05-29 17:57:46
author: "Stanford University"
categories: ["Paper Reviews", "Reinforcement-Learning"]
tags: ["Direct-Preference-Optimization", "Implicit-Reward-Modeling", "Bradley-Terry-Preference-Framework", "KL-Constrained-Reward-Maximization", "Binary-Cross-Entropy-Policy-Optimization", "Reward-Reparameterization", "Policy-Implicit-Reward-Function", "Closed-Form-Optimal-Policy-Extraction", "RL-Free-Preference-Learning", "Dynamic-Per-Example-Importance-Weighting"]
cover: /assets/images/default.jpg
use_math: true
---
### TL;DR
# Direct Preference Optimization: 언어 모델은 비밀리에 보상 모델입니다

## 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

대규모 언어 모델은 방대한 비지도 학습 데이터로부터 광범위한 세계 지식과 추론 능력을 습득하지만, 이러한 모델의 행동을 인간의 선호도에 맞춰 정밀하게 제어하는 것은 매우 어려운 과제입니다. 현재 표준적인 접근법인 인간 피드백 기반 강화학습(RLHF)은 먼저 인간 선호도 데이터로부터 보상 모델을 학습한 후, 강화학습 알고리즘을 사용하여 이 보상을 최대화하도록 언어 모델을 최적화합니다. 이 파이프라인은 여러 언어 모델을 동시에 훈련시켜야 하고, 훈련 루프에서 언어 모델 정책으로부터 샘플링을 수행해야 하므로 상당한 계산 비용이 발생합니다. 또한 보상 모델과 정책 모델을 별도로 유지하고 업데이트해야 하므로 구현이 복잡하며, 훈련 과정에서 불안정성이 발생할 수 있습니다.

이 논문의 핵심 동기는 이러한 복잡한 강화학습 파이프라인을 단순화하면서도 동일한 이론적 목적 함수를 최적화할 수 있는 방법을 찾는 것입니다. 저자들은 기존 RLHF 방법들이 사용하는 강화학습 기반 목적 함수를 단순한 이진 교차 엔트로피 목적 함수로 정확하게 최적화할 수 있음을 발견했습니다. 이를 통해 선호도 학습 파이프라인을 크게 단순화할 수 있으며, 명시적인 보상 모델 학습과 강화학습 훈련 루프를 제거하면서도 기존 방법들과 동등하거나 더 나은 성능을 달성할 수 있습니다. 이러한 접근법은 인간 선호도로부터 더 많은 언어 모델을 훈련시키는 데 있어 장벽을 의미 있게 낮추고, 선호도 기반 학습의 접근성을 크게 향상시킵니다.

## 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

**Direct Preference Optimization(DPO)**는 기존 RLHF 알고리즘과 동일한 목적 함수(KL 발산 제약이 있는 보상 최대화)를 암묵적으로 최적화하지만 구현이 간단하고 훈련이 직관적인 알고리즘입니다. DPO의 핵심 통찰은 보상 함수에서 최적 정책으로의 분석적 매핑을 활용하는 것입니다. 이를 통해 보상 함수에 대한 손실 함수를 정책에 대한 손실 함수로 변환할 수 있으며, 명시적이고 독립적인 보상 모델을 학습시키는 것을 피하면서도 Bradley-Terry 모델과 같은 기존의 인간 선호도 모델 하에서 최적화를 수행합니다. 본질적으로 정책 네트워크는 언어 모델과 암묵적 보상을 모두 표현하게 됩니다.

DPO의 수학적 기초는 다음과 같습니다. KL 제약이 있는 보상 최대화 문제의 최적 정책은 $\pi_r(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)$ 형태를 가집니다. 여기서 $Z(x)$는 분배 함수입니다. 이 식을 재배열하여 보상 함수를 정책의 로그 확률 비율로 표현하면 $r(x, y) = \beta \log \frac{\pi_r(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \beta \log Z(x)$가 됩니다. Bradley-Terry 모델은 두 응답 간의 보상 차이에만 의존하므로, 이 재매개변수화를 적용하면 분배 함수가 소거되고 인간 선호도 확률을 최적 정책과 참조 정책만의 관점에서 표현할 수 있습니다. 결과적으로 DPO는 다음의 단순한 이진 교차 엔트로피 손실 함수를 사용하여 정책을 직접 최적화합니다.

$$\mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}\right)\right]$$

DPO 업데이트의 메커니즘적 이해를 위해 손실 함수의 그래디언트를 분석하면, 선호되는 응답의 우도를 증가시키고 선호되지 않는 응답의 우도를 감소시키되, 예제들이 암묵적 보상 모델이 선호되지 않는 완성을 얼마나 더 높게 평가하는지에 따라 동적으로 가중치가 부여됨을 알 수 있습니다. 이러한 예제별 중요도 가중치는 순진한 확률 비율 목적 함수에서 발생하는 모델 퇴화를 방지하는 데 매우 중요합니다.

## 제안된 방법은 어떻게 구현되었습니까?

DPO의 파이프라인은 두 단계로 진행됩니다. 첫 번째 단계에서는 모든 프롬프트 $x$에 대해 완성 $y_1, y_2 \sim \pi_{\text{ref}}(\cdot \mid x)$를 샘플링하고, 인간 선호도로 레이블을 달아 오프라인 선호도 데이터셋 $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$을 구성합니다. 두 번째 단계에서는 주어진 $\pi_{\text{ref}}$와 $\mathcal{D}$, 그리고 원하는 $\beta$에 대해 $\mathcal{L}_{\text{DPO}}$를 최소화하도록 언어 모델 $\pi_\theta$를 최적화합니다. 실제로는 공개적으로 사용 가능한 선호도 데이터셋을 재사용하며, 선호도 데이터셋이 $\pi^{\text{SFT}}$를 사용하여 샘플링되었으므로 가능한 경우 $\pi_{\text{ref}} = \pi^{\text{SFT}}$로 초기화합니다. $\pi^{\text{SFT}}$를 사용할 수 없는 경우, 선호되는 완성의 우도를 최대화하여 $\pi_{\text{ref}}$를 초기화합니다.

실험은 세 가지 서로 다른 개방형 텍스트 생성 작업을 탐구합니다. **통제된 감정 생성** 작업에서는 IMDb 데이터셋의 영화 리뷰 접두사로부터 긍정적인 감정을 가진 텍스트를 생성하도록 훈련하며, 사전학습된 감정 분류기를 사용하여 선호도 쌍을 생성합니다. **요약** 작업에서는 Reddit TL;DR 데이터셋을 사용하여 포럼 게시물의 주요 요점을 요약하도록 훈련하며, 인간이 수집한 선호도 데이터를 활용합니다. **단일 턴 대화** 작업에서는 Anthropic Helpful and Harmless 데이터셋을 사용하여 다양한 인간 질의에 대해 매력적이고 유용한 응답을 생성하도록 훈련합니다. 모든 실험에서 기본 하이퍼파라미터는 $\beta = 0.1$, 배치 크기 64, 학습률 1e-6의 RMSprop 옵티마이저를 사용하며, 학습률은 0에서 1e-6까지 150 스텝에 걸쳐 선형적으로 워밍업합니다.

평가 방법론은 작업의 특성에 따라 다릅니다. 통제된 감정 생성 설정에서는 진짜 보상 함수(감정 분류기)에 접근할 수 있기 때문에 각 알고리즘이 달성한 보상과 참조 정책으로부터의 KL 발산의 프론티어로 평가합니다. 요약 및 단일 턴 대화 설정에서는 베이스라인 정책에 대한 승률로 알고리즘을 평가하며, 요약 품질과 응답 유용성에 대한 인간 평가의 대리 지표로 GPT-4를 사용합니다. GPT-4 평가의 신뢰성을 검증하기 위해 인간 연구를 수행했으며, 결과는 GPT-4가 인간이 서로 일치하는 만큼 인간과 일치하는 경향이 있어 인간 평가의 합리적인 대리 지표임을 보여줍니다.

## 이 연구의 결과가 가지는 의미는 무엇입니까?

DPO는 모든 실험 설정에서 기존 방법들과 비교하여 우수한 성능을 달성했습니다. **통제된 감정 생성** 실험에서 DPO는 가장 효율적인 보상-KL 프론티어를 생성하여 낮은 KL을 유지하면서 가장 높은 보상을 달성했습니다. 특히 DPO의 보상/KL 트레이드오프는 PPO를 엄격하게 지배했으며, PPO가 진짜 보상에 접근할 수 있는 경우(PPO-GT)에도 DPO가 더 나은 프론티어를 달성했습니다. **요약 작업**에서 DPO는 온도 0.0에서 약 61%의 승률을 달성하여 최적 샘플링 온도 0.0에서 57%의 PPO 성능을 초과했으며, Best of $N$ 베이스라인과 비교하여도 더 높은 최대 승률을 달성했습니다. 더욱이 DPO는 PPO보다 샘플링 온도에 훨씬 더 강건한 것으로 나타났으며, 다양한 온도에서 일관된 성능을 유지했습니다. **단일 턴 대화** 작업에서 DPO는 Anthropic-HH 데이터셋의 선호되는 완성을 개선하는 유일한 계산적으로 효율적인 방법이었으며, 계산적으로 까다로운 Best of 128 베이스라인과 유사하거나 더 나은 성능을 제공했습니다.

분포 이동 하에서의 일반화 능력도 검증되었습니다. Reddit TL;DR에서 훈련된 DPO와 PPO 정책을 다른 분포인 CNN/DailyMail 데이터셋의 테스트 분할에 있는 뉴스 기사에서 평가한 결과, DPO는 온도 0과 0.25에서 각각 0.36과 0.31의 승률을 달성하여 PPO의 0.26과 0.23을 상당한 차이로 능가했습니다. 이는 DPO 정책이 PPO가 사용하는 추가적인 레이블이 없는 Reddit TL;DR 프롬프트를 사용하지 않음에도 불구하고 PPO 정책과 유사하게 잘 일반화할 수 있음을 보여줍니다.

이 연구의 이론적 기여는 매우 중요합니다. DPO는 **언어 모델이 비밀리에 보상 모델**이라는 핵심 통찰을 제시하며, 이는 Plackett-Luce 선호도 모델 계열에서 모든 보상 동치 클래스가 주어진 참조 모델에 대해 어떤 모델을 사용하여 재매개변수화 $r(x, y) = \beta \log \frac{\pi(y \mid x)}{\pi_{\text{ref}}(y \mid x)}$로 표현될 수 있음을 의미합니다. 이는 DPO가 명시적 보상 모델링 없이도 전체 보상 공간을 탐색할 수 있음을 보장합니다. 또한 DPO는 액터-크리틱 알고리즘의 불안정성을 진단할 수 있는 이론적 프레임워크를 제공합니다. PPO와 같은 표준 강화학습 알고리즘에서 정규화 항이 최적해에 영향을 주지는 않지만, 이것이 없으면 목적 함수의 정책 경사가 높은 분산을 가질 수 있어 학습이 불안정해집니다. 대조적으로 DPO 재매개변수화는 어떠한 베이스라인도 요구하지 않는 보상 함수를 생성하므로, 분산을 줄이기 위한 추가적인 가치 함수 학습이나 몬테카를로 추정이 필요하지 않습니다.

실용적 관점에서 DPO는 선호도 기반 언어 모델 훈련의 접근성을 크게 향상시킵니다. 명시적인 보상 모델을 학습시키고 강화학습 훈련 루프를 유지해야 하는 복잡성을 제거하면서도 동일한 이론적 목적 함수를 최적화할 수 있다는 점은 매우 중요합니다. 이는 구현을 크게 단순화하고 계산 비용을 줄이며, 여러 모델을 동시에 유지하고 업데이트해야 하는 복잡성을 제거합니다. 하이퍼파라미터 튜닝이 거의 없이도 PPO를 기반으로 하는 것들을 포함한 기존 RLHF 알고리즘과 유사하거나 더 나은 성능을 보여주므로, DPO는 인간 선호도로부터 더 많은 언어 모델을 훈련시키는 데 있어 장벽을 의미 있게 낮춥니다.

향후 연구 방향으로는 여러 가지 중요한 질문들이 제기됩니다. 첫째, 명시적 보상 함수로부터 학습하는 것과 비교하여 DPO 정책이 분포 외 상황에서 어떻게 일반화하는지에 대한 더 포괄적인 연구가 필요합니다. 둘째, 보상 과최적화가 직접 선호도 최적화 설정에서 어떻게 나타나는지, 그리고 이를 완화할 수 있는 방법에 대한 이해가 필요합니다. 셋째, 이 논문에서는 최대 6B 파라미터의 모델을 평가했지만, DPO를 훨씬 더 큰 최신 모델로 확장하는 것은 흥미로운 향후 연구 방향입니다. 마지막으로 DPO의 이론적 프레임워크는 다른 선호도 모델링 접근법으로 확장될 수 있으며, 다중 선택지에 대한 순위나 조건부 선호도 등을 다루는 확장도 가능합니다. 이러한 확장은 더 풍부한 인간 피드백을 활용하여 모델의 정렬 품질을 향상시킬 수 있습니다.
- - -
## Direct Preference Optimization: 언어 모델은 비밀리에 보상 모델입니다

대규모 비지도 학습 언어 모델은 방대한 데이터셋으로부터 광범위한 세계 지식과 추론 능력을 습득하지만, 완전히 비지도 방식으로 훈련되기 때문에 모델의 행동을 정밀하게 제어하는 것은 매우 어려운 과제입니다. 이러한 모델들은 다양한 목표, 우선순위, 기술 수준을 가진 사람들이 생성한 데이터로 학습되기 때문에, 모든 학습 데이터가 바람직한 것은 아닙니다. 예를 들어, AI 코딩 어시스턴트가 일반적인 프로그래밍 실수를 이해하여 이를 수정할 수 있기를 원하지만, 코드를 생성할 때는 훈련 데이터에서 드물게 나타나는 고품질 코딩 능력 쪽으로 모델을 편향시키고 싶습니다. 마찬가지로, 언어 모델이 50%의 사람들이 믿는 일반적인 오해를 인식하기를 원하지만, 이 오해를 50%의 질문에서 사실이라고 주장하는 것은 원하지 않습니다.

기존의 방법들은 일반적으로 강화학습을 사용하여 언어 모델이 인간의 선호도와 일치하도록 조정하는데, 이 논문에서는 기존 방법들이 사용하는 강화학습 기반 목적 함수를 단순한 이진 교차 엔트로피 목적 함수로 정확하게 최적화할 수 있음을 보여줍니다. 이를 통해 선호도 학습 파이프라인을 크게 단순화할 수 있습니다.

![RLHF와 DPO 비교](https://arxiv.org/html/2305.18290/figures/diagrams/teaser.png)

위 그림은 인간 피드백을 활용한 언어 모델 파인튜닝의 두 가지 접근법을 비교합니다. 기존의 인간 피드백 기반 강화학습(RLHF) 방법은 먼저 프롬프트와 응답 쌍에 대한 인간 선호도 데이터셋에 보상 모델을 학습시킨 후, 강화학습을 사용하여 학습된 보상을 최대화하는 정책을 찾습니다. 반면 Direct Preference Optimization(DPO)은 단순한 분류 목적 함수로 선호도를 가장 잘 만족하는 정책을 직접 최적화하며, 암묵적 보상 모델을 학습시켜 해당하는 최적 정책을 닫힌 형태로 추출할 수 있습니다.

높은 수준에서 보면, 기존 방법들은 대규모 비지도 사전학습 후에 인간이 안전하고 유용하다고 판단하는 행동 유형을 나타내는 선별된 인간 선호도 집합을 사용하여 원하는 행동을 언어 모델에 주입합니다. 선호도 학습을 위한 가장 직관적인 접근법은 고품질 응답의 인간 시연에 대한 지도 파인튜닝이지만, 가장 성공적인 방법군은 인간(또는 AI) 피드백 기반 강화학습(RLHF/RLAIF)입니다. RLHF 방법들은 인간 선호도 데이터셋에 보상 모델을 학습시킨 후 강화학습을 사용하여 언어 모델 정책을 최적화하여 원본 모델로부터 과도하게 벗어나지 않으면서 높은 보상을 받는 응답을 생성하도록 합니다.

RLHF는 인상적인 대화 및 코딩 능력을 가진 모델을 생성하지만, RLHF 파이프라인은 지도 학습보다 상당히 복잡합니다. 여러 언어 모델을 훈련시켜야 하고 훈련 루프에서 언어 모델 정책으로부터 샘플링을 수행해야 하므로 상당한 계산 비용이 발생합니다. 이 논문에서는 명시적인 보상 모델링이나 강화학습 없이 언어 모델을 인간 선호도에 직접 최적화하는 방법을 제시합니다.

Direct Preference Optimization(DPO)은 기존 RLHF 알고리즘과 동일한 목적 함수(KL 발산 제약이 있는 보상 최대화)를 암묵적으로 최적화하지만 구현이 간단하고 훈련이 직관적인 알고리즘입니다. 직관적으로 DPO 업데이트는 선호되는 응답과 선호되지 않는 응답의 상대적 로그 확률을 증가시키지만, 순진한 확률 비율 목적 함수에서 발생하는 모델 퇴화를 방지하는 동적이고 예제별 중요도 가중치를 통합합니다.

DPO는 기존 알고리즘과 마찬가지로 주어진 보상 함수가 경험적 선호도 데이터와 얼마나 잘 일치하는지 측정하는 이론적 선호도 모델(예: Bradley-Terry 모델)에 의존합니다. 그러나 기존 방법들이 선호도 모델을 사용하여 보상 모델을 훈련시키는 선호도 손실을 정의한 다음 학습된 보상 모델을 최적화하는 정책을 훈련시키는 반면, DPO는 변수 변환을 사용하여 선호도 손실을 정책의 함수로 직접 정의합니다. 따라서 DPO는 모델 응답에 대한 인간 선호도 데이터셋이 주어지면 단순한 이진 교차 엔트로피 목적 함수를 사용하여 정책을 최적화할 수 있으며, 선호도 데이터에 맞춰진 암묵적 보상 함수에 대한 최적 정책을 생성합니다.

이 논문의 주요 기여는 선호도로부터 언어 모델을 훈련시키기 위한 단순한 강화학습 없는 알고리즘인 Direct Preference Optimization(DPO)입니다. 실험 결과는 DPO가 감정 조절, 요약, 대화와 같은 작업에서 선호도로부터 학습하는 데 있어 PPO 기반 RLHF를 포함한 기존 방법들만큼 효과적이며, 최대 6B 파라미터의 언어 모델을 사용한다는 것을 보여줍니다.
## 관련 연구

자기지도 학습 방식으로 훈련된 대규모 언어 모델은 규모가 증가함에 따라 일부 작업을 제로샷(zero-shot) 방식으로 수행하거나 퓨 샷(few-shot) 프롬프트를 통해 해결하는 능력을 학습합니다. 제로샷 학습은 모델이 사전에 해당 작업에 대한 명시적인 훈련 없이도 새로운 작업을 수행할 수 있는 능력을 의미하며, 퓨 샷 학습은 소수의 예시만으로도 작업을 이해하고 수행할 수 있는 능력을 나타냅니다. 그러나 이러한 모델들의 하위 작업 성능과 사용자 의도와의 정렬은 지시사항과 인간이 작성한 완성 예시로 구성된 데이터셋으로 파인튜닝을 수행함으로써 크게 향상될 수 있습니다.

이러한 지시사항 튜닝(instruction-tuning) 절차는 대규모 언어 모델이 지시사항 튜닝 세트 외부의 지시사항에도 일반화할 수 있게 하며, 전반적으로 모델의 사용성을 증가시킵니다. 지시사항 튜닝의 핵심 아이디어는 모델이 단순히 텍스트를 생성하는 것을 넘어서 사용자의 명령을 이해하고 따르도록 학습시키는 것입니다. 예를 들어, "[Ouyang et al.](https://arxiv.org/pdf/2203.02155v1)의 InstructGPT 연구"에서는 인간 시연 데이터와 모델 출력에 대한 인간 평가를 수집하여 보상 모델을 훈련시킨 후, 이 보상 모델의 점수를 최대화하도록 언어 모델을 파인튜닝하는 방법을 제시했습니다. 이러한 접근법은 원본 GPT-3 모델보다 다양한 지시사항 따르기 및 개방형 작업에서 우수한 성능을 보였습니다.

지시사항 튜닝의 성공에도 불구하고, 응답 품질에 대한 상대적인 인간 판단은 전문가 시연보다 수집하기 쉬운 경우가 많습니다. 이는 고품질의 시연 데이터를 생성하려면 전문가가 직접 완성된 응답을 작성해야 하지만, 선호도 데이터는 단순히 두 응답 중 어느 것이 더 나은지 선택하기만 하면 되기 때문입니다. 따라서 후속 연구들은 인간 선호도 데이터셋으로 대규모 언어 모델을 파인튜닝하여 번역, 요약, 스토리텔링, 지시사항 따르기 등의 작업에서 숙련도를 향상시켰습니다.

이러한 선호도 기반 방법들은 일반적으로 두 단계로 구성됩니다. 첫 번째 단계에서는 Bradley-Terry 모델과 같은 선호도 모델 하에서 선호도 데이터셋과의 호환성을 위해 신경망 보상 함수를 최적화합니다. Bradley-Terry 모델은 두 선택지 간의 선호도를 확률적으로 모델링하는 고전적인 방법으로, 두 응답 $y_1$과 $y_2$가 주어졌을 때 $y_1$이 선호될 확률을 다음과 같이 표현합니다.

$$P(y_1 \succ y_2 | x) = \frac{\exp(r(x, y_1))}{\exp(r(x, y_1)) + \exp(r(x, y_2))}$$

여기서 $r(x, y)$는 프롬프트 $x$와 응답 $y$에 대한 보상 함수입니다. 이 확률 모델은 체스 경기나 스포츠 경기에서 선수들의 상대적 실력을 평가하는 데 사용되던 방법을 언어 모델의 응답 품질 평가에 적용한 것입니다.

두 번째 단계에서는 강화학습 알고리즘을 사용하여 언어 모델을 파인튜닝하여 주어진 보상을 최대화합니다. 일반적으로 사용되는 알고리즘으로는 REINFORCE, Proximal Policy Optimization(PPO), 또는 이들의 변형이 있습니다. "[Schulman et al.](https://arxiv.org/pdf/1707.06347v2)의 PPO 연구"에서 제안된 PPO는 정책 업데이트를 신뢰 영역 내로 제한하는 클리핑된 대리 목적 함수를 사용하여 훈련 과정을 안정화시킵니다. PPO의 업데이트 규칙은 다음과 같이 표현됩니다.

$$\theta_{k+1} = \arg\max_\theta \mathbb{E}_{s,a\sim\pi_\theta} \left[ \min\left(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}, 1 \pm \epsilon\right) A^{\pi_{\theta_k}}(s,a) \right]$$

여기서 $\pi_\theta$는 정책, $A^{\pi_{\theta_k}}(s,a)$는 어드밴티지 함수, $\epsilon$은 신뢰 영역의 크기를 제어하는 하이퍼파라미터입니다. 이 방법은 정책이 한 번에 너무 크게 변하지 않도록 제약하여 훈련의 안정성을 보장합니다.

밀접하게 관련된 연구 방향으로는 지시사항 따르기를 위해 파인튜닝된 대규모 언어 모델을 활용하여 안전성이나 무해성과 같은 목표 속성에 대한 추가적인 합성 선호도 데이터를 생성하는 방법이 있습니다. "[Bai et al.](https://arxiv.org/pdf/2212.08073v1)의 Constitutional AI 연구"는 이러한 접근법의 대표적인 예로, 모델이 자신의 응답을 비평하고 수정하도록 하는 자기 비평(self-critique) 메커니즘을 도입했습니다. 이 방법은 인간으로부터 텍스트 형태의 원칙(rubric)만을 약한 감독으로 받아 모델의 주석을 생성합니다. 구체적으로, 지도 학습 단계에서 모델은 유해한 응답을 유도하는 프롬프트에 대해 자신의 응답을 생성하고, 이를 비평한 후 유해한 내용을 제거하도록 수정합니다. 강화학습 단계에서는 이렇게 개선된 모델을 사용하여 응답 쌍을 생성하고, 독립적인 피드백 모델이 16개의 원칙 중 무작위로 선택된 원칙에 따라 덜 유해한 응답을 선택합니다. 이러한 방법들은 두 가지 연구 분야의 융합을 나타냅니다. 하나는 다양한 목적을 위해 강화학습으로 언어 모델을 훈련시키는 연구이고, 다른 하나는 인간 선호도로부터 학습하는 일반적인 방법에 관한 연구입니다.

상대적 인간 선호도를 사용하는 것의 매력에도 불구하고, 강화학습으로 대규모 언어 모델을 파인튜닝하는 것은 여전히 주요한 실용적 과제로 남아 있습니다. 강화학습 기반 접근법은 여러 언어 모델을 훈련시켜야 하고, 훈련 루프에서 언어 모델 정책으로부터 샘플링을 수행해야 하므로 상당한 계산 비용이 발생합니다. 또한 보상 모델과 정책 모델을 별도로 유지하고 업데이트해야 하므로 구현이 복잡하며, 훈련 과정에서 불안정성이 발생할 수 있습니다. 이 논문은 강화학습 없이 상대적 선호도를 최적화하는 이론적으로 정당화된 접근법을 제공합니다.

언어 모델 외의 맥락에서 선호도로부터 정책을 학습하는 것은 밴딧(bandit)과 강화학습 설정 모두에서 연구되어 왔으며, 여러 접근법이 제안되었습니다. 보상이 아닌 행동의 선호도나 순위를 사용하는 문맥적 밴딧 학습은 문맥적 듀얼링 밴딧(Contextual Dueling Bandit, CDB)으로 알려져 있습니다. 절대적인 보상이 없는 상황에서 CDB의 이론적 분석은 최적 정책의 개념을 폰 노이만 승자(von Neumann winner)로 대체합니다. 폰 노이만 승자는 다른 모든 정책에 대해 기대 승률이 최소 50% 이상인 정책을 의미합니다. 이는 체스나 바둑과 같은 게임에서 어떤 전략이 다른 모든 전략에 대해 최소한 비기거나 이길 수 있다면 그 전략이 안정적이라고 볼 수 있는 것과 유사한 개념입니다.

그러나 CDB 설정에서는 선호도 레이블이 온라인으로 제공되는 반면, 인간 선호도로부터 학습하는 경우에는 일반적으로 오프라인 선호도 주석이 달린 행동 쌍의 고정된 배치로부터 학습합니다. 이는 실제 응용에서 중요한 차이점입니다. 온라인 설정에서는 모델이 행동을 취하고 즉시 피드백을 받을 수 있지만, 오프라인 설정에서는 미리 수집된 데이터로부터만 학습해야 합니다.

유사하게, 선호도 기반 강화학습(Preference-based RL, PbRL)은 보상이 아닌 알려지지 않은 '점수 매기기' 함수에 의해 생성된 이진 선호도로부터 학습합니다. "[Christiano et al.](https://arxiv.org/pdf/1706.03741v4)의 연구"는 이러한 접근법의 선구적인 예로, 인간 선호도로부터 보상 예측기를 학습하는 방법을 제시했습니다. 이 방법은 궤적 세그먼트 $\sigma = ((o_0, a_0), (o_1, a_1), \ldots, (o_{k-1}, a_{k-1}))$에 대한 인간의 선호도를 수집하고, 보상 예측기 $\hat{r}: \mathcal{O} \times \mathcal{A} \rightarrow \mathbb{R}$를 훈련시켜 다음의 교차 엔트로피 손실을 최소화합니다.

$$\text{loss}(\hat{r}) = -\sum_{(\sigma^1, \sigma^2, \mu) \in \mathcal{D}} \mu(1) \log \hat{P}[\sigma^1 \succ \sigma^2] + \mu(2) \log \hat{P}[\sigma^2 \succ \sigma^1]$$

여기서 $\hat{P}[\sigma^1 \succ \sigma^2] = \frac{\exp{\sum \hat{r}(o^1_t, a^1_t)}}{\exp{\sum \hat{r}(o^1_t, a^1_t)} + \exp{\sum \hat{r}(o^2_t, a^2_t)}}$는 $\sigma^1$이 $\sigma^2$보다 선호될 예측 확률입니다. 이 연구는 Atari 게임과 시뮬레이션된 로봇 운동과 같은 복잡한 강화학습 작업에서 인간 피드백만으로 새로운 복잡한 행동을 학습할 수 있음을 보여주었으며, 로봇이 백플립을 수행하는 것과 같은 작업을 약 1시간의 인간 피드백만으로 학습할 수 있었습니다.

PbRL을 위한 다양한 알고리즘이 존재하며, 오프폴리시(off-policy) 선호도 데이터를 재사용할 수 있는 방법들도 포함됩니다. "[Peng et al.](https://arxiv.org/pdf/1910.00177)의 Advantage-Weighted Regression(AWR) 연구"는 이러한 접근법의 한 예로, 경험 재생(experience replay)을 통해 오프폴리시 데이터를 활용할 수 있는 단순한 강화학습 알고리즘을 제시했습니다. AWR은 가치 함수 베이스라인을 학습하는 회귀 단계와 어드밴티지 가중 회귀를 사용하여 정책을 업데이트하는 단계로 구성됩니다. 최적 정책 업데이트는 다음과 같은 가중 회귀 문제의 형태를 취합니다.

$$\pi^*(a|s) = \frac{1}{Z(s)} \mu(a|s) \exp\left(\frac{1}{\beta}\left(\mathcal{R}_{s,a}^\mu - V^\mu(s)\right)\right)$$

여기서 가중치는 지수화된 어드밴티지로 주어집니다. 이 방법은 과거 정책들의 혼합으로부터 샘플링된 데이터를 사용할 수 있도록 확장되었으며, 가치 함수 베이스라인은 과거 정책들의 가치 함수의 가중 평균으로 추정됩니다.

그러나 이러한 방법들은 일반적으로 먼저 잠재적인 점수 매기기 함수(즉, 보상 모델)를 명시적으로 추정한 다음 이를 최적화하는 방식을 따릅니다. 이는 두 단계 프로세스로, 첫 번째 단계에서 보상 모델을 학습하고 두 번째 단계에서 이 보상 모델을 최대화하는 정책을 학습합니다. "[Ziegler et al.](https://arxiv.org/pdf/1909.08593v2)의 연구"와 "[Stiennon et al.](https://arxiv.org/pdf/2009.01325v3)의 요약 학습 연구"는 이러한 두 단계 접근법을 언어 모델에 적용한 대표적인 예입니다. 이들 연구는 먼저 인간 비교 데이터로부터 보상 모델을 훈련시킨 후, PPO를 사용하여 이 보상을 최대화하도록 정책을 파인튜닝했습니다.

이와 대조적으로, 이 논문은 선호도를 만족시키기 위해 정책을 직접 최적화하는 단일 단계 정책 학습 접근법을 제시합니다. 이는 명시적인 보상 모델을 학습하는 중간 단계를 거치지 않고, 선호도 데이터로부터 직접 정책을 최적화합니다. 이러한 접근법은 구현을 크게 단순화하고 계산 비용을 줄이며, 보상 모델과 정책 모델 간의 불일치로 인한 문제를 피할 수 있습니다. 또한 강화학습의 복잡성과 불안정성을 제거하면서도 동일한 이론적 목적 함수를 최적화할 수 있다는 점에서 기존 방법들에 비해 실용적인 이점을 제공합니다.
## 예비 지식

이 섹션에서는 논문의 핵심 방법론인 Direct Preference Optimization(DPO)을 이해하기 위해 필요한 기존의 인간 피드백 기반 강화학습(RLHF) 파이프라인을 상세히 설명합니다. RLHF는 대규모 언어 모델을 인간의 선호도에 맞춰 조정하는 표준적인 접근법으로, 일반적으로 세 가지 단계로 구성됩니다.

### 지도 파인튜닝 단계

RLHF 파이프라인의 첫 번째 단계는 지도 파인튜닝(Supervised Fine-Tuning, SFT)입니다. 이 단계에서는 사전학습된 언어 모델을 대화, 요약 등 목표로 하는 하위 작업에 대한 고품질 데이터로 지도 학습을 통해 파인튜닝하여 $\pi^{\text{SFT}}$ 모델을 얻습니다. 이 과정은 모델이 특정 작업의 기본적인 형식과 패턴을 학습하도록 하는 초기 단계로, 이후 선호도 학습의 기반이 됩니다.

### 보상 모델링 단계

두 번째 단계는 보상 모델링 단계로, 인간의 선호도 데이터를 수집하고 이를 바탕으로 보상 함수를 학습합니다. 이 단계는 다음과 같은 세부 과정으로 진행됩니다.

먼저 SFT 모델에 프롬프트 $x$를 제공하여 한 쌍의 응답 $(y_1, y_2) \sim \pi^{\text{SFT}}(y \mid x)$를 생성합니다. 이렇게 생성된 응답 쌍은 인간 평가자에게 제시되며, 평가자는 두 응답 중 어느 것을 선호하는지 표현합니다. 이러한 선호도는 $y_w \succ y_l \mid x$로 표기되며, 여기서 $y_w$는 선호되는 응답(chosen), $y_l$은 선호되지 않는 응답(rejected)을 나타냅니다.

이러한 선호도는 우리가 직접 접근할 수 없는 잠재적인 보상 모델 $r^*(y, x)$에 의해 생성된다고 가정합니다. 선호도를 모델링하는 여러 접근법이 존재하며, 그중 Bradley-Terry(BT) 모델이 널리 사용됩니다. 더 일반적인 Plackett-Luce 순위 모델도 여러 개의 순위가 매겨진 응답에 접근할 수 있는 경우 이 프레임워크와 호환됩니다.

Bradley-Terry 모델은 인간 선호도 분포 $p^*$를 다음과 같이 표현합니다.

$$p^*(y_1 \succ y_2 \mid x) = \frac{\exp(r^*(x, y_1))}{\exp(r^*(x, y_1)) + \exp(r^*(x, y_2))}$$

이 수식은 두 응답 간의 선호도를 보상 함수의 지수 함수 비율로 모델링합니다. 직관적으로 이해하면, 보상이 높은 응답일수록 선호될 확률이 기하급수적으로 증가하는 구조입니다. 이는 체스나 스포츠 경기에서 선수들의 상대적 실력을 평가하는 Elo 레이팅 시스템과 유사한 원리를 따릅니다.

$p^*$로부터 샘플링된 비교 데이터의 정적 데이터셋 $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$에 접근할 수 있다고 가정하면, 보상 모델 $r_\phi(x, y)$를 파라미터화하고 최대 우도 추정을 통해 파라미터를 추정할 수 있습니다. 이 문제를 이진 분류 문제로 프레이밍하면 다음과 같은 음의 로그 우도 손실 함수를 얻습니다.

$$\mathcal{L}_R(r_\phi, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}[\log\sigma(r_\phi(x, y_w) - r_\phi(x, y_l))]$$

여기서 $\sigma$는 로지스틱 함수입니다. 이 손실 함수는 선호되는 응답과 선호되지 않는 응답 간의 보상 차이를 최대화하도록 설계되었습니다. 로지스틱 함수를 사용함으로써 보상 차이가 클수록 손실이 작아지고, 모델이 잘못된 순서로 예측할 경우 큰 페널티를 받게 됩니다.

언어 모델의 맥락에서 보상 네트워크 $r_\phi(x, y)$는 종종 SFT 모델 $\pi^{\text{SFT}}(y \mid x)$로부터 초기화되며, 최종 트랜스포머 레이어 위에 단일 스칼라 보상 값을 생성하는 선형 레이어를 추가합니다. 이러한 구조는 사전학습된 언어 모델의 표현 능력을 활용하면서도 보상 예측이라는 새로운 작업에 적응할 수 있도록 합니다.

보상 함수의 분산을 낮추기 위해 선행 연구들은 보상을 정규화하여 모든 $x$에 대해 $\mathbb{E}_{x, y \sim \mathcal{D}}[r_\phi(x, y)] = 0$이 되도록 합니다. 이러한 정규화는 보상 값의 절대적 크기보다는 상대적 순서가 중요하다는 점을 반영하며, 훈련의 안정성을 향상시킵니다.

### 강화학습 파인튜닝 단계

강화학습 단계에서는 학습된 보상 함수를 사용하여 언어 모델에 피드백을 제공합니다. 선행 연구들을 따라 최적화 문제는 다음과 같이 정식화됩니다.

$$\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y \mid x)}[r_\phi(x, y)] - \beta \mathbb{D}_{\text{KL}}[\pi_\theta(y \mid x) \mid\mid \pi_{\text{ref}}(y \mid x)]$$

이 목적 함수는 두 가지 중요한 요소로 구성됩니다. 첫 번째 항은 학습된 보상 함수를 최대화하려는 목표를 나타내며, 두 번째 항은 KL 발산 제약을 통해 모델이 참조 정책 $\pi_{\text{ref}}$로부터 과도하게 벗어나는 것을 방지합니다. 여기서 $\beta$는 기본 참조 정책으로부터의 편차를 제어하는 파라미터이며, 참조 정책은 일반적으로 초기 SFT 모델 $\pi^{\text{SFT}}$입니다.

실제로 언어 모델 정책 $\pi_\theta$도 $\pi^{\text{SFT}}$로 초기화됩니다. 이러한 제약이 추가된 이유는 매우 중요합니다. 첫째, 보상 모델이 정확한 분포로부터 모델이 너무 멀리 벗어나는 것을 방지합니다. 보상 모델은 특정 데이터 분포에서 학습되었기 때문에, 정책이 이 분포를 크게 벗어나면 보상 모델의 예측이 신뢰할 수 없게 됩니다. 둘째, 생성 다양성을 유지하고 단일 고보상 응답으로의 모드 붕괴(mode collapse)를 방지합니다. 제약이 없다면 모델은 보상이 높은 몇 가지 응답만 반복적으로 생성할 수 있습니다.

언어 생성의 이산적 특성으로 인해 이 목적 함수는 미분 가능하지 않으며, 일반적으로 강화학습으로 최적화됩니다. 표준 접근법은 보상 함수를 $r(x, y) = r_\phi(x, y) - \beta(\log\pi_\theta(y \mid x) - \log\pi_{\text{ref}}(y \mid x))$로 구성하고, Proximal Policy Optimization(PPO)을 사용하여 최대화하는 것입니다.

이 보상 함수의 구조를 자세히 살펴보면, 학습된 보상 $r_\phi(x, y)$에서 KL 발산 페널티를 직접 빼는 형태입니다. KL 발산 항 $\beta(\log\pi_\theta(y \mid x) - \log\pi_{\text{ref}}(y \mid x))$는 현재 정책이 참조 정책으로부터 얼마나 벗어났는지를 측정하며, 이 값이 클수록 보상에서 큰 페널티가 부과됩니다. 이러한 구조는 정책이 높은 보상을 추구하면서도 참조 정책과의 유사성을 유지하도록 균형을 맞춥니다.

PPO는 정책 경사 방법의 일종으로, 정책 업데이트가 너무 크지 않도록 제한하는 클리핑된 대리 목적 함수를 사용합니다. 이는 훈련 과정의 안정성을 크게 향상시키지만, 여러 모델(정책 모델, 보상 모델, 가치 함수 모델)을 동시에 유지하고 업데이트해야 하므로 구현이 복잡하고 계산 비용이 높습니다.

이러한 RLHF 파이프라인은 InstructGPT와 같은 최신 언어 모델의 성공에 핵심적인 역할을 했지만, 복잡성과 계산 비용이라는 실용적 과제를 안고 있습니다. 이 논문에서 제안하는 DPO는 이러한 복잡한 강화학습 과정을 단순한 지도 학습 문제로 변환하여 동일한 목적을 달성하는 방법을 제시합니다.
## Direct Preference Optimization

대규모 언어 모델에 강화학습 알고리즘을 적용하는 것의 어려움에 동기를 받아, 이 연구의 목표는 선호도를 직접 사용하여 정책을 최적화하는 단순한 접근법을 도출하는 것입니다. 보상을 학습한 다음 강화학습을 통해 이를 최적화하는 기존 RLHF 방법들과 달리, 이 접근법은 특정한 보상 모델 파라미터화 선택을 활용하여 강화학습 훈련 루프 없이 닫힌 형태로 최적 정책을 추출할 수 있습니다.

핵심 통찰은 보상 함수에서 최적 정책으로의 분석적 매핑을 활용하는 것입니다. 이를 통해 보상 함수에 대한 손실 함수를 정책에 대한 손실 함수로 변환할 수 있습니다. 이러한 변수 변환 접근법은 명시적이고 독립적인 보상 모델을 학습시키는 것을 피하면서도, Bradley-Terry 모델과 같은 기존의 인간 선호도 모델 하에서 최적화를 수행합니다. 본질적으로 정책 네트워크는 언어 모델과 (암묵적) 보상을 모두 표현합니다.

### DPO 목적 함수 유도

유도 과정은 일반적인 보상 함수 $r$ 하에서 앞서 설명한 강화학습 목적 함수로부터 시작합니다. 선행 연구들을 따라, KL 제약이 있는 보상 최대화 목적 함수의 최적 해가 다음과 같은 형태를 취한다는 것을 보일 수 있습니다.

$$\pi_r(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)$$

여기서 $Z(x) = \sum_y \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)$는 분배 함수(partition function)입니다. 이 수식은 최적 정책이 참조 정책에 보상에 비례하는 지수 가중치를 곱한 형태로 표현됨을 보여줍니다.

이 결과를 직관적으로 이해하면, 높은 보상을 받는 응답은 참조 정책에 비해 지수적으로 더 높은 확률을 가지게 되며, $\beta$ 파라미터는 이러한 가중치의 강도를 조절합니다. $\beta$가 작을수록 보상의 영향이 커지고, $\beta$가 클수록 참조 정책에 더 가까워집니다.

진짜 보상 함수 $r^*$의 최대 우도 추정치 $r_\phi$를 사용하더라도, 분배 함수 $Z(x)$를 추정하는 것은 여전히 계산 비용이 많이 들기 때문에 이 표현을 실제로 활용하기 어렵습니다. 분배 함수는 모든 가능한 응답에 대해 합을 계산해야 하는데, 언어 모델의 경우 가능한 응답의 수가 어휘 크기의 시퀀스 길이 제곱에 비례하여 천문학적으로 많기 때문입니다.

그러나 위 수식을 재배열하여 보상 함수를 대응하는 최적 정책 $\pi_r$, 참조 정책 $\pi_{\text{ref}}$, 그리고 알려지지 않은 분배 함수 $Z(\cdot)$의 관점에서 표현할 수 있습니다. 구체적으로, 먼저 양변에 로그를 취한 다음 대수적 조작을 통해 다음을 얻습니다.

$$r(x, y) = \beta \log \frac{\pi_r(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \beta \log Z(x)$$

이 재매개변수화는 보상 함수를 정책의 로그 확률 비율과 분배 함수의 로그의 합으로 표현합니다. 이는 보상이 정책이 참조 정책으로부터 얼마나 벗어났는지를 측정하는 항과 정규화 항으로 분해됨을 의미합니다.

이 재매개변수화를 진짜 보상 $r^*$과 대응하는 최적 모델 $\pi^*$에 적용할 수 있습니다. 다행히도 Bradley-Terry 모델은 두 완성 간의 보상 차이에만 의존합니다. $p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))$. 앞서 유도한 재매개변수화를 $r^*(x, y)$에 대입하여 선호도 모델에 대입하면, 분배 함수가 소거되고 인간 선호도 확률을 최적 정책 $\pi^*$과 참조 정책 $\pi_{\text{ref}}$만의 관점에서 표현할 수 있습니다.

이것이 가능한 이유는 두 응답에 대한 보상 차이를 계산할 때 $\beta \log Z(x)$ 항이 양쪽에 모두 나타나 소거되기 때문입니다. 이는 매우 중요한 수학적 트릭으로, 계산하기 어려운 분배 함수를 제거할 수 있게 해줍니다.

따라서 Bradley-Terry 모델 하에서 최적 RLHF 정책 $\pi^*$은 다음의 선호도 모델을 만족합니다.

$$p^*(y_1 \succ y_2 \mid x) = \frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)} - \beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)}\right)}$$

이 수식은 선호도 확률을 두 응답에 대한 정책과 참조 정책의 로그 확률 비율의 차이로 표현합니다. 로지스틱 함수 형태를 취하고 있어, 첫 번째 응답이 두 번째 응답보다 참조 정책 대비 더 높은 확률을 가질수록 선호될 확률이 증가합니다.

이 유도는 Bradley-Terry 모델을 사용하지만, 더 일반적인 Plackett-Luce 모델 하에서도 유사하게 표현을 유도할 수 있습니다. Plackett-Luce 모델은 두 개 이상의 선택지에 대한 순위를 모델링할 수 있는 확장된 프레임워크입니다.

이제 보상 모델이 아닌 최적 정책의 관점에서 인간 선호도 데이터의 확률을 표현했으므로, 파라미터화된 정책 $\pi_\theta$에 대한 최대 우도 목적 함수를 정식화할 수 있습니다. 보상 모델링 접근법과 유사하게, 정책 목적 함수는 다음과 같이 됩니다.

$$\mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}\right)\right]$$

여기서 $y_w$는 선호되는 응답(chosen), $y_l$은 선호되지 않는 응답(rejected)을 나타냅니다. 이 손실 함수는 정책이 선호되는 응답에 대해 참조 정책 대비 더 높은 확률을 할당하고, 선호되지 않는 응답에 대해서는 더 낮은 확률을 할당하도록 유도합니다.

이러한 방식으로 대안적인 파라미터화를 사용하여 암묵적 보상을 학습시키며, 그 최적 정책은 단순히 $\pi_\theta$입니다. 더욱이 이 절차는 재매개변수화된 Bradley-Terry 모델을 학습시키는 것과 동등하므로, 선호도 데이터 분포에 대한 적절한 가정 하에서 일관성과 같은 특정 이론적 속성을 누립니다.

### DPO 업데이트의 메커니즘적 이해

DPO의 메커니즘적 이해를 위해서는 손실 함수 $\mathcal{L}_{\text{DPO}}$의 그래디언트를 분석하는 것이 유용합니다. 파라미터 $\theta$에 대한 그래디언트는 다음과 같이 쓸 수 있습니다.

$$\nabla_\theta \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = -\beta \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\left[\underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w))}_{\text{보상 추정이 틀렸을 때 더 높은 가중치}} \left[\underbrace{\nabla_\theta \log \pi(y_w \mid x)}_{\text{$y_w$의 우도 증가}} - \underbrace{\nabla_\theta \log \pi(y_l \mid x)}_{\text{$y_l$의 우도 감소}}\right]\right]$$

여기서 $\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\text{ref}}(y \mid x)}$는 언어 모델 $\pi_\theta$와 참조 모델 $\pi_{\text{ref}}$에 의해 암묵적으로 정의된 보상입니다.

이 그래디언트 수식을 자세히 살펴보면, 손실 함수의 그래디언트는 선호되는 완성 $y_w$의 우도를 증가시키고 선호되지 않는 완성 $y_l$의 우도를 감소시킵니다. 중요한 점은 예제들이 암묵적 보상 모델 $\hat{r}_\theta$가 선호되지 않는 완성을 얼마나 더 높게 평가하는지에 따라 가중치가 부여된다는 것입니다. 이는 $\beta$로 스케일되며, 즉 암묵적 보상 모델이 완성들을 얼마나 잘못 순서 지었는지를 나타내며 KL 제약의 강도를 고려합니다.

구체적으로 로지스틱 함수 $\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w))$는 모델이 선호되지 않는 응답에 더 높은 보상을 부여할수록 1에 가까워지고, 올바르게 순서를 매길수록 0에 가까워집니다. 따라서 모델이 틀렸을 때 더 큰 업데이트가 이루어지고, 이미 올바른 순서를 학습했을 때는 작은 업데이트가 이루어집니다.

실험 결과는 이러한 가중치의 중요성을 시사하며, 가중치 계수가 없는 순진한 버전의 이 방법은 언어 모델을 퇴화시킬 수 있습니다. 가중치가 없다면 모델은 선호되는 응답의 확률을 무한정 증가시키고 선호되지 않는 응답의 확률을 무한정 감소시키려 할 수 있으며, 이는 모드 붕괴나 의미 없는 출력으로 이어질 수 있습니다.

### DPO 파이프라인

일반적인 DPO 파이프라인은 다음과 같이 진행됩니다. 첫 번째 단계에서는 모든 프롬프트 $x$에 대해 완성 $y_1, y_2 \sim \pi_{\text{ref}}(\cdot \mid x)$를 샘플링하고, 인간 선호도로 레이블을 달아 오프라인 선호도 데이터셋 $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$을 구성합니다. 두 번째 단계에서는 주어진 $\pi_{\text{ref}}$와 $\mathcal{D}$, 그리고 원하는 $\beta$에 대해 $\mathcal{L}_{\text{DPO}}$를 최소화하도록 언어 모델 $\pi_\theta$를 최적화합니다.

실제로는 공개적으로 사용 가능한 선호도 데이터셋을 재사용하고자 하며, 샘플을 생성하고 인간 선호도를 수집하는 것보다 이것이 더 효율적입니다. 선호도 데이터셋이 $\pi^{\text{SFT}}$를 사용하여 샘플링되었으므로, 가능한 경우 $\pi_{\text{ref}} = \pi^{\text{SFT}}$로 초기화합니다.

그러나 $\pi^{\text{SFT}}$를 사용할 수 없는 경우, 선호되는 완성 $(x, y_w)$의 우도를 최대화하여 $\pi_{\text{ref}}$를 초기화합니다.

$$\pi_{\text{ref}} = \operatorname*{arg\,max}_\pi \mathbb{E}_{x, y_w \sim \mathcal{D}}[\log \pi(y_w \mid x)]$$

이 절차는 사용할 수 없는 진짜 참조 분포와 DPO가 사용하는 $\pi_{\text{ref}}$ 간의 분포 이동을 완화하는 데 도움이 됩니다. 참조 정책이 선호도 데이터를 생성한 정책과 너무 다르면, 암묵적 보상 모델의 추정이 부정확해질 수 있기 때문입니다.

이러한 초기화 전략은 DPO가 실제 데이터 분포에 더 가까운 참조 정책으로 시작할 수 있게 하여, 학습 과정의 안정성과 최종 성능을 향상시킵니다. 선호되는 완성에 대한 우도 최대화는 본질적으로 간단한 지도 파인튜닝을 수행하는 것이며, 이는 참조 정책이 적어도 고품질 응답을 생성할 수 있는 능력을 갖추도록 보장합니다.

DPO의 핵심 장점은 명시적인 보상 모델 학습과 강화학습 훈련 루프를 제거하면서도, 기존 RLHF 방법들과 동일한 이론적 목적 함수를 최적화한다는 것입니다. 이는 구현을 크게 단순화하고 계산 비용을 줄이며, 여러 모델을 동시에 유지하고 업데이트해야 하는 복잡성을 제거합니다. 동시에 Bradley-Terry 모델과 같은 확립된 선호도 모델의 이론적 기반을 유지하여, 적절한 가정 하에서 일관성과 같은 바람직한 속성을 보장합니다.
## DPO의 이론적 분석

이 섹션에서는 DPO 방법론에 대한 심층적인 이론적 해석을 제공하고, DPO가 가지는 이론적 장점을 PPO와 같은 액터-크리틱 알고리즘의 문제점과 연관지어 분석합니다.

### 언어 모델은 비밀리에 보상 모델입니다

DPO는 명시적인 보상 함수를 학습하거나 강화학습을 수행하지 않고도, 단일 최대 우도 목적 함수만으로 정책을 최적화합니다. DPO 최적화 목적 함수는 다음과 같은 보상 재매개변수화를 가진 Bradley-Terry 모델과 동등합니다.

$$r^*(x, y) = \beta \log \frac{\pi^*_\theta(y \mid x)}{\pi_{\text{ref}}(y \mid x)}$$

이 재매개변수화에서 파라미터화된 모델 $\pi_\theta$를 최적화하는 것은 변수 변환 하에서 보상 모델 최적화와 동등합니다. 이 섹션에서는 이러한 재매개변수화의 이론적 기반을 구축하고, 학습 가능한 보상 모델의 클래스를 제약하지 않으면서도 최적 정책의 정확한 복원이 가능함을 보입니다.

#### 보상 함수의 동치 관계

보상 함수들 사이의 동치 관계를 정의하는 것으로 시작합니다.

**Definition 1.** 두 보상 함수 $r(x, y)$와 $r'(x, y)$가 어떤 함수 $f$에 대해 $r(x, y) - r'(x, y) = f(x)$를 만족하면, 두 보상 함수는 동치라고 합니다.

이것이 실제로 동치 관계임을 쉽게 확인할 수 있으며, 이 관계는 보상 함수의 집합을 동치 클래스들로 분할합니다. 동치 관계의 핵심은 두 보상 함수의 차이가 오직 프롬프트 $x$에만 의존하고 응답 $y$에는 의존하지 않는다는 점입니다.

**Lemma 1.** Plackett-Luce 선호도 프레임워크, 특히 Bradley-Terry 모델 하에서, 동일한 클래스에 속한 두 보상 함수는 동일한 선호도 분포를 유도합니다.

**Lemma 2.** 동일한 동치 클래스에 속한 두 보상 함수는 제약된 강화학습 문제 하에서 동일한 최적 정책을 유도합니다.

이 두 보조정리의 증명은 직접적이며 부록에서 다룹니다. Lemma 1은 Plackett-Luce 모델 계열에서 잘 알려진 과소 명세 문제를 나타냅니다. 이러한 과소 명세로 인해, 최대 우도 추정치에 대한 보장을 달성하기 위해서는 일반적으로 추가적인 식별 가능성 제약을 부과해야 합니다. Lemma 2는 동일한 클래스의 모든 보상 함수가 동일한 최적 정책을 생성한다는 것을 명시하므로, 최종 목표를 위해서는 최적 클래스로부터 임의의 보상 함수를 복원하는 것만이 관심사입니다.

#### DPO 재매개변수화의 표현력

다음 정리를 증명합니다.

**Theorem 1.** 적절한 가정 하에서, Plackett-Luce 모델, 특히 Bradley-Terry 모델과 일치하는 모든 보상 클래스는 주어진 참조 모델 $\pi_{\text{ref}}(y \mid x)$에 대해 어떤 모델 $\pi(y \mid x)$를 사용하여 다음과 같은 재매개변수화로 표현될 수 있습니다.

$$r(x, y) = \beta \log \frac{\pi(y \mid x)}{\pi_{\text{ref}}(y \mid x)}$$

**증명 스케치.** 임의의 보상 함수 $r(x, y)$를 고려하고, 이것이 대응하는 최적 모델 $\pi_r(y \mid x)$를 유도한다고 합시다. $r$의 동치 클래스로부터의 보상 함수가 위의 재매개변수화를 사용하여 표현될 수 있음을 보입니다. 사영 연산자 $f$를 다음과 같이 정의합니다.

$$f(r; \pi_{\text{ref}}, \beta)(x, y) = r(x, y) - \beta \log \sum_y \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)$$

이 연산자 $f$는 보상 함수를 $\pi_r$의 분배 함수의 로그로 정규화합니다. 추가된 정규화 항은 오직 프롬프트 $x$의 함수이므로, $f(r; \pi_{\text{ref}}, \beta)(x, y)$는 $r(x, y)$의 동치 클래스에 속하는 보상 함수입니다. 최종적으로 $r$을 임의의 보상 함수에 대해 성립하는 관계식으로 대체하면, 다음을 얻습니다.

$$f(r; \pi_{\text{ref}}, \beta)(x, y) = \beta \log \frac{\pi_r(y \mid x)}{\pi_{\text{ref}}(y \mid x)}$$

즉, 사영 $f$는 $r$의 동치 클래스의 구성원을 원하는 형태로 생성하며, 제안된 재매개변수화로부터 보상 모델의 일반성을 잃지 않습니다. ∎

Theorem 1은 DPO 재매개변수화가 각 동치 클래스 내에서 정확히 어떤 보상 함수를 선택하는지를 명시하는 것으로도 해석할 수 있습니다. 이는 다음 조건을 만족하는 보상 함수입니다.

$$\sum_y \underbrace{\pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)}_{= \pi(y \mid x)\text{, Thm. 1 재매개변수화 사용}} = 1$$

즉, $\pi(y \mid x)$는 유효한 분포입니다(확률이 양수이고 합이 1입니다). 그러나 앞서 유도한 최적 정책의 형태를 따르면, 이 식은 보상 함수 $r(x, y)$에 의해 유도된 최적 정책의 분배 함수임을 알 수 있습니다.

DPO 알고리즘의 핵심 통찰은 과소 명세된 Plackett-Luce, 특히 Bradley-Terry 선호도 모델 계열에 특정 제약을 부과하여, 표현 가능한 보상 모델의 클래스는 보존하면서도 모든 프롬프트 $x$에 대해 최적 정책을 분석적으로 다루기 쉽게 만든다는 것입니다. 이러한 이론적 프레임워크는 DPO가 명시적 보상 모델링 없이도 전체 보상 공간을 탐색할 수 있음을 보장합니다.

### 액터-크리틱 알고리즘의 불안정성

이론적 프레임워크를 사용하여 PPO와 같은 RLHF에 사용되는 표준 액터-크리틱 알고리즘의 불안정성을 진단할 수 있습니다. RLHF 파이프라인을 따라 강화학습 파인튜닝 단계에 초점을 맞춥니다.

제약된 강화학습 문제에 대한 제어-추론 프레임워크(control as inference framework)와의 연결을 도출할 수 있습니다. 파라미터화된 모델 $\pi_\theta(y \mid x)$를 가정하고, 보상 함수 $r_\phi(y, x)$에 의해 유도된 최적 정책 $\pi^*$와 $\pi_\theta(y \mid x)$ 사이의 KL 발산 $\mathbb{D}_{\text{KL}}[\pi_\theta(y|x) \mid\mid \pi^*(y \mid x)]$를 최소화합니다. 대수적 조작을 통해 다음의 최적화 목적 함수를 얻습니다.

$$\max_{\pi_\theta} \mathbb{E}_{\pi_\theta(y \mid x)}\bigg[\underbrace{r_\phi(x, y) - \beta \log \sum_y \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r_\phi(x, y)\right)}_{f(r_\phi, \pi_{\text{ref}}, \beta)} - \underbrace{\beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\text{ref}}(y \mid x)}}_{\text{KL}}\bigg]$$

이는 DPO와 동등한 보상을 $r_\phi$의 보상 클래스에 대해 사용하여 이전 연구들에서 최적화된 것과 동일한 목적 함수입니다. 이 설정에서 $f(r_\phi, \pi_{\text{ref}}, \beta)$의 정규화 항은 참조 정책 $\pi_{\text{ref}}$의 소프트 가치 함수(soft value function)로 해석될 수 있습니다.

이 항이 최적해에 영향을 주지는 않지만, 이것이 없으면 목적 함수의 정책 경사가 높은 분산을 가질 수 있어 학습이 불안정해집니다. 학습된 가치 함수를 사용하여 정규화 항을 고려할 수 있지만, 이 역시 최적화하기 어려울 수 있습니다. 또는 이전 연구들에서는 인간 완성 베이스라인을 사용하여 보상을 정규화했는데, 이는 본질적으로 정규화 항의 단일 샘플 몬테카를로 추정치입니다.

대조적으로 DPO 재매개변수화는 어떠한 베이스라인도 요구하지 않는 보상 함수를 생성합니다. 정규화 항이 제거되었기 때문에, 분산을 줄이기 위한 추가적인 가치 함수 학습이나 몬테카를로 추정이 필요하지 않습니다. 이는 DPO가 단순한 분류 손실만으로도 안정적인 최적화를 달성할 수 있는 이론적 근거를 제공합니다.
## 실험

이 섹션에서는 DPO가 선호도로부터 정책을 직접 훈련시키는 능력을 실증적으로 평가합니다. 먼저 잘 통제된 텍스트 생성 환경에서 DPO가 PPO와 같은 일반적인 선호도 학습 알고리즘과 비교하여 보상 최대화와 참조 정책과의 KL 발산 최소화를 얼마나 효율적으로 균형 잡는지 분석합니다. 다음으로 DPO의 성능을 더 큰 모델과 요약 및 대화와 같은 더 어려운 RLHF 작업에서 평가합니다. 실험 결과 DPO는 하이퍼파라미터 튜닝이 거의 없이도 PPO를 사용한 RLHF와 같은 강력한 베이스라인만큼 또는 그보다 더 나은 성능을 보이며, 학습된 보상 함수 하에서 $N$개의 샘플링된 궤적 중 최선을 반환하는 방법과도 비슷한 성능을 달성합니다.

### 실험 설정

실험은 세 가지 서로 다른 개방형 텍스트 생성 작업을 탐구합니다. 모든 실험에서 알고리즘들은 선호도 데이터셋 $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$으로부터 정책을 학습합니다.

**통제된 감정 생성(Controlled Sentiment Generation)** 작업에서 $x$는 IMDb 데이터셋의 영화 리뷰 접두사이며, 정책은 긍정적인 감정을 가진 $y$를 생성해야 합니다. 통제된 평가를 수행하기 위해 이 실험에서는 사전학습된 감정 분류기를 사용하여 생성된 텍스트에 대한 선호도 쌍을 생성하는데, $p(\text{positive} \mid x, y_w) > p(\text{positive} \mid x, y_l)$을 만족하도록 합니다. SFT를 위해서는 IMDB 데이터셋의 훈련 분할에 있는 리뷰로 GPT-2-large를 수렴할 때까지 파인튜닝합니다.

**요약(Summarization)** 작업에서 $x$는 Reddit의 포럼 게시물이며, 정책은 게시물의 주요 요점을 요약한 $y$를 생성해야 합니다. 선행 연구를 따라 Reddit TL;DR 요약 데이터셋과 [Stiennon et al.](https://arxiv.org/pdf/2009.01325v3)이 수집한 인간 선호도를 사용합니다. TRLX 프레임워크로 RLHF를 위해 인간이 작성한 포럼 게시물 요약으로 파인튜닝된 SFT 모델을 사용합니다. 인간 선호도 데이터셋은 [Stiennon et al.](https://arxiv.org/pdf/2009.01325v3)이 다르지만 유사하게 훈련된 SFT 모델의 샘플에 대해 수집했습니다.

**단일 턴 대화(Single-turn Dialogue)** 작업에서 $x$는 천체물리학에 대한 질문부터 연애 조언 요청까지 다양한 인간의 질의이며, 정책은 사용자의 질의에 대해 매력적이고 유용한 응답 $y$를 생성해야 합니다. Anthropic Helpful and Harmless 대화 데이터셋을 사용하는데, 이는 인간과 자동화된 어시스턴트 간의 170,000개의 대화를 포함합니다. 각 대화 기록은 대규모(하지만 알려지지 않은) 언어 모델이 생성한 한 쌍의 응답과 인간이 선호하는 응답을 나타내는 선호도 레이블로 끝납니다. 이 설정에서는 사전학습된 SFT 모델을 사용할 수 없으므로, 선호되는 완성만으로 기성 언어 모델을 파인튜닝하여 SFT 모델을 형성합니다.

### 평가 방법

실험은 두 가지 서로 다른 평가 접근법을 사용합니다. 각 알고리즘이 제약된 보상 최대화 목적 함수를 최적화하는 효과를 분석하기 위해, 통제된 감정 생성 설정에서는 각 알고리즘이 달성한 보상과 참조 정책으로부터의 KL 발산의 프론티어로 평가합니다. 이 프론티어는 진짜 보상 함수(감정 분류기)에 접근할 수 있기 때문에 계산 가능합니다.

그러나 실제 세계에서는 진짜 보상 함수를 알 수 없으므로, 요약 및 단일 턴 대화 설정에서는 베이스라인 정책에 대한 승률로 알고리즘을 평가하며, 요약 품질과 응답 유용성에 대한 인간 평가의 대리 지표로 GPT-4를 사용합니다. 요약의 경우 테스트 세트의 참조 요약을 베이스라인으로 사용하고, 대화의 경우 테스트 데이터셋의 선호되는 응답을 베이스라인으로 사용합니다. 기존 연구들은 언어 모델이 기존 메트릭보다 더 나은 자동 평가자가 될 수 있음을 시사하지만, GPT-4를 평가에 사용하는 것을 정당화하기 위해 인간 연구를 수행합니다.

### 비교 방법

DPO 외에도 인간 선호도를 준수하도록 언어 모델을 훈련시키는 여러 기존 접근법을 평가합니다. 가장 단순하게는 요약 작업에서 GPT-J를 사용한 제로샷 프롬프팅과 대화 작업에서 Pythia-2.8B를 사용한 2샷 프롬프팅을 탐구합니다. 또한 SFT 모델과 **Preferred-FT**를 평가하는데, Preferred-FT는 SFT 모델(통제된 감정 및 요약) 또는 일반 언어 모델(단일 턴 대화)로부터 선택된 완성 $y_w$에 대해 지도 학습으로 파인튜닝된 모델입니다.

또 다른 유사 지도 방법은 **Unlikelihood**로, 정책이 $y_w$에 할당하는 확률을 최대화하고 $y_l$에 할당하는 확률을 최소화하도록 최적화합니다. 'unlikelihood' 항에 대한 선택적 계수 $\alpha \in [0, 1]$을 사용합니다. 선호도 데이터로부터 학습된 보상 함수를 사용하는 **PPO**와 통제된 감정 설정에서 사용 가능한 진짜 보상 함수로부터 학습하는 오라클인 **PPO-GT**도 고려합니다. 감정 실험에서는 PPO-GT의 두 가지 구현을 사용하는데, 하나는 기성 버전이고 다른 하나는 보상을 정규화하고 하이퍼파라미터를 추가로 튜닝하여 성능을 개선한 수정 버전입니다(학습된 보상으로 '일반' PPO를 실행할 때도 이러한 수정 사항을 사용합니다).

마지막으로 **Best of $N$** 베이스라인을 고려하는데, 이는 SFT 모델(대화에서는 Preferred-FT)로부터 $N$개의 응답을 샘플링하고 선호도 데이터셋으로부터 학습된 보상 함수에 따라 가장 높은 점수를 받은 응답을 반환합니다. 이 고성능 방법은 보상 모델의 품질을 PPO 최적화로부터 분리하지만, 테스트 시 모든 질의에 대해 $N$개의 완성을 샘플링해야 하므로 중간 정도의 $N$에 대해서도 계산적으로 비실용적입니다.

### DPO의 RLHF 목적 함수 최적화 능력

![보상-KL 프론티어와 요약 승률](https://arxiv.org/html/2305.18290/x1.png)

![대화 승률](https://arxiv.org/html/2305.18290/x2.png)

위 그림들은 DPO와 다른 방법들의 성능을 비교합니다. 왼쪽 그림은 다양한 알고리즘의 기대 보상 대 참조 정책과의 KL 발산 프론티어를 보여줍니다. DPO는 모든 KL 값에 대해 가장 높은 기대 보상을 제공하여 최적화의 품질을 입증합니다. 오른쪽 그림은 GPT-4를 평가자로 사용한 TL;DR 요약 작업에서 인간이 작성한 요약에 대한 승률을 샘플링 온도별로 보여줍니다. DPO는 PPO의 최상의 성능을 초과하면서도 샘플링 온도 변화에 더 강건합니다.

일반적인 RLHF 알고리즘에서 사용되는 KL 제약이 있는 보상 최대화 목적 함수는 보상의 활용과 참조 정책으로부터 크게 벗어나지 않도록 제한하는 것 사이의 균형을 맞춥니다. 따라서 알고리즘을 비교할 때는 달성된 보상과 KL 불일치를 모두 고려해야 합니다. 약간 더 높은 보상을 달성하지만 훨씬 더 높은 KL을 가지는 것은 반드시 바람직하지 않습니다.

감정 설정에서 다양한 알고리즘의 보상-KL 프론티어를 분석하기 위해 각 알고리즘에 대해 정책 보수성을 위한 다른 하이퍼파라미터를 사용하여 여러 훈련 실행을 수행합니다. PPO의 경우 목표 KL $\in \{3, 6, 9, 12\}$, DPO의 경우 $\beta \in \{0.05, 0.1, 1, 5\}$, unlikelihood의 경우 $\alpha \in \{0.05, 0.1, 0.5, 1\}$, preferred-FT의 경우 무작위 시드를 사용합니다. 이 스윕은 총 22회의 실행을 포함합니다.

수렴할 때까지 매 100 훈련 스텝마다 각 정책을 테스트 프롬프트 세트에서 평가하여 진짜 보상 함수 하에서의 평균 보상과 참조 정책 $\pi_{\text{ref}}$와의 평균 시퀀스 수준 KL 발산 $\text{KL}(\pi \mid\mid \pi_{\text{ref}})$를 계산합니다. 여기서 시퀀스 수준 KL은 타임스텝별 KL 발산의 합입니다.

실험 결과 DPO는 가장 효율적인 프론티어를 생성하여 낮은 KL을 유지하면서 가장 높은 보상을 달성합니다. 이 결과는 여러 이유로 특히 주목할 만합니다. 첫째, DPO와 PPO는 동일한 목적 함수를 최적화하지만 DPO가 훨씬 더 효율적입니다. DPO의 보상/KL 트레이드오프는 PPO를 엄격하게 지배합니다. 둘째, DPO는 PPO가 진짜 보상에 접근할 수 있는 경우(PPO-GT)에도 PPO보다 더 나은 프론티어를 달성합니다. 이는 DPO의 암묵적 보상 모델링과 직접 정책 최적화 접근법이 명시적 보상 모델과 강화학습을 사용하는 방법보다 더 효과적임을 보여줍니다.

### 실제 선호도 데이터셋으로의 확장

다음으로 요약 및 단일 턴 대화에서 DPO의 파인튜닝 성능을 평가합니다. 요약의 경우 ROUGE와 같은 자동 평가 메트릭은 인간 선호도와 상관관계가 낮을 수 있으며, 선행 연구는 인간 선호도에 대해 PPO를 사용하여 언어 모델을 파인튜닝하는 것이 더 효과적인 요약을 제공한다는 것을 발견했습니다.

TL;DR 요약 데이터셋의 테스트 분할에서 완성을 샘플링하고 테스트 세트의 참조 완성에 대한 평균 승률을 계산하여 다양한 방법을 평가합니다. 모든 방법의 완성은 0.0에서 1.0까지 다양한 온도에서 샘플링되며, 승률은 앞서 제시한 그림에 나타나 있습니다. DPO, PPO, Preferred-FT는 모두 동일한 GPT-J SFT 모델을 파인튜닝합니다.

실험 결과 DPO는 온도 0.0에서 약 61%의 승률을 달성하여 최적 샘플링 온도 0.0에서 57%의 PPO 성능을 초과합니다. DPO는 또한 Best of $N$ 베이스라인과 비교하여 더 높은 최대 승률을 달성합니다. DPO의 $\beta$ 하이퍼파라미터를 의미 있게 튜닝하지 않았으므로 이러한 결과는 DPO의 잠재력을 과소평가할 수 있습니다.

더욱이 DPO는 PPO보다 샘플링 온도에 훨씬 더 강건한 것으로 나타났습니다. PPO의 성능은 높은 온도에서 기본 GPT-J 모델 수준으로 저하될 수 있는 반면, DPO는 다양한 온도에서 일관된 성능을 유지합니다. Preferred-FT는 SFT 모델에 비해 크게 개선되지 않습니다.

단일 턴 대화에서는 Anthropic HH 데이터셋의 테스트 분할 중 한 단계의 인간-어시스턴트 상호작용이 있는 부분 집합에서 다양한 방법을 평가합니다. GPT-4 평가는 테스트의 선호되는 완성을 참조로 사용하여 다양한 방법의 승률을 계산합니다.

이 작업에 대한 표준 SFT 모델이 없으므로 사전학습된 Pythia-2.8B로 시작하여 Preferred-FT를 사용하여 선택된 완성에 대해 참조 모델을 훈련시켜 완성이 모델의 분포 내에 있도록 한 다음 DPO를 사용하여 훈련합니다. 또한 128개의 Preferred-FT 완성 중 최선(Best of 128)과 Pythia-2.8B 기본 모델의 2샷 프롬프트 버전과 비교하여, DPO가 각 방법의 최상 성능 온도에서 비슷하거나 더 나은 성능을 보임을 발견합니다.

![대화 작업 승률](https://arxiv.org/html/2305.18290/x3.png)

![훈련 과정의 승률 변화](https://arxiv.org/html/2305.18290/x4.png)

위 그림들은 Anthropic-HH 단일 턴 대화에 대한 GPT-4가 계산한 승률을 보여줍니다. 왼쪽 그림에서 DPO는 Anthropic-HH 테스트 세트의 선택된 요약을 개선하는 유일한 방법입니다. 오른쪽 그림은 훈련 과정에서 다양한 샘플링 온도에 대한 승률을 보여주며, DPO의 개선이 다양한 샘플링 온도에 대해 훈련 과정 전반에 걸쳐 상당히 안정적임을 보여줍니다.

Anthropic HH 데이터셋에서 PPO로 훈련된 RLHF 모델도 평가했지만, 기본 Pythia-2.8B 모델보다 나은 성능을 제공하는 프롬프트나 샘플링 온도를 찾을 수 없었습니다. TL;DR의 결과와 두 방법이 동일한 보상 함수를 최적화한다는 사실을 바탕으로 Best of 128을 PPO 수준 성능의 대략적인 대리 지표로 간주합니다.

전반적으로 DPO는 Anthropic HH 데이터셋의 선호되는 완성을 개선하는 유일한 계산적으로 효율적인 방법이며, 계산적으로 까다로운 Best of 128 베이스라인과 유사하거나 더 나은 성능을 제공합니다. 앞서 제시한 그림은 DPO가 최상의 성능에 비교적 빠르게 수렴함을 보여줍니다.

### 새로운 입력 분포로의 일반화

분포 이동 하에서 PPO와 DPO의 성능을 추가로 비교하기 위해, Reddit TL;DR 요약 실험의 PPO 및 DPO 정책을 다른 분포인 CNN/DailyMail 데이터셋의 테스트 분할에 있는 뉴스 기사에서 평가하며, TL;DR에서 최상의 샘플링 온도(0과 0.25)를 사용합니다.

| 알고리즘 | 온도 0 | 온도 0.25 |
|---------|--------|-----------|
| DPO | 0.36 | 0.31 |
| PPO | 0.26 | 0.23 |

위 표는 분포 외 CNN/DailyMail 입력 기사에 대한 진짜 요약과의 GPT-4 승률을 보여줍니다. 데이터셋의 진짜 요약에 대한 GPT-4 승률을 계산했으며, Reddit TL;DR에 사용한 것과 동일한 GPT-4 프롬프트를 사용하되 "포럼 게시물"이라는 단어를 "뉴스 기사"로 대체했습니다.

이 새로운 분포에서 DPO는 상당한 차이로 PPO 정책을 계속 능가합니다. 이 실험은 DPO 정책이 PPO가 사용하는 추가적인 레이블이 없는 Reddit TL;DR 프롬프트를 사용하지 않음에도 불구하고 PPO 정책과 유사하게 잘 일반화할 수 있다는 초기 증거를 제공합니다.

### GPT-4 판단의 인간 판단을 통한 검증

GPT-4 판단의 신뢰성을 검증하기 위해 TL;DR 요약 실험의 결과와 두 가지 다른 GPT-4 프롬프트를 사용하여 인간 연구를 수행합니다. GPT-4 (S)(simple) 프롬프트는 단순히 어느 요약이 게시물의 중요한 정보를 더 잘 요약하는지 묻습니다. GPT-4 (C)(concise) 프롬프트는 어느 요약이 더 간결한지도 묻는데, GPT-4 (S) 프롬프트로는 GPT-4가 인간보다 더 길고 반복적인 요약을 선호한다는 것을 발견했기 때문입니다.

세 가지 비교를 수행하는데, 가장 높은 성능(DPO, 온도 0.25), 가장 낮은 성능(PPO, 온도 1.0), 그리고 중간 성능(SFT, 온도 0.25) 방법을 사용하여 다양한 샘플 품질을 다루는 것을 목표로 합니다. 세 방법 모두 탐욕적으로 샘플링된 PPO(최상 성능 온도)와 비교됩니다.

| 비교 | DPO vs PPO-1 | SFT vs PPO-1 | PPO-1 vs PPO-N |
|------|--------------|--------------|----------------|
| 응답자 수 | 27 | 21 | 22 |
| GPT-4 (S) 승률 % | 47 | 27 | 13 |
| GPT-4 (C) 승률 % | 54 | 32 | 12 |
| 인간 승률 % | 58 | 43 | 17 |
| GPT-4 (S)-인간 일치 % | 70 | 77 | 86 |
| GPT-4 (C)-인간 일치 % | 67 | 79 | 85 |
| 인간-인간 일치 % | 65 | - | 87 |

위 표는 TL;DR 요약 샘플에 대한 인간과 GPT-4 승률 및 판단별 일치도를 비교합니다. 인간은 서로 일치하는 만큼 GPT-4와 일치합니다. 각 실험은 명시된 방법의 요약을 온도 0의 PPO 요약과 비교합니다.

두 프롬프트 모두에서 GPT-4는 인간이 서로 일치하는 만큼 인간과 일치하는 경향이 있어 GPT-4가 인간 평가의 합리적인 대리 지표임을 시사합니다. 제한된 인간 평가자로 인해 DPO와 PPO-1 비교에 대해서만 여러 인간 판단을 수집합니다. 전반적으로 GPT-4 (C) 프롬프트는 일반적으로 인간을 더 잘 대표하는 승률을 제공하므로 주요 결과에서 이 프롬프트를 사용합니다.

이러한 실험 결과들은 DPO가 명시적인 보상 모델링이나 강화학습 없이도 인간 선호도를 효과적으로 학습하고 최적화할 수 있음을 입증합니다. DPO는 통제된 환경과 실제 응용 모두에서 기존 방법들과 비슷하거나 더 나은 성능을 달성하면서도 구현이 훨씬 간단하고 계산 비용이 낮습니다.
## 논의

선호도로부터 학습하는 것은 유능하고 정렬된 언어 모델을 훈련시키기 위한 강력하고 확장 가능한 프레임워크입니다. 이 논문에서는 강화학습 없이 선호도로부터 언어 모델을 훈련시키는 단순한 훈련 패러다임인 DPO를 소개했습니다. 선호도 학습 문제를 표준 강화학습 설정으로 강제하여 기성 강화학습 알고리즘을 사용하는 대신, DPO는 언어 모델 정책과 보상 함수 간의 매핑을 식별하여 강화학습이나 일반성의 손실 없이 단순한 교차 엔트로피 손실로 인간 선호도를 직접 만족하도록 언어 모델을 훈련시킬 수 있게 합니다.

DPO는 하이퍼파라미터 튜닝이 거의 없이도 PPO를 기반으로 하는 것들을 포함한 기존 RLHF 알고리즘과 유사하거나 더 나은 성능을 보여줍니다. 따라서 DPO는 인간 선호도로부터 더 많은 언어 모델을 훈련시키는 데 있어 장벽을 의미 있게 낮춥니다. 이러한 결과는 DPO가 실용적인 관점에서 기존 방법들에 비해 상당한 이점을 제공함을 보여줍니다. 명시적인 보상 모델을 학습시키고 강화학습 훈련 루프를 유지해야 하는 복잡성을 제거하면서도, 동일한 이론적 목적 함수를 최적화할 수 있다는 점은 선호도 기반 언어 모델 훈련의 접근성을 크게 향상시킵니다.

### 한계점과 향후 연구 방향

이 논문의 결과는 향후 연구를 위한 몇 가지 중요한 질문을 제기합니다. 첫 번째로, 명시적 보상 함수로부터 학습하는 것과 비교하여 DPO 정책이 분포 외(out of distribution) 상황에서 어떻게 일반화하는지에 대한 질문입니다. 초기 결과는 DPO 정책이 PPO 기반 모델과 유사하게 일반화할 수 있음을 시사하지만, 더 포괄적인 연구가 필요합니다. 예를 들어, DPO 정책으로부터의 자기 레이블링(self-labeling)을 통한 훈련이 레이블이 없는 프롬프트를 효과적으로 활용할 수 있는지에 대한 탐구가 필요합니다. 이는 [Constitutional AI 연구](https://arxiv.org/pdf/2212.08073v1)에서 제안된 자기 비평 메커니즘과 유사한 접근법으로, 모델이 자신의 출력을 평가하고 개선하는 능력을 활용하는 것입니다.

두 번째로, 보상 과최적화(reward over-optimization)가 직접 선호도 최적화 설정에서 어떻게 나타나는지, 그리고 앞서 제시한 실험 결과 그림의 오른쪽에서 관찰된 성능의 약간의 감소가 이것의 사례인지에 대한 질문입니다. 보상 과최적화는 강화학습 설정에서 잘 알려진 현상으로, 모델이 보상 함수의 허점을 악용하여 실제로는 바람직하지 않은 행동을 하면서도 높은 보상을 받는 상황을 의미합니다. DPO가 암묵적 보상 모델을 사용하기 때문에 이러한 현상이 어떻게 나타나고 완화될 수 있는지에 대한 이해가 필요합니다.

세 번째로, 이 논문에서는 최대 6B 파라미터의 모델을 평가했지만, DPO를 훨씬 더 큰 최신 모델로 확장하는 것은 흥미로운 향후 연구 방향입니다. 현대의 대규모 언어 모델들은 수백억에서 수조 개의 파라미터를 가지고 있으며, 이러한 규모에서 DPO의 효율성과 효과성을 검증하는 것은 실용적으로 매우 중요합니다. 특히 DPO의 단순성과 계산 효율성이 대규모 모델에서도 유지되는지, 그리고 확장 과정에서 새로운 도전 과제가 발생하는지를 이해하는 것이 필요합니다.

평가와 관련하여, GPT-4가 계산한 승률이 프롬프트에 의해 영향을 받는다는 것을 발견했습니다. 향후 연구는 자동화된 시스템으로부터 고품질 판단을 이끌어내는 최선의 방법을 연구할 수 있습니다. 이 논문의 인간 연구 결과는 GPT-4가 인간 평가의 합리적인 대리 지표가 될 수 있음을 보여주었지만, 프롬프트 설계가 평가 결과에 미치는 영향을 더 체계적으로 이해하는 것이 중요합니다. 특히 요약의 간결성과 정보성 사이의 균형, 대화의 유용성과 안전성 사이의 균형과 같은 다차원적 품질 평가를 위한 프롬프트 설계 방법론이 필요합니다.

마지막으로, 인간 선호도로부터 언어 모델을 훈련시키는 것을 넘어서 DPO의 많은 가능한 응용이 존재합니다. 다른 모달리티의 생성 모델을 훈련시키는 것을 포함하여 다양한 영역으로의 확장이 가능합니다. 예를 들어, 이미지 생성 모델, 음악 생성 모델, 비디오 생성 모델 등에서도 인간 선호도를 직접 최적화하는 DPO의 접근법을 적용할 수 있습니다. 이러한 모달리티들에서도 명시적 보상 모델링과 강화학습의 복잡성을 피하면서 선호도를 효과적으로 학습할 수 있다면, DPO는 생성 AI 전반에 걸쳐 광범위한 영향을 미칠 수 있습니다.

또한 DPO의 이론적 프레임워크는 다른 선호도 모델링 접근법으로 확장될 수 있습니다. 이 논문에서는 주로 Bradley-Terry 모델을 사용했지만, 더 복잡한 선호도 구조를 모델링하는 방법들, 예를 들어 다중 선택지에 대한 순위나 조건부 선호도 등을 다루는 확장도 가능합니다. 이러한 확장은 더 풍부한 인간 피드백을 활용하여 모델의 정렬 품질을 향상시킬 수 있습니다.

DPO의 성공은 선호도 학습 분야에서 중요한 패러다임 전환을 나타냅니다. 강화학습이 필수적이라는 기존의 가정에 도전하여, 더 단순하고 직접적인 접근법이 동등하거나 더 나은 결과를 달성할 수 있음을 보여주었습니다. 이는 향후 연구자들이 복잡한 문제를 해결할 때 더 단순한 해결책을 먼저 탐구하도록 장려하며, 때로는 문제를 다시 정식화하는 것이 더 효과적인 알고리즘으로 이어질 수 있음을 시사합니다.

결론적으로, DPO는 인간 선호도로부터 언어 모델을 훈련시키는 실용적이고 효과적인 방법을 제공하며, 이 분야의 접근성을 크게 향상시킵니다. 향후 연구를 통해 DPO의 한계를 더 잘 이해하고, 더 큰 규모로 확장하며, 다양한 응용 분야로 확장함으로써, 선호도 기반 학습이 AI 정렬의 표준 접근법이 될 수 있는 기반을 마련할 것입니다.
## 참고문헌

이 논문은 인간 피드백 기반 강화학습, 선호도 모델링, 대규모 언어 모델 훈련, 평가 방법론 등 다양한 연구 분야의 51개 참고문헌을 인용하고 있습니다. 주요 인용 논문들은 DPO 방법론의 이론적 기반과 실증적 검증을 위한 비교 기준을 제공합니다.

핵심적으로 [Ouyang et al.](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)의 InstructGPT 연구는 표준적인 RLHF 파이프라인을 확립했으며 DPO가 개선하고자 하는 기존 방법론의 기반을 제공합니다. [Bradley and Terry](https://doi.org/10.2307/2334029)와 [Plackett](https://doi.org/10.2307/2346567)의 연구는 선호도 모델링의 이론적 토대인 Bradley-Terry 모델과 Plackett-Luce 모델을 제시했습니다. [Schulman et al.](https://arxiv.org/pdf/1707.06347v2)의 PPO 알고리즘은 DPO가 비교하는 주요 베이스라인 방법입니다.

언어 모델 아키텍처와 관련하여 [Brown et al.](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)의 GPT-3 연구와 [Touvron et al.](https://arxiv.org/pdf/2302.13971)의 LLaMA 모델은 실험에 사용된 기본 모델들의 배경을 제공합니다. 평가 방법론과 관련해서는 [Stiennon et al.](https://arxiv.org/pdf/2009.01325v3)의 요약 학습 연구와 [Bai et al.](https://arxiv.org/pdf/2212.08073v1)의 Constitutional AI 연구가 인간 선호도 데이터셋과 평가 프로토콜을 제공했습니다.

## 수학적 유도

이 부록 섹션들은 본문에서 제시된 DPO 방법론의 수학적 기반을 상세히 증명하고 확장합니다.

### KL 제약이 있는 보상 최대화 목적 함수의 최적해 유도

앞서 본문에서 제시한 최적 정책의 닫힌 형태 해를 완전히 유도합니다. 임의의 보상 함수 $r(x, y)$, 참조 모델 $\pi_{\text{ref}}$, 그리고 일반적인 비파라메트릭 정책 클래스 하에서 다음 목적 함수를 최적화합니다.

$$\max_{\pi} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi}[r(x, y)] - \beta \mathbb{D}_{\text{KL}}[\pi(y \mid x) \mid\mid \pi_{\text{ref}}(y \mid x)]$$

이 목적 함수는 먼저 KL 발산을 로그 확률 비율의 기댓값으로 확장하여 다음과 같이 재작성됩니다.

$$\max_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)} \left[r(x, y) - \beta \log \frac{\pi(y \mid x)}{\pi_{\text{ref}}(y \mid x)}\right]$$

최대화 문제를 최소화 문제로 변환하고, 분배 함수 $Z(x) = \sum_y \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)$를 도입하면:

$$\min_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{y \sim \pi(y \mid x)} \left[\log \frac{\pi(y \mid x)}{\frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)} - \log Z(x)\right]$$

여기서 핵심 통찰은 $Z(x)$가 $x$만의 함수이고 정책 $\pi$에 의존하지 않는다는 점입니다. 최적 정책 $\pi^*(y \mid x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)$를 정의하면, 이는 유효한 확률 분포가 됩니다. 왜냐하면 모든 $y$에 대해 $\pi^*(y \mid x) \geq 0$이고 $\sum_y \pi^*(y \mid x) = 1$이기 때문입니다.

목적 함수를 KL 발산으로 재구성하면:

$$\min_{\pi} \mathbb{E}_{x \sim \mathcal{D}} \left[\mathbb{D}_{\text{KL}}(\pi(y \mid x) \mid\mid \pi^*(y \mid x)) - \log Z(x)\right]$$

Gibbs의 부등식에 따르면 KL 발산은 두 분포가 동일할 때에만 0에서 최소값을 가지므로, 최적해는 $\pi(y \mid x) = \pi^*(y \mid x)$입니다. 이는 본문에서 제시한 정책의 최적 형태를 완전히 정당화합니다.

### Bradley-Terry 모델 하에서의 DPO 목적 함수 유도

Bradley-Terry 선호도 모델 하에서 DPO 목적 함수를 명시적으로 유도합니다. Bradley-Terry 모델은 다음과 같이 선호도 확률을 정의합니다.

$$p^*(y_1 \succ y_2 \mid x) = \frac{\exp(r^*(x, y_1))}{\exp(r^*(x, y_1)) + \exp(r^*(x, y_2))}$$

앞서 유도한 보상 함수의 재매개변수화를 대입하면:

$$r^*(x, y) = \beta \log \frac{\pi^*(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + \beta \log Z(x)$$

이를 Bradley-Terry 모델에 대입하면 분배 함수 $Z(x)$가 소거됩니다.

$$p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)}\right)}{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)}\right) + \exp\left(\beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)}\right)}$$

이는 다음과 같이 단순화됩니다.

$$p^*(y_1 \succ y_2 \mid x) = \sigma\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid x)} - \beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid x)}\right)$$

여기서 $\sigma$는 로지스틱 함수입니다. 이 형태는 계산하기 어려운 분배 함수 없이 정책만의 관점에서 선호도를 표현할 수 있게 하며, 본문의 DPO 손실 함수의 이론적 기반을 제공합니다.

### Plackett-Luce 모델로의 확장

DPO 유도를 더 일반적인 Plackett-Luce 순위 모델로 확장합니다. $K$개의 답변 $y_1, \ldots, y_K$와 순위 $\tau: [K] \to [K]$가 주어졌을 때, Plackett-Luce 모델은 다음과 같이 정의됩니다.

$$p^*(\tau \mid y_1, \ldots, y_K, x) = \prod_{k=1}^K \frac{\exp(r^*(x, y_{\tau(k)}))}{\sum_{j=k}^K \exp(r^*(x, y_{\tau(j)}))}$$

$K=2$일 때 이는 Bradley-Terry 모델로 축소됩니다. 동일한 보상 재매개변수화를 적용하면 분배 함수가 소거되고 다음을 얻습니다.

$$p^*(\tau \mid y_1, \ldots, y_K, x) = \prod_{k=1}^K \frac{\exp\left(\beta \log \frac{\pi^*(y_{\tau(k)} \mid x)}{\pi_{\text{ref}}(y_{\tau(k)} \mid x)}\right)}{\sum_{j=k}^K \exp\left(\beta \log \frac{\pi^*(y_{\tau(j)} \mid x)}{\pi_{\text{ref}}(y_{\tau(j)} \mid x)}\right)}$$

순위가 매겨진 선호도 데이터셋 $\mathcal{D} = \{\tau^{(i)}, y_1^{(i)}, \ldots, y_K^{(i)}, x^{(i)}\}_{i=1}^N$이 주어지면, 파라미터화된 정책 $\pi_\theta$에 대한 최대 우도 목적 함수는:

$$\mathcal{L}_{\text{DPO}}(\pi_\theta, \pi_{\text{ref}}) = -\mathbb{E}_{\tau, y_1, \ldots, y_K, x \sim \mathcal{D}} \left[\log \prod_{k=1}^K \frac{\exp\left(\beta \log \frac{\pi_\theta(y_{\tau(k)} \mid x)}{\pi_{\text{ref}}(y_{\tau(k)} \mid x)}\right)}{\sum_{j=k}^K \exp\left(\beta \log \frac{\pi_\theta(y_{\tau(j)} \mid x)}{\pi_{\text{ref}}(y_{\tau(j)} \mid x)}\right)}\right]$$

이는 DPO를 쌍대 비교를 넘어 완전한 순위 정보로 확장할 수 있음을 보여줍니다.

### DPO 목적 함수의 그래디언트

DPO 손실 함수의 그래디언트를 명시적으로 계산합니다. 암묵적 보상을 $\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\text{ref}}(y \mid x)}$로 정의하고, $u = \hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w)$로 놓으면:

$$\nabla_\theta \mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[\frac{\sigma'(u)}{\sigma(u)} \nabla_\theta u\right]$$

로지스틱 함수의 성질 $\sigma'(x) = \sigma(x)(1 - \sigma(x))$와 $\sigma(-x) = 1 - \sigma(x)$를 사용하면:

$$\nabla_\theta \mathcal{L}_{\text{DPO}} = -\beta \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[\sigma(\hat{r}_\theta(x, y_w) - \hat{r}_\theta(x, y_l)) \left[\nabla_\theta \log \pi(y_w \mid x) - \nabla_\theta \log \pi(y_l \mid x)\right]\right]$$

이 그래디언트 형태는 중요한 통찰을 제공합니다. 선호되는 완성 $y_w$의 로그 우도를 증가시키고 선호되지 않는 완성 $y_l$의 로그 우도를 감소시키는데, 가중치는 암묵적 보상 모델이 얼마나 잘못 순서를 매겼는지에 비례합니다. $\sigma(\hat{r}_\theta(x, y_w) - \hat{r}_\theta(x, y_l))$ 항은 모델이 이미 올바른 선호도를 학습했을 때는 0에 가까워지고, 잘못된 순서를 예측할 때는 1에 가까워져 더 큰 업데이트를 유도합니다.

### 보조정리 1과 2의 증명

**보조정리 1**: Plackett-Luce 선호도 프레임워크 하에서, 동일한 동치 클래스에 속한 두 보상 함수는 동일한 선호도 분포를 유도합니다.

**증명**: 두 보상 함수 $r(x, y)$와 $r'(x, y)$가 어떤 함수 $f$에 대해 $r'(x, y) = r(x, y) + f(x)$를 만족한다고 가정합니다. 프롬프트 $x$, 답변 $y_1, \ldots, y_K$, 그리고 순위 $\tau$에 대해:

$$p_{r'}(\tau \mid y_1, \ldots, y_K, x) = \prod_{k=1}^K \frac{\exp(r'(x, y_{\tau(k)}))}{\sum_{j=k}^K \exp(r'(x, y_{\tau(j)}))} = \prod_{k=1}^K \frac{\exp(r(x, y_{\tau(k)}) + f(x))}{\sum_{j=k}^K \exp(r(x, y_{\tau(j)}) + f(x))}$$

$\exp(f(x))$를 분자와 분모에서 인수분해하면:

$$= \prod_{k=1}^K \frac{\exp(f(x)) \exp(r(x, y_{\tau(k)}))}{\exp(f(x)) \sum_{j=k}^K \exp(r(x, y_{\tau(j)}))} = \prod_{k=1}^K \frac{\exp(r(x, y_{\tau(k)}))}{\sum_{j=k}^K \exp(r(x, y_{\tau(j)}))} = p_r(\tau \mid y_1, \ldots, y_K, x)$$

따라서 $p_{r'} = p_r$이며 동일한 선호도 분포를 유도합니다.

**보조정리 2**: 동일한 동치 클래스에 속한 두 보상 함수는 제약된 강화학습 문제 하에서 동일한 최적 정책을 유도합니다.

**증명**: $r'(x, y) = r(x, y) + f(x)$인 두 보상 함수를 고려하고, 대응하는 최적 정책을 $\pi_r$과 $\pi_{r'}$로 표기합니다. 앞서 유도한 최적 정책 형태에 따르면:

$$\pi_{r'}(y \mid x) = \frac{1}{\sum_y \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r'(x, y)\right)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r'(x, y)\right)$$

$r'(x, y) = r(x, y) + f(x)$를 대입하면:

$$= \frac{1}{\sum_y \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta}(r(x, y) + f(x))\right)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta}(r(x, y) + f(x))\right)$$

$\exp\left(\frac{1}{\beta} f(x)\right)$를 인수분해하면:

$$= \frac{1}{\exp\left(\frac{1}{\beta} f(x)\right) \sum_y \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)} \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right) \exp\left(\frac{1}{\beta} f(x)\right) = \pi_r(y \mid x)$$

따라서 $\pi_{r'} = \pi_r$이며 동일한 최적 정책을 생성합니다.

### 정리 1의 증명

**정리 1**: 적절한 가정 하에서, 모든 보상 동치 클래스는 어떤 모델 $\pi(y \mid x)$에 대해 재매개변수화 $r(x, y) = \beta \log \frac{\pi(y \mid x)}{\pi_{\text{ref}}(y \mid x)}$로 표현될 수 있습니다.

**증명**: 임의의 보상 함수 $r(x, y)$를 고려하고, 이것이 대응하는 최적 모델 $\pi_r(y \mid x)$를 유도한다고 합시다. 사영 연산자를 다음과 같이 정의합니다.

$$f(r; \pi_{\text{ref}}, \beta)(x, y) = r(x, y) - \beta \log \sum_y \pi_{\text{ref}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)$$

이 연산자는 분배 함수의 로그로 보상을 정규화하며, 추가된 항이 $x$만의 함수이므로 $f(r; \pi_{\text{ref}}, \beta)(x, y)$는 $r(x, y)$의 동치 클래스에 속합니다. 앞서 유도한 관계식을 사용하면:

$$f(r; \pi_{\text{ref}}, \beta)(x, y) = \beta \log \frac{\pi_r(y \mid x)}{\pi_{\text{ref}}(y \mid x)}$$

즉, 사영 $f$는 $r$의 동치 클래스로부터 원하는 형태의 보상 함수를 생성하며, 제안된 재매개변수화로부터 보상 모델의 일반성을 잃지 않습니다.

**유일성**: 동일한 동치 클래스의 두 보상 함수 $r(x, y)$와 $r'(x, y) = r(x, y) + f(x)$가 모두 재매개변수화 형태를 가진다고 가정합니다. 즉, $r'(x, y) = \beta \log \frac{\pi'(y \mid x)}{\pi_{\text{ref}}(y \mid x)}$이고 $r(x, y) = \beta \log \frac{\pi(y \mid x)}{\pi_{\text{ref}}(y \mid x)}$입니다. 그러면:

$$\beta \log \frac{\pi'(y \mid x)}{\pi_{\text{ref}}(y \mid x)} = \beta \log \frac{\pi(y \mid x)}{\pi_{\text{ref}}(y \mid x)} + f(x) = \beta \log \frac{\pi(y \mid x) \exp\left(\frac{1}{\beta} f(x)\right)}{\pi_{\text{ref}}(y \mid x)}$$

따라서 $\pi'(y \mid x) = \pi(y \mid x) \exp\left(\frac{1}{\beta} f(x)\right)$입니다. 이들이 분포이므로 $y$에 대해 합하면 $\exp\left(\frac{1}{\beta} f(x)\right) = 1$이 되어야 하고, $\beta > 0$이므로 $f(x) = 0$입니다. 따라서 $r(x, y) = r'(x, y)$이며 재매개변수화는 각 동치 클래스 내에서 유일합니다.

## DPO 구현 세부사항

### DPO 손실 함수의 PyTorch 구현

DPO는 비교적 직관적으로 구현할 수 있습니다. 다음은 DPO 손실 함수의 PyTorch 코드입니다.

```python
import torch.nn.functional as F

def dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta):
    """
    pi_logps: 정책 로그 확률, 형태 (B,)
    ref_logps: 참조 모델 로그 확률, 형태 (B,)
    yw_idxs: [0, B-1] 범위의 선호되는 완성 인덱스, 형태 (T,)
    yl_idxs: [0, B-1] 범위의 선호되지 않는 완성 인덱스, 형태 (T,)
    beta: KL 페널티 강도를 제어하는 온도 파라미터
    
    각 (yw_idxs[i], yl_idxs[i]) 쌍은 단일 선호도 쌍의 인덱스를 나타냅니다.
    """
    pi_yw_logps, pi_yl_logps = pi_logps[yw_idxs], pi_logps[yl_idxs]
    ref_yw_logps, ref_yl_logps = ref_logps[yw_idxs], ref_logps[yl_idxs]
    
    pi_logratios = pi_yw_logps - pi_yl_logps
    ref_logratios = ref_yw_logps - ref_yl_logps
    
    losses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))
    rewards = beta * (pi_logps - ref_logps).detach()
    
    return losses, rewards
```

이 구현은 DPO의 핵심을 간결하게 표현합니다. 로그 확률 비율을 계산하여 정책과 참조 모델 간의 차이를 측정하고, 이를 로지스틱 함수에 통과시켜 손실을 계산합니다.

### 하이퍼파라미터 설정

달리 명시되지 않는 한, $\beta = 0.1$, 배치 크기 64, 학습률 1e-6의 RMSprop 옵티마이저를 기본값으로 사용합니다. 학습률은 0에서 1e-6까지 150 스텝에 걸쳐 선형적으로 워밍업합니다. TL;DR 요약 작업의 경우 $\beta = 0.5$를 사용하며, 나머지 파라미터는 동일하게 유지합니다.

이러한 하이퍼파라미터는 최소한의 튜닝으로도 안정적인 성능을 제공하도록 선택되었습니다. 특히 $\beta$ 값은 보상 최대화와 참조 정책으로부터의 KL 발산 제약 사이의 균형을 제어하며, 작업의 복잡도에 따라 조정될 수 있습니다.

## 실험 설정의 추가 세부사항

### IMDb 감정 실험 설정

프롬프트는 길이가 2-8 토큰인 IMDB 데이터셋의 접두사입니다. 사전학습된 감정 분류기 `siebert/sentiment-roberta-large-english`를 진짜 보상 모델로 사용하고 `gpt2-large`를 기본 모델로 사용합니다. 이러한 더 큰 모델들을 사용하는 이유는 기본 크기 모델들이 저품질 텍스트를 생성하고 보상이 다소 부정확하다는 것을 발견했기 때문입니다.

먼저 IMDB 데이터의 부분집합에서 1 에폭 동안 지도 파인튜닝을 수행합니다. 그런 다음 이 모델을 사용하여 25,000개의 접두사에 대해 각각 4개의 완성을 샘플링하고, 진짜 보상 모델을 사용하여 각 접두사마다 6개의 선호도 쌍을 생성합니다. RLHF 보상 모델은 `gpt2-large` 모델에서 초기화되며 선호도 데이터셋에서 3 에폭 동안 훈련되고, 검증 세트 정확도가 가장 높은 체크포인트를 선택합니다.

"TRL" 실행은 TRL 라이브러리의 하이퍼파라미터를 사용하며, 자체 구현은 PPO 스텝당 1024의 더 큰 배치 샘플을 사용합니다.

### GPT-4 평가 프롬프트

요약과 대화 실험에서 승률을 계산하기 위해 사용된 GPT-4 프롬프트를 제공합니다. 모든 실험에서 `gpt-4-0314`를 사용하며, 모든 평가에서 요약 또는 응답의 순서를 무작위로 선택합니다.

**요약을 위한 GPT-4 승률 프롬프트 (S - Simple)**:

```
Which of the following summaries does a better job of summarizing the most 
important points in the given forum post?

Post: <post>

Summary A: <Summary A>

Summary B: <Summary B>

FIRST provide a one-sentence comparison of the two summaries, explaining which 
you prefer and why. SECOND, on a new line, state only "A" or "B" to indicate your 
choice. Your response should use the format:
Comparison: <one-sentence comparison and explanation>
Preferred: <"A" or "B">
```

**요약을 위한 GPT-4 승률 프롬프트 (C - Concise)**:

```
Which of the following summaries does a better job of summarizing the most 
important points in the given forum post, without including unimportant or 
irrelevant details? A good summary is both precise and concise.

Post: <post>

Summary A: <Summary A>

Summary B: <Summary B>

FIRST provide a one-sentence comparison of the two summaries, explaining which 
you prefer and why. SECOND, on a new line, state only "A" or "B" to indicate your 
choice. Your response should use the format:
Comparison: <one-sentence comparison and explanation>
Preferred: <"A" or "B">
```

**대화를 위한 GPT-4 승률 프롬프트**:

```
For the following query to a chatbot, which response is more helpful?

Query: <the user query>

Response A: <either the test method or baseline>

Response B: <the other response>

FIRST provide a one-sentence comparison of the two responses and explain 
which you feel is more helpful. SECOND, on a new line, state only "A" or 
"B" to indicate which response is more helpful. Your response should use 
the format:
Comparison: <one-sentence comparison and explanation>
More helpful: <"A" or "B">
```

이러한 프롬프트는 GPT-4가 일관되고 명확한 판단을 내릴 수 있도록 구조화되어 있습니다. 특히 Concise 프롬프트는 GPT-4가 긴 요약을 선호하는 경향을 완화하기 위해 도입되었습니다.

### Unlikelihood 베이스라인 제외 이유

Unlikelihood 베이스라인은 감정 실험에 포함했지만, 요약이나 대화 실험에서는 베이스라인으로 포함하지 않았습니다. 그 이유는 일반적으로 의미 없는 응답을 생성하기 때문인데, 이는 제약되지 않은 우도 최소화의 결과라고 판단됩니다.

다음 표는 TL;DR 프롬프트에서 온도 1.0으로 샘플링된 Unlikelihood 샘플들을 보여줍니다.

| 프롬프트 | Unlikelihood 생성 응답 |
|---------|----------------------|
| SUBREDDIT: r/relationships<br>TITLE: The girl [26 F] I [22 M] have been seeing for a month didn't respond to me at all yesterday while hanging out with a friend...<br>POST: She gets terrible service while at her house, but I texted her 3 times yesterday, 4-5 hours apart. She didn't call me until early this morning and left a voicemail that she was busy all day with a friend... | TL;DR: girl when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when... |
| SUBREDDIT: r/tifu<br>TITLE: TIFU by accidently kicking an old woman<br>POST: So this didn't happen today but actually about a year or two ago. I was at my granddads funeral so of course it was all very sad and full of lots of crying old people... | TL;DR: when an old woman was tripping the when when when when when when when when when when when when when when when when when when when when when when when when... |

일반적으로 Unlikelihood는 요약 및 대화와 같은 더 복잡한 문제에 대해 의미 있는 응답을 생성하지 못합니다. 이는 선호되지 않는 응답의 확률을 무한정 감소시키려는 목적 함수의 특성 때문에, 모델이 반복적이고 의미 없는 토큰을 생성하는 방향으로 수렴하기 때문입니다.

## 추가 실험 결과

### 다양한 $N$ 값에 대한 Best of $N$ 베이스라인 성능

Best of $N$ 베이스라인은 실험에서 강력하지만 계산적으로 비용이 많이 드는 베이스라인입니다. Anthropic-HH 대화와 TL;DR 요약에 대해 다양한 $N$ 값의 Best of $N$ 베이스라인을 평가한 결과를 제시합니다.

![Best of N 성능 곡선](https://arxiv.org/html/2305.18290/x5.png)

![TL;DR Best of N](https://arxiv.org/html/2305.18290/x6.png)

위 그림들은 $N = \{1, 4, 16, 64, 128\}$에 대한 Best of $N$ 베이스라인의 성능을 보여줍니다. 성능은 대략 64-128개의 샘플 이후에 정체됩니다. 첫 번째 그림은 Anthropic-HH 대화에서 선택된 샘플링 온도에 대한 승률을 보여주며, 두 번째 그림은 TL;DR 요약에서의 결과를 나타냅니다. 두 작업 모두에서 Best of 64와 Best of 128의 성능 차이가 미미하여, 추가 샘플링으로 인한 개선이 제한적임을 보여줍니다.

이러한 결과는 DPO가 Best of 128과 유사하거나 더 나은 성능을 달성하면서도 테스트 시 단일 샘플만 생성하면 되므로 계산적으로 훨씬 효율적임을 강조합니다.

### 샘플 응답 및 GPT-4 판단

다양한 방법들 간의 구체적인 차이를 이해하기 위해, DPO와 베이스라인 방법들의 샘플 응답과 GPT-4의 판단을 제시합니다.

**요약 샘플 예시 1**:

프롬프트: "SUBREDDIT: r/AskReddit... I've been ungrateful to parents who have wanted only the best for me... I'm 17, leaving for University... what can I do to make it up to them..."

DPO 요약: "I've been a shitty child and I want to make up for it, what can I do for my parents on their birthdays and christmas to show them how much I love them?"

PPO 요약: "I've been a shitty person to my parents, what can I do to make it up to them, before I go, and what can I do after university to make them happy, and what can I do to make them happy after university (if that is the case)"

GPT-4 판단: Summary B [DPO]가 특별한 날에 제스처를 통해 부모님을 행복하게 만드는 데 초점을 맞춰 게시물의 주요 요점을 더 효과적으로 포착하는 반면, Summary A는 반복적이고 덜 명확합니다.

이 예시에서 DPO는 핵심 의도를 간결하게 요약한 반면, PPO는 반복적이고 장황한 응답을 생성했습니다.

**요약 샘플 예시 2**:

프롬프트: "SUBREDDIT: r/relationships... I [22 M] am about to propose to my girlfriend [22F] of almost 2 years... I can't sleep. I am anxious as hell..."

DPO 요약: "About to propose to my girlfriend of almost 2 years, but I am extremely anxious and can't sleep. How can I relax?"

PPO 요약: "I am about to ask my girlfriend of almost 2 years to marry me, and I can't calm myself down. Any tips on how to relax? I would like to be able to sleep tonight (and the night before)…"

GPT-4 판단: Summary A [DPO]가 요청자의 불안과 긴장을 풀 방법을 찾는 목표를 간결하게 전달하며 게시물의 주요 요점을 더 효과적으로 요약합니다.

**대화 샘플 - GPT-4가 DPO를 선택한 경우**:

프롬프트: "Can you help me write an essay on the Civil Rights Movement?"

DPO 응답: "The Civil Rights Movement was a series of social and political movements that took place between the 1950s and the 1980s, leading to the integration of public spaces, higher education, employment, and voting rights... I'll be happy to write an essay on this topic for you. What kind of essay would you like? A paper on the history of the Civil Rights Movement? A paper on the causes of the Civil Rights Movement? Or a paper on the effects of the Civil Rights Movement? Any ideas? I'll be happy to..."

GT (Ground Truth) 응답: "Oh, I'm not sure if I can actually write an essay, but I can at least help you think through the process. First, can you tell me what you're trying to write about? In other words, what are the main points or questions you want to address?"

GPT-4 판단: Response B [DPO]가 Civil Rights Movement에 대한 더 상세한 정보를 제공하고 에세이 주제에 대한 구체적인 제안을 제공하여, 에세이를 작성하려는 사람에게 더 유용합니다.

**대화 샘플 - GPT-4가 GT를 선택한 경우**:

프롬프트: "what is 7 plus 2"

DPO 응답: "Hm, let me see… seven plus two is nine, I'm pretty sure. I'll be happy to read the input aloud and double check if I've got it right. Let me try: '7 + 2' I've got that right! I think our input was meant to be seven, plus two... The formula is called the 'arithmetic-prefix method'..."

GT 응답: "11"

GPT-4 판단: Response B [GT]가 질문에 대한 직접적이고 정확한 답변을 제공하는 반면, Response A는 지나치게 복잡하고 올바른 답변을 제공하지 않습니다.

이 마지막 예시는 흥미롭게도 GPT-4가 오류를 범한 경우입니다. DPO의 응답이 장황하고 불필요한 설명을 포함하고 있지만, 실제로는 "seven plus two is nine"라고 올바르게 답변했습니다. 반면 GT는 "11"이라고 답했는데 이는 명백히 틀렸습니다. 이는 GPT-4 평가의 한계를 보여주는 사례입니다.

### 인간 연구 세부사항

![인간 연구 설문 레이아웃](https://arxiv.org/html/2305.18290/figures/results/survey.png)

위 그림은 SurveyMonkey에서의 설문 레이아웃을 보여줍니다. 각 응답자는 유사하게 형식화된 25개의 판단을 완료했습니다.

GPT-4를 사용한 승률 계산을 검증하기 위해, TL;DR 요약 설정에서 여러 알고리즘 매치업에 대한 인간 선호도 데이터를 수집하는 인간 연구를 수행했습니다. DPO (온도 0.25), SFT (온도 0.25), 그리고 PPO (온도 1.0)를 참조 알고리즘인 PPO (온도 0)와 비교하는 세 가지 서로 다른 알고리즘 매치업을 선택했습니다.

DPO 대 PPO-0 비교의 150개 무작위 샘플과 PPO-1 대 PPO-0 비교의 100개 무작위 샘플을 추출하여, 각 비교에 두 명의 인간을 할당하여 DPO-PPO에 대해 275개의 판단(한 명의 자원봉사자가 응답하지 않았습니다), PPO-PPO에 대해 200개의 판단을 생성했습니다. SFT 비교는 125개를 샘플링하여 각각에 한 명의 인간을 할당했습니다.

인간이 타이로 레이블한 판단은 무시했으며(약 1%에 불과), 인간 A와 인간 B 간의 원시 일치 비율(두 명의 인간 주석자가 있는 비교, 즉 SFT가 아닌 경우)과 각 인간과 GPT-4 간의 일치 비율을 측정했습니다.

참가자는 총 25명의 자원봉사 평가자로, 각각 25개의 요약을 비교했습니다. 평가자들은 Stanford 학생(학부생부터 박사과정까지) 또는 최근 Stanford 졸업생 또는 방문자로, STEM(주로 CS)에 중점을 둔 사람들이었습니다.

다음 표는 TL;DR 요약 샘플에 대한 인간과 GPT-4 승률 및 판단별 일치도를 비교합니다.

| 비교 | DPO vs PPO-1 | SFT vs PPO-1 | PPO-1 vs PPO-0 |
|------|--------------|--------------|----------------|
| 응답자 수 | 27 | 21 | 22 |
| GPT-4 (S) 승률 % | 47 | 27 | 13 |
| GPT-4 (C) 승률 % | 54 | 32 | 12 |
| 인간 승률 % | 58 | 43 | 17 |
| GPT-4 (S)-인간 일치 % | 70 | 77 | 86 |
| GPT-4 (C)-인간 일치 % | 67 | 79 | 85 |
| 인간-인간 일치 % | 65 | - | 87 |

두 프롬프트 모두에서 GPT-4는 인간이 서로 일치하는 만큼 인간과 일치하는 경향이 있어, GPT-4가 인간 평가의 합리적인 대리 지표임을 시사합니다. 제한된 인간 평가자로 인해 DPO 대 PPO-1 비교에 대해서만 여러 인간 판단을 수집했습니다. 전반적으로 GPT-4 (C) 프롬프트는 일반적으로 인간을 더 잘 대표하는 승률을 제공하므로 주요 결과에서 이 프롬프트를 사용했습니다.

이러한 결과는 GPT-4가 인간 평가의 신뢰할 수 있는 대리 지표로 사용될 수 있음을 검증하지만, 프롬프트 설계가 평가 결과에 영향을 미칠 수 있음을 보여줍니다. Concise 프롬프트가 인간 판단과 더 잘 일치하는 경향이 있어, 이후 분석에서는 이 프롬프트를 사용했습니다.
- - -
### References
* [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](http://arxiv.org/pdf/2305.18290v3)