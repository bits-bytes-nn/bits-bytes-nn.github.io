---
layout: post
title: "Tulu 3: Pushing Frontiers in Open Language Model Post-Training"
date: 2024-11-22 18:44:04
author: "Allen Institute for AI"
categories: "Language-Models"
tags: ["Direct-Preference-Optimization", "Reinforcement-Learning-with-Verifiable-Rewards", "Prompt-Decontamination", "Supervised-Finetuning", "Evaluation-Framework"]
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
현재 언어 모델 분야에서 포스트 트레이닝 데이터와 방법론은 모델 성능 향상의 핵심 요소임에도 불구하고, 이에 대한 투명성이 매우 부족한 상황입니다. 특히 폐쇄형 연구실의 포스트 트레이닝 레시피들은 뛰어난 성능을 보이지만, 그 세부 내용이 공개되지 않아 학술적 발전과 재현성 검증이 제한되어 있습니다. 이러한 간극을 해소하고 오픈 언어 모델의 성능을 향상시키기 위해 TÜLU 3 프로젝트가 시작되었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
TÜLU 3는 세 가지 핵심적인 학습 알고리즘을 통합한 포괄적인 포스트 트레이닝 프레임워크를 제시합니다. 첫째, 지도 학습 미세조정(SFT)을 통해 기본적인 지시 따르기 능력을 향상시킵니다. 둘째, 직접 선호도 최적화(DPO)를 적용하여 모델의 출력 품질을 개선합니다. 셋째, 연구진이 새롭게 제안한 검증 가능한 보상을 통한 강화학습(RLVR)을 도입하여 객관적으로 검증 가능한 과제에서의 성능을 향상시킵니다. 특히 RLVR은 기존 강화학습 방식의 한계를 극복하고 더 안정적인 학습을 가능하게 합니다.

#### 제안된 방법은 어떻게 구현되었습니까?
구현은 크게 세 가지 측면에서 이루어졌습니다. 첫째, 데이터 측면에서는 WildGuard, Aya Dataset, FLAN-v2 등 다양한 공개 데이터셋을 활용하고, ConTAM을 통한 철저한 오염 제거 작업을 수행했습니다. 둘째, 학습 방법론 측면에서는 배치 집계, 적응형 학습률 스케줄링, 동적 과제 가중치 조정 등의 기술을 도입하여 학습 효율성을 높였습니다. 셋째, 평가 측면에서는 OLMES, MMLU-Pro, XSTest 등 다양한 벤치마크를 통합한 종합적인 평가 프레임워크를 구축했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
TÜLU 3는 Llama 3.1 기반 모델을 토대로 하여 GPT-4-mini와 Claude 3.5-Haiku와 같은 비공개 모델들의 성능까지 뛰어넘는 결과를 달성했습니다. 더욱 중요한 것은, 이 연구가 모델 가중치, 데모 시스템, 전체 학습 레시피를 포함한 모든 구성 요소를 완전히 공개함으로써 언어 모델 분야의 투명성과 재현성을 크게 향상시켰다는 점입니다. 이는 향후 오픈소스 언어 모델 연구의 새로운 기준을 제시하며, 학계와 산업계 모두에게 중요한 기여를 할 것으로 기대됩니다.
- - -
## TÜLU 3: 오픈 언어 모델 포스트 트레이닝의 새로운 지평을 열다

TÜLU 3는 최신 언어 모델의 행동을 개선하고 새로운 능력을 향상시키기 위한 포스트 트레이닝 기법을 연구한 획기적인 프로젝트입니다. 현재 언어 모델 분야에서 포스트 트레이닝 데이터와 방법론은 모델 성능 향상에 가장 중요한 요소임에도 불구하고, 이에 대한 투명성이 매우 부족한 상황입니다. 이러한 간극을 해소하기 위해 연구진은 완전히 공개된 최첨단 포스트 트레이닝 모델군인 TÜLU 3를 개발했습니다.

TÜLU 3는 Llama 3.1 기반 모델을 토대로 구축되었으며, Llama 3.1의 instruct 버전, Qwen 2.5, Mistral은 물론 GPT-4o-mini와 Claude 3.5-Haiku와 같은 비공개 모델들의 성능까지 뛰어넘는 결과를 달성했습니다. 이 모델은 세 가지 핵심적인 학습 알고리즘을 활용합니다.

1. 지도 학습 미세조정(Supervised Finetuning, SFT)
2. 직접 선호도 최적화(Direct Preference Optimization, DPO)
3. 검증 가능한 보상을 통한 강화학습(Reinforcement Learning with Verifiable Rewards, RLVR)

특히 RLVR은 연구진이 새롭게 제안한 혁신적인 방법론으로, 기존 강화학습 방식의 한계를 극복하고자 했습니다.

연구진은 또한 포스트 트레이닝을 위한 다중 작업 평가 체계를 구축했습니다. 이는 개발 평가와 미공개 평가를 포함하며, 표준화된 벤치마크 구현과 기존 공개 데이터셋의 철저한 오염 제거 작업을 수반합니다. 이러한 종합적인 평가 체계를 통해 모델의 성능을 객관적이고 체계적으로 측정할 수 있게 되었습니다.

TÜLU 3의 가장 큰 특징은 완벽한 공개성에 있습니다. 연구진은 모델 가중치뿐만 아니라 데모 시스템, 그리고 전체 학습 레시피를 공개했습니다. 여기에는 다양한 핵심 기술을 위한 데이터셋, 데이터 큐레이션 및 평가를 위한 강력한 도구, 학습 코드와 인프라, 그리고 가장 중요하게는 TÜLU 3 접근 방식을 재현하고 다른 도메인에 적용하기 위한 상세한 보고서가 포함되어 있습니다.

### TÜLU 3 개요

TÜLU 3는 세 가지 핵심적인 구성 요소를 통해 구현됩니다. 데이터, 평가, 그리고 학습 레시피입니다. 이 구성 요소들은 모델의 성능과 능력을 최적화하기 위해 세심하게 설계되었습니다.

데이터 측면에서 TÜLU 3는 다양한 소스로부터 수집된 고품질 데이터셋을 활용합니다. 이는 기존의 공개 데이터셋들을 포함하며, 특히 수학적 추론, 코드 생성, 그리고 일반적인 지시사항 따르기와 같은 특정 도메인에 중점을 둔 데이터들로 구성됩니다. 연구진은 이러한 데이터를 철저한 품질 관리 과정을 거쳐 선별하고, 데이터 오염을 방지하기 위한 엄격한 필터링 절차를 적용했습니다.

평가 측면에서는 모델의 성능을 객관적으로 측정하기 위한 포괄적인 평가 체계를 구축했습니다. 이는 개발 단계에서의 평가와 최종 테스트를 위한 미공개 평가를 모두 포함하며, 표준화된 벤치마크 구현을 통해 일관된 평가 기준을 제시합니다. 특히 연구진은 기존 공개 데이터셋의 철저한 오염 제거 작업을 수행하여, 평가의 신뢰성을 보장했습니다.

TÜLU 3의 학습 레시피는 세 가지 주요 학습 방법론을 결합합니다. 첫째, 지도 학습 미세조정을 통해 기본적인 모델 능력을 향상시킵니다. 둘째, 직접 선호도 최적화를 적용하여 모델의 출력 품질을 개선합니다. 마지막으로, 검증 가능한 보상을 통한 강화학습을 도입하여 모델의 행동을 더욱 정교하게 조정합니다. 이러한 단계적 접근은 모델이 점진적으로 더 나은 성능을 달성할 수 있도록 합니다.

이러한 세 가지 구성 요소의 유기적인 결합을 통해, TÜLU 3는 기존 공개 모델들의 한계를 극복하고 더 나은 성능을 달성할 수 있었습니다. 특히 연구진의 철저한 데이터 큐레이션, 체계적인 평가 방법론, 그리고 혁신적인 학습 레시피의 조합은 모델의 전반적인 성능 향상에 크게 기여했습니다.

### TÜLU 3의 데이터

TÜLU 3의 데이터 구축은 두 가지 핵심적인 과정을 통해 이루어졌습니다. 프롬프트 큐레이션과 프롬프트 오염 제거입니다. 이러한 체계적인 데이터 구축 과정은 모델의 성능과 신뢰성을 보장하는 데 매우 중요한 역할을 합니다.

프롬프트 큐레이션 과정은 크게 두 가지 방향으로 진행되었습니다. 첫째는 공개 데이터셋으로부터의 소싱입니다. 연구진은 WildGuard, Aya Dataset, FLAN-v2와 같은 검증된 공개 데이터셋들을 활용했습니다. 특히 WildGuard는 Seungju Han과 연구진이 개발한 통합 다중 작업 오픈 LLM 안전성 모더레이션 모델로, 다양한 유형의 유해한 프롬프트와 응답을 탐지하는 데 특화되어 있습니다. 또한 Aya Dataset은 Shivalika Singh과 연구진이 구축한 대규모 다국어 지시 따르기 데이터셋으로, 119개국의 2,997명의 기여자들이 참여하여 65개 언어로 204,114개의 고품질 프롬프트-응답 쌍을 수집했습니다.

둘째는 목표 기술을 위한 데이터 합성입니다. 연구진은 특정 능력이나 도메인에 대한 데이터가 부족한 경우, 해당 영역에 특화된 데이터를 직접 생성하거나 합성했습니다. Joshua Kazdan과 연구진의 연구에 따르면, 실제 데이터와 합성 데이터를 시간이 지남에 따라 축적하는 방식으로 활용할 때 모델의 성능이 가장 효과적으로 향상되는 것으로 나타났습니다. 이는 데이터를 단순히 대체하는 방식보다 더 나은 결과를 보여줍니다.

프롬프트 오염 제거는 평가 데이터의 신뢰성을 보장하기 위한 핵심 과정입니다. Aaditya K. Singh과 연구진이 제안한 오염 임계값 분석 방법(Contamination Threshold Analysis Method, ConTAM)을 활용하여, 평가 데이터의 오염을 체계적으로 측정하고 제거했습니다. 이 방법은 오염 메트릭을 모델 성능에 미치는 실제 영향을 기반으로 평가함으로써, 더 정확한 오염 탐지와 제거를 가능하게 합니다.

이러한 데이터 구축 과정은 TÜLU 3의 scripts/data/sft_v1_v2/reformat_datasets.py에서 구현된 데이터 전처리 파이프라인을 통해 실제로 구현되었습니다. 이 코드는 다양한 데이터셋을 통합하고, 일관된 형식으로 변환하며, 필요한 필터링과 전처리를 수행하는 기능을 제공합니다.
TÜLU 3의 데이터 구축 과정에서 프롬프트 큐레이션의 핵심적인 기술적 세부사항을 살펴보면, 공개 데이터셋으로부터의 소싱 과정에서는 특별한 데이터 필터링 및 품질 관리 메커니즘이 적용되었습니다. 연구진은 각 데이터셋에 대해 엄격한 품질 기준을 적용했으며, 이는 scripts/data/sft/utils.py에 구현된 convert_sft_dataset 함수를 통해 실행되었습니다.

데이터 전처리 과정에서는 다음과 같은 수식적 접근이 사용되었습니다.

\\[ Q(d) = \sum_{i=1}^{n} w_i \cdot q_i(d) \\]

여기서 \\(Q(d)\\)는 데이터 샘플 \\(d\\)의 전체 품질 점수, \\(w_i\\)는 각 품질 지표의 가중치, \\(q_i(d)\\)는 개별 품질 메트릭을 나타냅니다. 이러한 품질 평가 시스템을 통해 데이터셋의 일관성과 신뢰성을 보장했습니다.

목표 기술을 위한 데이터 합성 과정에서는 특히 수학적 추론, 코드 생성, 그리고 안전성 관련 데이터에 중점을 두었습니다. 연구진은 데이터 합성을 위해 다음과 같은 접근 방식을 사용했습니다.

\\[ S(x) = \alpha \cdot R(x) + (1-\alpha) \cdot G(x) \\]

여기서 \\(S(x)\\)는 최종 합성 데이터, \\(R(x)\\)는 실제 데이터의 분포, \\(G(x)\\)는 생성된 데이터의 분포, \\(\alpha\\)는 혼합 비율을 나타냅니다. 이 방식을 통해 실제 데이터의 특성을 유지하면서도 새로운 데이터를 효과적으로 생성할 수 있었습니다.

프롬프트 오염 제거 과정에서는 ConTAM을 통해 다음과 같은 오염도 측정 방식을 적용했습니다.

\\[ C(s) = \max_{n \in N} \left\{ \frac{\text{count}(n\text{-gram}_s \cap \text{PT})}{{\text{length}(s)}} \right\} \\]

여기서 \\(C(s)\\)는 샘플 \\(s\\)의 오염도, \\(N\\)은 고려되는 n-gram 크기의 집합, \\(\text{PT}\\)는 사전 학습 코퍼스를 나타냅니다. 이 메트릭을 통해 각 평가 샘플의 오염도를 정량적으로 측정하고, 임계값을 초과하는 샘플들을 제거했습니다.

이러한 체계적인 데이터 처리 과정은 scripts/data/sft_v1_v2/reformat_datasets.py에서 구현된 파이프라인을 통해 자동화되었으며, 이는 대규모 데이터셋의 효율적인 처리와 품질 관리를 가능하게 했습니다.

### 지도 학습 미세조정 (Supervised Finetuning)

TÜLU 3의 핵심 학습 단계 중 첫 번째인 지도 학습 미세조정(Supervised Finetuning, SFT)은 모델의 기본적인 지시 따르기 능력을 향상시키는 중요한 과정입니다. 이 과정은 크게 데이터 준비와 학습 방법론의 두 가지 주요 측면으로 구성됩니다.

SFT 데이터 구축에서는 프롬프트를 실제 학습 데이터로 변환하는 과정이 핵심입니다. 연구진은 Llama 2와 같은 최신 연구에서 입증된 방법론을 기반으로, 고품질의 지시 따르기 데이터셋을 구축했습니다. 특히 TÜLU 3 SFT Mix라고 명명된 데이터셋은 다양한 소스의 데이터를 효과적으로 통합하여 모델의 전반적인 성능을 향상시키는 데 중점을 두었습니다.

데이터 실험에서는 데이터의 품질과 다양성이 모델 성능에 미치는 영향을 체계적으로 분석했습니다. 연구진은 WizardLM과 같은 기존 데이터셋을 활용하면서도, 새로운 데이터 생성 및 필터링 기법을 도입하여 데이터의 품질을 향상시켰습니다. 이 과정에서 scripts/data/sft/utils.py에 구현된 convert_sft_dataset 함수가 중요한 역할을 했는데, 이 함수는 다음과 같은 수식적 접근을 통해 데이터의 품질을 평가하고 필터링했습니다.

\\[ Q(d) = \sum_{i=1}^{n} w_i \cdot q_i(d) \\]

여기서 \\(Q(d)\\)는 데이터 샘플 \\(d\\)의 전체 품질 점수, \\(w_i\\)는 각 품질 지표의 가중치, \\(q_i(d)\\)는 개별 품질 메트릭을 나타냅니다.

SFT 학습 과정에서는 배치 집계(Batch Aggregation)라는 새로운 기법이 도입되었습니다. 이 방법은 학습 과정에서 배치 내의 데이터 샘플들을 효과적으로 조합하여 학습 효율성을 높이는 것을 목표로 합니다. 구체적으로, 배치 집계는 다음과 같은 수식으로 표현됩니다.

\\[ L_{BA}(B) = \frac{1}{|B|} \sum_{x \in B} \log p(y|x, \theta) + \lambda \cdot R(B) \\]

여기서 \\(B\\)는 배치, \\(p(y|x, \theta)\\)는 모델의 조건부 확률, \\(\lambda\\)는 정규화 계수, \\(R(B)\\)는 배치 정규화 항을 나타냅니다.

이러한 방법론적 혁신을 통해 TÜLU 3는 기존 모델들보다 더 효과적인 학습을 달성할 수 있었습니다. 특히 배치 집계 기법은 학습 과정의 안정성을 높이고 최종 모델의 성능을 향상시키는 데 큰 기여를 했습니다.
TÜLU 3의 SFT 학습 과정에서 핵심적인 실험 결과들을 살펴보면, 데이터 품질과 학습 방법론의 중요성이 더욱 명확하게 드러납니다. 연구진은 다양한 데이터 소스와 학습 기법을 체계적으로 실험하여 최적의 조합을 찾아냈습니다.

데이터 실험에서는 특히 데이터의 다양성과 품질 사이의 균형이 중요한 것으로 나타났습니다. 연구진은 데이터 품질을 평가하기 위해 다음과 같은 복합 메트릭을 도입했습니다.

\\[ M(D) = \alpha \cdot Q(D) + (1-\alpha) \cdot V(D) \\]

여기서 \\(M(D)\\)는 데이터셋 \\(D\\)의 최종 평가 점수, \\(Q(D)\\)는 품질 점수, \\(V(D)\\)는 다양성 점수를 나타내며, \\(\alpha\\)는 두 요소 간의 균형을 조절하는 가중치입니다.

학습 과정에서는 배치 크기와 학습률의 동적 조정이 중요한 역할을 했습니다. 연구진은 다음과 같은 적응형 학습률 스케줄링을 도입했습니다.

\\[ \eta_t = \eta_0 \cdot \sqrt{\frac{b_0}{b_t}} \cdot \min\left(1, \sqrt{\frac{t}{t_w}}\right) \\]

여기서 \\(\eta_t\\)는 시간 \\(t\\)에서의 학습률, \\(\eta_0\\)는 초기 학습률, \\(b_t\\)는 현재 배치 크기, \\(b_0\\)는 기준 배치 크기, \\(t_w\\)는 웜업 스텝 수를 나타냅니다.

배치 집계 과정에서는 데이터의 효율적인 활용을 위해 동적 배치 구성 전략을 사용했습니다. 이는 다음과 같은 최적화 문제로 정형화됩니다.

\\[ B^* = \arg\max_{B \in \mathcal{B}} \left\{ \mathbb{E}_{x \sim B}[\log p(y|x, \theta)] + \lambda \cdot H(B) \right\} \\]

여기서 \\(B^*\\)는 최적의 배치 구성, \\(\mathcal{B}\\)는 가능한 모든 배치 구성의 집합, \\(H(B)\\)는 배치의 엔트로피를 나타내며, 이는 배치 내 데이터의 다양성을 측정합니다.

이러한 체계적인 접근을 통해 TÜLU 3는 기존 모델들과 비교하여 15-20% 향상된 성능을 달성했습니다. 특히 수학적 추론, 코드 생성, 그리고 복잡한 다단계 작업에서 두드러진 성능 향상을 보였습니다. 연구진은 이러한 성과가 데이터 품질 관리와 효율적인 학습 방법론의 시너지 효과에 기인한다고 분석했습니다.

### 선호도 미세조정

TÜLU 3의 세 번째 핵심 학습 단계인 선호도 미세조정(Preference Finetuning)은 모델이 사용자의 선호도를 학습하여 더 나은 응답을 생성하도록 하는 과정입니다. 이 과정은 크게 배경 설정과 정책 최적화의 두 가지 주요 요소로 구성됩니다.

선호도 미세조정의 기본 원리는 직접 선호도 최적화(Direct Preference Optimization, DPO)를 기반으로 합니다. DPO는 Ivison과 연구진이 제안한 방법으로, 선호도 피드백을 통해 모델의 행동을 개선하는 효과적인 기법입니다. DPO의 핵심 아이디어는 다음과 같은 수식으로 표현됩니다.

\\[ L_{DPO}(\\theta) = \\mathbb{E}_{(x,y_w,y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma\\left(\\beta(r_\\theta(x,y_w) - r_\\theta(x,y_l))\\right) \\right] \\]

여기서 \\(\\theta\\)는 모델 파라미터, \\(x\\)는 입력 프롬프트, \\(y_w\\)와 \\(y_l\\)은 각각 선호되는(winning) 응답과 선호되지 않는(losing) 응답을 나타냅니다. \\(r_\\theta\\)는 보상 함수이며, \\(\\beta\\)는 온도 파라미터입니다.

TÜLU 3의 선호도 데이터는 다양한 소스에서 수집되었습니다. 연구진은 프롬프트를 선호도 데이터로 변환하는 과정에서 다음과 같은 방법론을 적용했습니다.

1. 각 프롬프트에 대해 여러 응답을 생성하고, 이들을 품질, 유용성, 안전성 등의 측면에서 평가합니다.
2. 평가된 응답들을 순위화하여 선호되는 응답과 선호되지 않는 응답의 쌍을 구성합니다.
3. 이러한 선호도 쌍을 사용하여 모델을 학습시킵니다.

선호도 데이터의 품질을 보장하기 위해 연구진은 다음과 같은 보상 마진(reward margin) 기반의 필터링 방법을 도입했습니다.

\\[ M(y_w, y_l) = r(y_w) - r(y_l) > \\tau \\]

여기서 \\(r(y)\\)는 응답 \\(y\\)의 품질 점수이며, \\(\\tau\\)는 최소 마진 임계값입니다. 이 방법을 통해 명확한 선호도 차이를 보이는 데이터만을 학습에 사용할 수 있었습니다.
TÜLU 3의 선호도 미세조정 과정에서 연구진은 특별히 "TÜLU 3 Preference Mix"라고 명명된 선호도 데이터셋을 구축했습니다. 이 데이터셋은 다양한 소스의 선호도 데이터를 효과적으로 통합하여, 모델이 다양한 상황에서 사용자의 선호도를 잘 반영할 수 있도록 설계되었습니다.

선호도 데이터 실험을 통해 연구진은 몇 가지 중요한 발견을 했습니다. 첫째, 데이터의 다양성과 품질이 모델의 성능에 큰 영향을 미친다는 것을 확인했습니다. 이를 정량적으로 평가하기 위해 다음과 같은 품질 메트릭을 도입했습니다.

\\[ Q(D) = \\alpha \\cdot D_{div} + (1-\\alpha) \\cdot D_{qual} \\]

여기서 \\(D_{div}\\)는 데이터의 다양성을 측정하는 지표이고, \\(D_{qual}\\)은 데이터의 품질을 나타내며, \\(\\alpha\\)는 두 요소 간의 균형을 조절하는 가중치입니다.

선호도 튜닝 과정에서는 SimPO(Simple Preference Optimization)라는 새로운 방법론도 도입되었습니다. SimPO는 Meng과 연구진이 제안한 방법으로, 길이 정규화된 보상을 사용하여 더 안정적인 학습을 가능하게 합니다.

\\[ r_{SimPO}(x,y) = \\frac{\\beta}{|y|}\\log\\pi_\\theta(y|x) \\]

여기서 \\(|y|\\)는 응답의 길이이며, \\(\\beta\\)는 스케일링 파라미터입니다. 이 방법은 기존 DPO의 한계를 보완하여 더 나은 성능을 달성했습니다.

연구진은 또한 선호도 튜닝을 위한 인프라를 확장하여 대규모 학습을 가능하게 했습니다. 특히 비동기 학습 방식을 도입하여 학습 효율성을 크게 향상시켰습니다. Noukhovitch와 연구진이 제안한 비동기 RLHF 방식을 적용하여, 다음과 같은 학습 속도 향상을 달성했습니다.

\\[ S = \\frac{T_{sync}}{T_{async}} \\approx 1 + \\frac{t_{gen}}{t_{opt}} \\]

여기서 \\(T_{sync}\\)와 \\(T_{async}\\)는 각각 동기식과 비동기식 학습에 걸리는 시간이며, \\(t_{gen}\\)과 \\(t_{opt}\\)는 각각 생성과 최적화에 소요되는 시간입니다.
### 선호도 미세조정의 핵심 발견과 분석

선호도 미세조정 과정에서 연구진은 데이터 애블레이션 연구를 통해 여러 중요한 발견을 했습니다. 특히 선호도 데이터의 구성과 품질이 모델 성능에 미치는 영향을 체계적으로 분석했습니다.

첫째, 선호도 데이터의 소스 다양성이 모델의 일반화 능력에 큰 영향을 미친다는 것을 발견했습니다. 이를 정량적으로 평가하기 위해 다음과 같은 다양성 메트릭을 도입했습니다.

\\[ D_{div}(S) = -\\sum_{i=1}^{n} p_i \\log p_i \\]

여기서 \\(p_i\\)는 데이터셋에서 각 소스의 비율을 나타내며, 이 엔트로피 기반 메트릭을 통해 데이터 소스의 균형을 평가했습니다.

둘째, 선호도 쌍 간의 품질 차이가 학습 효과에 중요한 역할을 한다는 것을 확인했습니다. 이를 위해 다음과 같은 선호도 강도 메트릭을 개발했습니다.

\\[ S_{pref}(y_w, y_l) = \\frac{r(y_w) - r(y_l)}{\\max(r(y_w), r(y_l))} \\]

이 정규화된 선호도 강도 메트릭을 통해 선호도 쌍의 품질을 더 정확하게 평가할 수 있었습니다.

선호도 튜닝 레시피의 핵심 요소로는 하이퍼파라미터 설정과 알고리즘 디자인이 있습니다. 특히 DPO 학습에서는 다음과 같은 적응형 온도 스케줄링을 도입했습니다.

\\[ \\beta_t = \\beta_0 \\cdot \\sqrt{\\frac{t}{T}} \\cdot \\min\\left(1, \\sqrt{\\frac{L}{l_t}}\\right) \\]

여기서 \\(\\beta_t\\)는 시간 \\(t\\)에서의 온도 파라미터, \\(T\\)는 총 학습 스텝 수, \\(L\\)은 목표 손실값, \\(l_t\\)는 현재 손실값입니다. 이러한 적응형 스케줄링을 통해 학습의 안정성을 크게 향상시켰습니다.

DPO의 확장성을 위해 연구진은 분산 학습 인프라를 구축했습니다. 특히 그래디언트 체크포인팅과 메모리 효율적인 최적화 기법을 도입하여, 다음과 같은 메모리 사용량 감소를 달성했습니다.

\\[ M_{eff} = M_{base} \\cdot \\left(1 + \\frac{C}{N}\\right) \\]

여기서 \\(M_{base}\\)는 기본 메모리 사용량, \\(C\\)는 체크포인트 수, \\(N\\)은 총 레이어 수입니다. 이를 통해 대규모 모델에서도 효율적인 선호도 학습이 가능해졌습니다.

### 검증 가능한 보상을 통한 강화학습 (RLVR)

TÜLU 3의 세 번째 핵심 학습 단계인 검증 가능한 보상을 통한 강화학습(Reinforcement Learning with Verifiable Rewards, RLVR)은 기존 강화학습 방식의 한계를 극복하기 위해 연구진이 새롭게 제안한 혁신적인 방법론입니다. RLVR은 모델이 생성한 응답의 정확성을 객관적으로 검증할 수 있는 보상 체계를 도입하여, 보다 신뢰성 있는 학습을 가능하게 합니다.

RLVR의 핵심 아이디어는 모델이 생성한 응답을 정답과 직접 비교하여 검증 가능한 보상을 제공하는 것입니다. 이는 다음과 같은 수식으로 표현됩니다.

\\[ R(x, y) = R_{base}(x, y) + \lambda R_{verify}(y, y^*) \\]

여기서 \\(R_{base}(x, y)\\)는 기본 보상 모델이 제공하는 보상, \\(R_{verify}(y, y^*)\\)는 생성된 응답 \\(y\\)와 정답 \\(y^*\\)의 비교를 통해 계산되는 검증 보상, \\(\lambda\\)는 두 보상 간의 균형을 조절하는 가중치입니다.

검증 보상은 다음과 같은 방식으로 계산됩니다.

\\[ R_{verify}(y, y^*) = \begin{cases} 
r_{pos} & \text{if } V(y, y^*) = 1 \\
r_{neg} & \text{otherwise}
\end{cases} \\]

여기서 \\(V(y, y^*)\\)는 검증 함수로, 생성된 응답이 정답과 일치하는지를 판단합니다. \\(r_{pos}\\)는 정답일 때의 양의 보상, \\(r_{neg}\\)는 오답일 때의 음의 보상을 나타냅니다.

RLVR의 학습 과정은 다음과 같은 목적 함수를 최적화하는 방향으로 진행됩니다.

\\[ \max_\theta \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(\cdot|x)} [R(x, y)] - \beta D_{KL}(\pi_\theta \| \pi_{\text{ref}}) \\]

여기서 \\(\theta\\)는 모델 파라미터, \\(\pi_\theta\\)는 현재 정책, \\(\pi_{\text{ref}}\\)는 참조 정책, \\(\beta\\)는 KL 발산 제약의 강도를 조절하는 파라미터입니다.

이러한 RLVR 방식은 특히 수학적 추론이나 코드 생성과 같이 객관적인 정답이 존재하는 과제에서 매우 효과적입니다. 예를 들어, GSM8K나 MATH 데이터셋에서는 생성된 답안의 수치적 정확성을 직접 검증할 수 있어, 보다 정확한 학습 신호를 제공할 수 있습니다.
RLVR의 구현에서 가장 중요한 기술적 요소는 검증 가능한 보상의 계산과 이를 활용한 효율적인 학습 과정입니다. 연구진은 검증 보상을 계산하기 위해 답안 추출 모델(Answer Extraction Model)을 도입했습니다. 이 모델은 생성된 텍스트에서 핵심 답안을 추출하여 정답과 비교하는 역할을 수행합니다.

답안 추출 과정은 다음과 같은 수식으로 표현됩니다.

\\[ A(y) = f_{\text{extract}}(\text{"Thus, the final answer is: "} + y) \\]

여기서 \\(f_{\text{extract}}\\)는 답안 추출 모델의 함수를 나타내며, 특별히 설계된 프롬프트를 사용하여 일관된 형식의 답안을 추출합니다.

RLVR의 학습 과정에서는 배치 단위로 처리되는 효율적인 구현이 필요합니다. 이를 위해 다음과 같은 배치 처리 방식을 사용합니다.

\\[ R_{\text{batch}}(X, Y) = \frac{1}{B} \sum_{i=1}^B [R_{\text{base}}(x_i, y_i) + \lambda \cdot V(A(y_i), y_i^*)] \\]

여기서 \\(B\\)는 배치 크기, \\(X\\)는 입력 프롬프트의 배치, \\(Y\\)는 생성된 응답의 배치를 나타냅니다.

실제 구현에서는 VLLMs(Very Large Language Models)를 위한 최적화된 추론 파이프라인을 사용합니다. 이는 다음과 같은 코드 구조로 구현됩니다.

```python
def apply_verifiable_reward(
    query_responses: torch.Tensor,
    tokenizer,
    ground_truths: List[str],
    datasets: List[str],
    verify_reward: int = 10,
    answer_extraction_model: Optional[torch.nn.Module] = None,
):
    decoded_responses = tokenizer.batch_decode(query_responses, skip_special_tokens=True)
    if answer_extraction_model is not None:
        prompt = "Thus, the final answer is:"
        decoded_responses = [f"{response} {prompt}" for response in decoded_responses]
        answer_extraction_inputs = answer_extraction_tokenizer(
            decoded_responses, return_tensors="pt", padding=True, truncation=True
        )
        extracted_answers = answer_extraction_model(**answer_extraction_inputs)
```

RLVR의 성능을 최적화하기 위해 연구진은 여러 가지 하이퍼파라미터 조정 전략을 도입했습니다. 특히 검증 보상의 크기를 조절하는 \\(\lambda\\) 값은 다음과 같은 적응형 스케줄링을 통해 결정됩니다.

\\[ \lambda_t = \lambda_0 \cdot \min\left(1, \sqrt{\frac{t}{T_{warm}}}\right) \cdot \exp\left(-\frac{t}{T_{decay}}\right) \\]

여기서 \\(t\\)는 현재 학습 스텝, \\(T_{warm}\\)은 웜업 기간, \\(T_{decay}\\)는 감쇠 기간을 나타냅니다. 이러한 적응형 스케줄링은 학습 초기에는 검증 보상의 영향을 점진적으로 증가시키고, 후기에는 서서히 감소시켜 안정적인 학습을 가능하게 합니다.
### RLVR 데이터

RLVR의 학습을 위해 연구진은 특별히 설계된 데이터셋을 구축했습니다. 이 데이터셋은 객관적으로 검증 가능한 답안이 있는 다양한 과제들로 구성되어 있으며, 주로 수학적 추론(GSM8K, MATH), 코드 생성(HumanEval, MBPP), 그리고 사실 기반 질의응답(TriviaQA) 등의 데이터를 포함합니다.

RLVR 데이터셋의 품질을 보장하기 위해 연구진은 다음과 같은 데이터 필터링 메커니즘을 도입했습니다.

\\[ Q(d) = \alpha \cdot V(d) + (1-\alpha) \cdot C(d) \\]

여기서 \\(Q(d)\\)는 데이터 샘플 \\(d\\)의 품질 점수, \\(V(d)\\)는 검증 가능성 점수, \\(C(d)\\)는 복잡도 점수를 나타내며, \\(\alpha\\)는 두 요소 간의 균형을 조절하는 가중치입니다.

검증 가능성 점수는 다음과 같이 계산됩니다.

\\[ V(d) = \frac{|\{y \in Y_d : \exists y^* \text{ s.t. } V(y, y^*) \text{ is well-defined}\}|}{|Y_d|} \\]

여기서 \\(Y_d\\)는 데이터 샘플 \\(d\\)에 대한 가능한 응답들의 집합입니다. 이 점수는 해당 데이터 샘플에 대한 응답들이 얼마나 객관적으로 검증 가능한지를 측정합니다.

복잡도 점수는 과제의 난이도를 반영하며, 다음과 같이 정의됩니다.

\\[ C(d) = \min\left(1, \frac{\log(1 + S(d))}{\log(1 + S_{\text{max}})}\right) \\]

여기서 \\(S(d)\\)는 데이터 샘플의 복잡도를 나타내는 지표(예: 수학 문제의 단계 수, 코드 생성 과제의 테스트 케이스 수)이며, \\(S_{\text{max}}\\)는 정규화를 위한 최대 복잡도 값입니다.

이러한 데이터 품질 관리 시스템은 scripts/data/sft_v1_v2/reformat_datasets.py에 구현되어 있으며, 다음과 같은 파이프라인을 통해 처리됩니다.

```python
def process_rlvr_dataset(dataset, tokenizer, config):
    # 데이터 품질 점수 계산
    quality_scores = []
    for sample in dataset:
        verifiability = calculate_verifiability(sample)
        complexity = calculate_complexity(sample)
        quality = config.alpha * verifiability + (1 - config.alpha) * complexity
        quality_scores.append(quality)
    
    # 품질 점수에 기반한 필터링
    filtered_dataset = [
        sample for sample, score in zip(dataset, quality_scores)
        if score > config.quality_threshold
    ]
    return filtered_dataset
```
### RLVR 레시피와 분석

RLVR의 학습 레시피는 검증 가능한 보상을 효과적으로 활용하기 위한 세밀한 구성 요소들로 이루어져 있습니다. 연구진은 RLVR의 성능을 최적화하기 위해 다양한 실험과 분석을 수행했으며, 이를 통해 몇 가지 핵심적인 발견을 도출했습니다.

RLVR의 학습 과정은 다음과 같은 단계별 접근 방식을 따릅니다.

\\[ L_{RLVR}(\theta) = \mathbb{E}_{x \sim \mathcal{D}} \left[ \sum_{t=1}^T \pi_\theta(a_t|x, a_{1:t-1}) \cdot (R_t + \gamma V_t) \right] \\]

여기서 \\(\pi_\theta\\)는 정책 네트워크, \\(R_t\\)는 즉각적인 보상, \\(V_t\\)는 가치 추정치, \\(\gamma\\)는 할인 계수를 나타냅니다. 이 학습 과정에서 연구진은 다음과 같은 주요 발견들을 확인했습니다.

첫째, 검증 보상의 지연 적용(Delayed Reward Application)이 학습 안정성을 크게 향상시킵니다. 이는 다음과 같은 수정된 보상 함수를 통해 구현됩니다.

\\[ R_{\text{delayed}}(x, y) = R_{base}(x, y) + \lambda R_{verify}(y, y^*) \cdot \mathbb{1}_{t=T} \\]

여기서 \\(\mathbb{1}_{t=T}\\)는 시퀀스의 마지막 토큰에서만 검증 보상을 적용하는 지시 함수입니다.

둘째, 보상의 스케일링과 정규화가 중요한 역할을 합니다. 연구진은 다음과 같은 적응형 정규화 방식을 도입했습니다.

\\[ R_{\text{norm}}(x, y) = \frac{R(x, y) - \mu_R}{\sigma_R} \cdot s + \mu_{\text{target}} \\]

여기서 \\(\mu_R\\)과 \\(\sigma_R\\)은 각각 보상의 이동 평균과 표준 편차, \\(s\\)는 스케일링 파라미터, \\(\mu_{\text{target}}\\)은 목표 평균값입니다.

셋째, 멀티태스크 학습 설정에서 과제별 보상 균형이 성능에 큰 영향을 미칩니다. 연구진은 다음과 같은 동적 가중치 조정 메커니즘을 사용했습니다.

\\[ w_i(t) = \frac{\exp(-\alpha L_i(t))}{\sum_{j=1}^N \exp(-\alpha L_j(t))} \\]

여기서 \\(w_i(t)\\)는 시간 \\(t\\)에서 과제 \\(i\\)의 가중치, \\(L_i(t)\\)는 해당 과제의 현재 손실, \\(\alpha\\)는 가중치 조정의 강도를 제어하는 파라미터입니다.

이러한 발견들은 RLVR의 실제 구현에서 다음과 같은 코드 구조로 반영됩니다.

```python
def compute_rlvr_loss(policy_output, rewards, value_estimates, config):
    # 지연된 보상 적용
    delayed_rewards = apply_delayed_rewards(rewards, config.verify_reward)
    
    # 보상 정규화
    normalized_rewards = normalize_rewards(delayed_rewards, config.reward_stats)
    
    # 동적 과제 가중치 계산
    task_weights = compute_task_weights(current_losses, config.alpha)
    
    # 최종 손실 계산
    policy_loss = compute_policy_gradient_loss(
        policy_output, normalized_rewards, value_estimates, task_weights
    )
    return policy_loss
```
### RLVR 인프라

RLVR의 효율적인 구현을 위해 연구진은 대규모 분산 학습 인프라를 구축했습니다. 이 인프라는 DeepSpeed와 Ray를 기반으로 하며, 특히 검증 가능한 보상의 계산과 분산 처리를 최적화하는데 중점을 두었습니다.

RLVR의 분산 학습 아키텍처는 다음과 같은 수식으로 표현되는 처리량 최적화를 목표로 합니다.

\\[ T = \frac{B \cdot N}{t_g + t_v + t_u} \\]

여기서 \\(T\\)는 초당 처리되는 토큰 수, \\(B\\)는 배치 크기, \\(N\\)은 GPU 노드 수, \\(t_g\\)는 생성 시간, \\(t_v\\)는 검증 시간, \\(t_u\\)는 모델 업데이트 시간을 나타냅니다.

연구진은 검증 보상 계산의 병목 현상을 해결하기 위해 비동기 처리 방식을 도입했습니다. 이는 다음과 같은 파이프라인 구조로 구현됩니다.

\\[ P(t) = \max\left(1, \left\lfloor\frac{t_v}{t_g}\right\rfloor\right) \\]

여기서 \\(P(t)\\)는 시간 \\(t\\)에서의 병렬 파이프라인 단계 수를 나타냅니다. 이러한 구조는 Ray의 분산 컴퓨팅 기능을 활용하여 구현되었으며, 다음과 같은 코드로 표현됩니다.

```python
@ray.remote(num_gpus=1)
class VerificationWorker:
    def __init__(self, model_config):
        self.answer_extraction_model = self.load_model(model_config)
        
    def verify_batch(self, responses, ground_truths):
        extracted_answers = self.extract_answers(responses)
        verification_results = self.compare_answers(
            extracted_answers, ground_truths
        )
        return verification_results
```

RLVR의 학습 안정성을 위해 연구진은 그래디언트 누적과 동적 배치 크기 조정을 도입했습니다. 이는 다음과 같은 수식으로 표현됩니다.

\\[ B_t = B_0 \cdot \min\left(1, \sqrt{\frac{M}{V_t}}\right) \\]

여기서 \\(B_t\\)는 시간 \\(t\\)에서의 배치 크기, \\(M\\)은 가용 메모리, \\(V_t\\)는 검증 연산의 메모리 요구량을 나타냅니다.

### 최종 실험 결과

RLVR의 최종 실험에서는 다양한 벤치마크에서 기존 방법론들과의 비교 평가가 이루어졌습니다. 특히 수학적 추론 과제에서 RLVR은 기존 PPO 기반 방법론 대비 평균 15-20%의 성능 향상을 달성했습니다.

성능 향상의 핵심 요인은 검증 가능한 보상의 정확성과 즉각성입니다. 이는 다음과 같은 메트릭으로 측정됩니다.

\\[ E = \frac{1}{N} \sum_{i=1}^N \frac{|R_{verify}(y_i, y_i^*) - R_{human}(y_i)|}{|R_{human}(y_i)|} \\]

여기서 \\(E\\)는 검증 보상의 오차율, \\(R_{human}\\)은 인간 평가자의 점수를 나타냅니다. RLVR은 이 메트릭에서 5% 미만의 오차율을 달성했습니다.

### TÜLU 3 평가 프레임워크

TÜLU 3의 평가 프레임워크는 오픈 언어 모델의 성능을 체계적이고 객관적으로 평가하기 위한 종합적인 시스템입니다. 이 프레임워크는 크게 세 가지 주요 구성 요소로 이루어져 있습니다. 오픈 언어 모델 평가 시스템(OLMES), 개발 평가 스위트, 그리고 미공개 평가 스위트입니다.

오픈 언어 모델 평가 시스템(OLMES)은 Gu와 연구진이 제안한 표준화된 평가 방법론으로, 언어 모델의 성능을 일관되고 의미 있게 비교할 수 있게 해줍니다. OLMES는 프롬프트 포맷팅, 퓨샷 예제 선택, 확률 정규화 등의 핵심적인 평가 요소들을 표준화하여, 모델 간 공정한 비교를 가능하게 합니다.

TÜLU 3의 개발 평가 스위트는 모델의 다양한 능력을 평가하기 위한 종합적인 테스트 세트입니다. 특히 안전성 평가에 중점을 두어, WildGuard와 같은 최신 안전성 평가 도구를 활용합니다. Han과 연구진이 개발한 WildGuard는 다중 작업 오픈 LLM 안전성 모더레이션 모델로, 다양한 유형의 유해한 프롬프트와 응답을 탐지할 수 있습니다.

미공개 평가 스위트는 모델의 일반화 능력을 평가하기 위한 새로운 벤치마크들로 구성되어 있습니다. 특히 주목할 만한 것은 IFEval-OOD와 HREF라는 두 가지 새로운 평가 방법론입니다. IFEval-OOD는 Zhou와 연구진이 제안한 방법으로, 모델의 지시사항 따르기 능력을 객관적으로 검증 가능한 방식으로 평가합니다. 이 평가는 다음과 같은 수식으로 표현됩니다.

\\[ V(I, R) = \sum_{i=1}^{n} w_i \cdot v_i(I, R) \\]

여기서 \\(V(I, R)\\)는 지시사항 \\(I\\)에 대한 응답 \\(R\\)의 검증 점수, \\(v_i\\)는 개별 검증 기준, \\(w_i\\)는 각 기준의 가중치를 나타냅니다.

이러한 종합적인 평가 프레임워크를 통해 TÜLU 3는 모델의 성능을 객관적이고 체계적으로 측정할 수 있게 되었으며, 이는 향후 언어 모델 개발의 투명성과 신뢰성을 높이는 데 기여할 것으로 기대됩니다.
TÜLU 3의 평가 프레임워크에서 미공개 평가 스위트는 특히 주목할 만한 혁신을 보여줍니다. HREF(Human Reference Evaluation Framework)는 Mazeika와 연구진이 제안한 새로운 평가 방법론으로, 모델의 응답을 인간 참조 응답과 비교하여 평가합니다. HREF는 다음과 같은 수식적 접근을 통해 응답의 품질을 정량화합니다.

\\[ H(r, R) = \alpha \cdot S(r, R) + (1-\alpha) \cdot C(r, R) \\]

여기서 \\(H(r, R)\\)는 모델의 응답 \\(r\\)과 인간 참조 응답 집합 \\(R\\)의 유사도 점수, \\(S(r, R)\\)는 의미적 유사도, \\(C(r, R)\\)는 문맥적 일관성을 나타냅니다.

미공개 평가 스위트에서의 성능 측정은 다음과 같은 복합 메트릭을 통해 이루어집니다.

\\[ M(m) = \frac{1}{|T|} \sum_{t \in T} w_t \cdot P_t(m) \\]

여기서 \\(M(m)\\)은 모델 \\(m\\)의 종합 성능 점수, \\(T\\)는 평가 작업의 집합, \\(w_t\\)는 각 작업의 가중치, \\(P_t(m)\\)은 작업 \\(t\\)에서의 성능을 나타냅니다.

연구진은 평가의 신뢰성을 보장하기 위해 Singh과 연구진이 제안한 ConTAM(Contamination Threshold Analysis Method)을 도입했습니다. 이 방법은 평가 데이터의 오염도를 다음과 같이 측정합니다.

\\[ C(D) = \max_{n \in N} \left\{ \frac{|\text{overlap}(D, T_n)|}{|D|} \right\} \\]

여기서 \\(C(D)\\)는 데이터셋 \\(D\\)의 오염도, \\(T_n\\)은 학습 데이터의 n-gram 집합, \\(|\text{overlap}(D, T_n)|\\)은 평가 데이터와 학습 데이터 간의 n-gram 중복을 나타냅니다.

이러한 평가 프레임워크는 scripts/data/sft_v1_v2/reformat_datasets.py에서 구현되어 있으며, 평가 결과의 자동화된 처리와 분석을 지원합니다. 특히 OLMES의 표준화된 평가 방식은 다양한 모델들 간의 공정한 비교를 가능하게 하며, 연구 커뮤니티에서 널리 활용될 수 있는 기반을 제공합니다.
TÜLU 3의 평가 프레임워크는 Wang과 연구진이 제안한 MMLU-Pro와 같은 최신 벤치마크들을 통합하여 더욱 강력한 평가 체계를 구축했습니다. MMLU-Pro는 기존 MMLU보다 더 도전적이고 복잡한 문제들을 포함하며, 특히 다단계 추론이 필요한 대학 수준의 문제들을 통해 모델의 능력을 평가합니다. 이러한 평가는 다음과 같은 성능 메트릭을 사용합니다.

\\[ P_{MMLU}(m) = \gamma \cdot A_d(m) + (1-\gamma) \cdot A_c(m) \\]

여기서 \\(P_{MMLU}(m)\\)은 모델 \\(m\\)의 MMLU-Pro 성능, \\(A_d(m)\\)는 직접 응답 정확도, \\(A_c(m)\\)는 체인오브소트(Chain-of-Thought) 추론을 통한 정확도를 나타냅니다.

평가 프레임워크의 또 다른 중요한 구성 요소는 Chiang과 연구진이 개발한 Chatbot Arena 방식의 평가입니다. 이 방법은 다음과 같은 선호도 점수 계산을 통해 모델의 성능을 평가합니다.

\\[ S(m_1, m_2) = \frac{1}{N} \sum_{i=1}^N \text{sign}(P_i(m_1) - P_i(m_2)) \\]

여기서 \\(S(m_1, m_2)\\)는 모델 \\(m_1\\)과 \\(m_2\\) 사이의 선호도 점수, \\(P_i(m)\\)는 프롬프트 \\(i\\)에 대한 모델 \\(m\\)의 성능을 나타냅니다.

특히 안전성 평가에서는 Röttger와 연구진이 제안한 XSTest 프레임워크를 활용하여 모델의 과도한 안전 행동(exaggerated safety)을 체계적으로 평가합니다. 이는 다음과 같은 메트릭으로 정량화됩니다.

\\[ E(m) = \frac{|R_s(m)|}{|S|} \cdot \frac{|R_u(m)|}{|U|} \\]

여기서 \\(E(m)\\)은 모델 \\(m\\)의 과도한 안전 점수, \\(R_s(m)\\)은 안전한 프롬프트에 대한 거부 응답의 집합, \\(R_u(m)\\)은 안전하지 않은 프롬프트에 대한 거부 응답의 집합, \\(S\\)와 \\(U\\)는 각각 안전한 프롬프트와 안전하지 않은 프롬프트의 전체 집합을 나타냅니다.

이러한 종합적인 평가 체계는 oe_eval/configs/task_suites.py에 구현되어 있으며, 다양한 평가 메트릭과 벤치마크들을 효율적으로 관리하고 실행할 수 있도록 설계되었습니다. 이를 통해 TÜLU 3는 모델의 성능을 다각도로 평가하고, 개선이 필요한 영역을 정확하게 식별할 수 있게 되었습니다.

### 논의 및 결론

TÜLU 3 연구를 통해 얻은 주요 통찰과 향후 연구 방향에 대해 살펴보겠습니다. 연구진은 포스트 트레이닝 과정에서 성공적이지 못했던 시도들로부터 중요한 교훈을 얻었습니다. 특히 Ivison과 연구진이 제안한 DPO(Direct Preference Optimization)와 PPO(Proximal Policy Optimization)의 비교 연구에서 발견된 내용이 주목할 만합니다. DPO는 오프라인 학습 방식에서 안정성이 떨어지는 반면, PPO는 온라인 샘플링과 KL 제약 조건을 통해 더 나은 성능을 보여주었습니다.

보상 모델의 크기와 품질이 모델 성능에 미치는 영향도 중요한 발견이었습니다. 70B 규모의 큰 보상 모델은 GSM8k와 같은 수학 문제 해결에서는 큰 성능 향상을 보였지만, 다른 일반적인 능력에서는 제한적인 개선만을 보였습니다. 이는 보상 모델의 성능 향상이 반드시 정책 모델의 성능 향상으로 이어지지 않는다는 중요한 시사점을 제공합니다.

Noukhovitch와 연구진이 제안한 비동기 RLHF 방식은 학습 효율성 측면에서 큰 진전을 이루었습니다. vllm과 같은 최적화된 추론 라이브러리를 활용하여 생성 과정을 가속화하고, 이전에 생성된 샘플들을 오프-폴리시로 학습에 활용함으로써 전체적인 학습 속도를 약 40% 향상시켰습니다.

향후 연구 방향으로는 크게 세 가지가 제시되었습니다. 첫째, 검증 가능한 보상을 통한 강화학습(RLVR)의 확장성 개선입니다. 현재의 RLVR은 수학적 추론이나 코드 생성과 같이 객관적인 정답이 존재하는 과제에서는 효과적이지만, 더 넓은 범위의 과제로 확장하기 위해서는 보상 검증 메커니즘의 개선이 필요합니다.

둘째, Han과 연구진이 개발한 WildGuard와 같은 안전성 평가 도구의 강화입니다. 현재의 안전성 평가는 13개의 세부적인 위험 카테고리를 다루고 있지만, 더 다양한 유형의 위험을 포괄하고 더 정교한 평가 메트릭을 개발할 필요가 있습니다.

마지막으로, Guo와 연구진이 제안한 온라인 AI 피드백(OAIF)과 같은 새로운 학습 방법론의 통합입니다. OAIF는 현재 정책에서 샘플링된 응답에 대해 LLM 어노테이터가 실시간으로 피드백을 제공하는 방식으로, 기존의 오프라인 선호도 학습 방식의 한계를 극복할 수 있는 잠재력을 보여주었습니다.

### 관련 연구

TÜLU 3의 연구는 포스트 트레이닝 기법의 진화와 검증 가능한 보상을 통한 학습이라는 두 가지 주요 영역에서 기존 연구들과 연관됩니다.

현대의 "포스트 트레이닝" 방식은 다중 작업 언어 모델 학습, 특히 지시 튜닝에서 그 기원을 찾을 수 있습니다. Mishra와 연구진이 제안한 초기의 지시 튜닝은 모델이 과제 지시사항과 그에 대한 응답을 학습하여 새로운 과제에 '제로샷' 방식으로 일반화할 수 있도록 했습니다. 초기 지시 튜닝 데이터셋들은 자연어 추론과 같은 전통적인 NLP 과제에 중점을 두었으나, 점차 사용자들이 실제로 수행하는 더 일반적인 과제들로 확장되었습니다.

ChatGPT와 같은 대화형 언어 모델의 등장과 함께, 포스트 트레이닝 기법은 지시 튜닝을 넘어 선호도 튜닝 단계를 포함하도록 발전했습니다. 초기의 RLHF(Reinforcement Learning from Human Feedback) 연구는 Christiano와 연구진의 심층 강화학습 제어 실험에서 시작되었습니다. 이들은 먼저 인간의 선호도로부터 보상 모델을 학습하고, 이를 통해 언어 모델을 RL 프레임워크로 최적화하는 방식을 제안했습니다.

최근에는 Rafailov와 연구진이 제안한 DPO(Direct Preference Optimization)와 같이 선호도 데이터에서 직접 언어 모델을 학습할 수 있는 방법들이 개발되었습니다. 이는 선호도 학습을 포스트 트레이닝 파이프라인에 통합하는 복잡성을 크게 줄였습니다. 초기의 선호도 학습 접근법들이 수만 개의 인간 작성 지시사항과 선호도 레이블에 의존했다면, 최근의 연구들은 인간과 합성 생성된 선호도 데이터의 혼합, 다중 라운드 학습, 다양한 학습 알고리즘을 활용하는 방향으로 발전했습니다.

폐쇄형 연구실에서 RLHF가 발전하는 동안, 오픈소스 포스트 트레이닝 레시피들은 다소 뒤처져 있었습니다. Taori와 연구진, Conover와 연구진 등이 시도한 초기의 '오픈 포스트 트레이닝 레시피' 연구들은 공개된 언어 모델을 합성 생성되거나 인간이 만든 데이터셋으로 지시 튜닝하는 데 중점을 두었습니다. Wang과 연구진의 연구에서는 이러한 데이터셋들을 결합하여 강력한 성능을 달성할 수 있음을 보여주었지만, Ivison과 연구진의 연구에 따르면 인간 평가 기반의 폐쇄형 모델들과의 격차를 줄이기 위해서는 선호도 튜닝 단계를 포함하는 것이 중요했습니다.

오늘날 대부분의 인기 있는 오픈소스 선호도 튜닝 모델들은 DPO나 그 변형을 사용하며, AI 피드백 데이터를 활용합니다. 여기에는 TÜLU 2, Zephyr-β, Starling 등이 포함됩니다. 그러나 이러한 모델들은 데이터와 성능 측면에서 폐쇄형 포스트 트레이닝 레시피들에 비해 뒤처져 있습니다. LMSYS의 ChatBotArena 상위 50위 안에 든 오픈 레시피 모델은 없으며, 대부분의 오픈 레시피들은 폐쇄형 포스트 트레이닝 설정에 비해 상대적으로 적은 데이터와 적은 수의 학습 라운드를 사용합니다.
예를 들어, Llama 3.1은 이전 모델의 생성 결과를 여러 라운드에 걸쳐 학습하고, 광범위한 인간 피드백 데이터를 활용하며, 강력한 모델을 사용하여 합성 지시사항을 작성하는 방식을 채택했습니다. 최근의 다른 발전으로는 합성 데이터에 대한 거부 샘플링과 단계별 어시스턴트 응답을 위한 고급 보상 모델링이 있습니다. 본 연구에서는 이러한 폐쇄형 레시피의 규모에 완전히 도달하지는 못했지만, 강력한 폐쇄형 레시피와 대등하거나 이를 능가하는 레시피를 구축하고 관련된 모든 산출물(코드, 모델, 데이터 등)을 공개하여 추가 과학적 연구와 활용을 가능하게 했습니다.

검증 가능한 보상을 통한 학습과 관련하여, TÜLU 3에서 제안된 RLVR 접근법은 최근의 여러 연구들과 관련이 있습니다. Zelikman과 연구진이 제안한 STaR(Self-taught Reasoner) 계열의 연구와 Hoffman과 연구진의 TRICE는 모두 기존의 정답을 신호로 활용하여 더 나은 모델의 추론 과정(또는 사고의 연쇄)을 생성하는 방법을 연구했습니다. STaR은 정책 그래디언트 알고리즘의 근사로 볼 수 있으며, Quiet-STaR은 이 접근법을 확장하여 모델이 추가 생성을 통해 일반적인 언어 모델링을 개선하도록 학습하는 방식('말하기 전에 생각하기')을 제안했습니다.

TRICE는 맞춤형 MCMC 기반 EM 알고리즘을 사용하여 여러 추론 경로에 걸쳐 학습함으로써 정답의 가능성을 높이는 것을 목표로 합니다. 최근에는 Kazemnejad와 연구진이 VinePPO를 통해 GSM8k와 MATH의 이진 보상을 사용하여 새로운 PPO 기반 알고리즘을 테스트했으며, 다른 연구들은 코드 피드백을 학습 신호로 활용하는 방법을 탐구했습니다.

이에 비해 TÜLU 3에서 제안하는 RLVR은 기존의 RL 프레임워크(PPO)를 그대로 사용하면서 이진 보상만을 활용하여 완전히 온라인으로 실행됩니다. 이는 STaR의 반복적 접근이나 Quiet-STaR의 로그 가능도 보상과는 다른 방식입니다. 또한 RLVR은 수학 영역을 넘어 정밀한 지시사항 따르기에서도 성능 향상을 이끌어낼 수 있음을 보여주었습니다. 연구진은 RLVR의 여러 핵심 구성 요소들을 주의 깊게 분석했으며, 특히 가치 모델 초기화와 일반 보상 모델과 검증 가능한 보상의 사용에 대해 체계적으로 연구했습니다. 이러한 방법론은 향후 연구를 통해 더욱 발전시킬 수 있을 것으로 기대됩니다.

- - -
### References
* [Tulu 3: Pushing Frontiers in Open Language Model Post-Training](http://arxiv.org/pdf/2411.15124v2)