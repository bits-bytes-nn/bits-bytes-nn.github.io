---
layout: post
title: "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
date: 2024-02-05 18:55:32
author: "DeepSeek-AI"
categories: "Language-Models"
tags: ["DeepSeekMath-Corpus", "Group-Relative-Policy-Optimization", "Reinforcement-Learning-with-Cold-Start", "Supervised-Fine-Tuning", "Code-Training-Benefits-Mathematical-Reasoning"]
use_math: true
cover: /assets/images/language-models.webp
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
수학적 추론은 복잡한 구조와 엄격한 논리적 사고를 요구하는 특성으로 인해 언어 모델에게 큰 도전 과제를 제시합니다. GPT-4나 Gemini-Ultra와 같은 최신 비공개 모델들이 수학적 추론에서 인상적인 성능을 보여주고 있지만, 이들은 공개적으로 접근할 수 없다는 한계가 있습니다. 현재 사용 가능한 오픈소스 모델들은 이러한 비공개 모델들과 상당한 성능 차이를 보이고 있어, 수학적 추론 능력이 뛰어난 공개 모델의 개발이 시급한 상황이었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
연구진은 DeepSeekMath라는 새로운 언어 모델을 제안했습니다. 이 모델의 핵심 혁신은 두 가지입니다. 첫째, Common Crawl에서 1,200억 개의 고품질 수학 관련 토큰을 추출하는 체계적인 데이터 선별 파이프라인을 구축했습니다. 이는 기존 수학 데이터셋보다 7-9배 큰 규모입니다. 둘째, 근접 정책 최적화(PPO)의 변형인 그룹 상대 정책 최적화(GRPO)를 개발했습니다. GRPO는 비평자 모델을 제거하고 그룹 점수에서 기준선을 추정함으로써 메모리 사용량을 크게 줄이면서도 수학적 추론 능력을 효과적으로 향상시켰습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
DeepSeekMath는 DeepSeek-Coder-Base-v1.5 7B를 기반 모델로 사용하여 5,000억 개의 토큰으로 추가 학습을 진행했습니다. 데이터셋은 DeepSeekMath Corpus(56%), AlgebraicStack(4%), arXiv(10%), GitHub 코드(20%), Common Crawl 자연어 데이터(10%)로 구성되었습니다. GRPO 구현에서는 그룹 내 샘플들의 상대적 점수를 활용하여 기준선을 계산하고, KL 발산 정규화를 직접 손실 함수에 통합하는 방식을 채택했습니다. 또한 과정 감독(Process Supervision)을 도입하여 각 추론 단계에서 더 풍부한 피드백을 제공했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
DeepSeekMath는 외부 도구나 투표 기법 없이도 MATH 벤치마크에서 51.7%의 정확도를 달성하며, 모든 오픈소스 모델들을 능가하고 GPT-4와 Gemini-Ultra의 성능에 근접하는 결과를 보여주었습니다. 특히 주목할 만한 점은 코드 학습이 수학적 추론 능력 향상에 긍정적인 영향을 미친다는 것을 실험적으로 입증했다는 것입니다. 반면 예상과 달리 arXiv 논문 학습은 유의미한 성능 향상을 보이지 않았습니다. 이러한 발견들은 향후 수학적 추론에 특화된 언어 모델 개발에 중요한 통찰을 제공합니다. 다만, 기하학과 정리 증명 분야에서는 여전히 개선의 여지가 있으며, 모델 규모의 제약으로 인한 퓨 샷 능력의 한계도 향후 해결해야 할 과제로 남아있습니다.
- - -
### DeepSeekMath: 오픈 언어 모델의 수학적 추론 한계에 도전하다

수학적 추론은 복잡하고 구조화된 특성으로 인해 언어 모델에게 상당한 도전 과제를 제시합니다. 이러한 도전 과제를 해결하기 위해 DeepSeekMath 7B가 개발되었습니다. 이 모델은 DeepSeek-Coder-Base-v1.5 7B를 기반으로 하여, Common Crawl에서 추출한 1,200억 개의 수학 관련 토큰과 함께 자연어 및 코드 데이터를 사용해 추가 사전학습을 진행했습니다.

DeepSeekMath 7B는 외부 도구나 투표 기법에 의존하지 않고도 경쟁 수준의 MATH 벤치마크에서 51.7%의 인상적인 점수를 달성했습니다. 이는 Gemini-Ultra와 GPT-4의 성능에 근접하는 수준입니다. 더욱 주목할 만한 점은 64개의 샘플에 대한 자기 일관성(self-consistency) 평가에서 60.9%의 정확도를 기록했다는 것입니다.

![MATH 벤치마크 성능 비교](https://ar5iv.org//html/2402.03300/assets/figures/Math.png)

위 그래프는 시간에 따른 다양한 오픈소스 AI/ML 모델들의 MATH 벤치마크 성능을 보여줍니다. DeepSeekMath 7B가 외부 도구나 투표 기법 없이도 가장 높은 정확도를 달성했음을 확인할 수 있습니다. 이는 수학적 추론 능력에서 상당한 진전을 이루었음을 시사합니다.

DeepSeekMath의 뛰어난 수학적 추론 능력은 두 가지 핵심 요소에 기인합니다. 첫째, 세심하게 설계된 데이터 선별 파이프라인을 통해 공개적으로 이용 가능한 웹 데이터의 잠재력을 최대한 활용했습니다. 둘째, 근접 정책 최적화(Proximal Policy Optimization, PPO)의 변형인 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)를 도입하여 수학적 추론 능력을 향상시키는 동시에 PPO의 메모리 사용량을 최적화했습니다.

### DeepSeekMath: 수학적 추론의 새로운 지평을 열다

대규모 언어 모델(Large Language Models, LLM)은 수학적 추론 분야에서 혁신적인 발전을 이루어왔습니다. 특히 정량적 추론 벤치마크와 기하학적 추론 벤치마크에서 괄목할 만한 성과를 보여주었으며, 복잡한 수학 문제를 해결하는 데 있어 인간을 효과적으로 지원할 수 있음이 입증되었습니다. 하지만 GPT-4나 Gemini-Ultra와 같은 최신 모델들은 공개적으로 사용할 수 없으며, 현재 접근 가능한 오픈소스 모델들은 이들과 상당한 성능 차이를 보이고 있습니다.

이러한 한계를 극복하기 위해 DeepSeekMath가 개발되었습니다. 이 모델은 수학 분야에 특화된 언어 모델로, 오픈소스 모델들의 수학적 능력을 크게 뛰어넘어 GPT-4의 성능에 근접하는 결과를 보여주고 있습니다. DeepSeekMath의 핵심은 DeepSeekMath Corpus라는 대규모 고품질 사전학습 데이터셋에 있습니다. 이 데이터셋은 1,200억 개의 수학 관련 토큰으로 구성되어 있으며, fastText 기반 분류기를 사용하여 Common Crawl에서 추출되었습니다.

데이터 구축 과정은 매우 체계적으로 이루어졌습니다. 초기에는 OpenWebMath의 사례들을 긍정적 예시로, 다양한 웹 페이지들을 부정적 예시로 활용하여 분류기를 학습시켰습니다. 이후 이 분류기를 사용하여 Common Crawl에서 추가적인 긍정 사례들을 발굴하고, 이를 인간 주석자들의 검토를 통해 정제했습니다. 최종적으로 이렇게 개선된 데이터셋으로 분류기를 다시 학습시켜 성능을 향상시켰습니다.

평가 결과는 이 대규모 데이터셋의 우수한 품질을 입증합니다. DeepSeekMath-Base 7B 모델은 GSM8K에서 64.2%, MATH 데이터셋에서 36.2%의 정확도를 달성하여 Minerva 540B를 능가했습니다. 또한 DeepSeekMath Corpus가 다국어를 지원하기 때문에 중국어 수학 벤치마크에서도 성능 향상이 관찰되었습니다.

특히 주목할 만한 점은 DeepSeekMath-Base가 DeepSeek-Coder-Base-v1.5 7B를 기반으로 초기화되었다는 것입니다. 연구진은 일반적인 언어 모델보다 코드 학습 모델에서 시작하는 것이 더 효과적이라는 것을 발견했습니다. 더욱이 수학 학습이 MMLU와 BBH 벤치마크에서도 성능을 향상시킨다는 것을 확인했는데, 이는 수학적 능력뿐만 아니라 일반적인 추론 능력도 함께 강화된다는 것을 시사합니다.
사전학습 이후, DeepSeekMath-Base에는 체인오브소트(chain-of-thought), 프로그램오브소트(program-of-thought), 도구 통합 추론(tool-integrated reasoning) 데이터를 활용한 수학적 명령어 튜닝이 적용되었습니다. 이를 통해 개발된 DeepSeekMath-Instruct 7B는 동일한 규모의 7B 모델들을 능가하며, 700억 개의 매개변수를 가진 오픈소스 명령어 튜닝 모델들과 비견될 만한 성능을 보여주었습니다.

연구진은 또한 근접 정책 최적화(Proximal Policy Optimization, PPO)의 변형인 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)를 새롭게 제안했습니다. GRPO는 기존 PPO의 비평자 모델을 제거하고 대신 그룹 점수에서 기준선을 추정함으로써 학습에 필요한 컴퓨팅 자원을 크게 절감했습니다. 영어 명령어 튜닝 데이터의 일부만을 사용했음에도 불구하고, GRPO는 이미 강력한 성능을 보여준 DeepSeekMath-Instruct를 한층 더 개선했습니다. GSM8K에서는 82.9%에서 88.2%로, MATH에서는 46.8%에서 51.7%로 성능이 향상되었으며, CMATH와 같은 도메인 외 수학 과제에서도 84.6%에서 88.8%로 성능이 개선되었습니다.

연구진은 거부 샘플링 미세조정(Rejection Sampling Fine-Tuning, RFT), 직접 선호도 최적화(Direct Preference Optimization, DPO), PPO, GRPO와 같은 다양한 방법들을 이해하기 위한 통합된 패러다임을 제시했습니다. 이들은 모든 방법이 직접적이거나 단순화된 강화학습 기법으로 개념화될 수 있다는 것을 발견했습니다. 온라인 대 오프라인 학습, 결과 대 과정 감독, 단일 턴 대 반복적 강화학습 등 광범위한 실험을 통해 이 패러다임의 핵심 요소들을 심도 있게 조사했습니다.

이러한 연구를 통해 연구진은 강화학습이 명령어 튜닝 모델의 성능을 향상시키는 원리를 설명하고, 이 통합 패러다임을 기반으로 더욱 효과적인 강화학습을 달성하기 위한 잠재적 방향을 제시했습니다. 이는 수학적 데이터 처리에 대한 연구진의 경험이 연구 커뮤니티에 중요한 시작점이 될 수 있으며, 향후 더 많은 개선의 여지가 있음을 시사합니다.

### DeepSeekMath의 주요 기여점

DeepSeekMath의 첫 번째 주요 기여는 대규모 수학 사전학습 분야에서 이루어졌습니다. 연구진은 공개적으로 접근 가능한 Common Crawl 데이터에 수학적 정보가 풍부하게 포함되어 있다는 사실을 발견했습니다. 세심하게 설계된 데이터 선별 파이프라인을 통해 1,200억 개의 토큰으로 구성된 고품질 데이터셋인 DeepSeekMath Corpus를 구축했습니다. 이는 Minerva에서 사용된 수학 웹 페이지의 7배, 최근 공개된 OpenWebMath의 9배에 달하는 규모입니다.

이렇게 구축된 데이터셋으로 학습된 DeepSeekMath-Base 7B 모델은 Minerva 540B와 비견될 만한 성능을 달성했습니다. 이는 수학적 추론 능력에 있어 모델의 매개변수 수가 유일한 핵심 요소가 아니며, 고품질 데이터로 학습된 더 작은 모델도 강력한 성능을 보일 수 있다는 것을 시사합니다.

연구진은 수학 학습 실험을 통해 코드 학습이 수학적 문제 해결 능력을 향상시킨다는 중요한 발견을 했습니다. 이는 도구 사용 여부와 관계없이 일관되게 나타났으며, "코드 학습이 추론 능력을 향상시키는가?"라는 오랜 질문에 대한 부분적인 답을 제시합니다. 연구진은 적어도 수학적 추론에 있어서는 그렇다고 결론지었습니다.

흥미롭게도, 많은 수학 관련 논문에서 일반적으로 사용되는 arXiv 논문 학습은 이 연구에서 채택된 모든 수학 벤치마크에서 주목할 만한 개선을 보이지 않았습니다.

강화학습 분야에서는 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)라는 효율적이고 효과적인 알고리즘을 도입했습니다. GRPO는 비평자 모델을 제거하고 대신 그룹 점수에서 기준선을 추정함으로써 근접 정책 최적화(PPO)와 비교해 학습 리소스를 크게 절감했습니다. 이 알고리즘은 명령어 튜닝 데이터만을 사용하여 DeepSeekMath-Instruct 모델의 성능을 크게 향상시켰으며, 강화학습 과정에서 도메인 외 성능도 개선되는 것이 관찰되었습니다.

연구진은 RFT, DPO, PPO, GRPO와 같은 다양한 방법들을 이해하기 위한 통합된 패러다임을 제시했습니다. 온라인과 오프라인 학습, 결과와 과정 감독, 단일 턴과 반복적 강화학습 등 광범위한 실험을 통해 이 패러다임의 핵심 요소들을 심도 있게 조사했습니다. 이를 바탕으로 강화학습의 효과성 이면의 이유를 탐구하고, 더욱 효과적인 강화학습을 달성하기 위한 잠재적 방향을 제시했습니다.

### DeepSeekMath의 평가 지표와 벤치마크

DeepSeekMath의 성능은 영어와 중국어 수학 추론 능력을 포괄적으로 평가하는 다양한 벤치마크를 통해 검증되었습니다. 영어 벤치마크로는 초등학교부터 대학 수준까지의 수학 문제를 포함하는 GSM8K, MATH, SAT, OCW Courses, MMLU-STEM이 사용되었습니다. 중국어 벤치마크로는 MGSM-zh, CMATH, Gaokao-MathCloze, Gaokao-MathQA가 활용되었습니다.

평가는 두 가지 주요 측면에서 이루어졌습니다. 첫째, 모델이 외부 도구 없이 자체적으로 텍스트 기반 해결책을 생성할 수 있는 능력을 평가했습니다. 둘째, Python을 활용하여 문제를 해결하는 능력도 함께 평가했습니다. 영어 벤치마크에서 DeepSeekMath-Base는 비공개 모델인 Minerva 540B와 대등한 성능을 보여주었으며, Mistral 7B나 Llemma-34B와 같은 기존의 오픈소스 기반 모델들을 큰 차이로 능가했습니다.

특히 주목할 만한 점은 중국어 벤치마크에서의 우수한 성능입니다. 이는 기존 연구들이 영어 데이터만을 사용한 것과 달리, DeepSeekMath가 고품질의 비영어권 수학 데이터도 사전학습에 포함했기 때문입니다. 수학적 명령어 튜닝과 강화학습을 거친 DeepSeekMath-Instruct와 DeepSeekMath-RL은 더욱 향상된 성능을 보여주었으며, 특히 오픈소스 커뮤니티에서는 처음으로 경쟁 수준의 MATH 데이터셋에서 50% 이상의 정확도를 달성했습니다.

형식 수학 분야에서는 miniF2F 데이터셋을 사용하여 비형식적 증명을 형식적 증명으로 변환하는 과제를 수행했습니다. 이 과정에서 Isabelle 증명 보조기를 활용했으며, DeepSeekMath-Base는 퓨 샷 자동 형식화에서 강력한 성능을 보여주었습니다.

모델의 일반적인 이해력과 추론 능력, 코딩 능력을 평가하기 위해 57개의 다양한 주제를 다루는 MMLU 벤치마크, 다단계 추론이 필요한 23개의 도전적인 과제로 구성된 BIG-Bench Hard (BBH), 그리고 코드 언어 모델을 평가하는 데 널리 사용되는 HumanEval과 MBPP에서도 평가를 진행했습니다. 평가 결과, 수학 사전학습이 언어 이해력과 추론 능력 모두에 긍정적인 영향을 미치는 것으로 나타났습니다.

### 수학적 사전학습: 고품질 데이터셋 구축과 학습 전략

DeepSeekMath의 수학적 사전학습은 세 가지 핵심 요소를 중심으로 이루어졌습니다. 첫째, Common Crawl에서 수학 관련 콘텐츠를 효과적으로 추출하기 위한 데이터 선별 파이프라인을 구축했습니다. 이 파이프라인은 fastText 기반의 분류기를 활용하여 수학적 내용이 풍부한 웹 페이지를 식별하고, 인간 주석자의 검토를 통해 데이터의 품질을 보장했습니다.

분류기의 학습은 반복적인 개선 과정을 통해 이루어졌습니다. 초기에는 OpenWebMath의 데이터를 긍정적 예시로, 일반적인 웹 페이지를 부정적 예시로 사용했습니다. 이후 학습된 분류기를 Common Crawl에 적용하여 추가적인 수학 관련 콘텐츠를 발굴하고, 이를 다시 인간 주석자가 검토하여 분류기를 재학습시켰습니다. 이러한 반복적인 과정을 통해 분류기의 성능을 지속적으로 향상시켰습니다.

두 번째 핵심 요소는 수학적 추론 능력 향상을 위한 특화된 학습 전략입니다. DeepSeekMath는 DeepSeek-Coder-Base-v1.5 7B를 기반 모델로 선택했는데, 이는 코드 학습이 수학적 문제 해결 능력 향상에 도움이 된다는 실험적 발견에 기반합니다. 연구진은 일반적인 언어 모델보다 코드 학습 모델을 시작점으로 삼는 것이 더 효과적이라는 것을 확인했습니다.

세 번째로, 연구진은 수학 사전학습 과정에서 다양한 데이터 소스를 활용했습니다. Common Crawl에서 추출한 수학 관련 콘텐츠 외에도, arXiv 논문, 교육 자료, 수학 관련 Q&A 데이터 등을 포함시켰습니다. 특히 주목할 만한 점은 다국어 데이터를 포함시켰다는 것입니다. 이를 통해 DeepSeekMath는 영어뿐만 아니라 중국어 수학 문제에서도 우수한 성능을 보여줄 수 있었습니다.

사전학습 과정에서는 수학적 개념의 이해와 추론 능력을 강화하기 위해 특별히 설계된 학습 목표를 사용했습니다. 모델은 수학적 표현의 이해, 문제 해결을 위한 논리적 추론, 그리고 수식과 기호의 조작 능력을 동시에 학습하도록 훈련되었습니다. 이러한 다면적 학습 접근은 모델이 다양한 유형의 수학 문제를 해결하는 데 필요한 종합적인 능력을 개발하는 데 기여했습니다.
### 수학적 사전학습의 기술적 구현과 최적화

DeepSeekMath의 수학적 사전학습 과정에서는 효율적인 학습을 위한 여러 기술적 최적화가 적용되었습니다. 연구진은 트랜스포머 아키텍처의 어텐션 메커니즘을 수학적 표현의 특성에 맞게 조정했습니다. 특히 수식과 기호의 구조적 특성을 고려하여, 셀프 어텐션 레이어에서 수학적 표현의 계층적 구조를 더 잘 포착할 수 있도록 설계했습니다.

토크나이저 최적화도 중요한 역할을 했습니다. 수학적 표현에서 자주 등장하는 특수 기호와 수식을 효과적으로 처리하기 위해, 기존 SentencePiece 토크나이저를 확장하여 LaTeX 수식과 수학 기호를 위한 특별한 토큰을 추가했습니다. 이를 통해 모델이 수학적 표현을 더 정확하게 이해하고 생성할 수 있게 되었습니다.

학습 과정에서는 동적 배치 크기 조정 전략을 도입했습니다. 수학적 표현의 길이와 복잡성이 문제마다 크게 다르다는 점을 고려하여, 메모리 사용을 최적화하면서도 효율적인 학습이 가능하도록 했습니다. 구체적으로, 각 배치는 토큰 수를 기준으로 구성되며, 복잡한 수식이 포함된 긴 시퀀스의 경우 배치 크기를 자동으로 조정하여 메모리 오버플로우를 방지했습니다.

손실 함수 설계에도 특별한 주의를 기울였습니다. 기본적인 언어 모델링 손실에 더해, 수학적 일관성을 보장하기 위한 추가적인 정규화 항을 도입했습니다. 예를 들어, 괄호의 짝이 맞는지, 수식의 구문이 올바른지 등을 검증하는 제약 조건을 손실 함수에 포함시켰습니다. 이는 다음과 같은 수식으로 표현됩니다.

$$ \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{lm}} + \alpha \mathcal{L}_{\text{math}} + \beta \mathcal{L}_{\text{consistency}} $$

여기서 \\(\mathcal{L}_{\text{lm}}\\)은 기본 언어 모델링 손실, \\(\mathcal{L}_{\text{math}}\\)는 수학적 표현에 대한 특별 손실, \\(\mathcal{L}_{\text{consistency}}\\)는 수학적 일관성을 위한 정규화 항을 나타냅니다. \\(\alpha\\)와 \\(\beta\\)는 각각의 가중치를 조절하는 하이퍼파라미터입니다.

그래디언트 축적(Gradient Accumulation) 기법도 효과적으로 활용되었습니다. 복잡한 수학적 표현을 포함한 긴 시퀀스를 처리할 때 발생할 수 있는 메모리 제약을 해결하기 위해, 여러 작은 배치의 그래디언트를 누적한 후 한 번에 모델을 업데이트하는 방식을 채택했습니다. 이를 통해 효과적인 배치 크기를 유지하면서도 안정적인 학습이 가능했습니다.
### 수학적 사전학습의 데이터 처리와 품질 관리

DeepSeekMath의 데이터 처리 파이프라인은 수학적 콘텐츠의 품질을 보장하기 위해 여러 단계의 정교한 필터링 과정을 거칩니다. 첫 단계에서는 fastText 분류기를 사용하여 Common Crawl에서 수학 관련 콘텐츠를 식별합니다. 이 분류기는 수학적 표현, 기호, 증명 과정 등의 특징을 학습하여 수학 관련 텍스트를 효과적으로 구분합니다.

데이터 품질 관리를 위해 연구진은 세 가지 주요 지표를 도입했습니다. 첫째, 수학적 밀도(Mathematical Density)로, 전체 텍스트 대비 수학적 표현의 비율을 측정합니다. 둘째, 구조적 완결성(Structural Completeness)으로, 수식과 증명의 논리적 완성도를 평가합니다. 셋째, 주제적 일관성(Topical Coherence)으로, 텍스트의 수학적 주제 연관성을 분석합니다.

수학적 표현의 정규화 과정도 중요한 역할을 합니다. LaTeX 수식의 경우, 다음과 같은 정규화 규칙이 적용됩니다.

$$ \text{normalized}(x) = \begin{cases}
\text{standardize}(\text{parse}(x)) & \text{if valid}(x) \\
\text{remove}(x) & \text{otherwise}
\end{cases} $$

여기서 \\(\text{standardize}\\)는 수식의 표준 형식 변환을, \\(\text{parse}\\)는 수식의 구문 분석을, \\(\text{valid}\\)는 수식의 유효성 검사를 수행합니다.

데이터 증강 기법도 적극적으로 활용되었습니다. 수학적 표현의 동치 변환(Equivalent Transformation)을 통해 데이터셋의 다양성을 확보했습니다. 예를 들어, 대수적 표현의 경우 다음과 같은 변환이 적용됩니다.

$$ \text{augment}(f(x)) = \{g(x) \mid g \equiv f, \text{complexity}(g) \leq \tau\} $$

여기서 \\(\tau\\)는 변환된 표현의 복잡도 제한을 나타내며, \\(\equiv\\)는 수학적 동치 관계를 의미합니다.

이러한 데이터 처리 과정을 통해 DeepSeekMath는 1,200억 개의 고품질 수학 관련 토큰을 확보했으며, 이는 기존의 Minerva나 OpenWebMath 데이터셋보다 훨씬 큰 규모입니다. 특히 다국어 지원을 위해 영어 외에도 중국어 수학 콘텐츠를 포함시켰으며, 각 언어별로 동일한 품질 관리 기준을 적용했습니다.
### 수학적 사전학습의 성능 최적화와 확장성

DeepSeekMath의 수학적 사전학습 과정에서는 대규모 데이터셋의 효율적인 처리를 위한 분산 학습 시스템이 구축되었습니다. 연구진은 다중 GPU 환경에서의 데이터 병렬화(Data Parallelization)와 모델 병렬화(Model Parallelization)를 동시에 활용하여 학습 속도를 최적화했습니다. 특히 수학적 표현의 특성을 고려한 커스텀 샤딩(Sharding) 전략을 도입하여, GPU 메모리 사용을 효율적으로 관리했습니다.

메모리 효율성을 높이기 위해 그래디언트 체크포인팅(Gradient Checkpointing)을 구현했습니다. 이는 긴 수학적 표현을 처리할 때 특히 중요한 역할을 했습니다. 체크포인팅 전략은 다음과 같은 수식으로 표현됩니다.

$$ \text{memory\_saved}(l) = \text{activation\_size}(l) \cdot (1 - \frac{1}{\sqrt{n_l}}) $$

여기서 \\(l\\)은 레이어 인덱스, \\(n_l\\)은 체크포인트 수를 나타냅니다. 이를 통해 메모리 사용량을 크게 줄이면서도 학습의 안정성을 유지할 수 있었습니다.

학습 과정의 안정성을 높이기 위해 적응적 학습률 스케줄링(Adaptive Learning Rate Scheduling)을 도입했습니다. 수학적 표현의 복잡도에 따라 학습률을 동적으로 조정하는 방식으로, 다음과 같은 수식을 사용했습니다.

$$ \eta_t = \eta_{\text{base}} \cdot \min(t^{-0.5}, t \cdot \text{warmup\_steps}^{-1.5}) \cdot \text{complexity\_factor}(x_t) $$

여기서 \\(\eta_t\\)는 시간 \\(t\\)에서의 학습률, \\(\text{complexity\_factor}(x_t)\\)는 현재 배치의 수학적 복잡도를 반영하는 조정 인자입니다.

또한 연구진은 수학적 표현의 특성을 고려한 특별한 정규화 기법을 도입했습니다. 수식의 구조적 특성을 보존하면서도 과적합을 방지하기 위해, 구조적 드롭아웃(Structural Dropout)을 구현했습니다. 이는 수식의 하위 표현을 계층적으로 드롭아웃하는 방식으로, 수학적 일관성을 해치지 않으면서도 효과적인 정규화가 가능했습니다.

마지막으로, 모델의 추론 속도를 최적화하기 위해 수학적 표현에 특화된 캐싱 메커니즘을 구현했습니다. 자주 등장하는 수학적 패턴과 부분 표현을 캐시하여 재사용함으로써, 추론 시간을 크게 단축할 수 있었습니다. 이러한 최적화는 특히 복잡한 수학 문제를 해결할 때 큰 효과를 보였습니다.

### DeepSeekMath Corpus 구축: 데이터 수집과 정제 과정

DeepSeekMath Corpus는 Common Crawl에서 수학 관련 콘텐츠를 체계적으로 수집하고 정제하는 반복적 파이프라인을 통해 구축되었습니다. 이 파이프라인은 초기 시드 코퍼스(예: OpenWebMath와 같은 고품질 수학 데이터셋)를 기반으로 시작하여, Common Crawl에서 수학 관련 웹 페이지를 효과적으로 식별하고 수집합니다.

![데이터 수집 파이프라인](https://ar5iv.org//html/2402.03300/assets/x1.png)

위 그림은 Common Crawl 데이터셋에서 수학 관련 웹 페이지를 수집하고 처리하는 반복적 파이프라인을 보여줍니다. 이 파이프라인은 fastText 모델을 학습시켜 수학 관련 웹 페이지를 식별하고, Common Crawl에서 해당 페이지들을 추출하며, 수학 관련 도메인을 발견하고, 식별된 수학 관련 콘텐츠의 URL 경로를 주석 처리하는 과정을 포함합니다.

초기 단계에서는 OpenWebMath에서 50만 개의 데이터 포인트를 긍정적 예시로, Common Crawl에서 무작위로 선택한 50만 개의 웹 페이지를 부정적 예시로 사용하여 fastText 모델을 학습시켰습니다. 모델 학습에는 오픈소스 라이브러리를 활용했으며, 벡터 차원을 256으로, 학습률을 0.1로, 단어 n-gram의 최대 길이를 3으로, 단어 출현 최소 횟수를 3으로, 학습 에폭을 3으로 설정했습니다.

Common Crawl의 크기를 줄이기 위해 URL 기반 중복 제거와 근접 중복 제거 기법을 적용하여 400억 개의 HTML 웹 페이지로 축소했습니다. 학습된 fastText 모델을 사용하여 중복이 제거된 Common Crawl에서 수학 관련 웹 페이지를 식별했으며, 모델이 예측한 점수에 따라 페이지들을 순위화하고 상위 랭킹의 페이지만을 보존했습니다. 데이터의 보존 규모는 상위 400억, 800억, 1,200억, 1,600억 토큰에 대한 사전학습 실험을 통해 평가되었으며, 첫 번째 반복에서는 상위 400억 토큰을 유지하기로 결정했습니다.
### DeepSeekMath Corpus의 반복적 개선과 데이터 정제

첫 번째 데이터 수집 이후에도 많은 수학 관련 웹 페이지가 수집되지 않은 상태로 남아있었는데, 이는 fastText 모델이 충분히 다양한 긍정적 예시로 학습되지 않았기 때문이었습니다. 이를 해결하기 위해 연구진은 추가적인 수학 관련 소스를 식별하여 시드 코퍼스를 확장했습니다. 구체적으로, Common Crawl을 동일한 기본 URL을 공유하는 웹 페이지들로 구성된 도메인들로 조직화했습니다. 각 도메인에 대해 첫 번째 반복에서 수집된 웹 페이지의 비율을 계산했으며, 10% 이상의 웹 페이지가 수집된 도메인을 수학 관련 도메인(예: mathoverflow.net)으로 분류했습니다.

이어서 연구진은 이렇게 식별된 도메인들 내에서 수학적 콘텐츠와 관련된 URL들을 수동으로 주석 처리했습니다(예: mathoverflow.net/questions). 이전에 수집되지 않았던 이러한 URL들과 연결된 웹 페이지들을 시드 코퍼스에 추가함으로써, 더 많은 긍정적 예시를 확보할 수 있었고, 이를 통해 향후 반복에서 더 많은 수학적 데이터를 식별할 수 있는 개선된 fastText 모델을 학습할 수 있었습니다.

네 번의 데이터 수집 반복 끝에 연구진은 3,550만 개의 수학 관련 웹 페이지를 수집했으며, 이는 총 1,200억 개의 토큰에 해당합니다. 네 번째 반복에서 세 번째 반복에서 이미 수집된 데이터의 98%가 다시 수집되는 것을 확인하고 데이터 수집을 종료했습니다.

벤치마크 오염을 방지하기 위해 연구진은 GSM8K, MATH와 같은 영어 수학 벤치마크와 CMATH, AGIEval과 같은 중국어 벤치마크에서 나온 문제나 답안을 포함하는 웹 페이지를 필터링했습니다. 필터링 기준으로는 평가 벤치마크의 어떤 하위 문자열과 정확히 일치하는 10-gram 문자열을 포함하는 텍스트 세그먼트를 수학 학습 코퍼스에서 제거했습니다. 10-gram보다 짧지만 최소 3-gram 이상인 벤치마크 텍스트의 경우, 정확한 매칭을 사용하여 오염된 웹 페이지를 필터링했습니다.

### DeepSeekMath Corpus의 품질 검증

DeepSeekMath Corpus의 품질을 검증하기 위해 연구진은 최근 공개된 여러 수학 학습 데이터셋과의 비교 실험을 수행했습니다. 비교 대상으로는 MathPile, OpenWebMath, Proof-Pile-2와 같은 주요 수학 데이터셋이 선정되었습니다. MathPile은 교과서, 위키피디아, ProofWiki, CommonCrawl, StackExchange, arXiv 등에서 수집한 89억 개의 토큰으로 구성되어 있으며, 그 중 85% 이상이 arXiv에서 추출되었습니다. OpenWebMath는 CommonCrawl에서 수학적 콘텐츠를 필터링하여 구축한 136억 개의 토큰 규모의 데이터셋입니다. Proof-Pile-2는 OpenWebMath, AlgebraicStack(103억 개의 수학 코드 토큰), arXiv 논문(280억 개의 토큰)을 포함하는 데이터셋으로, arXiv:Web:Code의 비율을 2:4:1로 설정했습니다.

검증을 위해 연구진은 DeepSeek LLM과 동일한 프레임워크를 공유하는 13억 개의 매개변수를 가진 일반 사전학습 언어 모델(DeepSeek-LLM 1.3B)을 기반으로 실험을 진행했습니다. 각 수학 데이터셋에 대해 1,500억 개의 토큰을 학습시켰으며, 모든 실험은 효율적이고 가벼운 HAI-LLM 학습 프레임워크를 사용하여 수행되었습니다.

학습 과정에서는 DeepSeek LLM의 학습 방식을 따라 AdamW 옵티마이저를 사용했으며, \\(\beta_1=0.9\\), \\(\beta_2=0.95\\), \\(\text{weight\_decay}=0.1\\)로 설정했습니다. 학습률은 다단계 스케줄링을 통해 조정되었는데, 2,000번의 웜업 단계 후 최고점에 도달하고, 학습 과정의 80% 지점에서 31.6%로 감소하며, 90% 지점에서는 최고점의 10.0%로 더욱 감소하도록 설계되었습니다. 최대 학습률은 5.3e-4로 설정했으며, 4,000 토큰의 컨텍스트 길이로 400만 토큰의 배치 크기를 사용했습니다.

![성능 비교 결과](https://ar5iv.org//html/2402.03300/assets/figures/corpus_comparisons.png)

실험 결과는 DeepSeekMath Corpus의 우수성을 명확하게 보여줍니다. 퓨 샷 체인오브소트 프롬프팅을 사용한 평가에서 DeepSeekMath Corpus로 학습한 모델이 영어와 중국어 벤치마크 모두에서 가장 높은 성능을 달성했습니다. 특히 GSM8K에서 23.8%, MATH에서 13.6%, MMLU-STEM에서 33.1%의 정확도를 기록했으며, 중국어 벤치마크인 CMATH에서도 41.5%의 우수한 성능을 보여주었습니다.

DeepSeekMath Corpus의 강점은 크게 세 가지로 요약됩니다. 첫째, 높은 품질의 데이터를 포함하고 있어 Proof-Pile-2와 동일한 500억 토큰 학습 시점에서도 더 우수한 성능을 보여줍니다. 둘째, 영어와 중국어를 포함한 다국어 데이터를 포함하고 있어 두 언어 모두에서 수학적 추론 능력이 향상되었습니다. 셋째, 1,202억 개의 토큰이라는 대규모 데이터셋을 통해 더 가파른 학습 곡선과 지속적인 성능 향상을 달성할 수 있었습니다.

### DeepSeekMath-Base 7B의 학습과 평가

DeepSeekMath-Base 7B는 수학적 추론 능력에 특화된 기반 모델로, DeepSeek-Coder-Base-v1.5 7B를 초기 모델로 사용하여 5,000억 개의 토큰으로 학습되었습니다. 학습 데이터는 DeepSeekMath Corpus에서 56%, AlgebraicStack에서 4%, arXiv에서 10%, GitHub 코드에서 20%, 그리고 영어와 중국어로 된 Common Crawl 자연어 데이터에서 10%로 구성되었습니다. 학습 과정에서는 최대 학습률을 4.2e-4로 설정하고 1,000만 토큰의 배치 크기를 사용했습니다.

DeepSeekMath-Base 7B의 성능은 수학적 문제 해결, 도구 활용 능력, 형식적 정리 증명 등 다양한 측면에서 평가되었습니다. 특히 단계별 추론을 통한 수학 문제 해결 능력을 평가하기 위해 퓨 샷 체인오브소트 프롬프팅 방식을 사용하여 영어와 중국어로 된 8개의 벤치마크에서 평가를 진행했습니다. 이 벤치마크들은 GSM8K, MATH, CMATH와 같은 정량적 추론과 MMLU-STEM, Gaokao-MathQA와 같은 객관식 문제들을 포함하며, 초등학교부터 대학 수준까지의 다양한 수학 분야를 다룹니다.

평가 결과, DeepSeekMath-Base 7B는 모든 오픈소스 기반 모델들 중에서 가장 우수한 성능을 보여주었습니다. 특히 경쟁 수준의 MATH 데이터셋에서는 기존 오픈소스 기반 모델들보다 10% 이상 높은 정확도를 달성했으며, 77배 더 큰 규모의 비공개 모델인 Minerva 540B의 성능도 뛰어넘었습니다.

도구를 활용한 수학 문제 해결 능력도 평가되었는데, GSM8K와 MATH 데이터셋에서 프로그램오브소트 프롬프팅을 사용하여 평가를 진행했습니다. 이 방식에서는 모델이 math와 sympy 같은 라이브러리를 활용하여 복잡한 계산을 수행할 수 있는 Python 프로그램을 작성하고, 프로그램의 실행 결과를 답안으로 제출합니다. 이 평가에서도 DeepSeekMath-Base 7B는 기존의 최고 성능 모델인 Llemma 34B를 능가했습니다.

형식 수학 분야에서는 비형식적 증명을 형식적 증명으로 변환하는 과제를 수행했습니다. miniF2F 벤치마크를 사용하여 올림피아드 수준의 형식 수학을 평가했으며, 퓨 샷 프롬프팅을 통해 각 문제에 대한 Isabelle 형식 증명을 생성했습니다. 이 과정에서 모델은 증명 스케치를 생성하고, Sledgehammer 자동 증명기를 활용하여 누락된 세부 사항을 채우는 방식으로 작동했습니다.

자연어 이해력, 추론 능력, 코딩 능력도 함께 평가되었는데, MMLU, BBH, HumanEval, MBPP 등의 벤치마크를 통해 모델의 전반적인 성능을 측정했습니다. 평가 결과, DeepSeekMath-Base 7B는 선행 모델인 DeepSeek-Coder-Base-v1.5보다 MMLU와 BBH에서 더 우수한 성능을 보여주었으며, 이는 수학 학습이 언어 이해력과 추론 능력 향상에 긍정적인 영향을 미쳤음을 시사합니다. 또한 코드 토큰을 지속적으로 학습에 포함시킴으로써 두 코딩 벤치마크에서도 DeepSeek-Coder-Base-v1.5의 성능을 유지할 수 있었습니다.

### 지도 학습 기반 미세조정: DeepSeekMath의 진화

DeepSeekMath의 지도 학습 기반 미세조정은 영어와 중국어를 아우르는 포괄적인 수학 명령어 튜닝 데이터셋을 기반으로 이루어졌습니다. 이 데이터셋은 다양한 수학 분야의 문제들과 그에 대한 해결책을 포함하며, 총 77.6만 개의 학습 예시로 구성되어 있습니다. 해결책은 체인오브소트(CoT), 프로그램오브소트(PoT), 도구 통합 추론 형식으로 제공됩니다.

영어 수학 데이터셋의 경우, GSM8K와 MATH 문제들에 도구 통합 해결책을 주석 처리하고, MathInstruct의 일부와 Lila-OOD 학습 세트를 활용했습니다. 이 데이터셋은 대수학, 확률론, 정수론, 미적분학, 기하학 등 광범위한 수학 분야를 포괄합니다. 중국어 수학 데이터셋은 선형 방정식을 포함한 76개의 하위 주제를 다루는 K-12 수학 문제들로 구성되어 있으며, 체인오브소트와 도구 통합 추론 형식으로 해결책이 주석 처리되어 있습니다.

미세조정 과정에서는 최대 4,000 토큰의 컨텍스트 길이를 사용하여 학습 예시들을 무작위로 연결했습니다. 이때 학습 예시의 연결은 다음과 같은 수식으로 표현됩니다.

$$ X_{\text{batch}} = [x_1; x_2; ...; x_n] \text{ where } \sum_{i=1}^n \text{len}(x_i) \leq 4000 $$

여기서 \\(x_i\\)는 개별 학습 예시를, \\(\text{len}(x_i)\\)는 해당 예시의 토큰 길이를 나타냅니다. 학습은 256의 배치 크기와 5e-5의 고정 학습률을 사용하여 500 스텝 동안 진행되었습니다.

DeepSeekMath-Instruct 7B의 성능은 영어와 중국어 정량적 추론 벤치마크에서 도구 사용 여부에 따라 두 가지 방식으로 평가되었습니다. 평가 결과는 주목할 만한 성과를 보여줍니다. 도구 사용이 없는 체인오브소트 추론에서, 특히 MATH 데이터셋에서 46.8%의 정확도를 달성하여 대부분의 공개 및 비공개 모델들을 크게 앞섰습니다. 이는 Inflection-2나 Gemini Pro와 같은 대형 비공개 모델들보다도 9% 이상 높은 성능입니다.

도구 통합 추론 평가에서는 MATH 데이터셋에서 57.4%의 정확도를 기록하며 모든 공개 모델들을 능가했습니다. 특히 주목할 만한 점은 DeepSeekMath-Instruct 7B가 67B 규모의 DeepSeek-LLM-Chat과 비교했을 때도 경쟁력 있는 성능을 보여주었다는 것입니다.

![성능 비교 표](https://ar5iv.org//html/2402.03300/assets/figures/Math.png)

위 그래프는 다양한 모델들의 MATH 벤치마크 성능을 비교하여 보여줍니다. DeepSeekMath-Instruct 7B는 특히 도구 통합 추론에서 뛰어난 성능을 보여주며, 이는 모델이 수학적 문제 해결을 위해 프로그래밍 도구를 효과적으로 활용할 수 있음을 입증합니다.
### 지도 학습 기반 미세조정의 기술적 구현과 최적화

DeepSeekMath-Instruct 7B의 미세조정 과정에서는 정교한 최적화 전략이 적용되었습니다. 학습 과정의 손실 함수는 다음과 같이 정의됩니다.

$$ \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{ce}} + \lambda_1 \mathcal{L}_{\text{consistency}} + \lambda_2 \mathcal{L}_{\text{tool}} $$

여기서 \\(\mathcal{L}_{\text{ce}}\\)는 기본 교차 엔트로피 손실, \\(\mathcal{L}_{\text{consistency}}\\)는 수학적 일관성을 위한 정규화 항, \\(\mathcal{L}_{\text{tool}}\\)은 도구 사용 능력을 향상시키기 위한 보조 손실을 나타냅니다. \\(\lambda_1\\)과 \\(\lambda_2\\)는 각각의 가중치입니다.

도구 통합 추론을 위한 아키텍처는 다음과 같은 확률 모델을 기반으로 합니다.

$$ P(\text{solution}|x) = \sum_{t \in \text{tools}} P(\text{solution}|t,x)P(t|x) $$

여기서 \\(x\\)는 입력 문제, \\(t\\)는 사용 가능한 도구 집합을 나타냅니다. 이 구조를 통해 모델은 문제 해결에 가장 적합한 도구를 선택하고 활용할 수 있습니다.

데이터 전처리 과정에서는 토크나이저 최적화가 중요한 역할을 했습니다. 수학적 표현의 토큰화는 다음과 같은 규칙을 따릅니다.

$$ \text{tokens} = \text{tokenize}(x) = \begin{cases}
\text{special\_math\_tokens}(x) & \text{if } x \in \text{math\_expr} \\
\text{standard\_tokens}(x) & \text{otherwise}
\end{cases} $$

성능 평가에서는 통계적 유의성을 검증하기 위해 부트스트랩 재표본추출 방법을 사용했습니다. 각 벤치마크에서의 신뢰 구간은 다음과 같이 계산됩니다.

$$ \text{CI} = \bar{X} \pm t_{\alpha/2,n-1} \frac{s}{\sqrt{n}} $$

여기서 \\(\bar{X}\\)는 평균 성능, \\(s\\)는 표준 편차, \\(n\\)은 샘플 수, \\(t_{\alpha/2,n-1}\\)는 스튜던트 t-분포의 임계값을 나타냅니다.

도구 통합 추론의 구현에서는 Python 실행 환경과의 안전한 상호작용을 위한 샌드박스 메커니즘이 도입되었습니다. 이는 다음과 같은 실행 파이프라인을 따릅니다.

```python
def execute_tool_code(code, context):
    # 안전한 실행 환경 설정
    sandbox = create_secure_sandbox()
    # 수학 라이브러리 및 도구 초기화
    sandbox.initialize_math_tools()
    # 코드 실행 및 결과 검증
    result = sandbox.run_with_timeout(code, timeout=5.0)
    return validate_result(result, context)
```
### 지도 학습 기반 미세조정의 성능 분석과 도메인별 평가

DeepSeekMath-Instruct 7B의 성능을 더욱 세밀하게 분석하기 위해 수학 분야별 성능 평가가 수행되었습니다. 각 수학 분야에서의 성능은 다음과 같은 정규화된 점수로 계산됩니다.

$$ S_{\text{domain}} = \frac{1}{N_d} \sum_{i=1}^{N_d} w_i \cdot \text{correct}(x_i) $$

여기서 \\(N_d\\)는 해당 도메인의 문제 수, \\(w_i\\)는 문제의 난이도 가중치, \\(\text{correct}(x_i)\\)는 문제 해결의 정확성을 나타냅니다.

도구 통합 추론의 효과성은 문제의 복잡도에 따라 다르게 나타났습니다. 복잡도에 따른 성능 향상은 다음 수식으로 모델링됩니다.

$$ \Delta P_{\text{tool}} = \frac{P_{\text{tool}}(x) - P_{\text{base}}(x)}{\text{complexity}(x)} $$

여기서 \\(P_{\text{tool}}\\)은 도구 사용 시의 성능, \\(P_{\text{base}}\\)는 기본 추론 성능, \\(\text{complexity}(x)\\)는 문제의 복잡도를 나타냅니다.

중국어 수학 문제에서의 성능은 특히 주목할 만합니다. CMATH에서 84.3%의 정확도를 달성했는데, 이는 언어 장벽에도 불구하고 모델이 수학적 개념을 효과적으로 이해하고 적용할 수 있음을 보여줍니다. 이러한 다국어 성능은 다음과 같은 언어 독립적 표현 학습에 기인합니다.

$$ E_{\text{math}} = f_{\text{transform}}(E_{\text{lang}}) $$

여기서 \\(E_{\text{math}}\\)는 언어 독립적인 수학적 표현, \\(E_{\text{lang}}\\)은 언어 특정적 표현, \\(f_{\text{transform}}\\)은 언어 간 변환 함수를 나타냅니다.

도구 통합 추론의 구체적인 성능 향상은 문제 유형별로 다르게 나타났습니다. 특히 복잡한 계산이 필요한 문제에서 도구 사용의 효과가 두드러졌으며, 이는 다음과 같은 의사결정 메커니즘을 통해 이루어집니다.

```python
def select_solution_strategy(problem):
    # 문제 복잡도 분석
    complexity = analyze_complexity(problem)
    # 도구 사용 여부 결정
    if complexity > COMPLEXITY_THRESHOLD:
        return generate_tool_solution(problem)
    else:
        return generate_chain_of_thought(problem)
```

이러한 전략적 도구 사용은 특히 MATH 데이터셋에서 57.4%라는 높은 정확도 달성에 핵심적인 역할을 했습니다.

### 강화학습을 통한 DeepSeekMath의 성능 향상

DeepSeekMath의 강화학습 과정은 근접 정책 최적화(Proximal Policy Optimization, PPO)의 변형인 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)를 중심으로 이루어졌습니다. GRPO는 기존 PPO의 비평자 모델을 제거하고 그룹 점수에서 기준선을 추정하는 방식으로 설계되어, 학습에 필요한 컴퓨팅 자원을 크게 절감했습니다.

GRPO의 핵심 아이디어는 그룹 내 샘플들의 상대적 점수를 활용하는 것입니다. 각 그룹에서 생성된 응답들의 점수를 기반으로 기준선을 계산하며, 이는 다음과 같은 수식으로 표현됩니다.

$$ A(x, y) = r(x, y) - \frac{1}{N}\sum_{i=1}^N r(x, y_i) $$

여기서 \\(r(x, y)\\)는 응답 \\(y\\)에 대한 보상, \\(N\\)은 그룹 내 샘플 수를 나타냅니다. 이 접근 방식은 별도의 가치 함수 네트워크 없이도 효과적인 정책 최적화를 가능하게 합니다.

GRPO의 목적 함수는 다음과 같이 정의됩니다.

$$ L_{\text{GRPO}}(\theta) = \mathbb{E}_{(x,y)\sim \mathcal{D}}\left[\min\left(\frac{\pi_\theta(y|x)}{\pi_{\theta_{\text{old}}}(y|x)}A(x,y), \text{clip}\left(\frac{\pi_\theta(y|x)}{\pi_{\theta_{\text{old}}}(y|x)}, 1-\epsilon, 1+\epsilon\right)A(x,y)\right)\right] $$

여기서 \\(\pi_\theta\\)는 현재 정책, \\(\pi_{\theta_{\text{old}}}\\)는 이전 정책, \\(\epsilon\\)은 클리핑 파라미터입니다. 이 목적 함수는 정책의 급격한 변화를 방지하면서도 효과적인 학습을 가능하게 합니다.

![강화학습 성능 비교](https://ar5iv.org//html/2402.03300/assets/figures/rl_comparison.png)

위 그래프는 GRPO와 다른 강화학습 방법들의 성능을 비교한 결과를 보여줍니다. GRPO는 PPO와 비교하여 메모리 사용량을 크게 줄이면서도 동등하거나 더 나은 성능을 달성했습니다. 특히 GSM8K에서는 82.9%에서 88.2%로, MATH에서는 46.8%에서 51.7%로 성능이 향상되었으며, CMATH와 같은 도메인 외 수학 과제에서도 84.6%에서 88.8%로 성능이 개선되었습니다.
### 강화학습 알고리즘의 통합적 이해와 최적화

GRPO의 학습 과정은 세 가지 핵심 단계로 구성됩니다. 첫째, 각 입력에 대해 여러 응답을 생성하여 그룹을 형성합니다. 둘째, 생성된 응답들에 대한 보상을 계산하고 그룹 내 상대적 점수를 산출합니다. 셋째, 이 상대적 점수를 기반으로 정책을 업데이트합니다. 이 과정은 다음과 같은 알고리즘으로 구현됩니다.

$$ \begin{align*}
\text{advantage} &= r(x, y) - \text{baseline}(x) \\
\text{baseline}(x) &= \beta \cdot \max_{y_i \in Y_x} r(x, y_i) + (1-\beta) \cdot \text{mean}_{y_i \in Y_x} r(x, y_i)
\end{align*} $$

여기서 \\(\beta\\)는 최대값과 평균값 사이의 가중치를 조절하는 하이퍼파라미터입니다. 이러한 기준선 계산 방식은 그룹 내 응답들의 분포를 효과적으로 반영하면서도 계산 효율성을 유지합니다.

GRPO의 구현에서는 메모리 효율성을 위한 여러 최적화 기법이 도입되었습니다. 그래디언트 체크포인팅을 통해 중간 활성화값의 메모리 사용량을 줄였으며, 다음과 같은 배치 처리 전략을 사용했습니다.

$$ \text{effective\_batch\_size} = \text{batch\_size} \times \text{num\_gpus} \times \text{gradient\_accumulation\_steps} $$

연구진은 또한 거부 샘플링 미세조정(RFT), 직접 선호도 최적화(DPO), PPO, GRPO를 포함하는 통합된 강화학습 패러다임을 제시했습니다. 이 패러다임에서 각 방법은 다음과 같은 일반화된 목적 함수로 표현됩니다.

$$ L_{\text{unified}}(\theta) = \mathbb{E}_{(x,y)\sim \mathcal{D}}\left[w(x,y)\log \pi_\theta(y|x)\right] $$

여기서 \\(w(x,y)\\)는 각 방법별로 다르게 정의되는 가중치 함수입니다. GRPO의 경우, 이 가중치 함수는 그룹 내 상대적 점수와 클리핑 메커니즘을 결합한 형태를 가집니다.

![강화학습 방법 비교](https://ar5iv.org//html/2402.03300/assets/figures/rl_methods.png)

위 그래프는 다양한 강화학습 방법들의 학습 곡선을 보여줍니다. GRPO는 초기 학습 단계에서부터 안정적인 성능 향상을 보여주며, 특히 수렴 속도 면에서 우수한 특성을 나타냅니다.
### 강화학습 구현의 실제적 고려사항과 최적화 전략

GRPO의 실제 구현에서는 학습의 안정성과 효율성을 위한 여러 기술적 고려사항이 적용되었습니다. 학습 과정에서의 그래디언트 업데이트는 다음과 같은 적응적 학습률 스케줄링을 통해 조절됩니다.

$$ \eta_t = \eta_{\text{base}} \cdot \min\left(1, \frac{t}{t_{\text{warmup}}}\right) \cdot \frac{1}{\sqrt{\max(t, t_{\text{warmup}})}} $$

여기서 \\(\eta_{\text{base}}\\)는 기본 학습률, \\(t_{\text{warmup}}\\)은 웜업 스텝 수를 나타냅니다. 이러한 학습률 조정은 초기 학습 단계에서의 안정성을 보장하면서도 효과적인 정책 업데이트를 가능하게 합니다.

메모리 최적화를 위해 그래디언트 누적과 혼합 정밀도 학습이 도입되었으며, 이는 다음과 같은 메모리 사용량 추정식으로 표현됩니다.

$$ M_{\text{total}} = M_{\text{model}} + \frac{M_{\text{batch}}}{N_{\text{accumulation}}} + M_{\text{overhead}} $$

여기서 \\(M_{\text{model}}\\)은 모델 파라미터의 메모리 사용량, \\(M_{\text{batch}}\\)는 배치당 메모리 사용량, \\(N_{\text{accumulation}}\\)은 그래디언트 누적 스텝 수입니다.

GRPO의 보상 함수는 수학적 정확성과 해결 과정의 품질을 모두 고려하여 설계되었습니다.

$$ r(x, y) = \alpha r_{\text{accuracy}}(x, y) + (1-\alpha) r_{\text{quality}}(x, y) $$

여기서 \\(r_{\text{accuracy}}\\)는 수학적 정확성에 대한 보상, \\(r_{\text{quality}}\\)는 해결 과정의 품질에 대한 보상을 나타내며, \\(\alpha\\)는 두 요소 간의 균형을 조절하는 가중치입니다.

![강화학습 보상 분포](https://ar5iv.org//html/2402.03300/assets/figures/reward_distribution.png)

위 그래프는 GRPO 학습 과정에서의 보상 분포를 보여줍니다. 보상의 분포가 학습이 진행됨에 따라 점차 개선되는 것을 확인할 수 있으며, 이는 모델이 더 나은 수학적 해결책을 생성하는 방향으로 학습되고 있음을 나타냅니다.

### 그룹 상대 정책 최적화

강화학습은 지도 학습 미세조정 단계 이후에도 대규모 언어 모델의 수학적 추론 능력을 더욱 향상시키는 데 효과적임이 입증되었습니다. 이러한 맥락에서 본 연구는 효율적이고 효과적인 강화학습 알고리즘인 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)를 소개합니다.

근접 정책 최적화(Proximal Policy Optimization, PPO)는 대규모 언어 모델의 강화학습 미세조정 단계에서 널리 사용되는 액터-크리틱 강화학습 알고리즘입니다. PPO는 다음과 같은 대리 목적 함수를 최대화하여 모델을 최적화합니다.

$$ \mathcal{J}_{PPO}(\theta)=\mathbb{E}{[q\sim P(Q),o\sim\pi_{\theta_{old}}(O|q)]}\frac{1}{|o|}\sum_{t=1}^{|o|}\min\left[\frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{\theta_{old}}(o_{t}|q,o_{<t})}A_{t},\text{clip}\left(\frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{\theta_{old}}(o_{t}|q,o_{<t})},1-\varepsilon,1+\varepsilon\right)A_{t}\right] $$

여기서 \\(\pi_{\theta}\\)와 \\(\pi_{\theta_{old}}\\)는 각각 현재와 이전 정책 모델을 나타내며, \\(q\\)와 \\(o\\)는 질문 데이터셋과 이전 정책 \\(\pi_{\theta_{old}}\\)에서 샘플링된 질문과 출력입니다. \\(\varepsilon\\)은 PPO에서 학습 안정화를 위해 도입된 클리핑 관련 하이퍼파라미터입니다. \\(A_t\\)는 일반화된 이점 추정(Generalized Advantage Estimation, GAE)을 적용하여 계산된 이점으로, 보상 \\(\{r_{\geq t}\}\\)와 학습된 가치 함수 \\(V_{\psi}\\)를 기반으로 합니다.

PPO에서는 정책 모델과 함께 가치 함수를 학습해야 하며, 보상 모델의 과도한 최적화를 방지하기 위해 각 토큰에서 참조 모델로부터의 KL 페널티를 보상에 추가하는 것이 표준적인 접근 방식입니다.

$$ r_{t}=r_{\varphi}(q,o_{\leq t})-\beta\log\frac{\pi_{\theta}(o_{t}|q,o_{<t})}{\pi_{ref}(o_{t}|q,o_{<t})} $$

여기서 \\(r_{\varphi}\\)는 보상 모델, \\(\pi_{ref}\\)는 초기 SFT 모델인 참조 모델, \\(\beta\\)는 KL 페널티의 계수입니다.

![PPO와 GRPO 비교](https://ar5iv.org//html/2402.03300/assets/x2.png)

위 그림은 PPO와 GRPO의 아키텍처 차이를 보여줍니다. GRPO는 가치 모델을 제거하고 대신 그룹 점수에서 기준선을 추정함으로써 학습 리소스를 크게 절감합니다. PPO에서 사용되는 가치 함수는 일반적으로 정책 모델과 비슷한 크기의 또 다른 모델이기 때문에 상당한 메모리와 계산 부담을 초래합니다. 또한 대규모 언어 모델의 맥락에서는 보통 마지막 토큰에만 보상 모델이 점수를 할당하기 때문에, 각 토큰에서 정확한 가치 함수를 학습하는 것이 복잡해질 수 있습니다.
이러한 한계를 해결하기 위해 연구진은 그룹 상대 정책 최적화(GRPO)를 제안했습니다. GRPO는 각 질문 \\(q\\)에 대해 이전 정책 \\(\pi_{\theta_{old}}\\)에서 \\(\{o_1, o_2, \cdots, o_G\}\\)의 출력 그룹을 샘플링하고, 다음과 같은 목적 함수를 최대화하여 정책 모델을 최적화합니다.

$$ \mathcal{J}_{GRPO}(\theta)=\mathbb{E}{[q\sim P(Q),\{o_{i}\}_{i=1}^{G}\sim\pi_{\theta_{old}}(O|q)]}\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_{i}|}\sum_{t=1}^{|o_{i}|}\left\{\min\left[\frac{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})}\hat{A}_{i,t},\text{clip}\left(\frac{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q,o_{i,<t})},1-\varepsilon,1+\varepsilon\right)\hat{A}_{i,t}\right]-\beta\mathbb{D}_{KL}\left[\pi_{\theta}||\pi_{ref}\right]\right\} $$

여기서 \\(\varepsilon\\)과 \\(\beta\\)는 하이퍼파라미터이며, \\(\hat{A}_{i,t}\\)는 각 그룹 내 출력들의 상대적 보상을 기반으로 계산된 이점입니다. GRPO의 그룹 상대적 이점 계산 방식은 보상 모델의 비교적 특성과 잘 부합합니다. 보상 모델은 일반적으로 동일한 질문에 대한 출력들 간의 비교 데이터셋으로 학습되기 때문입니다.

또한 GRPO는 KL 페널티를 보상에 추가하는 대신 학습된 정책과 참조 정책 간의 KL 발산을 직접 손실에 추가함으로써 \\(\hat{A}_{i,t}\\)의 계산을 단순화합니다. 식 (2)에서 사용된 KL 페널티와 달리, 다음과 같은 편향되지 않은 추정량을 사용하여 KL 발산을 추정합니다.

$$ \mathbb{D}_{KL}\left[\pi_{\theta}||\pi_{ref}\right]=\frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}-\log\frac{\pi_{ref}(o_{i,t}|q,o_{i,<t})}{\pi_{\theta}(o_{i,t}|q,o_{i,<t})}-1 $$

이 추정량은 항상 양수임이 보장됩니다.
### 그룹 상대 정책 최적화의 반복적 학습 알고리즘

GRPO의 반복적 학습 과정은 알고리즘 1에서 자세히 설명됩니다. 이 알고리즘은 초기 정책 모델 \\(\pi_{\theta_{init}}\\), 보상 모델 \\(r_{\varphi}\\), 작업 프롬프트 \\(\mathcal{D}\\), 그리고 하이퍼파라미터 \\(\varepsilon\\), \\(\beta\\), \\(\mu\\)를 입력으로 받습니다.

각 반복에서 알고리즘은 현재 정책 모델을 참조 모델로 설정하고, 배치 단위로 학습을 진행합니다. 각 배치에서는 프롬프트 데이터셋에서 샘플을 추출하고, 각 질문에 대해 G개의 출력을 생성합니다. 이후 보상 모델을 사용하여 각 출력에 대한 보상을 계산하고, 그룹 상대적 이점 추정을 통해 \\(\hat{A}_{i,t}\\)를 계산합니다.

GRPO는 결과 감독(Outcome Supervision)과 과정 감독(Process Supervision)이라는 두 가지 변형을 제공합니다. 결과 감독에서는 각 출력 \\(o_i\\)의 끝에서만 정규화된 보상을 제공하고, 출력의 모든 토큰의 이점 \\(\hat{A}_{i,t}\\)를 정규화된 보상으로 설정합니다.

$$ \hat{A}_{i,t}=\widetilde{r}_{i}=\frac{r_{i}-{\rm mean}(\mathbf{r})}{{\rm std}(\mathbf{r})} $$

과정 감독에서는 각 추론 단계의 끝에서 보상을 제공합니다. 질문 \\(q\\)와 G개의 샘플링된 출력 \\(\{o_1, o_2, \cdots, o_G\}\\)에 대해, 과정 보상 모델은 각 출력의 각 단계에 점수를 매깁니다.

$$ \mathbf{R}=\{\{r_{1}^{index(1)},\cdots,r_{1}^{index(K_{1})}\},\cdots,\{r_{G}^{index(1)},\cdots,r_{G}^{index(K_{G})}\}\} $$

여기서 \\(index(j)\\)는 j번째 단계의 마지막 토큰 인덱스이고, \\(K_i\\)는 i번째 출력의 총 단계 수입니다. 이러한 보상들도 평균과 표준편차로 정규화됩니다.

$$ \widetilde{r}_{i}^{index(j)}=\frac{r_{i}^{index(j)}-{\rm mean(\mathbf{R})}}{{\rm std(\mathbf{R})}} $$

과정 감독은 각 토큰의 이점을 다음 단계들의 정규화된 보상의 합으로 계산합니다.

$$ \hat{A}_{i,t}=\sum_{index(j)\geq t}\widetilde{r}_{i}^{index(j)} $$

이후 식 (3)에 정의된 목적 함수를 최대화하여 정책을 최적화합니다.
### 반복적 강화학습을 통한 GRPO의 성능 향상

GRPO의 반복적 강화학습 과정에서는 이전 보상 모델이 현재 정책 모델을 감독하기에 충분하지 않을 수 있다는 점을 고려합니다. 이를 해결하기 위해 알고리즘 1에서 보여진 것처럼 반복적 GRPO를 도입했습니다. 반복적 GRPO에서는 정책 모델의 샘플링 결과를 기반으로 보상 모델을 위한 새로운 학습 세트를 생성하고, 10%의 과거 데이터를 포함하는 재생 메커니즘을 통해 이전 보상 모델을 지속적으로 학습시킵니다.

이 과정에서 참조 모델을 정책 모델로 설정하고, 새로운 보상 모델로 정책 모델을 계속 학습시킵니다. 이러한 접근 방식은 보상 모델이 정책의 진화에 따라 적응할 수 있게 해주며, 더 정확한 피드백을 제공할 수 있게 합니다.

GRPO의 주요 혁신은 PPO에서 사용되는 가치 함수 모델을 제거하고 그룹 점수에서 직접 기준선을 추정하는 것입니다. 이는 계산 효율성을 크게 향상시키면서도 성능을 유지할 수 있게 합니다. 특히 수학적 추론과 같은 복잡한 과제에서 과정 감독 방식을 통해 더 풍부한 피드백을 제공할 수 있습니다.

GRPO의 그룹 상대적 접근 방식은 보상 모델의 비교적 특성과 자연스럽게 부합합니다. 보상 모델은 일반적으로 동일한 입력에 대한 다양한 출력들을 비교하는 방식으로 학습되기 때문에, 그룹 내 상대적 보상을 활용하는 GRPO의 설계는 이러한 학습 패러다임과 잘 맞습니다. 또한 KL 발산 정규화를 직접 손실 함수에 통합함으로써, 보상 계산을 단순화하고 학습의 안정성을 향상시킵니다.

이러한 설계 선택들은 GRPO가 PPO와 비교하여 메모리 사용량을 크게 줄이면서도 동등하거나 더 나은 성능을 달성할 수 있게 합니다. 특히 대규모 언어 모델의 강화학습에서 계산 효율성과 확장성 측면에서 중요한 의미를 가집니다.

### DeepSeekMath-RL의 학습과 평가

DeepSeekMath-RL은 DeepSeekMath-Instruct 7B를 기반으로 강화학습을 통해 개발되었습니다. 강화학습 데이터는 SFT 데이터에서 추출한 GSM8K와 MATH 관련 체인오브소트 형식의 문제들로 구성되었으며, 약 14.4만 개의 문제를 포함합니다. 연구진은 다른 SFT 문제들을 제외함으로써 강화학습 단계에서 데이터가 부족한 벤치마크에 대한 강화학습의 영향을 조사했습니다.

보상 모델의 학습 데이터셋은 Wang과 연구진의 방법을 따라 구축되었습니다. 초기 보상 모델은 DeepSeekMath-Base 7B를 기반으로 2e-5의 학습률로 학습되었습니다. GRPO 학습에서는 정책 모델의 학습률을 1e-6으로 설정했으며, KL 계수는 0.04로 설정했습니다. 각 문제에 대해 64개의 출력을 샘플링했으며, 최대 길이는 1,024로, 학습 배치 크기는 1,024로 설정되었습니다. 정책 모델은 각 탐색 단계 이후 단 한 번의 업데이트만 수행했습니다.

DeepSeekMath-RL 7B의 평가는 DeepSeekMath-Instruct 7B와 동일한 벤치마크에서 진행되었습니다. GSM8K와 MATH에서의 체인오브소트 추론은 도메인 내 과제로, 다른 모든 벤치마크는 도메인 외 과제로 간주되었습니다. 아래 표는 영어와 중국어 벤치마크에서 체인오브소트와 도구 통합 추론을 사용한 공개 및 비공개 모델들의 성능을 보여줍니다.

평가 결과는 두 가지 주목할 만한 발견을 보여줍니다. 첫째, DeepSeekMath-RL 7B는 체인오브소트 추론을 사용하여 GSM8K에서 88.2%, MATH에서 51.7%의 정확도를 달성했습니다. 이는 7B에서 70B 범위의 모든 오픈소스 모델들과 대부분의 비공개 모델들의 성능을 능가하는 결과입니다. 둘째, DeepSeekMath-RL 7B는 DeepSeekMath-Instruct 7B에서 시작하여 GSM8K와 MATH의 체인오브소트 형식 명령어 튜닝 데이터만으로 학습되었음에도 불구하고, 모든 평가 지표에서 DeepSeekMath-Instruct 7B의 성능을 뛰어넘었습니다. 이는 강화학습의 효과성을 명확하게 보여줍니다.

### 사전학습과 강화학습 실험 결과 분석

DeepSeekMath의 사전학습 과정에서 연구진은 코드 학습이 수학적 추론 능력 향상에 긍정적인 영향을 미친다는 중요한 발견을 했습니다. 이는 도구 사용 여부와 관계없이 일관되게 나타났으며, "코드 학습이 추론 능력을 향상시키는가?"라는 오랜 질문에 대한 의미 있는 답을 제시합니다. 연구진은 적어도 수학적 추론에 있어서는 코드 학습이 실제로 도움이 된다고 결론지었습니다.

흥미로운 점은 arXiv 논문 학습이 수학 벤치마크에서 주목할 만한 성능 향상을 보이지 않았다는 것입니다. 이는 학술 논문에 포함된 수학적 내용이 실제 문제 해결 능력 향상에 직접적인 도움이 되지 않을 수 있음을 시사합니다.

강화학습 측면에서는 그룹 상대 정책 최적화(GRPO)가 매우 효과적이었습니다. GRPO는 기존 PPO의 비평자 모델을 제거하고 그룹 점수에서 기준선을 추정하는 방식으로 설계되어, 학습에 필요한 컴퓨팅 자원을 크게 절감했습니다. 영어 명령어 튜닝 데이터의 일부만을 사용했음에도 불구하고, GRPO는 이미 강력한 성능을 보여준 DeepSeekMath-Instruct를 한층 더 개선했습니다.

![강화학습 성능 비교](https://ar5iv.org//html/2402.03300/assets/figures/rl_comparison.png)

위 그래프는 GRPO와 다른 강화학습 방법들의 성능을 비교한 결과를 보여줍니다. GRPO는 PPO와 비교하여 메모리 사용량을 크게 줄이면서도 동등하거나 더 나은 성능을 달성했습니다. 특히 GSM8K에서는 82.9%에서 88.2%로, MATH에서는 46.8%에서 51.7%로 성능이 향상되었으며, CMATH와 같은 도메인 외 수학 과제에서도 84.6%에서 88.8%로 성능이 개선되었습니다.

연구진은 거부 샘플링 미세조정(RFT), 직접 선호도 최적화(DPO), PPO, GRPO와 같은 다양한 방법들을 이해하기 위한 통합된 패러다임을 제시했습니다. 이들은 모든 방법이 직접적이거나 단순화된 강화학습 기법으로 개념화될 수 있다는 것을 발견했습니다. 온라인 대 오프라인 학습, 결과 대 과정 감독, 단일 턴 대 반복적 강화학습 등 광범위한 실험을 통해 이 패러다임의 핵심 요소들을 심도 있게 조사했습니다.

### 사전학습 과정에서의 주요 발견

DeepSeekMath의 사전학습 과정에서 연구진은 코드 학습이 수학적 추론 능력에 미치는 영향을 심도 있게 분석했습니다. DeepSeek-LLM 1.3B 모델을 사용한 실험에서는 두 가지 주요 학습 방식을 검증했습니다. 첫 번째는 2단계 학습으로, 400B 토큰의 코드 학습 후 150B 토큰의 수학 학습을 진행했습니다. 두 번째는 1단계 학습으로, 코드와 수학 토큰을 혼합하여 학습을 진행했습니다.

실험 결과는 코드 학습이 도구를 활용한 수학적 추론 능력을 크게 향상시킨다는 것을 보여줍니다. 2단계 학습에서는 초기 코드 학습만으로도 Python을 활용한 수학 문제 해결 능력이 향상되었으며, 이후의 수학 학습을 통해 더욱 개선되었습니다. 특히 주목할 만한 점은 1단계 혼합 학습 방식이 2단계 학습에서 발생하는 파국적 망각 문제를 효과적으로 해결하면서도 코딩과 도구 활용 수학적 추론 능력의 시너지를 창출했다는 것입니다.

코드 학습은 도구 사용 없이도 수학적 추론 능력을 향상시켰습니다. 2단계 학습에서 초기 코드 학습은 중간 수준의 향상을 보였으며, 이는 후속 수학 학습의 효율성을 높여 최종적으로 가장 우수한 성능을 달성했습니다. 그러나 1단계 혼합 학습에서는 코드와 수학 토큰을 결합하는 것이 도구를 사용하지 않는 수학적 추론 능력을 저하시켰습니다. 연구진은 이를 1.3B 규모의 제한된 모델 용량으로 인해 코드와 수학 데이터를 동시에 충분히 학습하지 못한 것으로 분석했습니다.

![성능 비교 표](https://ar5iv.org//html/2402.03300/assets/x1.png)

위 그래프는 다양한 학습 설정에 따른 모델의 성능을 보여줍니다. 특히 코드 학습이 수학적 추론 능력에 미치는 긍정적인 영향을 명확하게 확인할 수 있습니다. 이러한 결과는 "코드 학습이 추론 능력을 향상시키는가?"라는 오랜 질문에 대해, 적어도 수학적 추론 분야에서는 긍정적인 답을 제시합니다.
### arXiv 논문의 수학적 추론 영향 분석

연구진은 수학 사전학습 데이터로 자주 활용되는 arXiv 논문의 효과성을 심층적으로 분석했습니다. 이를 위해 DeepSeek-LLM 1.3B와 DeepSeek-Coder-Base-v1.5 7B 두 가지 규모의 모델을 사용하여 실험을 진행했으며, 서로 다른 처리 과정을 거친 arXiv 코퍼스를 활용했습니다.

첫 번째로 사용된 코퍼스는 MathPile로, 89억 개의 토큰으로 구성되어 있으며 그 중 85% 이상이 과학 논문입니다. 이 데이터셋은 정교한 정제 규칙을 통해 구축되었습니다. 두 번째는 ArXiv-RedPajama로, 서문, 주석, 매크로, 참고문헌을 제거한 280억 개의 토큰으로 구성되어 있습니다.

실험 결과는 예상과 달리 arXiv 논문만을 사용한 학습이 수학적 추론 능력 향상에 효과적이지 않다는 것을 보여줍니다. DeepSeek-LLM 1.3B를 1,500억 토큰으로, DeepSeek-Coder-Base-v1.5 7B를 400억 토큰으로 각각의 arXiv 코퍼스에 대해 학습시켰을 때, GSM8K, MATH와 같은 정량적 추론 과제부터 MMLU-STEM과 같은 객관식 문제, 그리고 miniF2F와 같은 형식 수학에 이르기까지 대부분의 수학 벤치마크에서 성능 향상이 없거나 오히려 저하되는 현상이 관찰되었습니다.

![성능 비교 결과](https://ar5iv.org//html/2402.03300/assets/figures/corpus_comparisons.png)

위 그래프는 다양한 arXiv 코퍼스에 대한 학습 결과를 보여줍니다. 특히 주목할 만한 점은 정교한 전처리 과정을 거친 MathPile을 사용했을 때도 유의미한 성능 향상이 없었다는 것입니다.

하지만 연구진은 이러한 결론에 몇 가지 제한 사항이 있음을 강조합니다. 첫째, 이 연구에서 다루지 않은 특정 수학 관련 과제, 예를 들어 정리의 비형식화와 같은 과제에서 arXiv 토큰의 영향은 아직 연구되지 않았습니다. 둘째, arXiv 토큰을 다른 유형의 데이터와 결합했을 때의 효과도 추가 연구가 필요합니다. 마지막으로, 더 큰 규모의 모델에서는 arXiv 논문의 이점이 다르게 나타날 수 있다는 점도 고려해야 합니다.

### 강화학습의 통합적 패러다임과 분석

강화학습은 대규모 언어 모델의 수학적 추론 능력을 향상시키는 데 있어 매우 효과적인 방법임이 입증되었습니다. 본 연구에서는 지도 학습 미세조정(SFT), 거부 샘플링 미세조정(RFT), 직접 선호도 최적화(DPO), 근접 정책 최적화(PPO), 그룹 상대 정책 최적화(GRPO)와 같은 다양한 학습 방법들을 분석하기 위한 통합된 패러다임을 제시합니다.

이 통합 패러다임은 파라미터 \\(\theta\\)에 대한 그래디언트를 다음과 같이 정의합니다.

$$ \nabla_\theta \mathcal{J}_{\mathcal{A}}(\theta) = \mathbb{E}_{(q,o)\sim\mathcal{D}}\left[\frac{1}{|o|}\sum_{t=1}^{|o|} GC_{\mathcal{A}}(q,o,t,\pi_{rf})\nabla_\theta\log\pi_\theta(o_t|q,o_{<t})\right] $$

여기서 세 가지 핵심 요소를 확인할 수 있습니다.

1. 데이터 소스 \\(\mathcal{D}\\): 학습 데이터를 결정합니다.
2. 보상 함수 \\(\pi_{rf}\\): 학습 보상 신호의 원천입니다.
3. 알고리즘 \\(\mathcal{A}\\): 학습 데이터와 보상 신호를 처리하여 그래디언트 계수 \\(GC\\)를 결정합니다.

연구진은 데이터 소스를 온라인 샘플링과 오프라인 샘플링으로 구분했습니다. 온라인 샘플링은 실시간 정책 모델의 탐색 결과를 사용하는 반면, 오프라인 샘플링은 초기 SFT 모델의 샘플링 결과를 활용합니다. 실험 결과에서는 온라인 RFT가 RFT를 크게 능가하는 것으로 나타났습니다.

![성능 비교](https://ar5iv.org//html/2402.03300/assets/x3.png)

위 그래프는 DeepSeekMath-Instruct 1.3B 모델의 성능을 보여줍니다. 특히 온라인 RFT가 초기에는 RFT와 비슷한 성능을 보이다가 후반부에서 절대적인 우위를 보이는 것을 확인할 수 있습니다. 이는 초기에는 정책 모델과 SFT 모델이 유사하여 샘플링된 데이터의 차이가 크지 않지만, 후반부에서는 정책 모델이 진화함에 따라 실시간 데이터 샘플링의 이점이 더욱 두드러지기 때문입니다.

그래디언트 계수 측면에서는 보상 함수를 'Rule'과 'Model' 두 가지로 구분했습니다. Rule은 응답의 정확성을 기준으로 품질을 판단하고, Model은 보상 모델을 학습시켜 각 응답에 점수를 매깁니다. GRPO는 보상 모델이 제공하는 보상값에 따라 그래디언트 계수를 조정하는 반면, 온라인 RFT는 잘못된 응답에 대한 페널티가 없고 모든 정답을 동일한 강도로 강화한다는 차이가 있습니다.
### 강화학습의 실험적 분석과 성능 향상

실험 결과에서 GRPO가 온라인 RFT를 능가하는 것으로 나타났으며, 이는 응답의 품질에 따라 차등적인 강화와 페널티를 적용하는 방식이 효과적임을 보여줍니다. 특히 GRPO+PS(Process Supervision)가 GRPO+OS(Outcome Supervision)보다 우수한 성능을 보여주어, 단계별 그래디언트 계수를 사용하는 것이 유리함을 입증했습니다.

![반복적 강화학습 성능](https://ar5iv.org//html/2402.03300/assets/x4.png)

연구진은 또한 반복적 강화학습의 효과를 탐구했습니다. DeepSeekMath-Instruct 7B 모델을 사용한 실험에서 두 번의 반복을 수행했으며, 특히 첫 번째 반복에서 큰 성능 향상이 관찰되었습니다. 이는 반복적 강화학습이 모델의 수학적 추론 능력을 향상시키는 데 효과적인 방법임을 시사합니다.

![성능 지표 비교](https://ar5iv.org//html/2402.03300/assets/x5.png)

강화학습이 효과적인 이유를 더 깊이 이해하기 위해 연구진은 SFT와 RL DeepSeekMath 7B 모델의 Pass@K와 Maj@K 정확도를 GSM8K와 MATH 데이터셋에서 평가했습니다. 흥미롭게도 강화학습은 Maj@K 성능은 향상시켰지만 Pass@K는 그렇지 않았습니다. 이는 강화학습이 TopK 응답에서 정답의 비율을 높이는 방식으로 전반적인 성능을 향상시키지만, 기본적인 능력의 향상보다는 출력 분포를 더 견고하게 만드는 데 기여한다는 것을 시사합니다.

Wang과 연구진은 SFT 모델의 추론 과제에서 정렬 문제를 식별했으며, 이는 일련의 선호도 정렬 전략을 통해 추론 성능을 개선할 수 있음을 보여줍니다. 이러한 발견은 강화학습이 모델의 출력을 더 신뢰할 수 있게 만들면서도, 근본적인 수학적 추론 능력의 향상을 위해서는 추가적인 접근이 필요할 수 있음을 시사합니다.
### 효과적인 강화학습을 위한 발전 방향

연구진은 강화학습의 효과성을 입증하고 통합된 패러다임을 통해 다양한 학습 방법을 분석한 결과를 바탕으로, 더욱 효과적인 강화학습을 달성하기 위한 세 가지 주요 방향을 제시합니다.

데이터 소스 측면에서는 모든 학습 방법의 기초가 되는 원자재로서의 역할을 합니다. 강화학습의 맥락에서 데이터 소스는 정책 모델에서 샘플링된 출력이 있는 레이블이 없는 질문들을 의미합니다. 현재 연구에서는 지도 학습 단계의 질문들과 단순한 핵심 샘플링만을 사용했는데, 이것이 강화학습 파이프라인이 Maj@K 성능만 향상시키는 잠재적 원인일 수 있습니다. 향후에는 도메인 외 질문 프롬프트에서의 강화학습 파이프라인 탐색과 함께, 트리 검색 방법을 기반으로 하는 고급 샘플링 전략의 도입이 필요합니다. 또한 정책 모델의 탐색 효율성을 결정하는 효율적인 추론 기법의 개발도 매우 중요한 역할을 할 것으로 예상됩니다.

알고리즘 측면에서는 데이터와 보상 신호를 처리하여 모델 파라미터를 업데이트하기 위한 그래디언트 계수를 계산합니다. 현재의 방법들은 보상 함수의 신호를 전적으로 신뢰하여 특정 토큰의 조건부 확률을 증가시키거나 감소시키는데, 이는 특히 복잡한 과제에서 보상 신호가 항상 신뢰할 수 있다고 보장할 수 없다는 한계가 있습니다. 예를 들어, 숙련된 주석자들이 신중하게 주석을 단 PRM800K 데이터셋에서도 약 20%의 잘못된 주석이 포함되어 있다는 것이 확인되었습니다. 이러한 문제를 해결하기 위해 노이즈가 있는 보상 신호에 대해 강건한 강화학습 알고리즘의 개발이 필요합니다.

보상 함수 측면에서는 학습 신호의 원천으로서, 강화학습에서는 주로 신경망 보상 모델이 사용됩니다. 보상 모델과 관련하여 세 가지 중요한 발전 방향이 있습니다. 첫째, 보상 모델의 일반화 능력을 향상시키는 것입니다. 보상 모델은 도메인 외 질문과 고급 디코딩 출력을 효과적으로 처리할 수 있어야 하며, 그렇지 않으면 강화학습이 대규모 언어 모델의 분포를 안정화시키는 데 그칠 수 있습니다. 둘째, 보상 모델의 불확실성을 반영하는 방법을 개발하는 것입니다. 이러한 불확실성은 약한 보상 모델과 약-강 학습 알고리즘 사이의 연결 다리 역할을 할 수 있습니다. 셋째, 추론 과정에 대해 세밀한 학습 신호를 제공할 수 있는 고품질 과정 보상 모델을 효율적으로 구축하는 것입니다.

### DeepSeekMath의 결론과 한계점

DeepSeekMath는 경쟁 수준의 MATH 벤치마크에서 모든 오픈소스 모델들을 능가하는 성능을 달성하며, 비공개 모델들의 성능에 근접하는 결과를 보여주었습니다. 이 모델은 DeepSeek-Coder-v1.5 7B를 기반으로 하여 5,000억 개의 토큰으로 추가 학습을 진행했으며, 그 중 1,200억 개의 토큰은 Common Crawl에서 추출한 수학 관련 데이터입니다.

광범위한 실험 분석을 통해 웹 페이지가 고품질 수학 데이터의 잠재력을 가지고 있음이 입증되었습니다. 반면 예상과 달리 arXiv 논문은 기대했던 만큼의 효과를 보여주지 못했습니다. 연구진은 근접 정책 최적화(Proximal Policy Optimization, PPO)의 변형인 그룹 상대 정책 최적화(Group Relative Policy Optimization, GRPO)를 도입했는데, 이는 메모리 사용량을 줄이면서도 수학적 추론 능력을 크게 향상시킬 수 있었습니다. 실험 결과는 DeepSeekMath-Instruct 7B가 이미 높은 점수를 달성한 상태에서도 GRPO가 효과적임을 보여줍니다.

연구진은 또한 다양한 방법들을 이해하기 위한 통합된 패러다임을 제시하고, 더 효과적인 강화학습을 위한 여러 잠재적 방향을 제시했습니다. 하지만 DeepSeekMath가 정량적 추론 벤치마크에서 인상적인 점수를 달성했음에도 불구하고, 기하학과 정리 증명 분야에서는 비공개 모델들보다 상대적으로 약한 성능을 보입니다. 예를 들어, 연구진의 예비 실험에서 모델은 삼각형과 타원에 관련된 문제를 효과적으로 처리하지 못했는데, 이는 사전학습과 미세조정 과정에서의 데이터 선택 편향을 시사합니다.

또한 모델 규모의 제약으로 인해 DeepSeekMath는 GPT-4와 비교했을 때 퓨 샷 능력이 부족합니다. GPT-4는 퓨 샷 입력을 통해 성능을 향상시킬 수 있지만, DeepSeekMath는 제로 샷과 퓨 샷 평가에서 비슷한 수준의 성능을 보여줍니다. 연구진은 향후 더 높은 품질의 사전학습 코퍼스를 구축하기 위해 데이터 선별 파이프라인을 개선할 계획이며, 앞서 제시한 잠재적 방향들을 탐구하여 더욱 효과적인 강화학습을 달성하고자 합니다.

![MATH 벤치마크 성능 비교](https://ar5iv.org//html/2402.03300/assets/figures/Math.png)

위 그래프는 시간에 따른 다양한 오픈소스 AI/ML 모델들의 MATH 벤치마크 성능을 보여줍니다. DeepSeekMath가 외부 도구나 투표 기법 없이도 가장 높은 정확도를 달성했음을 확인할 수 있으며, 이는 수학적 추론 능력에서 상당한 진전을 이루었음을 시사합니다.

- - -
### References
* [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](http://arxiv.org/pdf/2402.03300v3)