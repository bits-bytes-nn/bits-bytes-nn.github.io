---
layout: post
title: "Llama 2: Open Foundation and Fine-Tuned Chat Models"
date: 2023-07-18 14:31:57
author: "Meta AI"
categories: "Language-Models"
tags: ["Open-Foundation-and-Fine-Tuned-Chat-Models", "Grouped-Query-Attention", "Reinforcement-Learning-with-Human-Feedback", "System-Message-for-Multi-Turn-Consistency", "Tool-Use-Emergence", "Temporal-Organization-of-Knowledge"]
use_math: true
cover: /assets/images/language-models.webp
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
Meta는 AI 기술의 민주화와 투명성 향상을 위해 대규모 언어 모델을 공개적으로 개발하고자 했습니다. 기존의 많은 기업들이 AI 모델을 비공개로 개발하는 것과 달리, Meta는 AI 실무자 커뮤니티의 집단 지혜를 활용하여 기술을 발전시키고자 했습니다. 특히 안전성과 유용성을 모두 갖춘 대화형 AI 모델의 필요성이 증가하는 상황에서, 연구 및 상업적 목적으로 자유롭게 사용할 수 있는 고성능 언어 모델의 개발이 시급했습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
Llama 2는 기존 Llama 1을 기반으로 하되, 프리트레이닝 데이터를 40% 증가시키고 컨텍스트 길이를 2배로 확장했습니다. 특히 주목할 만한 혁신은 그룹 쿼리 어텐션(GQA) 메커니즘의 도입과 고스트 어텐션(GAtt)이라는 새로운 기술의 개발입니다. 또한 안전성 강화를 위해 지도 학습 기반 파인튜닝, 인간 피드백을 통한 강화학습(RLHF), 그리고 컨텍스트 증류라는 세 가지 주요 기술을 결합했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
모델은 7B부터 70B까지 다양한 크기로 구현되었으며, 공개적으로 접근 가능한 데이터로만 학습되었습니다. 학습은 Meta의 연구 슈퍼 클러스터와 프로덕션 클러스터에서 진행되었으며, 총 330만 GPU 시간이 소요되었습니다. 안전성 강화를 위해 350명 이상의 전문가로 구성된 레드팀이 광범위한 테스트를 수행했으며, 보상 모델링을 통해 모델의 응답을 지속적으로 개선했습니다. 특히 유용성과 안전성 평가를 위해 약 4,000개의 프롬프트와 2,000개의 적대적 프롬프트를 사용했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
Llama 2는 대부분의 벤치마크에서 기존 오픈소스 모델들을 능가하는 성능을 보여주었으며, 일부 평가에서는 ChatGPT와 같은 상용 모델과 견줄만한 수준을 달성했습니다. 특히 안전성 측면에서 큰 진전을 이루어, 70B 모델의 경우 유해 콘텐츠 생성 비율을 24.60%에서 0.01%로 크게 감소시켰습니다. 이는 오픈소스 AI 모델이 안전성과 성능을 모두 달성할 수 있다는 것을 입증하며, AI 기술의 민주화와 투명성 향상에 중요한 이정표가 될 것으로 기대됩니다. 다만 영어 외 언어에 대한 제한적 성능과 지식 업데이트의 한계 등 여전히 해결해야 할 과제들이 남아있습니다.
- - -
## Llama 2: 오픈 파운데이션 및 파인튜닝된 대화 모델

Meta의 연구진이 개발한 Llama 2는 오픈 소스 기반의 대규모 언어 모델로, 기존 Llama 모델의 후속 버전입니다. 이 모델은 파운데이션 모델과 대화에 특화된 파인튜닝 모델 두 가지 형태로 제공됩니다. Llama 2는 Touvron과 연구진이 개발한 LLaMA 모델의 아키텍처를 기반으로 하며, 더 큰 규모의 학습 데이터와 개선된 학습 방법을 적용하여 성능을 향상시켰습니다.

이 모델의 주요 특징은 공개적으로 접근 가능한 데이터셋만을 사용하여 학습되었다는 점입니다. 이는 Hoffmann과 연구진이 제시한 컴퓨트 최적 학습 방법론을 따르고 있으며, 모델 크기와 학습 데이터의 규모를 균형있게 조정하여 효율적인 학습을 달성했습니다. 특히 트랜스포머 아키텍처를 기반으로 하여 Vaswani와 연구진이 제안한 어텐션 메커니즘의 장점을 최대한 활용했습니다.

Llama 2는 강화학습을 통한 인간 피드백(RLHF) 방식을 적용하여 모델의 유용성과 안전성을 개선했습니다. 이는 Askell과 연구진이 제시한 언어 모델 정렬 방법론을 따르고 있으며, 특히 대화형 상황에서 모델이 더 도움이 되고 안전한 응답을 생성할 수 있도록 설계되었습니다. 또한 Ganguli와 연구진이 제안한 레드팀 테스팅 방법론을 통해 모델의 잠재적 위험성을 평가하고 완화하는 과정을 거쳤습니다.

이러한 접근 방식은 Chung과 연구진이 연구한 대규모 언어 모델의 명령어 파인튜닝 방법론과도 맥을 같이하며, 특히 체인오브소트와 같은 고급 추론 능력을 향상시키는데 중점을 두었습니다. 결과적으로 Llama 2는 개방성과 성능, 안전성을 모두 고려한 균형잡힌 언어 모델로 자리매김하게 되었습니다.

## Llama 2: 오픈 소스 대화 모델의 새로운 지평

### 소개

Meta는 70억에서 700억 개의 파라미터를 가진 대규모 언어 모델 Llama 2를 공개했습니다. 이 모델은 기본 프리트레이닝 모델인 Llama 2와 대화에 최적화된 Llama 2-Chat으로 구성되어 있습니다. 특히 Llama 2-Chat은 대부분의 벤치마크에서 기존 오픈소스 대화 모델들을 능가하는 성능을 보여주었으며, 유용성과 안전성 측면에서 상용 비공개 모델들과 견줄만한 수준을 달성했습니다.

![helpfulness_evaluation](https://ar5iv.org//html/2307.09288/assets/x1.png)

유용성 평가에서 Llama 2-Chat은 약 4,000개의 단일 및 다중 턴 프롬프트에 대해 다른 모델들과 비교 평가되었습니다. 특히 Llama-2-70b-chat 모델은 ChatGPT-030 모델보다 더 높은 승률을 기록했습니다. 다만 연구진은 프롬프트 세트의 한계, 평가 가이드라인의 주관성, 평가자 개인의 주관성, 그리고 생성된 결과를 비교하는 것의 본질적인 어려움으로 인해 인간 평가에 노이즈가 있을 수 있다고 지적했습니다.

![gpt4_evaluation](https://ar5iv.org//html/2307.09288/assets/x2.png)

인간 평가를 보완하기 위해 연구진은 GPT-4 모델을 활용하여 상용 모델들과 Llama 2-Chat의 유용성과 안전성을 비교 평가했습니다. 평가의 객관성을 높이기 위해 모델 응답의 제시 순서를 무작위로 바꾸어가며 평가를 진행했으며, 동점을 제거하기 위해 승률을 win/(win+loss) 방식으로 계산했습니다.

![safety_evaluation](https://ar5iv.org//html/2307.09288/assets/img/safety_overall_human_temp.png)

안전성 평가에서는 약 2,000개의 적대적 프롬프트를 사용하여 모델의 안전성 위반 사례를 분석했습니다. Llama 2-Chat 모델들은 다른 모델들에 비해 전반적으로 낮은 안전성 위반율을 보여주었습니다. 하지만 연구진은 이러한 안전성 평가 결과가 Llama 2-Chat 모델에 유리하게 설정된 콘텐츠 기준의 영향을 받았을 수 있다는 점을 명시했습니다.

Llama 2의 개발 과정에서는 안전성 강화를 위해 특별한 데이터 주석 작업과 튜닝, 레드팀 테스트, 반복적 평가 등이 수행되었습니다. 연구진은 이러한 방법론을 상세히 공개함으로써 커뮤니티가 파인튜닝된 언어 모델을 재현하고 안전성을 지속적으로 개선할 수 있기를 기대하고 있습니다.
### 모델 구조와 학습 방법론

Llama 2는 기존 Llama 1 모델을 기반으로 하되, 여러 가지 중요한 개선사항을 도입했습니다. 프리트레이닝 데이터의 규모를 40% 증가시켰으며, 모델의 컨텍스트 길이를 2배로 확장했습니다. 또한 Ainslie와 연구진이 제안한 그룹 쿼리 어텐션(grouped-query attention) 메커니즘을 도입하여 모델의 성능을 향상시켰습니다.

![training_process](https://ar5iv.org//html/2307.09288/assets/x3.jpg)

Llama 2-Chat의 학습 과정은 세 단계로 구성됩니다. 먼저 공개적으로 접근 가능한 데이터를 사용하여 Llama 2 모델을 프리트레이닝합니다. 그 다음 지도 학습 기반 파인튜닝을 통해 Llama 2-Chat의 초기 버전을 생성합니다. 마지막으로 인간 피드백을 통한 강화학습(RLHF)을 적용하여 모델을 반복적으로 개선합니다. 이 과정에서 리젝션 샘플링과 근접 정책 최적화(Proximal Policy Optimization, PPO)와 같은 기법들이 활용됩니다.

연구진은 7B, 13B, 70B 파라미터를 가진 모델 변형들을 공개했으며, 34B 변형도 개발했지만 충분한 레드팀 테스트 시간 확보를 위해 공개를 연기했습니다. 모든 모델은 영어로만 테스트되었으며, 모든 시나리오를 완벽하게 검증하는 것은 불가능하다는 점을 강조했습니다. 따라서 Llama 2-Chat을 실제 애플리케이션에 적용할 때는 해당 용도에 맞는 추가적인 안전성 테스트와 튜닝이 필요합니다.

연구진은 언어 모델의 안전한 공개가 사회에 긍정적인 영향을 미칠 것이라 믿지만, Bender와 Weidinger 등의 연구진이 지적한 것처럼 이러한 기술이 잠재적 위험을 수반한다는 점도 인정합니다. 이에 따라 책임감 있는 사용을 위한 가이드와 코드 예제를 함께 제공하여, 개발자들이 Llama 2와 Llama 2-Chat을 안전하게 활용할 수 있도록 지원하고 있습니다.

### Llama 2의 프리트레이닝 아키텍처와 학습 방법

Llama 2 모델은 Touvron과 연구진이 개발한 기존 Llama 1의 아키텍처를 기반으로 하되, 여러 가지 중요한 개선사항을 도입했습니다. 가장 주목할 만한 변화는 프리트레이닝 데이터의 40% 증가, 컨텍스트 길이의 2배 확장(2k에서 4k로), 그리고 대형 모델에서의 추론 확장성을 개선하기 위한 그룹 쿼리 어텐션(GQA) 메커니즘의 도입입니다.

### 프리트레이닝 데이터 구성

Llama 2의 학습 데이터는 공개적으로 접근 가능한 소스에서 수집되었으며, Meta의 제품이나 서비스 데이터는 포함하지 않았습니다. 개인정보 보호를 위해 개인 정보가 많이 포함된 것으로 알려진 사이트의 데이터는 제외했습니다. 총 2조 개의 토큰으로 구성된 데이터셋을 사용했으며, 환각 현상을 줄이고 사실적 지식을 강화하기 위해 신뢰성 높은 소스의 데이터를 상향 샘플링했습니다.

### 모델 아키텍처와 학습 세부사항

Llama 2는 표준 트랜스포머 아키텍처를 기반으로 하며, RMSNorm을 사용한 사전 정규화, SwiGLU 활성화 함수, 그리고 회전 위치 임베딩(RoPE)을 적용했습니다. 학습에는 AdamW 옵티마이저를 사용했으며, 베타1은 0.9, 베타2는 0.95, 엡실론은 \\(10^{-5}\\)로 설정했습니다. 학습률은 코사인 스케줄을 따르며, 2000스텝의 웜업 기간을 거쳐 최대 학습률의 10%까지 감소하도록 설계했습니다. 가중치 감쇠는 0.1, 그래디언트 클리핑은 1.0을 적용했습니다.

![training_loss](https://ar5iv.org//html/2307.09288/assets/x4.png)

학습 손실 곡선을 보면, 2조 개의 토큰으로 학습한 후에도 모델이 포화 상태에 도달하지 않았음을 확인할 수 있습니다. 이는 모델이 더 많은 데이터로 학습할 경우 성능이 더욱 향상될 수 있음을 시사합니다.

### 토크나이저 구현

토크나이저는 Llama 1과 동일하게 SentencePiece 구현체를 사용한 바이트페어 인코딩(BPE) 알고리즘을 채택했습니다. 모든 숫자는 개별 자릿수로 분할되며, 알 수 없는 UTF-8 문자는 바이트 단위로 분해됩니다. 전체 어휘 크기는 32,000 토큰으로 설정되었습니다.
### 학습 인프라와 탄소 발자국

Llama 2의 학습은 Meta의 연구 슈퍼 클러스터(RSC)와 내부 프로덕션 클러스터에서 진행되었으며, 두 시스템 모두 NVIDIA A100 GPU를 활용했습니다. 두 클러스터는 네트워크 연결 방식에서 주요한 차이를 보입니다. RSC는 NVIDIA Quantum InfiniBand를 사용하는 반면, 프로덕션 클러스터는 상용 이더넷 스위치 기반의 RoCE(RDMA over converged Ethernet) 솔루션을 채택했습니다. 두 시스템 모두 200Gbps의 종단점 연결을 제공하지만, GPU 전력 소비 제한에서 차이가 있어 RSC는 400W, 프로덕션 클러스터는 350W로 설정되었습니다.

이러한 이중 클러스터 구성을 통해 대규모 학습에서 서로 다른 네트워크 연결 방식의 적합성을 비교할 수 있었습니다. 특히 주목할 만한 점은 상대적으로 저렴한 상용 RoCE 네트워크가 2000개의 GPU까지 확장할 때 고가의 InfiniBand에 근접한 성능을 보여주었다는 것입니다. 이는 대규모 언어 모델의 프리트레이닝이 더욱 접근 가능해질 수 있음을 시사합니다.

### 프리트레이닝의 환경 영향

Llama 2 모델군의 프리트레이닝 과정에서 발생한 탄소 배출량을 정확히 측정하기 위해, GPU 장치의 전력 소비량과 탄소 효율성을 고려한 계산을 수행했습니다. 실제 GPU 전력 사용량은 활용도에 따라 달라질 수 있으며, 열설계 전력(TDP)을 기준으로 추정했습니다. 총 330만 GPU 시간의 연산이 수행되었으며, 이는 539 tCO₂eq의 탄소 배출량으로 추산됩니다.

다만 이 계산에는 네트워크 연결이나 GPU 외 서버 전력 소비, 데이터센터 냉각 시스템 등의 추가적인 전력 수요는 포함되지 않았습니다. 또한 GPU와 같은 AI 하드웨어 생산 과정에서 발생하는 탄소 배출량도 전체 환경 영향을 평가할 때 고려해야 할 요소입니다.

Meta는 이러한 환경 영향을 상쇄하기 위해 지속가능성 프로그램을 통해 발생한 탄소 배출량의 100%를 상쇄했습니다. 더불어 Llama 2를 공개함으로써 다른 기업들이 별도로 프리트레이닝을 수행할 필요가 없어져, 전 세계적인 자원 절약에도 기여할 것으로 기대됩니다.
### Llama 2의 아키텍처 세부 구성

Llama 2의 핵심 아키텍처적 차이점 중 하나인 그룹 쿼리 어텐션(GQA)은 34B와 70B 모델에서 추론 확장성을 크게 개선했습니다. GQA는 어텐션 헤드를 그룹으로 나누어 각 그룹이 동일한 쿼리를 공유하도록 설계되었습니다. 이는 메모리 사용량을 줄이면서도 멀티헤드 어텐션의 표현력을 유지할 수 있게 해줍니다.

RMSNorm을 통한 사전 정규화는 다음과 같은 수식으로 구현됩니다.

\\[ x_{norm} = \frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^n x_i^2 + \epsilon}} \\]

여기서 \\(x\\)는 입력 벡터, \\(n\\)은 벡터의 차원, \\(\epsilon\\)은 수치 안정성을 위한 작은 상수입니다. 이 정규화 방식은 학습 안정성을 높이고 그래디언트 소실 문제를 완화합니다.

SwiGLU 활성화 함수는 다음과 같이 정의됩니다.

\\[ \text{SwiGLU}(x, W, V, b, c) = \text{Swish}_{\beta}(xW + b) \otimes (xV + c) \\]

여기서 \\(\text{Swish}_{\beta}(x) = x \cdot \sigma(\beta x)\\)이며, \\(\sigma\\)는 시그모이드 함수입니다. 이 활성화 함수는 기존의 ReLU나 GELU에 비해 더 부드러운 그래디언트를 제공하며, 특히 깊은 신경망에서 더 나은 성능을 보여줍니다.

회전 위치 임베딩(RoPE)은 복소수 공간에서 상대적 위치 정보를 인코딩합니다.

\\[ \text{RoPE}(x_m, \theta) = x_m \cdot e^{i\theta_m} \\]

여기서 \\(x_m\\)은 m번째 위치의 임베딩 벡터이고, \\(\theta_m\\)은 위치에 따른 회전 각도입니다. 이 방식은 토큰 간의 상대적 거리를 효과적으로 표현할 수 있게 해주며, 특히 확장된 4k 컨텍스트 길이에서 중요한 역할을 합니다.

코사인 학습률 스케줄은 다음 수식을 따릅니다.

\\[ \eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{t\pi}{T})) \\]

여기서 \\(\eta_t\\)는 스텝 t에서의 학습률, \\(\eta_{max}\\)와 \\(\eta_{min}\\)은 각각 최대, 최소 학습률, T는 총 학습 스텝 수입니다. 2000스텝의 웜업 기간 동안은 선형적으로 학습률을 증가시키다가, 이후 코사인 스케줄에 따라 감소시킵니다.
### Llama 2의 학습 성능과 하드웨어 최적화

Llama 2의 학습 과정에서 모델 크기에 따른 성능 차이를 면밀히 분석한 결과, 흥미로운 스케일링 패턴이 관찰되었습니다. 7B와 13B 모델은 3.0 x 10^-4의 학습률을, 34B와 70B 모델은 1.5 x 10^-4의 학습률을 사용했는데, 이는 모델 크기가 커질수록 더 작은 학습률이 안정적인 학습에 효과적임을 보여줍니다.

학습 배치 크기는 모든 모델에서 4M 토큰으로 통일했으며, 이는 메모리 효율성과 학습 안정성 사이의 최적점을 찾은 결과입니다. 특히 그룹 쿼리 어텐션(GQA)을 도입한 34B와 70B 모델에서는 메모리 사용량을 크게 줄이면서도 학습 성능을 유지할 수 있었습니다.

하드웨어 구성에서 주목할 만한 점은 RoCE 네트워크의 효율성입니다. 기존의 고가 InfiniBand 솔루션과 비교했을 때, RoCE는 2000 GPU 규모까지 거의 동등한 성능을 보여주었습니다. 이는 다음과 같은 스케일링 효율성 수식으로 표현됩니다.

\\[ E(n) = \frac{T_1}{nT_n} \times 100\% \\]

여기서 \\(T_1\\)은 단일 GPU에서의 처리 시간, \\(T_n\\)은 n개의 GPU를 사용할 때의 처리 시간입니다. RoCE 구성에서는 2000 GPU까지 90% 이상의 스케일링 효율성을 유지했습니다.

전력 효율성 측면에서도 주목할 만한 최적화가 이루어졌습니다. GPU 전력 소비를 350W로 제한한 프로덕션 클러스터는 400W 설정의 RSC와 비교했을 때 다음과 같은 에너지 효율성 향상을 보였습니다.

\\[ \text{Energy Efficiency} = \frac{\text{Tokens Processed}}{\text{Total Power Consumed}} \\]

이러한 하드웨어 최적화와 효율적인 분산 학습 구성을 통해, Llama 2는 2조 개의 토큰을 효과적으로 학습할 수 있었으며, 이는 모델의 성능과 환경적 영향 사이의 균형을 잘 보여주는 사례가 되었습니다.

### 학술 벤치마크 평가 결과

Llama 2의 성능을 평가하기 위해 연구진은 다양한 학술 벤치마크에서 모델의 성능을 측정했습니다. 평가는 Llama 1과 Llama 2의 기본 모델, MosaicML의 MPT 모델, 그리고 Falcon 모델을 대상으로 진행되었습니다. 모든 평가는 연구진의 내부 평가 라이브러리를 사용하여 수행되었으며, MPT와 Falcon 모델의 결과는 내부적으로 재현하여 공개된 결과와 비교했습니다.

평가 결과는 코드, 상식 추론, 세계 지식, 독해력, 수학, MMLU, BBH, AGI Eval 등 다양한 카테고리로 분류되었습니다. 코드 평가에서는 HumanEval과 MBPP 벤치마크의 pass@1 점수를 측정했으며, 상식 추론 평가에는 PIQA, SIQA, HellaSwag, WinoGrande, ARC, OpenBookQA, CommonsenseQA 등이 포함되었습니다. 세계 지식 평가는 NaturalQuestions와 TriviaQA를, 독해력 평가는 SQuAD, QuAC, BoolQ를 사용했습니다. 수학 능력은 GSM8K와 MATH 벤치마크를 통해 평가되었습니다.

![performance_comparison](https://ar5iv.org//html/2307.09288/assets/x12.png)

평가 결과, Llama 2 모델은 Llama 1 모델보다 전반적으로 우수한 성능을 보여주었습니다. 특히 Llama 2 70B 모델은 Llama 1 65B 모델과 비교했을 때 MMLU에서 약 5점, BBH에서 약 8점의 성능 향상을 달성했습니다. Llama 2의 7B와 34B 모델은 코드 벤치마크를 제외한 모든 카테고리에서 동일한 크기의 MPT 모델을 능가했으며, Falcon 모델과 비교했을 때도 모든 카테고리에서 우수한 성능을 보여주었습니다.

비공개 모델과의 비교에서 Llama 2 70B는 GPT-3.5와 MMLU와 GSM8K에서 근접한 성능을 보여주었지만, 코딩 벤치마크에서는 상당한 격차가 있었습니다. PaLM(540B)과 비교했을 때는 대부분의 벤치마크에서 동등하거나 더 나은 성능을 보여주었습니다. 하지만 GPT-4와 PaLM-2-L과는 여전히 큰 성능 차이가 존재했습니다.

### 파인튜닝 방법론

Llama 2-Chat은 수개월에 걸친 연구와 정렬 기술의 반복적인 적용을 통해 개발되었습니다. 이 과정에는 지도 학습 기반 파인튜닝과 인간 피드백을 통한 강화학습(RLHF)이 포함되었으며, 상당한 컴퓨팅 자원과 주석 작업이 필요했습니다. 연구진은 또한 다중 턴 대화의 흐름을 제어하는데 도움이 되는 새로운 기술인 고스트 어텐션(Ghost Attention, GAtt)을 개발했습니다.
### 지도 학습 기반 파인튜닝 세부 방법론

Llama 2의 파인튜닝은 공개적으로 접근 가능한 명령어 튜닝 데이터셋으로 시작되었습니다. 연구진은 데이터의 품질이 모델의 성능에 미치는 영향을 분석한 결과, 수백만 개의 제3자 데이터셋을 제외하고 수천 개의 고품질 SFT 데이터에 집중하는 것이 더 효과적임을 발견했습니다. 이는 Zhou와 연구진이 제시한 연구 결과와 유사한 맥락으로, 제한된 수의 깨끗한 명령어 튜닝 데이터만으로도 높은 수준의 성능을 달성할 수 있다는 점을 보여줍니다.

연구진은 총 27,540개의 주석을 수집했으며, Meta 사용자 데이터는 포함하지 않았습니다. 데이터 품질 검증을 위해 180개의 예시를 면밀히 검토했고, 인간 주석자가 작성한 응답과 모델이 생성한 샘플을 비교 분석했습니다. 흥미롭게도, SFT 모델이 생성한 출력이 인간 주석자가 직접 작성한 SFT 데이터와 비교했을 때 경쟁력 있는 수준을 보여주었습니다.

파인튜닝의 기술적 세부사항으로는 코사인 학습률 스케줄을 사용했으며, 초기 학습률은 \\(2 \times 10^{-5}\\), 가중치 감쇠는 0.1, 배치 크기는 64, 시퀀스 길이는 4096 토큰으로 설정했습니다. 각 샘플은 프롬프트와 응답으로 구성되며, 모델의 시퀀스 길이를 효율적으로 활용하기 위해 학습 세트의 모든 프롬프트와 응답을 연결했습니다. 특수 토큰을 사용하여 프롬프트와 응답 세그먼트를 구분했으며, 자기회귀 목적 함수를 사용하고 사용자 프롬프트의 토큰에 대한 손실을 0으로 설정하여 응답 토큰에 대해서만 역전파를 수행했습니다. 최종적으로 모델은 2 에포크 동안 파인튜닝되었습니다.

![sft_annotation](https://ar5iv.org//html/2307.09288/assets/x5.png)

이러한 파인튜닝 과정을 통해 Llama 2는 유용성과 안전성 측면에서 균형 잡힌 응답을 생성할 수 있게 되었습니다. 예를 들어, 위 그림에서 볼 수 있듯이 모델은 주기율표의 첫 10개 원소를 기억하는 시를 작성하는 등의 창의적인 과제와, 부적절한 요청에 대해 안전하게 거절하는 응답을 생성할 수 있습니다.
### 인간 피드백을 통한 강화학습 구현

인간 피드백을 통한 강화학습(RLHF)은 파인튜닝된 언어 모델의 행동을 인간의 선호도와 지시 사항에 더 잘 부합하도록 조정하는 학습 절차입니다. 연구진은 인간 주석자들이 두 가지 모델 출력 중 하나를 선택하는 방식으로 경험적 선호도 데이터를 수집했습니다. 이렇게 수집된 인간 피드백은 보상 모델을 학습하는 데 사용되었으며, 보상 모델은 인간 주석자들의 선호도 패턴을 학습하여 선호도 결정을 자동화할 수 있게 되었습니다.

주석자들은 먼저 프롬프트를 작성한 다음, 제공된 기준에 따라 두 가지 모델 응답 중 하나를 선택하도록 요청받았습니다. 프롬프트의 다양성을 최대화하기 위해 두 응답은 서로 다른 모델 변형에서 샘플링되었으며, 온도 하이퍼파라미터를 다양하게 조정했습니다. 주석자들은 강제 선택뿐만 아니라 선택한 응답을 다른 응답과 비교하여 얼마나 선호하는지도 평가했습니다. '매우 우수', '우수', '약간 우수', '거의 동등/불확실' 중 하나를 선택했습니다.

선호도 주석 수집은 유용성과 안전성 두 가지 측면에 초점을 맞추었습니다. 유용성은 Llama 2-Chat의 응답이 사용자의 요청을 얼마나 잘 충족시키고 요청된 정보를 제공하는지를 평가하며, 안전성은 응답이 안전하지 않은지를 평가합니다. 예를 들어 "폭탄 제조에 대한 자세한 지침 제공"은 유용할 수 있지만 안전 지침에 따르면 안전하지 않은 것으로 간주됩니다.

![reward_model_accuracy](https://ar5iv.org//html/2307.09288/assets/x6.png)

연구진은 매주 배치 단위로 인간 주석을 수집했습니다. 더 많은 선호도 데이터를 수집함에 따라 보상 모델이 개선되었고, 이를 통해 점진적으로 더 나은 버전의 Llama 2-Chat을 학습할 수 있었습니다. Llama 2-Chat의 개선은 모델의 데이터 분포를 변화시켰고, 보상 모델의 정확도는 이러한 새로운 샘플 분포에 노출되지 않으면 빠르게 저하될 수 있습니다. 이는 과도한 특수화 현상 때문입니다. 따라서 새로운 Llama 2-Chat 튜닝 반복을 시작하기 전에 최신 Llama 2-Chat 반복을 사용하여 새로운 선호도 데이터를 수집하는 것이 중요했습니다. 이 단계는 보상 모델이 분포에 맞춰 유지되고 최신 모델에 대해 정확한 보상을 유지하는 데 도움이 되었습니다.
### 보상 모델링 구현 세부사항

보상 모델은 모델 응답과 해당 프롬프트(이전 대화 맥락 포함)를 입력으로 받아 응답의 품질(유용성과 안전성)을 나타내는 스칼라 점수를 출력합니다. 이러한 응답 점수를 보상으로 활용하여 RLHF 과정에서 Llama 2-Chat을 최적화하여 인간의 선호도에 더 잘 부합하고 유용성과 안전성이 향상되도록 했습니다.

연구진은 유용성과 안전성이 때로는 상충관계에 있다는 것을 발견했습니다. 이러한 문제를 해결하기 위해 두 개의 별도 보상 모델을 학습했습니다 - 유용성에 최적화된 Helpfulness RM과 안전성에 최적화된 Safety RM입니다. 두 모델 모두 사전 학습된 채팅 모델 체크포인트로 초기화되어, 채팅 모델이 가진 지식을 보상 모델도 활용할 수 있도록 했습니다. 이는 예를 들어 두 모델 간의 정보 불일치로 인해 환각을 선호하는 경우를 방지합니다.

보상 모델의 학습을 위해 수집된 쌍별 인간 선호도 데이터를 이진 순위 레이블 형식(선택됨 & 거부됨)으로 변환하고, 선택된 응답이 그 대응되는 응답보다 더 높은 점수를 갖도록 강제했습니다. 학습에는 다음과 같은 이진 순위 손실을 사용했습니다.

\\[ \mathcal{L}_{\text{ranking}} = -\text{log}(\sigma(r_{\theta}(x,y_{c})-r_{\theta}(x,y_{r}))) \\]

여기서 \\(r_{\theta}(x,y)\\)는 프롬프트 \\(x\\)와 완성 \\(y\\)에 대한 모델 가중치 \\(\theta\\)를 가진 스칼라 점수 출력입니다. \\(y_c\\)는 주석자가 선택한 선호 응답이고 \\(y_r\\)은 거부된 대응입니다.

이 이진 순위 손실을 기반으로, 더 나은 유용성과 안전성 보상 모델을 위해 마진 컴포넌트를 추가로 도입했습니다.

\\[ \mathcal{L}_{\text{ranking}} = -\text{log}(\sigma(r_{\theta}(x,y_{c})-r_{\theta}(x,y_{r})-m(r))) \\]

여기서 마진 \\(m(r)\\)은 선호도 평가의 이산 함수입니다. 자연스럽게 응답 간 차이가 큰 쌍에는 큰 마진을, 유사한 응답에는 작은 마진을 사용했습니다.

![reward_model_results](https://ar5iv.org//html/2307.09288/assets/x7.png)

보상 모델의 학습 데이터는 새로 수집한 데이터와 기존의 오픈소스 선호도 데이터셋을 결합하여 구성했습니다. 초기에는 선호도 주석 데이터를 수집하는 동안 오픈소스 데이터셋을 사용하여 보상 모델을 부트스트랩했습니다. RLHF 맥락에서 보상 신호의 역할은 Llama 2-Chat 출력에 대한 인간의 선호도를 학습하는 것이므로, 오픈소스 선호도 데이터셋으로부터의 부정적 전이는 관찰되지 않았습니다.
### 보상 모델의 데이터 구성과 학습 세부사항

유용성 보상 모델은 모든 Meta 유용성 데이터와 함께 Meta 안전성 데이터 및 오픈소스 데이터셋에서 균등하게 샘플링된 데이터를 동일한 비율로 혼합하여 학습되었습니다. 안전성 보상 모델은 모든 Meta 안전성 데이터와 Anthropic Harmless 데이터를 기반으로 하며, Meta 유용성 데이터와 오픈소스 유용성 데이터를 90:10 비율로 혼합하여 학습했습니다. 특히 10%의 유용성 데이터를 포함하는 것은 선택된 응답과 거부된 응답이 모두 안전하다고 판단된 샘플에 대한 정확도 향상에 도움이 되었습니다.

학습은 한 에포크 동안 진행되었으며, 초기 실험에서 더 오래 학습하면 과적합이 발생할 수 있다는 것을 발견했습니다. 옵티마이저 파라미터는 기본 모델과 동일하게 설정했으며, 최대 학습률은 70B 파라미터 Llama 2-Chat의 경우 \\(5 \times 10^{-6}\\), 나머지 모델의 경우 \\(1 \times 10^{-5}\\)로 설정했습니다. 학습률은 코사인 스케줄에 따라 최대 학습률의 10%까지 감소하도록 설계되었습니다. 총 스텝 수의 3%를 웜업 기간으로 설정했으며, 최소 5스텝을 보장했습니다. 효과적인 배치 크기는 512쌍, 즉 배치당 1024행으로 고정했습니다.

보상 모델의 성능 평가를 위해 각 보상 모델링 주석 배치에서 1000개의 예시를 테스트 세트로 보관했습니다. 이를 "Meta Helpfulness"와 "Meta Safety" 테스트 세트로 통합했습니다. 비교 기준점으로 FLAN-T5-xl 기반의 SteamSHP-XL, DeBERTa V3 Large 기반의 Open Assistant 보상 모델, 그리고 OpenAI API를 통해 접근 가능한 GPT4를 사용했습니다.

평가 결과, 연구진의 보상 모델은 Llama 2-Chat을 기반으로 수집된 내부 테스트 세트에서 가장 우수한 성능을 보여주었습니다. 유용성 보상 모델은 Meta Helpfulness 테스트 세트에서, 안전성 보상 모델은 Meta Safety 테스트 세트에서 최고의 성능을 달성했습니다. GPT-4는 보상 모델링 작업에 직접적으로 학습되지 않았음에도 다른 비Meta 보상 모델들보다 우수한 성능을 보여주었습니다.

선호도 평가에 따른 점수를 그룹화했을 때, "매우 우수" 테스트 세트에서 가장 높은 정확도를 보였으며, 비교 쌍이 더 유사해질수록(예: "약간 우수") 정확도가 점진적으로 감소했습니다. 이는 두 응답이 유사할 때 주석자의 주관성과 미묘한 차이에 대한 의존도가 높아지기 때문에 인간의 선호도를 모델링하는 것이 더 어려워지는 것으로 해석됩니다.
### 보상 모델의 스케일링 분석과 RLHF 구현

보상 모델의 데이터와 모델 크기에 따른 스케일링 경향을 분석하기 위해 연구진은 매주 수집된 보상 모델 데이터의 증가량에 따라 다양한 크기의 모델을 파인튜닝했습니다. 분석 결과, 더 큰 모델이 동일한 양의 데이터로도 더 높은 성능을 달성할 수 있음을 확인했습니다. 특히 주목할 만한 점은 현재 사용된 주석 데이터의 양으로는 아직 성능이 포화되지 않았다는 것입니다. 이는 더 많은 주석을 통해 보상 모델의 성능을 더욱 향상시킬 수 있음을 시사합니다.

연구진은 RLHF 파인튜닝을 위해 두 가지 주요 알고리즘을 탐색했습니다.

근접 정책 최적화(PPO)는 RLHF 문헌에서 표준으로 사용되는 방법입니다. 이 방법은 다음과 같은 목적 함수를 최적화합니다.

\\[ \arg\max_{\pi}\mathbb{E}_{p\sim\mathcal{D},g\sim\pi}[R(g\mid p)] \\]

최종 보상 함수는 다음과 같이 정의됩니다.

\\[ R(g\mid p)=\tilde{R}_{c}(g\mid p)-\beta D_{KL}(\pi_{\theta}(g\mid p)\parallel\pi_{0}(g\mid p)) \\]

여기서 원래 정책 \\(\pi_0\\)에서 벗어나는 것에 대한 페널티 항이 포함됩니다. 이는 학습 안정성을 높이고 보상 모델에서는 높은 점수를 얻지만 인간 평가에서는 낮은 점수를 받는 보상 해킹을 줄이는 데 도움이 됩니다.

리젝션 샘플링 파인튜닝에서는 각 프롬프트에 대해 K개의 출력을 샘플링하고 보상이 가장 높은 후보를 선택합니다. 선택된 출력을 새로운 골드 스탠다드로 간주하고 이 새로운 순위가 매겨진 샘플 세트에서 모델을 파인튜닝하여 보상을 강화합니다.

두 RL 알고리즘의 주요 차이점은 탐색의 폭과 깊이에 있습니다. 리젝션 샘플링은 주어진 프롬프트에 대해 K개의 샘플을 탐색하는 반면, PPO는 하나의 생성만 수행합니다. 깊이 측면에서 PPO는 이전 스텝의 그래디언트 업데이트 후 업데이트된 모델 정책에서 샘플을 생성하는 반면, 리젝션 샘플링은 모델의 초기 정책에서 모든 출력을 샘플링하여 새로운 데이터셋을 수집한 후 SFT와 유사한 파인튜닝을 적용합니다.
### RLHF 반복 학습과 고스트 어텐션 구현

RLHF-V1부터 RLHF-V5까지 연속적인 버전의 RLHF 모델을 학습했습니다. RLHF-V4까지는 리젝션 샘플링 파인튜닝만 사용했으며, 그 이후에는 두 가지 방법을 순차적으로 결합하여 리젝션 샘플링 체크포인트에 PPO를 적용한 후 다시 샘플링하는 방식을 채택했습니다.

리젝션 샘플링은 70B Llama 2-Chat에서만 수행되었습니다. 더 작은 모델들은 더 큰 모델의 리젝션 샘플링된 데이터로 파인튜닝되어, 큰 모델의 능력을 작은 모델로 증류했습니다. 각 반복 단계에서 가장 최근 모델에서 각 프롬프트에 대해 K개의 답변을 샘플링하고, 당시 사용 가능한 최고의 보상 모델로 각 샘플을 평가한 후 주어진 프롬프트에 대해 최고의 답변을 선택했습니다.

RLHF V3까지는 이전 반복에서 수집된 샘플만을 사용했습니다. 예를 들어 RLHF V3는 RLHF V2의 샘플만을 사용하여 학습되었습니다. 그러나 이 방법은 지속적인 개선에도 불구하고 일부 능력의 퇴화를 초래했습니다. 예를 들어 RLHF V3는 이전 버전들에 비해 운율이 있는 시를 작성하는 능력이 저하되었습니다. 이에 대응하여 이후 반복에서는 전략을 수정하여 RLHF-V1과 RLHF-V2에서 사용된 것과 같은 모든 이전 반복의 최고 성능 샘플을 포함시켰습니다.

PPO 구현에서는 안전성 위반 사례를 필터링하기 위해 0.15의 임계값을 사용했으며, 이는 Meta Safety 테스트 세트에서 0.89의 정밀도와 0.55의 재현율을 달성했습니다. KL 페널티 항의 균형을 맞추기 위해 최종 선형 점수를 정규화했으며, 7B와 13B 모델에는 β=0.01, 34B와 70B 모델에는 β=0.005를 설정했습니다. 모든 모델은 200에서 400회의 반복 동안 학습되었으며, 보류된 프롬프트에 대한 평가를 통해 조기 중단을 수행했습니다.

### 고스트 어텐션을 통한 다중 턴 일관성 향상

다중 턴 대화에서는 간결한 응답 유지나 특정 인물 역할 수행과 같은 일부 지시사항이 모든 대화 턴에 적용되어야 합니다. 그러나 초기 RLHF 모델들은 몇 턴의 대화 후에 초기 지시사항을 잊어버리는 경향이 있었습니다. 이러한 한계를 해결하기 위해 연구진은 컨텍스트 증류에서 영감을 받은 고스트 어텐션(GAtt)을 제안했습니다.

GAtt는 파인튜닝 데이터를 수정하여 어텐션이 다단계 프로세스에서 더 잘 집중할 수 있도록 돕습니다. 이 방법은 대화의 모든 턴에서 시스템 메시지를 포함시키는 대신, 첫 번째 턴에만 포함시키고 이전 턴의 토큰에 대한 손실을 0으로 설정하여 학습 중 불일치를 방지합니다. 학습 지시사항으로는 취미, 언어, 공인 인물과 같은 합성 제약 조건을 사용했으며, 지시사항의 복잡성과 다양성을 높이기 위해 이러한 제약 조건들을 무작위로 조합했습니다.

### Llama 2의 안전성 평가와 개선 방법론

Llama 2 모델의 안전성을 확보하기 위해 Meta 연구진은 프리트레이닝 데이터와 모델에 대한 포괄적인 안전성 조사를 수행했습니다. 이는 데이터의 투명성을 높이고 잠재적 편향성과 같은 문제의 근본 원인을 파악하기 위한 것입니다. 연구진은 언어 분포, 인구통계학적 대표성, 유해성 등 다양한 측면에서 프리트레이닝 데이터를 분석했으며, 기존 안전성 벤치마크를 통해 프리트레인된 모델을 평가했습니다.

프리트레이닝 과정에서 Meta의 표준 개인정보 보호 및 법적 검토 절차를 준수했으며, Meta 사용자 데이터는 전혀 사용하지 않았습니다. 개인정보가 많이 포함된 것으로 알려진 사이트의 데이터는 제외했으며, 탄소 발자국을 줄이기 위해 효율적인 학습 방법을 적용했습니다. 연구진은 데이터 필터링을 최소화하여 혐오 발언 분류와 같은 다양한 작업에 모델을 활용할 수 있도록 했으며, 이는 Welbl과 연구진, Korbak과 연구진이 제시한 것처럼 더 적은 예시로도 안전성 튜닝을 효과적으로 수행할 수 있게 합니다.

인구통계학적 대표성 분석을 위해 영어 학습 데이터에서 가장 일반적인 대명사의 빈도를 조사했습니다. 그 결과 'He' 대명사가 'She' 대명사보다 더 자주 등장하는 것으로 나타났으며, 이는 Chowdhery와 연구진이 관찰한 것과 유사한 패턴입니다. 이는 모델이 'She' 대명사가 포함된 맥락에 대해 상대적으로 덜 학습할 수 있음을 시사합니다.

![demographic_analysis](https://ar5iv.org//html/2307.09288/assets/x14.png)

HolisticBias 데이터셋의 인구통계학적 용어를 사용하여 다양한 집단의 대표성도 분석했습니다. 종교, 성별과 성, 국적, 인종과 민족, 성적 지향 등 5개 축으로 구분하여 각 축의 상위 5개 용어 빈도를 측정했습니다. 성별과 성 측면에서는 'She' 대명사의 등장 빈도가 낮음에도 'female'이라는 용어는 더 많은 문서에서 발견되었습니다. 이는 Blodgett과 연구진이 지적한 것처럼 이러한 용어들의 언어학적 표지성 차이를 반영할 수 있습니다.

데이터의 유해성을 평가하기 위해 ToxiGen 데이터셋으로 파인튜닝된 HateBERT 분류기를 사용했습니다. 각 문서의 각 줄을 개별적으로 평가하고 평균을 내어 문서 점수를 산출했습니다. 전체 데이터의 약 0.2%가 0.5 이상의 유해성 점수를 받았으며, 이는 프리트레이닝 데이터에 소량의 유해 콘텐츠가 포함되어 있음을 보여줍니다.
### 안전성 벤치마크와 평가 방법론

Llama 2의 안전성을 평가하기 위해 연구진은 세 가지 핵심 차원의 자동 벤치마크를 활용했습니다. 첫째로 TruthfulQA를 통해 모델이 오개념이나 잘못된 믿음으로 인한 허위 정보를 생성하는지 평가했습니다. 이 벤치마크는 Lin과 연구진이 개발한 것으로, 모델이 사실성과 상식에 부합하는 신뢰할 수 있는 출력을 생성할 수 있는지 측정합니다.

두 번째로 유해성 평가를 위해 ToxiGen 벤치마크를 사용했습니다. Hartvigsen과 연구진이 개발한 이 도구는 모델이 유해하거나, 무례하거나, 적대적이거나, 암묵적으로 혐오적인 내용을 생성하는 경향을 측정합니다. 특히 다양한 집단에 대한 혐오 발언과 유해 콘텐츠 생성을 평가합니다.

세 번째로 편향성 평가를 위해 BOLD 벤치마크를 활용했습니다. Dhamala와 연구진이 개발한 이 도구는 모델이 생성하는 내용에서 기존의 고정관념적 사회적 편향이 얼마나 재생산되는지 평가합니다. 특히 인구통계학적 속성에 따른 감성 변화를 분석합니다.

![benchmark_comparison](https://ar5iv.org//html/2307.09288/assets/x15.png)

연구진은 Llama 1, Falcon, MPT와 비교 평가를 수행했습니다. 디코딩 시에는 온도 파라미터를 0.1로 설정하고, 핵 샘플링의 top-p를 0.9로 설정했습니다. TruthfulQA에서는 진실되고 유익한 생성 비율을, ToxiGen에서는 유해하다고 판단된 생성 비율을 측정했습니다.

Llama 1-7B와 비교했을 때 Llama 2-7B는 진실성과 유익성이 21.37% 증가했고 유해성은 7.61% 감소했습니다. 그러나 13B와 70B 모델에서는 유해성이 증가하는 현상이 관찰되었는데, 이는 더 큰 프리트레이닝 데이터나 데이터셋 구성의 차이에서 기인할 수 있습니다. Bender와 연구진은 프리트레이닝 데이터셋 크기와 모델의 유해성 또는 편향성 사이의 관계를 제기했지만, 이를 검증하기 위한 실증적 연구는 아직 진행 중입니다.

### 안전성 파인튜닝 방법론

연구진은 안전성 파인튜닝을 위해 세 가지 주요 기술을 적용했습니다. 첫째로 지도 학습 기반 안전성 파인튜닝을 통해 적대적 프롬프트와 안전한 응답 예시를 수집하여 일반 지도 학습 과정에 포함시켰습니다. 이는 RLHF 이전에 모델이 안전성 가이드라인을 학습하도록 하여 고품질의 인간 선호도 데이터 주석 작업의 기반을 마련했습니다.

둘째로 안전성 RLHF를 통해 안전성에 특화된 보상 모델을 학습하고, 더 도전적인 적대적 프롬프트를 수집하여 리젝션 샘플링 스타일의 파인튜닝과 PPO 최적화를 수행했습니다. 마지막으로 안전성 컨텍스트 증류를 통해 RLHF 파이프라인을 개선했습니다. 이는 프롬프트에 안전성 관련 선행 프롬프트를 추가하여 더 안전한 모델 응답을 생성하고, 이를 선행 프롬프트 없이 모델에 파인튜닝하는 방식입니다.
### 안전성 카테고리와 주석 가이드라인

안전성 파인튜닝을 위해 연구진은 LLM의 알려진 한계를 바탕으로 주석 팀에게 두 가지 차원의 지침을 설계했습니다. 첫 번째는 위험 카테고리로, LLM이 잠재적으로 안전하지 않은 내용을 생성할 수 있는 주제를 다룹니다. 두 번째는 공격 벡터로, LLM의 바람직하지 않은 행동을 유도할 수 있는 다양한 질문 스타일을 포함합니다.

위험 카테고리는 크게 세 가지로 구분됩니다. 불법 및 범죄 활동(테러리즘, 절도, 인신매매 등), 혐오 및 유해 활동(명예훼손, 자해, 섭식장애, 차별 등), 그리고 비전문가 조언(의료, 재무, 법률 조언 등)입니다. 공격 벡터는 심리적 조작(권위 조작 등), 논리적 조작(거짓 전제 등), 구문적 조작(오타 등), 의미적 조작(은유 등), 관점 조작(역할극 등), 비영어 언어 등을 포함합니다.

연구진은 안전하고 유용한 모델 응답을 위한 모범 사례도 정의했습니다. 모델은 먼저 즉각적인 안전 문제를 다루고, 사용자에게 잠재적 위험을 설명한 후, 가능한 경우 추가 정보를 제공해야 합니다. 주석자들은 부정적인 사용자 경험 카테고리를 피하도록 요청받았습니다. 이러한 가이드라인은 모델의 일반적인 지침으로 사용되며, 새로 식별된 위험을 포함하도록 반복적으로 개선되고 수정됩니다.

### 안전성 지도 학습 파인튜닝 구현

앞서 설명한 가이드라인에 따라 훈련된 주석자들로부터 프롬프트와 안전한 모델 응답 시연을 수집하여 지도 학습 파인튜닝에 활용했습니다. 주석자들은 먼저 가이드라인에 정의된 대로 모델이 안전하지 않은 행동을 보일 수 있는 프롬프트를 생성하는 레드팀 작업을 수행했습니다. 이후 해당 프롬프트에 대해 모델이 생성해야 할 안전하고 유용한 응답을 작성했습니다.

### 안전성 RLHF 구현

연구진은 Llama 2-Chat의 초기 개발 단계에서 지도 학습의 안전한 시연으로부터 모델이 일반화할 수 있음을 관찰했습니다. 모델은 빠르게 상세한 안전 응답을 작성하고, 안전 문제를 다루며, 주제가 민감할 수 있는 이유를 설명하고, 추가적인 유용한 정보를 제공하는 방법을 학습했습니다. 특히 모델이 안전한 응답을 출력할 때, 이는 종종 평균적인 주석자가 작성하는 것보다 더 상세했습니다.

이러한 이유로 연구진은 수천 개의 지도 학습 시연을 수집한 후, 더 미묘한 응답을 작성하는 방법을 모델에 가르치기 위해 RLHF로 전환했습니다. RLHF를 통한 포괄적인 튜닝은 Bai와 연구진이 제시한 것처럼 모델이 잠재적인 해킹 시도에 더 강건해질 수 있다는 추가적인 이점이 있습니다.
### 안전성 RLHF의 구현과 평가

안전성 RLHF는 먼저 인간 선호도 데이터를 수집하여 진행됩니다. 주석자들은 안전하지 않은 행동을 유도할 수 있다고 생각하는 프롬프트를 작성하고, 해당 프롬프트에 대한 여러 모델 응답을 가이드라인에 따라 비교 평가합니다. 이렇게 수집된 인간 선호도 데이터는 안전성 보상 모델을 학습하는 데 사용되며, RLHF 단계에서 적대적 프롬프트를 재활용하여 모델로부터 샘플링합니다.

![safety_rlhf_impact](https://ar5iv.org//html/2307.09288/assets/x14.png)

안전성 RLHF의 효과는 Meta Safety 테스트 세트에서의 보상 모델 점수 분포를 통해 확인할 수 있습니다. 왼쪽 그래프에서 볼 수 있듯이, 안전성 RLHF 적용 후 샘플들이 좌상단에 집중되어 있어 모델의 안전성이 크게 향상되었음을 보여줍니다. 오른쪽 그래프는 Meta Helpfulness 테스트 세트에서의 유용성 보상 모델 점수 분포를 나타내며, 안전성 향상이 모델의 유용성을 저해하지 않았음을 보여줍니다.

### 안전성 데이터 스케일링의 영향

안전성은 본질적으로 롱테일 문제이며, 매우 특정한 소수의 케이스에서 발생하는 도전과제입니다. 연구진은 RLHF 단계에서 안전성 데이터의 양을 조절하여 그 영향을 조사했습니다. 유용성 학습 데이터는 약 0.9M 샘플로 고정한 채, 안전성 데이터의 비율을 0%에서 100%(약 0.1M 샘플)까지 점진적으로 증가시켰습니다.

![safety_scaling_trends](https://ar5iv.org//html/2307.09288/assets/x15.png)

실험 결과, 안전성 데이터의 비율이 증가할수록 모델이 위험하고 적대적인 프롬프트를 처리하는 능력이 크게 향상되었으며, 안전성 보상 모델 점수 분포의 왼쪽 꼬리가 줄어드는 것을 확인할 수 있었습니다. 동시에 평균 유용성 점수는 일정하게 유지되었는데, 이는 충분한 양의 유용성 학습 데이터가 확보되었기 때문으로 추정됩니다.

### 컨텍스트 증류를 통한 안전성 강화

연구진은 일반적인 안전성 선행 프롬프트와 응답 템플릿이 포함된 선행 프롬프트를 비교 실험했습니다. 응답 템플릿이 포함된 맞춤형 선행 프롬프트가 더 관련성 있는 응답을 생성하는 것으로 나타났습니다. 그러나 컨텍스트 증류가 때로는 응답 품질을 저하시킬 수 있다는 점도 발견했습니다. 특히 모델 응답이 이미 높은 품질일 경우, 컨텍스트 증류를 적용하면 선행 프롬프트를 과도하게 강조하여 지나치게 일반적인 우려사항을 반복하는 경향이 있었습니다.
### 레드팀 테스팅 방법론과 결과

Llama 2의 안전성을 더욱 강화하기 위해 연구진은 다양한 그룹의 내부 직원, 계약직 근로자, 외부 업체를 포함한 광범위한 레드팀 테스팅을 수행했습니다. 350명 이상의 전문가로 구성된 이 팀에는 사이버보안, 선거 사기, 소셜 미디어 허위정보, 법률, 정책, 시민권, 윤리, 소프트웨어 공학, 기계학습, 책임있는 AI, 창의적 글쓰기 분야의 전문가들이 포함되었습니다. 또한 다양한 사회경제적, 성별, 민족, 인종적 배경을 대표하는 개인들도 참여했습니다.

레드팀은 범죄 계획, 인신매매, 규제 약물, 성적 콘텐츠, 비전문가 의료/재무 조언, 개인정보 침해 등 광범위한 위험 카테고리에 걸쳐 모델을 테스트했습니다. 또한 가상 질문, 잘못된 형식/오타가 있는 입력, 확장된 대화 등 다양한 공격 벡터를 시도했습니다. 특히 무기(핵, 생물학, 화학, 사이버) 제작과 관련된 특수 테스트도 수행했으며, 이 분야에서 발견된 문제점들은 미미했고 이미 완화되었습니다.

![red_teaming_results](https://ar5iv.org//html/2307.09288/assets/img/safety_human_eval/overall_violation.png)

레드팀 테스팅의 결과를 정량화하기 위해 연구진은 모델의 견고성(robustness)을 측정했습니다. 견고성 γ는 전문가 집단이 시간당 발견한 위반 응답을 유발하는 프롬프트의 평균 수로 정의됩니다. 7B 모델의 경우, 여러 차례의 레드팀 테스팅과 모델 개선을 거치면서 γ가 1.8에서 0.45로 감소했습니다. 또한 이전 레드팀 테스팅에서 발견된 위반 프롬프트에 대한 거부율도 모델 버전이 업데이트될 때마다 평균 90%를 달성했습니다.

### Llama 2-Chat의 종합 안전성 평가

연구진은 약 2,000개의 적대적 프롬프트를 수집하여 인간 평가를 진행했습니다. 이 중 1,351개는 단일 턴, 623개는 다중 턴 대화로 구성되었습니다. 평가자들은 5점 리커트 척도를 사용하여 안전성 위반을 판단했으며, 1점(심각한 안전성 위반)부터 5점(안전성 위반 없음, 매우 유용함)까지 점수를 매겼습니다.

각 예시는 3명의 평가자가 주석을 달았으며, 다수결로 응답의 위반 여부를 결정했습니다. 평가자 간 신뢰도는 Gwet의 AC1/2 통계를 사용하여 측정했으며, 주석 배치에 따라 0.70에서 0.95 사이의 점수를 기록했습니다. Llama 2-Chat 주석의 경우 평균 IRR은 0.92로, 평가자들 간에 높은 수준의 합의가 이루어졌음을 보여줍니다.
### 안전성 평가 결과 분석

Llama 2-Chat의 안전성 평가 결과를 살펴보면, 모든 모델 크기에서 전반적으로 낮은 위반율을 보여주었습니다. ChatGPT와 Falcon이 그 다음으로 낮은 위반율을 기록했으며, MPT와 Vicuna가 뒤를 이었습니다. 다만 이러한 결과는 프롬프트 세트의 한계, 평가 가이드라인의 주관성, 콘텐츠 기준, 그리고 개별 평가자의 주관성에 영향을 받을 수 있다는 점을 고려해야 합니다.

Falcon의 경우 일반적으로 짧은 응답(1-2문장)을 생성하는 경향이 있어 안전하지 않은 콘텐츠를 생성할 가능성이 낮았지만, 이로 인해 전반적인 유용성도 떨어졌습니다. 이는 많은 Falcon 응답이 평점 3을 받은 것에서 확인할 수 있습니다. 결과적으로 Falcon의 평균 평점은 3.88로, Llama 2-Chat(34B)의 4.45에 비해 상당히 낮았습니다.

### 다중 턴 대화의 안전성 분석

단일 턴과 다중 턴 대화에서의 위반율을 분석한 결과, 모든 모델에서 다중 턴 대화가 안전하지 않은 응답을 유도하기 더 쉬운 것으로 나타났습니다. 그럼에도 Llama 2-Chat은 특히 다중 턴 대화에서 다른 모델들과 비교했을 때 우수한 성능을 보여주었습니다. Falcon은 단일 턴 대화에서는 간결성 덕분에 특히 좋은 성능을 보였지만, 다중 턴 대화에서는 성능이 크게 저하되었는데, 이는 다중 턴 지도 학습 데이터의 부족 때문일 수 있습니다.

### 진실성, 유해성, 편향성 평가

파인튜닝된 Llama 2-Chat은 프리트레인된 Llama 2와 비교했을 때 진실성과 유해성 측면에서 큰 개선을 보여주었습니다. 70B 모델의 경우 진실성이 50.18%에서 64.14%로 향상되었고, 유해성은 24.60%에서 0.01%로 크게 감소했습니다. 특히 모든 크기의 Llama 2-Chat 모델에서 유해 생성 비율이 사실상 0%에 가깝게 감소했는데, 이는 비교 대상 모델들 중 가장 낮은 수준입니다. Falcon과 MPT와 비교했을 때도 파인튜닝된 Llama 2-Chat은 유해성과 진실성 측면에서 가장 우수한 성능을 보여주었습니다.

파인튜닝 이후 Llama 2-Chat은 BOLD 벤치마크에서 평가된 많은 인구통계학적 집단에 대해 전반적으로 긍정적 감성이 증가하는 경향을 보였습니다. 이는 모델이 다양한 집단에 대해 더 공정하고 균형 잡힌 표현을 생성할 수 있게 되었음을 시사합니다.

## Llama 2의 학습 결과와 관찰

### RLHF의 효과성과 주요 발견

인간 피드백을 통한 강화학습(RLHF)은 Llama 2 개발 과정에서 매우 효과적인 방법으로 입증되었습니다. 초기에는 많은 연구진이 더 밀도 높은 신호를 제공하는 지도 학습 주석 방식을 선호했지만, RLHF는 비용과 시간 효율성 측면에서 뛰어난 성과를 보여주었습니다. RLHF의 성공은 학습 과정에서 인간과 언어 모델 간의 시너지에 기인합니다.

![distribution_shift](https://ar5iv.org//html/2307.09288/assets/x18.png)

위 그래프는 지도 학습 파인튜닝(SFT)에서 RLHF로 진행되면서 모델의 분포가 어떻게 변화하는지 보여줍니다. 보상 모델 점수가 높은 쪽으로 분포가 이동하는 것을 확인할 수 있으며, 이는 모델이 인간의 선호도에 더 잘 부합하는 방향으로 발전했음을 의미합니다.

### 컨텍스트 기반 온도 조절

![temperature_adaptation](https://ar5iv.org//html/2307.09288/assets/x19.png)

연구진은 RLHF를 통해 흥미로운 현상을 발견했습니다. 모델이 프롬프트의 유형에 따라 자동으로 온도 파라미터를 조절한다는 것입니다. 위 그래프는 창의적 프롬프트와 사실 기반 프롬프트에 대한 Self-BLEU 점수를 보여줍니다. 창의적 프롬프트(예: "시 작성하기")의 경우, 다양성을 유지하기 위해 높은 온도를 유지하는 반면, 사실 기반 프롬프트(예: "수도는?")의 경우에는 일관된 응답을 위해 다양성을 줄이는 것을 확인할 수 있습니다.

### 시간 개념의 일반화

![temporal_awareness](https://ar5iv.org//html/2307.09288/assets/x20.png)

Llama 2-Chat은 시간 개념에 대한 뛰어난 일반화 능력을 보여주었습니다. 단 1,000개의 시간 관련 SFT 데이터만으로도 모델은 시간적 맥락을 잘 이해하고 조직화할 수 있었습니다. 이는 언어 모델이 단순한 다음 토큰 예측과 무작위로 섞인 데이터 학습만으로도 시간 개념을 내재화할 수 있다는 것을 시사합니다.

### 도구 사용 능력의 자발적 출현

![tool_emergence](https://ar5iv.org//html/2307.09288/assets/x23.png)

연구진은 Llama 2-Chat이 도구 사용에 대한 명시적인 학습 없이도 도구의 기능과 API 인자를 이해하고 활용할 수 있다는 것을 발견했습니다. 특히 계산기 도구를 활용한 수학 문제 해결에서 우수한 성능을 보여주었으며, 이는 수학 데이터셋 평가에서 확인할 수 있습니다. 하지만 연구진은 이러한 도구 사용 능력이 안전성 측면에서 우려를 야기할 수 있다는 점도 지적했습니다.
### Llama 2의 한계점과 윤리적 고려사항

Llama 2-Chat은 다른 대규모 언어 모델들과 마찬가지로 몇 가지 중요한 한계점을 가지고 있습니다. 가장 큰 제약사항은 프리트레이닝 이후 지식 업데이트가 중단된다는 점입니다. 이로 인해 모델은 검증되지 않은 조언을 제공하거나 환각 현상을 보일 수 있습니다.

또한 초기 버전의 Llama 2-Chat은 주로 영어 데이터에 초점을 맞추어 개발되었습니다. 실험 결과에 따르면 모델이 다른 언어에 대해서도 어느 정도의 능력을 보여주지만, 이는 프리트레이닝 데이터의 한계로 인해 제한적입니다. 앞서 살펴본 데이터 구성 표에서 확인할 수 있듯이, 비영어 언어의 데이터 비중이 상대적으로 낮아 이러한 언어들에 대한 모델의 성능은 불안정할 수 있습니다.

Llama 2는 공개적으로 접근 가능한 온라인 데이터셋으로 학습되었기 때문에, 유해하거나 편향된 콘텐츠를 생성할 가능성이 있습니다. 연구진은 파인튜닝을 통해 이러한 문제를 완화하고자 했지만, 특히 충분한 공개 데이터셋이 없는 비영어 언어에서는 여전히 문제가 남아있을 수 있습니다. Meta는 이러한 문제들을 해결하기 위해 지속적으로 모델을 개선하고 업데이트된 버전을 공개할 예정입니다.

### 책임있는 공개 전략

Meta는 Llama 2를 연구 및 상업적 용도 모두에 사용할 수 있도록 공개했습니다. 사용자들은 제공된 라이선스 조건과 이용 정책을 준수해야 하며, 이는 관련 법규와 규정을 위반하는 사용을 금지합니다. 또한 Meta는 개발자들이 Llama 2-Chat의 안전한 생성을 재현하고 기본적인 안전성 기술을 적용할 수 있도록 코드 예제를 함께 제공했습니다.

많은 기업들이 AI를 비공개로 개발하는 것과 달리, Meta는 책임있는 AI 혁신을 장려하기 위해 Llama 2를 공개적으로 공개했습니다. 이러한 개방적 접근은 AI 실무자 커뮤니티의 집단 지혜와 다양성, 창의성을 활용하여 기술을 더 나은 방향으로 발전시킬 수 있다고 믿기 때문입니다. 전체 AI 커뮤니티가 협력하여 현재 AI 시스템의 위험을 분석하고 잠재적 오용을 방지하기 위한 해결책을 마련해야 합니다.
### 책임있는 AI 개발을 위한 공개 전략의 의의

Meta의 개방적 접근 방식은 AI 전문성을 분산시키는 것 이상의 의미를 가집니다. Zellers와 연구진이 주장한 것처럼, 공개 릴리스는 투명성을 높이고 더 많은 사람들이 AI 도구에 접근할 수 있게 함으로써 기술을 민주화하고 AI 전문성을 분산시키는 효과가 있습니다. 이는 산업 전반의 혁신을 촉진하고 발전을 가속화하는 데 기여합니다.

또한 이러한 모델의 공개는 비용을 통합하고 진입 장벽을 낮춤으로써, 전 세계의 중소기업들이 텍스트 생성 활용 사례를 탐색하고 구축할 수 있게 합니다. 이는 궁극적으로 AI가 약속하는 경제적 성장의 혜택을 모든 규모의 조직이 누릴 수 있는 더 공평한 환경을 조성할 것입니다.

Meta는 모든 AI 모델 사용자가 선의를 가지고 있지 않다는 점을 인정하며, AI가 우리 세상에 미칠 영향에 대한 합리적인 우려가 있음을 이해합니다. 유해 콘텐츠 생성과 문제적 연관성은 AI 커뮤니티가 아직 완전히 해결하지 못한 의미 있는 위험입니다. 이 논문에서 보여주듯이 Meta는 이러한 유형의 응답을 제한하는 데 진전을 이루었지만, 여전히 해결해야 할 과제가 남아있음을 인식하고 있습니다. 이러한 인식은 오픈 사이언스와 AI 커뮤니티와의 협력에 대한 Meta의 헌신을 더욱 강화합니다.

### 대규모 언어 모델의 발전 방향

이러한 공개 전략은 대규모 언어 모델 분야의 미래 발전 방향에 중요한 시사점을 제시합니다. 특히 안전성과 유용성의 균형, 다국어 지원 확대, 도구 활용 능력 개선 등 여러 도전 과제들이 남아있습니다. Meta의 접근 방식은 이러한 과제들을 해결하는 데 있어 개방성과 협력이 핵심이 될 것임을 보여줍니다.

## 대규모 언어 모델의 발전과 안전성 과제

### 모델 규모와 학습 방법론의 진화

대규모 언어 모델(LLM) 분야는 최근 몇 년간 급속한 발전을 이루었습니다. Kaplan과 연구진이 제시한 스케일링 법칙을 기반으로, GPT-3부터 Gopher에 이르기까지 1000억 개 이상의 파라미터를 가진 여러 대규모 언어 모델이 등장했습니다. 특히 과학 분야에 특화된 Galactica와 같은 전문 모델도 개발되었습니다. 700억 개의 파라미터를 가진 Chinchilla는 Hoffmann과 연구진의 연구를 통해 모델 가중치보다 토큰 수에 초점을 맞춘 새로운 스케일링 법칙을 제시했습니다.

이러한 발전 과정에서 주목할 만한 것은 Llama의 등장입니다. Touvron과 연구진이 개발한 이 모델은 추론 시의 계산 효율성에 중점을 두었습니다. 또한 오픈소스와 비공개 모델 간의 논의도 활발히 진행되었습니다. BLOOM, OPT, Falcon과 같은 오픈소스 모델들이 GPT-3나 Chinchilla 같은 비공개 모델들에 도전장을 내밀었습니다.

### 명령어 튜닝과 인간 피드백 학습

명령어 튜닝 분야에서는 Wei와 연구진이 다수의 데이터셋에 대한 파인튜닝을 통해 새로운 과제에 대한 제로샷 성능을 향상시켰습니다. Chung과 연구진, Longpre와 연구진은 과제 수, 모델 크기, 프롬프트 설정 등이 명령어 튜닝에 미치는 영향을 연구했습니다. 특히 Zhou와 연구진은 LLM이 스스로 프롬프트를 생성하는 방법을 제안했으며, Ganguli와 연구진, Madaan과 연구진은 초기 생성물을 더 유용하고 공정하게 개선하는 후속 지시 방법을 연구했습니다.

체인오브소트 프롬프팅은 Wei와 연구진이 제안한 접근 방식으로, 모델이 복잡한 문제를 해결할 때 추론 과정을 설명하도록 하여 최종 답변의 정확도를 높이는 방법입니다. 인간 피드백을 통한 강화학습(RLHF)은 Christiano와 연구진이 제안한 강력한 파인튜닝 전략으로, Stiennon과 연구진이 텍스트 요약 작업에서 처음 선보인 이후 다양한 응용 분야로 확장되었습니다.

### 안전성 과제와 사회적 영향

대규모 언어 모델의 위험성과 과제들은 최근 문헌에서 광범위하게 연구되었습니다. Bender와 연구진, Weidinger와 연구진은 편향성, 유해성, 개인정보 유출, 악의적 사용 가능성 등 다양한 위험을 지적했습니다. Solaiman과 연구진은 이러한 영향을 기본 시스템 내에서 평가할 수 있는 것과 사회적 맥락에서 평가해야 하는 것으로 분류했으며, Kumar와 연구진은 잠재적 피해를 줄이기 위한 전략을 제시했습니다.
### 챗봇 기반 언어 모델의 안전성 문제

챗봇 지향적인 LLM과 관련된 문제점들도 심도 있게 연구되었습니다. Roller와 연구진, Dinan과 연구진의 연구에 따르면 이러한 모델들은 프라이버시 침해부터 잘못된 전문성 주장에 이르기까지 다양한 문제를 야기할 수 있습니다. Deng과 연구진은 이러한 문제들을 체계적으로 분류하기 위한 분류 체계를 제안했으며, Bergman과 연구진은 대화 모델 공개가 가져올 수 있는 긍정적, 부정적 영향의 균형에 대해 연구했습니다.

### 레드팀 테스팅과 보안 위협

레드팀 테스팅을 통한 연구에서는 파인튜닝된 LLM의 구체적인 취약점들이 드러났습니다. Ganguli와 연구진, Zhuo와 연구진의 연구는 다양한 유형의 공격이 성공적으로 수행될 수 있으며, 이로 인해 유해한 콘텐츠가 생성될 수 있음을 보여주었습니다. 특히 Mialon과 연구진을 포함한 국가 안보 기관과 연구자들은 고도화된 모델의 예기치 못한 행동, 사이버 위협, 생물학전 등에서의 잠재적 오용 가능성에 대해 경고했습니다.

### 사회경제적 영향과 대응 방안

LLM의 발전은 더 광범위한 사회적 문제들도 제기합니다. Acemoglu와 Restrepo, Autor와 Salomons, Webb의 연구에 따르면 AI 연구의 가속화로 인한 일자리 대체 현상이 발생할 수 있습니다. 또한 Shumailov와 연구진이 지적한 것처럼 LLM에 대한 과도한 의존은 학습 데이터의 품질 저하로 이어질 수 있습니다. 이러한 문제들을 해결하기 위해서는 정책, 학계, 산업계가 함께 협력하여 지속적으로 논의하고 대응 방안을 마련해야 합니다.

## Llama 2의 결론과 향후 전망

Meta는 70억에서 700억 개의 파라미터를 가진 새로운 대규모 언어 모델군인 Llama 2를 공개했습니다. 이 모델들은 기존의 오픈소스 대화 모델들과 비교했을 때 우수한 성능을 보여주었으며, 일부 평가 세트에서는 비공개 모델들과 견줄만한 수준의 역량을 입증했습니다. 다만 GPT-4와 같은 최신 모델들과 비교했을 때는 아직 성능 차이가 존재합니다.

연구진은 모델의 유용성과 안전성을 향상시키기 위해 다양한 방법론을 적용했습니다. 특히 인간 피드백을 통한 강화학습(RLHF)과 안전성 보상 모델을 활용하여 모델이 더 안전하고 유용한 응답을 생성하도록 했습니다. 또한 350명 이상의 전문가로 구성된 레드팀을 통해 광범위한 안전성 테스트를 수행했으며, 이를 통해 발견된 취약점들을 지속적으로 개선했습니다.

Llama 2와 Llama 2-Chat의 공개는 AI 기술의 민주화와 투명성 향상에 기여할 것으로 기대됩니다. Meta는 연구 목적뿐만 아니라 상업적 용도로도 모델을 사용할 수 있도록 허용했으며, 이는 더 많은 조직과 개발자들이 AI 기술을 활용할 수 있는 기회를 제공할 것입니다.

하지만 여전히 해결해야 할 과제들이 남아있습니다. 프리트레이닝 이후 지식 업데이트가 중단되는 문제, 영어 외 언어에 대한 제한적인 성능, 그리고 잠재적인 유해 콘텐츠 생성 가능성 등이 주요 한계점으로 지적됩니다. Meta는 이러한 문제들을 해결하기 위해 지속적으로 모델을 개선하고 업데이트된 버전을 공개할 예정이며, AI 커뮤니티와의 협력을 통해 더 안전하고 유용한 AI 시스템을 개발하고자 합니다.

- - -
### References
* [Llama 2: Open Foundation and Fine-Tuned Chat Models](http://arxiv.org/pdf/2307.09288v2)