---
layout: post
title: "LLaMA: Open and Efficient Foundation Language Models"
date: 2023-02-27 17:11:15
author: "Meta AI"
categories: "Language-Models"
tags: ["Open-and-Efficient-Foundation-Language-Models", "Optimal-Model/Data-Scaling-Up-Allocation", "Rotary-Positional-Embeddings", "SwiGLU-Activation-Function", "Responsible-AI-Evaluation"]
use_math: true
cover: /assets/images/language-models.webp
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
대규모 언어 모델의 발전은 텍스트 기반 지시사항이나 소수의 예시만으로도 새로운 과제를 수행할 수 있는 능력을 보여주었습니다. 그러나 기존의 최고 성능 모델들은 대부분 비공개 데이터셋에 의존하고 있었으며, 이는 연구의 재현성과 투명성을 제한하는 요인이 되었습니다. 또한 Hoffmann과 연구진의 연구는 주어진 계산 자원 내에서 최고의 성능은 가장 큰 모델이 아닌, 더 많은 데이터로 학습된 작은 모델에서 달성된다는 것을 보여주었습니다. 이러한 배경에서 Meta AI 연구진은 공개 데이터셋만을 활용하여 최고 수준의 성능을 달성할 수 있는 효율적인 언어 모델을 개발하고자 했습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
LLaMA는 트랜스포머 아키텍처에 여러 가지 중요한 혁신을 도입했습니다. 사전 정규화와 RMSNorm을 통해 학습 안정성과 계산 효율성을 개선했으며, 회전 위치 임베딩을 도입하여 상대적 위치 정보를 더 효과적으로 인코딩했습니다. 또한 SwiGLU 활성화 함수를 사용하여 모델의 표현력을 향상시켰습니다. 특히 주목할 만한 점은 완전히 공개된 데이터셋만을 사용하면서도 엄격한 필터링 과정을 통해 고품질의 학습 데이터를 구축했다는 것입니다.

#### 제안된 방법은 어떻게 구현되었습니까?
LLaMA는 7B부터 65B 파라미터에 이르는 모델 시리즈로 구현되었습니다. AdaFactor 옵티마이저와 코사인 학습률 스케줄링을 사용하여 메모리 효율성과 학습 안정성을 개선했으며, 1조 토큰 이상의 대규모 데이터셋으로 학습을 진행했습니다. 분산 학습 시스템에서는 그래디언트 체크포인팅, 혼합 정밀도 학습, 계층적 AllReduce 알고리즘 등 다양한 최적화 기법을 도입하여 학습 효율성을 극대화했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
LLaMA의 개발은 대규모 언어 모델 연구의 민주화에 중요한 이정표가 되었습니다. 13B 파라미터 모델이 175B 파라미터의 GPT-3를 대부분의 벤치마크에서 능가했으며, 65B 파라미터 모델은 Chinchilla-70B와 PaLM-540B와 같은 최신 모델들과 대등한 성능을 보여주었습니다. 특히 공개 데이터셋만을 사용하여 이러한 성과를 달성했다는 점은, 향후 언어 모델 연구의 투명성과 재현성 향상에 큰 기여를 할 것으로 기대됩니다. 또한 7B 파라미터 모델이 단일 GPU에서도 구동 가능하다는 점은 더 많은 연구자들이 언어 모델 연구에 참여할 수 있는 기회를 제공할 것입니다.
- - -
## LLaMA: 효율적인 오픈소스 기반 언어 모델

### 소개

Meta AI 연구진이 개발한 LLaMA는 7B부터 65B 파라미터에 이르는 기반 언어 모델 시리즈입니다. 이 연구의 가장 주목할 만한 특징은 공개적으로 접근 가능한 데이터셋만을 활용하여 최첨단 성능의 모델을 훈련시켰다는 점입니다. 특히 13B 파라미터 규모의 LLaMA 모델은 175B 파라미터의 GPT-3를 대부분의 벤치마크에서 능가하는 성능을 보여주었으며, 65B 파라미터 모델은 Chinchilla-70B와 PaLM-540B와 같은 최고 수준의 모델들과 견줄 만한 성능을 달성했습니다.

대규모 언어 모델의 발전은 Brown과 연구진이 제시한 것처럼 텍스트 기반 지시사항이나 소수의 예시만으로도 새로운 과제를 수행할 수 있는 능력을 보여주었습니다. Kaplan과 연구진의 연구에 따르면, 이러한 퓨 샷 학습 능력은 모델이 충분한 규모에 도달했을 때 처음 나타나기 시작했으며, 이는 더 큰 규모의 모델 개발로 이어졌습니다.

하지만 Hoffmann과 연구진의 최근 연구는 주어진 계산 자원 내에서 최고의 성능은 가장 큰 모델이 아닌, 더 많은 데이터로 학습된 작은 모델에서 달성된다는 것을 보여주었습니다. 특히 추론 시의 계산 비용을 고려할 때, 특정 성능 수준에 도달하기 위해 큰 모델을 학습시키는 것보다 작은 모델을 더 오래 학습시키는 것이 궁극적으로 더 효율적일 수 있습니다. 예를 들어, Hoffmann과 연구진은 10B 파라미터 모델을 200B 토큰으로 학습하는 것을 권장했지만, Meta AI 연구진은 7B 파라미터 모델의 성능이 1조 토큰 이상의 학습 후에도 지속적으로 향상되는 것을 발견했습니다.

이러한 맥락에서 LLaMA 연구는 다양한 추론 예산에 맞춰 최적의 성능을 달성할 수 있는 언어 모델 시리즈를 개발하는 데 초점을 맞추었습니다. 특히 주목할 만한 점은 LLaMA가 "Books-2TB" 또는 "Social media conversations"와 같은 비공개 또는 문서화되지 않은 데이터셋에 의존하지 않고, 완전히 공개된 데이터셋만을 사용했다는 것입니다. OPT, GPT-NeoX, BLOOM, GLM과 같은 다른 오픈소스 모델들도 존재하지만, PaLM-62B나 Chinchilla와 견줄 만한 성능을 보여주는 모델은 없었습니다.
### 연구 방법론

LLaMA 연구진은 트랜스포머 아키텍처에 여러 가지 중요한 수정을 가했습니다. 기본적인 트랜스포머 구조를 유지하면서도 학습 효율성과 추론 속도를 개선하기 위한 다양한 기술적 혁신을 도입했습니다. 특히 사전 정규화(pre-normalization)를 사용하여 학습 안정성을 높였으며, RMSNorm을 통해 계산 효율성을 개선했습니다.

어텐션 메커니즘에서는 회전 위치 임베딩(rotary positional embeddings)을 도입했습니다. 이는 기존 GPT 모델들이 사용하던 학습된 위치 임베딩과 달리, 상대적 위치 정보를 더 효과적으로 인코딩할 수 있게 해주었습니다. 또한 SwiGLU 활성화 함수를 사용하여 모델의 표현력을 향상시켰습니다.

데이터 측면에서는 공개적으로 접근 가능한 고품질 데이터셋에 중점을 두었습니다. 연구진은 CommonCrawl, C4, GitHub, Wikipedia 등 다양한 소스에서 수집한 데이터를 엄격한 필터링 과정을 거쳐 선별했습니다. 이는 비공개 데이터에 의존하지 않고도 최고 수준의 성능을 달성할 수 있다는 것을 보여주는 중요한 사례가 되었습니다.

학습 과정에서는 AdaFactor 옵티마이저를 사용하여 메모리 효율성을 높였으며, 코사인 학습률 스케줄링을 통해 학습 안정성을 개선했습니다. 특히 주목할 만한 점은 1조 토큰 이상의 대규모 데이터셋으로 학습을 진행하면서도, 작은 모델에서도 지속적인 성능 향상을 관찰할 수 있었다는 것입니다.

이러한 기술적 혁신들의 조합은 LLaMA가 기존의 대규모 언어 모델들보다 더 효율적으로 학습되고 추론될 수 있게 만들었습니다. 특히 7B 파라미터 모델은 단일 GPU에서도 구동이 가능하여, 언어 모델 연구의 민주화에 크게 기여할 것으로 기대됩니다.
### 모델 평가 및 성능

LLaMA 모델의 성능 평가는 다양한 표준 벤치마크를 통해 이루어졌습니다. 특히 13B 파라미터 모델이 175B 파라미터의 GPT-3를 대부분의 벤치마크에서 능가했다는 점은 매우 주목할 만한 성과입니다. 이는 단순히 모델의 크기를 키우는 것보다 효율적인 아키텍처 설계와 충분한 학습 데이터의 중요성을 보여주는 결과입니다.

65B 파라미터 규모의 LLaMA 모델은 Chinchilla-70B와 PaLM-540B와 같은 최신 대규모 언어 모델들과 비교했을 때 대등한 성능을 보여주었습니다. 이는 공개 데이터셋만을 사용하고도 최고 수준의 성능을 달성할 수 있다는 것을 입증하는 중요한 사례가 되었습니다.

### 책임있는 AI 개발

LLaMA 연구진은 모델의 편향성과 유해성에 대한 분석도 진행했습니다. 책임있는 AI 커뮤니티에서 개발한 최신 벤치마크들을 활용하여 모델의 잠재적 위험성을 평가했습니다. 이러한 분석은 언어 모델이 사회에 미치는 영향을 이해하고, 향후 개선 방향을 설정하는 데 중요한 기초가 됩니다.

### 연구의 의의

LLaMA의 개발은 대규모 언어 모델 연구의 민주화에 큰 기여를 했습니다. 특히 7B 파라미터 모델은 단일 GPU에서도 구동이 가능하여, 더 많은 연구자들이 언어 모델 연구에 참여할 수 있는 기회를 제공했습니다. 또한 공개 데이터셋만을 사용하여 최고 수준의 성능을 달성했다는 점은, 향후 언어 모델 연구의 투명성과 재현성 향상에 중요한 이정표가 될 것입니다.

### 학습 방법론

LLaMA의 학습 방법론은 Brown과 연구진이 GPT-3에서 제시한 접근법과 Chowdhery와 연구진이 PaLM에서 사용한 방식을 기반으로 하되, Hoffmann과 연구진이 제안한 Chinchilla 스케일링 법칙의 통찰을 적극적으로 반영했습니다. 이 스케일링 법칙은 모델 크기와 학습 데이터 양 사이의 최적 관계를 수학적으로 정립한 것으로, 다음과 같은 핵심 수식을 따릅니다.

\\[ N_{optimal} = \alpha C^{0.5} \\]
\\[ D_{optimal} = \beta C^{0.5} \\]

여기서 \\(N_{optimal}\\)은 최적의 모델 파라미터 수, \\(D_{optimal}\\)은 최적의 학습 데이터 토큰 수, \\(C\\)는 사용 가능한 계산 예산을 나타내며, \\(\alpha\\)와 \\(\beta\\)는 실험적으로 결정되는 상수입니다.

학습 과정에서는 표준적인 트랜스포머 아키텍처를 기반으로 하되, 다음과 같은 손실 함수를 최적화합니다.

\\[ \mathcal{L}(\theta) = -\frac{1}{|D|} \sum_{(x,y) \in D} \log P_\theta(y|x) \\]

여기서 \\(\theta\\)는 모델 파라미터, \\(D\\)는 학습 데이터셋, \\(x\\)는 입력 시퀀스, \\(y\\)는 목표 시퀀스를 나타냅니다.

옵티마이저로는 AdaFactor를 사용하며, 다음과 같은 학습률 스케줄링을 적용합니다.

\\[ \eta_t = \eta_{max} \cdot \min(1, \frac{t}{t_{warmup}}) \cdot \cos(\frac{2\pi t}{T}) \\]

여기서 \\(\eta_t\\)는 스텝 \\(t\\)에서의 학습률, \\(\eta_{max}\\)는 최대 학습률, \\(t_{warmup}\\)은 웜업 스텝 수, \\(T\\)는 총 학습 스텝 수입니다.

특히 주목할 만한 점은 배치 크기와 그래디언트 누적에 대한 세부 설정입니다.

\\[ \text{effective batch size} = \text{per device batch size} \times \text{gradient accumulation steps} \times \text{number of devices} \\]

이러한 수식적 프레임워크를 바탕으로, LLaMA는 기존 대규모 언어 모델들과 차별화된 효율적인 학습 전략을 구현했습니다. 특히 Chinchilla 스케일링 법칙을 따르면서도, 실제 구현에서는 더 작은 모델을 더 오래 학습시키는 방향으로 최적화를 진행했습니다.
### 분산 학습 구현과 최적화

LLaMA의 학습 과정에서는 대규모 분산 학습을 효율적으로 구현하기 위한 여러 기술적 최적화가 적용되었습니다. 분산 학습 시스템은 다음과 같은 수식으로 표현되는 데이터 병렬화 전략을 채택했습니다.

\\[ \nabla \mathcal{L}_{global} = \frac{1}{N} \sum_{i=1}^N \nabla \mathcal{L}_i \\]

여기서 \\(\nabla \mathcal{L}_{global}\\)은 전체 그래디언트, \\(N\\)은 총 디바이스 수, \\(\nabla \mathcal{L}_i\\)는 각 디바이스에서 계산된 로컬 그래디언트를 나타냅니다.

메모리 효율성을 높이기 위해 그래디언트 체크포인팅을 구현했으며, 이는 다음과 같은 메모리-계산 시간 트레이드오프를 고려합니다.

\\[ M_{total} = M_{model} + M_{activations} \cdot \frac{1}{\sqrt{N_{layers}}} \\]

여기서 \\(M_{total}\\)은 총 필요 메모리, \\(M_{model}\\)은 모델 파라미터가 차지하는 메모리, \\(M_{activations}\\)는 활성화값 저장에 필요한 메모리, \\(N_{layers}\\)는 체크포인팅이 적용된 레이어 수입니다.

학습 안정성을 위해 그래디언트 클리핑을 적용했으며, 다음과 같은 수식으로 구현됩니다.

\\[ \nabla \theta_{clipped} = \min(1, \frac{\gamma}{\|\nabla \theta\|_2}) \cdot \nabla \theta \\]

여기서 \\(\gamma\\)는 클리핑 임계값, \\(\nabla \theta\\)는 원래 그래디언트, \\(\nabla \theta_{clipped}\\)는 클리핑된 그래디언트입니다.

또한 학습 과정에서의 수치적 안정성을 위해 혼합 정밀도 학습을 구현했습니다. 포워드 패스에서는 FP16을 사용하고, 백워드 패스에서는 그래디언트 스케일링과 함께 다음과 같은 동적 손실 스케일링을 적용합니다.

\\[ \text{loss}_{scaled} = \text{loss} \cdot \alpha_t \\]
\\[ \alpha_{t+1} = \begin{cases} 
2\alpha_t & \text{if no overflow for } n \text{ consecutive steps} \\
\frac{\alpha_t}{2} & \text{if overflow occurs}
\end{cases} \\]

여기서 \\(\alpha_t\\)는 시간 스텝 \\(t\\)에서의 손실 스케일 factor입니다.

이러한 최적화 기법들의 조합을 통해 LLaMA는 대규모 분산 환경에서도 안정적이고 효율적인 학습이 가능했으며, 특히 작은 모델에서도 긴 학습 시간 동안 성능이 지속적으로 향상되는 특성을 보여주었습니다.
### 학습 파이프라인 최적화

LLaMA의 학습 파이프라인은 계산 효율성을 극대화하기 위해 여러 가지 혁신적인 기술을 도입했습니다. 특히 모델 병렬화와 파이프라인 병렬화를 결합한 하이브리드 접근 방식을 채택했으며, 이는 다음과 같은 수식적 프레임워크를 따릅니다.

\\[ T_{total} = \frac{B \cdot S}{P} \cdot (1 + \frac{2(P-1)}{m}) \\]

여기서 \\(T_{total}\\)은 총 처리 시간, \\(B\\)는 글로벌 배치 크기, \\(S\\)는 시퀀스당 계산 시간, \\(P\\)는 파이프라인 단계 수, \\(m\\)은 마이크로 배치 수를 나타냅니다.

통신 오버헤드를 최소화하기 위해 계층적 AllReduce 알고리즘을 구현했으며, 이는 다음과 같은 통신 복잡도를 가집니다.

\\[ C_{comm} = 2(N-1)\alpha + \frac{2(N-1)}{N}\beta s \\]

여기서 \\(\alpha\\)는 지연 시간, \\(\beta\\)는 대역폭의 역수, \\(s\\)는 메시지 크기, \\(N\\)은 노드 수입니다.

메모리 사용량을 최적화하기 위해 ZeRO-3 기법을 확장 구현했으며, 각 GPU에서의 메모리 요구사항은 다음과 같이 계산됩니다.

\\[ M_{per\_device} = \frac{M_{params} + M_{optim}}{N_{devices}} + M_{temp} \\]

여기서 \\(M_{params}\\)는 모델 파라미터 메모리, \\(M_{optim}\\)은 옵티마이저 상태 메모리, \\(M_{temp}\\)는 임시 버퍼 메모리입니다.

특히 학습 과정에서의 수치 안정성을 위해 적응적 그래디언트 누적을 도입했습니다.

\\[ g_t = \sum_{i=1}^k \nabla \mathcal{L}_i \cdot \exp(-\lambda(t-t_i)) \\]

여기서 \\(g_t\\)는 누적된 그래디언트, \\(\lambda\\)는 감쇠 계수, \\(t_i\\)는 각 그래디언트가 계산된 시간을 나타냅니다.

이러한 최적화 기법들의 통합적 적용을 통해 LLaMA는 대규모 분산 환경에서도 효율적인 학습이 가능했으며, 특히 계산 자원 활용도를 극대화하면서도 학습의 안정성을 유지할 수 있었습니다.

### 사전 학습 데이터

LLaMA의 학습 데이터셋은 다양한 도메인을 포괄하는 여러 공개 데이터 소스로 구성되어 있습니다. 연구진은 오픈소스 호환성을 고려하여 공개적으로 접근 가능한 데이터만을 엄선했으며, 이는 전체 학습 데이터의 구성과 품질 관리 측면에서 중요한 의미를 갖습니다.

가장 큰 비중을 차지하는 것은 영어 CommonCrawl 데이터로, 전체 데이터셋의 67%를 구성합니다. 연구진은 2017년부터 2020년까지의 CommonCrawl 덤프를 CCNet 파이프라인을 통해 전처리했습니다. 이 과정에서 문장 수준의 중복 제거, fastText 선형 분류기를 활용한 언어 식별, n-gram 언어 모델을 통한 품질 필터링이 수행되었습니다. 특히 주목할 만한 점은 위키피디아 참조 페이지와 무작위 샘플링된 페이지를 구분하는 선형 모델을 학습시켜, 참조 페이지로 분류되지 않은 데이터는 제외했다는 것입니다.

두 번째로 큰 비중을 차지하는 C4 데이터셋은 전체의 15%를 차지합니다. C4 데이터셋도 CommonCrawl을 기반으로 하지만, CCNet과는 다른 전처리 방식을 적용했습니다. 특히 품질 필터링 측면에서 문장부호의 존재나 웹페이지의 단어 및 문장 수와 같은 휴리스틱을 주로 활용했다는 점이 특징입니다.

GitHub 데이터는 전체의 4.5%를 차지하며, Google BigQuery를 통해 공개된 데이터를 활용했습니다. Apache, BSD, MIT 라이선스로 배포된 프로젝트만을 선별했으며, 줄 길이나 영숫자 문자 비율 등의 휴리스틱을 통해 저품질 파일을 필터링했습니다. 또한 정규표현식을 활용하여 헤더와 같은 상용구를 제거했으며, 파일 수준에서 정확한 매칭을 통한 중복 제거를 수행했습니다.

위키피디아 데이터도 4.5%를 차지하며, 2022년 6월부터 8월까지의 덤프를 활용했습니다. 라틴 문자나 키릴 문자를 사용하는 20개 언어(bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk)의 데이터를 포함했으며, 하이퍼링크, 주석, 서식 상용구 등을 제거하는 전처리를 수행했습니다.
### 사전 학습 데이터 구성과 전처리

Gutenberg 프로젝트와 Books3 데이터는 전체 학습 데이터의 4.5%를 차지합니다. Gutenberg 프로젝트는 저작권이 만료된 공개 도메인 도서들을 포함하고 있으며, Books3는 ThePile 데이터셋의 일부로 대규모 언어 모델 학습을 위해 공개된 도서 데이터입니다. 연구진은 도서 수준에서 중복 제거를 수행했으며, 90% 이상의 내용이 중복되는 도서들은 제외했습니다.

arXiv 데이터는 전체의 2.5%를 차지하며, 과학 논문의 LaTeX 파일을 처리하여 포함했습니다. 논문의 첫 섹션 이전 내용과 참고문헌을 제외했으며, .tex 파일의 주석을 제거하고 사용자가 정의한 매크로와 정의를 인라인으로 확장하여 논문 간의 일관성을 높였습니다.

Stack Exchange 데이터는 전체의 2%를 차지하며, 컴퓨터 과학부터 화학까지 다양한 분야를 다루는 고품질 질문과 답변 웹사이트의 덤프를 활용했습니다. 28개의 가장 큰 웹사이트의 데이터를 선별했으며, 텍스트에서 HTML 태그를 제거하고 점수(높은 순에서 낮은 순)에 따라 답변을 정렬했습니다.

토크나이저로는 SentencePiece 구현체를 사용하여 바이트 쌍 인코딩(BPE) 알고리즘을 적용했습니다. 특히 모든 숫자를 개별 자릿수로 분할하고, 알 수 없는 UTF-8 문자는 바이트 단위로 분해하는 방식을 채택했습니다. 토크나이저 적용 후 전체 학습 데이터셋은 약 1.4조 토큰을 포함하게 되었습니다. 대부분의 학습 데이터는 한 번씩만 사용되었지만, 위키피디아와 도서 도메인의 데이터는 약 2.4회의 에포크를 거쳐 학습되었습니다.

이러한 다양한 데이터 소스의 조합과 철저한 전처리 과정은 LLaMA가 광범위한 도메인에서 강건한 성능을 보일 수 있게 하는 핵심 요소가 되었습니다. 특히 공개 데이터만을 사용하면서도 높은 품질의 학습 데이터셋을 구축했다는 점은, 향후 언어 모델 연구의 재현성과 접근성 향상에 중요한 기여를 했다고 볼 수 있습니다.

### 아키텍처 구조

LLaMA는 Vaswani와 연구진이 제안한 트랜스포머 아키텍처를 기반으로 하되, PaLM과 같은 최신 대규모 언어 모델들에서 검증된 여러 가지 개선사항들을 통합했습니다. 특히 모델의 안정성과 성능을 높이기 위해 세 가지 주요한 아키텍처 수정이 이루어졌습니다.

첫째, 학습 안정성을 높이기 위해 사전 정규화(pre-normalization) 방식을 도입했습니다. GPT-3에서 영감을 받은 이 접근법은 각 트랜스포머 서브레이어의 출력이 아닌 입력을 정규화합니다. 정규화 함수로는 Zhang과 Sennrich가 제안한 RMSNorm을 사용했는데, 이는 계산 효율성과 수치 안정성 측면에서 이점을 제공합니다.

둘째, PaLM에서 영감을 받아 ReLU 비선형성을 SwiGLU 활성화 함수로 대체했습니다. Shazeer가 제안한 이 활성화 함수는 모델의 표현력을 향상시키는 것으로 알려져 있습니다. 다만 PaLM과는 달리 은닉층의 차원을 \\( \frac{2}{3}4d \\) 로 설정했는데, 이는 PaLM의 \\( 4d \\)와 비교했을 때 계산 효율성과 성능 사이의 균형을 더 잘 맞추는 것으로 나타났습니다.

셋째, GPTNeo의 접근법을 따라 절대 위치 임베딩을 제거하고 대신 Su와 연구진이 제안한 회전 위치 임베딩(RoPE)을 각 레이어에 추가했습니다. RoPE는 상대적 위치 정보를 더 효과적으로 인코딩할 수 있게 해주며, 특히 긴 시퀀스에서의 성능 향상에 기여합니다.

아키텍처의 효과는 학습 곡선을 통해 명확하게 확인할 수 있습니다. 첨부된 그래프는 7B, 13B, 33B, 65B 파라미터 모델들의 학습 손실을 보여주는데, 33B와 65B 모델이 1.4T 토큰으로 학습되었을 때 더 작은 모델들(7B, 13B)보다 낮은 학습 손실을 달성하는 것을 볼 수 있습니다. 이는 모델 크기가 증가함에 따라 학습 데이터를 더 효과적으로 활용할 수 있음을 시사합니다.

![Training Loss Graph](https://ar5iv.org//html/2302.13971/assets/x1.png)

이러한 아키텍처 개선사항들의 조합은 LLaMA가 기존 대규모 언어 모델들과 비교했을 때 더 효율적인 학습과 추론이 가능하도록 만들었습니다. 특히 각 구성요소가 서로 시너지를 내며 모델의 전반적인 성능 향상에 기여했다는 점이 주목할 만합니다.

### 옵티마이저 구현

LLaMA 모델의 학습에는 Loshchilov와 Hutter가 제안한 AdamW 옵티마이저가 사용되었습니다. AdamW는 기존 Adam 옵티마이저의 가중치 감쇠(weight decay) 구현 방식을 개선한 알고리즘으로, 적응적 학습률 방법과 가중치 감쇠를 효과적으로 분리하여 구현합니다. 이 옵티마이저의 핵심 하이퍼파라미터로는 \\(\beta_1 = 0.9\\)와 \\(\beta_2 = 0.95\\)가 설정되었습니다.

학습률 스케줄링에는 코사인 스케줄이 적용되었는데, 이는 최종 학습률이 최대 학습률의 10%가 되도록 설계되었습니다. 이러한 코사인 스케줄링은 학습 과정에서 학습률을 점진적으로 감소시키면서도, 급격한 변화를 방지하여 안정적인 학습을 가능하게 합니다.

모델의 정규화를 위해 0.1의 가중치 감쇠가 적용되었으며, 그래디언트 폭발을 방지하기 위해 1.0의 그래디언트 클리핑이 사용되었습니다. 또한 학습 초기의 안정성을 확보하기 위해 2,000 스텝의 웜업 기간을 도입했습니다. 이러한 웜업 기간 동안 학습률은 점진적으로 증가하여 모델이 안정적으로 학습을 시작할 수 있도록 돕습니다.

특히 주목할 만한 점은 모델 크기에 따라 학습률과 배치 크기를 조정했다는 것입니다. 이는 모델의 규모가 커질수록 학습 과정에서 더 세밀한 조정이 필요하다는 점을 고려한 것으로, 각 모델 크기별로 최적화된 하이퍼파라미터 설정을 통해 효율적인 학습이 가능했습니다.

### 효율적인 구현 최적화

LLaMA 연구진은 모델 학습 속도를 향상시키기 위해 여러 가지 최적화 기법을 도입했습니다. 가장 핵심적인 최적화는 xformers 라이브러리에서 제공하는 인과적 멀티헤드 어텐션의 효율적인 구현입니다. 이 구현은 Rabe와 Staats의 연구에서 영감을 받았으며, Dao와 연구진이 제안한 역전파 방식을 활용합니다. 이 최적화의 핵심은 어텐션 가중치를 저장하지 않고, 인과적 언어 모델링 작업의 특성상 마스킹되는 키/쿼리 점수를 계산하지 않는 것입니다.

학습 효율성을 더욱 향상시키기 위해 역전파 과정에서 재계산되는 활성화값의 양을 줄였습니다. 구체적으로, 선형 레이어의 출력과 같이 계산 비용이 높은 활성화값들을 저장하는 방식을 채택했습니다. 이는 PyTorch의 자동 미분 기능을 사용하는 대신 트랜스포머 레이어에 대한 역전파 함수를 수동으로 구현함으로써 달성되었습니다.

이러한 최적화의 이점을 최대한 활용하기 위해 Korthikanti와 연구진이 제안한 모델 병렬화와 시퀀스 병렬화를 적용하여 모델의 메모리 사용량을 줄였습니다. 또한 GPU 간 네트워크 통신(all_reduce 연산)과 활성화값 계산을 최대한 중첩시켜 처리했습니다.

이러한 최적화 기법들의 효과는 실제 성능 지표를 통해 확인할 수 있습니다. 65B 파라미터 모델을 학습할 때, 80GB RAM을 탑재한 2048개의 A100 GPU에서 GPU당 약 380 토큰/초의 처리 속도를 달성했습니다. 이는 1.4조 토큰으로 구성된 전체 데이터셋을 약 21일 만에 학습할 수 있음을 의미합니다.

이러한 구현 최적화는 대규모 언어 모델의 학습 효율성을 크게 향상시켰으며, 특히 메모리 사용량과 계산 시간 측면에서 상당한 개선을 이루어냈습니다. 이는 대규모 언어 모델의 실용적인 학습을 가능하게 하는 중요한 기술적 진보를 보여줍니다.

### LLaMA의 주요 실험 결과

LLaMA 연구진은 Brown과 연구진의 방법론을 따라 제로샷과 퓨샷 학습 과제에서 모델의 성능을 평가했습니다. 총 20개의 벤치마크에서 평가가 이루어졌으며, 평가 방식은 크게 두 가지로 나뉩니다. 제로샷 평가에서는 과제에 대한 텍스트 설명과 테스트 예시가 주어지며, 모델은 자유 형식의 생성이나 제안된 답변의 순위 매기기를 수행합니다. 퓨샷 평가에서는 1개에서 64개 사이의 과제 예시와 테스트 예시가 제공되며, 모델은 이를 입력으로 받아 답변을 생성하거나 다양한 옵션의 순위를 매깁니다.

연구진은 LLaMA를 GPT-3, Gopher, Chinchilla, PaLM과 같은 비공개 언어 모델들과 비교했으며, OPT, GPT-J, GPT-Neo와 같은 오픈소스 모델들과도 성능을 비교했습니다. 또한 OPT-IML이나 Flan-PaLM과 같은 지시어 튜닝 모델들과도 간단한 비교를 수행했습니다.

평가는 자유 형식 생성 과제와 객관식 과제로 나누어 진행되었습니다. 객관식 과제에서는 주어진 맥락을 바탕으로 가장 적절한 답변을 선택하는 것이 목표입니다. 대부분의 경우 Gao와 연구진의 방법을 따라 답변의 문자 수로 정규화된 가능도를 기준으로 답변을 선택했습니다. 다만 OpenBookQA와 BoolQ와 같은 일부 데이터셋에서는 Brown과 연구진의 방법을 따라 "Answer:"를 맥락으로 했을 때의 가능도로 정규화된 가능도를 사용했습니다.

### 상식 추론 능력 평가

연구진은 BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC easy/challenge, OpenBookQA와 같은 8개의 표준 상식 추론 벤치마크에서 모델을 평가했습니다. 이러한 데이터셋들은 Cloze 스타일 과제와 Winograd 스타일 과제, 그리고 객관식 문답을 포함합니다. 언어 모델링 커뮤니티의 관행에 따라 제로샷 설정에서 평가가 이루어졌습니다.

평가 결과, LLaMA-65B는 BoolQ를 제외한 모든 벤치마크에서 Chinchilla-70B의 성능을 능가했습니다. 또한 BoolQ와 WinoGrande를 제외한 모든 벤치마크에서 PaLM-540B의 성능을 상회했습니다. 특히 주목할 만한 점은 LLaMA-13B 모델이 10배 더 큰 GPT-3를 대부분의 벤치마크에서 능가했다는 것입니다.

### 폐쇄형 질의응답 평가

Natural Questions와 TriviaQA 두 가지 폐쇄형 질의응답 벤치마크에서 LLaMA의 성능을 평가했습니다. 두 벤치마크 모두에서 정확한 일치(exact match) 성능을 측정했으며, 증거 문서에 대한 접근 없이 질문에 답변하는 폐쇄형 설정에서 평가가 이루어졌습니다.

평가 결과, LLaMA-65B는 제로샷과 퓨샷 설정 모두에서 최고 수준의 성능을 달성했습니다. 더욱 주목할 만한 점은 LLaMA-13B가 5-10배 더 큰 GPT-3와 Chinchilla와 비교했을 때도 경쟁력 있는 성능을 보여주었다는 것입니다. 이 모델은 추론 시 단일 V100 GPU에서도 구동이 가능합니다.
### 독해 능력 평가

LLaMA의 독해 능력은 RACE 벤치마크를 통해 평가되었습니다. RACE는 중국 중고등학생들을 대상으로 하는 영어 독해 시험에서 수집된 데이터셋으로, Brown과 연구진이 제시한 평가 방식을 따라 실험이 진행되었습니다. 평가 결과, LLaMA-65B는 PaLM-540B와 대등한 성능을 보여주었으며, LLaMA-13B는 GPT-3보다 몇 퍼센트 더 높은 성능을 달성했습니다.

### 수학적 추론 능력 평가

수학적 추론 능력은 MATH와 GSM8k 두 벤치마크를 통해 평가되었습니다. MATH는 LaTeX로 작성된 12K개의 중고등학교 수학 문제로 구성되어 있으며, GSM8k는 중학교 수준의 수학 문제들로 이루어져 있습니다. 연구진은 PaLM과 Minerva 모델과의 비교를 수행했는데, Minerva는 ArXiv와 수학 웹페이지에서 추출한 38.5B 토큰으로 파인튜닝된 PaLM 모델 시리즈입니다.

특히 주목할 만한 점은 LLaMA-65B가 수학 데이터로 파인튜닝되지 않았음에도 GSM8k에서 Minerva-62B의 성능을 능가했다는 것입니다. 평가는 maj1@k 방식을 사용하여 진행되었는데, 이는 각 문제에 대해 k개의 샘플을 생성하고 다수결 투표를 수행하는 Wang과 연구진의 방법입니다.

### 코드 생성 능력 평가

코드 생성 능력은 HumanEval과 MBPP 두 벤치마크를 통해 평가되었습니다. 두 과제 모두에서 모델은 프로그램에 대한 자연어 설명과 몇 가지 입출력 예시를 받습니다. HumanEval의 경우 함수 시그니처도 함께 제공되며, 프롬프트는 텍스트 설명과 테스트가 독스트링에 포함된 자연스러운 코드 형태로 구성됩니다.

평가 결과, 비슷한 규모의 모델들을 비교했을 때 LLaMA는 LaMDA나 PaLM과 같은 일반 모델들보다 우수한 성능을 보여주었습니다. 특히 13B 이상의 파라미터를 가진 LLaMA 모델들은 HumanEval과 MBPP 모두에서 137B 파라미터의 LaMDA를 능가했으며, LLaMA-65B는 PaLM-62B보다도 더 나은 성능을 달성했습니다.

코드 생성 평가에서 pass@1 결과는 0.1의 온도로 샘플링하여 얻었으며, pass@100과 pass@80 메트릭은 0.8의 온도로 측정되었습니다. Chen과 연구진의 방법을 따라 편향되지 않은 pass@k 추정치를 얻었습니다. 연구진은 코드 특화 토큰으로 파인튜닝을 통해 성능을 더욱 향상시킬 수 있다고 언급했지만, 이는 본 연구의 범위를 벗어나는 것으로 판단했습니다.
### 대규모 다중 과제 언어 이해 평가

LLaMA의 광범위한 지식 이해 능력을 평가하기 위해 Hendrycks와 연구진이 제안한 대규모 다중 과제 언어 이해(MMLU) 벤치마크에서 성능을 측정했습니다. MMLU는 인문학, STEM, 사회과학 등 다양한 지식 영역을 포괄하는 객관식 문제들로 구성되어 있습니다. 벤치마크에서 제공하는 5개의 예시를 활용한 5-샷 설정으로 평가가 진행되었습니다.

평가 결과, LLaMA-65B는 평균적으로 Chinchilla-70B와 PaLM-540B보다 몇 퍼센트 낮은 성능을 보여주었으며, 이러한 경향은 대부분의 영역에서 일관되게 나타났습니다. 연구진은 이러한 성능 차이의 원인으로 학습 데이터의 구성을 지목했습니다. LLaMA의 학습 데이터에서 ArXiv, Gutenberg, Books3와 같은 도서와 학술 논문은 총 177GB에 불과한 반면, 다른 모델들은 최대 2TB의 도서 데이터를 학습에 활용했습니다. 이러한 도서 데이터의 차이는 Gopher가 GPT-3보다 MMLU에서 더 우수한 성능을 보이는 이유이기도 합니다.

### 학습 과정에서의 성능 변화 분석

연구진은 학습 과정에서 질의응답과 상식 추론 벤치마크에서의 성능 변화를 추적했습니다. 대부분의 벤치마크에서 성능은 꾸준히 향상되었으며, 모델의 학습 퍼플렉시티와 강한 상관관계를 보였습니다. 그러나 SIQA와 WinoGrande는 예외적인 패턴을 보여주었습니다.

특히 SIQA의 경우 성능의 변동성이 매우 컸는데, 이는 해당 벤치마크의 신뢰성에 의문을 제기하게 만드는 결과입니다. WinoGrande에서는 학습 퍼플렉시티와의 상관관계가 상대적으로 약했으며, LLaMA-33B와 LLaMA-65B가 학습 과정에서 유사한 성능을 보여주었습니다.

### 지시어 파인튜닝 효과 분석

연구진은 Chung과 연구진의 프로토콜을 따라 간단한 지시어 파인튜닝 실험을 수행했습니다. 파인튜닝되지 않은 LLaMA-65B도 이미 기본적인 지시를 따를 수 있었지만, 소량의 파인튜닝만으로도 MMLU에서의 성능이 향상되었으며, 지시를 따르는 능력도 더욱 개선되었습니다.

LLaMA-I로 명명된 지시어 파인튜닝 모델은 MMLU에서 68.9%의 성능을 달성했으며, 이는 OPT-IML이나 Flan-PaLM과 같은 기존의 중규모 지시어 파인튜닝 모델들을 능가하는 결과입니다. 그러나 이는 여전히 GPT code-davinci-002의 77.4%에는 미치지 못하는 수준입니다.

### 편향성, 유해성 및 허위정보 평가

대규모 언어 모델은 학습 데이터에 존재하는 편향을 재생산하고 증폭시키는 경향이 있으며, 유해하거나 공격적인 내용을 생성할 수 있다는 우려가 있습니다. LLaMA의 학습 데이터셋이 웹에서 수집된 데이터를 상당 부분 포함하고 있다는 점을 고려할 때, 이러한 잠재적 위험성을 평가하는 것은 매우 중요합니다.

연구진은 RealToxicityPrompts 벤치마크를 통해 LLaMA의 유해 콘텐츠 생성 가능성을 평가했습니다. 이 벤치마크는 약 10만 개의 프롬프트로 구성되어 있으며, 모델이 생성한 텍스트의 유해성을 PerspectiveAPI를 통해 자동으로 평가합니다. 평가는 기본 프롬프트와 "다음 문장을 공손하고, 존중하며, 편향되지 않은 방식으로 완성하시오"라는 지시문이 포함된 "존중" 프롬프트 두 가지 방식으로 진행되었습니다.

평가 결과, 모델의 크기가 커질수록 유해성 점수가 증가하는 경향을 보였으며, 특히 "존중" 프롬프트에서 이러한 경향이 더욱 두드러졌습니다. 이는 Zhang과 연구진의 이전 연구에서도 관찰된 현상입니다. 다만 Hoffmann과 연구진의 연구에서는 Chinchilla와 Gopher 사이에서 이러한 차이가 관찰되지 않았는데, 이는 더 큰 모델인 Gopher가 Chinchilla보다 성능이 낮았기 때문일 수 있습니다. 이는 유해성과 모델 크기의 관계가 동일한 모델 계열 내에서만 적용될 수 있음을 시사합니다.

CrowS-Pairs 벤치마크를 통해 모델의 편향성도 평가했습니다. 이 데이터셋은 성별, 종교, 인종/피부색, 성적 지향, 연령, 국적, 장애, 외모, 사회경제적 지위 등 9개 범주에서의 편향을 측정합니다. 각 예시는 고정관념을 반영하는 문장과 그에 대응하는 반고정관념 문장으로 구성되어 있으며, 제로샷 설정에서 두 문장의 퍼플렉시티를 비교하여 모델의 편향성을 측정합니다.

GPT-3와 OPT-175B와 비교했을 때, LLaMA는 평균적으로 약간 더 나은 성능을 보여주었습니다. 그러나 종교 범주에서는 OPT-175B보다 10% 더 높은 편향성을 보였으며, 연령과 성별 범주에서도 상당한 편향성이 관찰되었습니다. 이러한 편향성은 여러 필터링 단계를 거쳤음에도 CommonCrawl 데이터에서 기인한 것으로 추정됩니다.
### 성별 편향성 심층 분석

WinoGender 벤치마크를 통해 LLaMA의 성별 관련 편향성을 더욱 심층적으로 분석했습니다. WinoGender는 공동참조 해결 데이터셋으로, 각 문장은 "직업", "참여자", "대명사" 세 가지 요소를 포함하며, 대명사가 직업이나 참여자 중 어느 것을 지칭하는지 판단하는 과제입니다. 예를 들어 "간호사가 환자에게 자신의 근무 시간이 한 시간 후에 끝난다고 알렸다"라는 문장에서 "자신의"가 간호사를 지칭하는지 환자를 지칭하는지를 판단해야 합니다.

평가는 "her/her/she", "his/him/he", "their/them/someone"과 같은 세 가지 유형의 대명사에 대해 수행되었습니다. LLaMA-65B는 성 중립적인 "their/them/someone" 대명사에서 가장 높은 성능(81.7%)을 보였으며, "her/her/she"(78.8%)와 "his/him/he"(72.1%) 대명사에서는 상대적으로 낮은 성능을 보였습니다. 이러한 성능 차이는 Rae와 연구진, Hoffmann과 연구진의 이전 연구에서도 관찰된 현상으로, 모델이 성별 편향을 학습했음을 시사합니다.

특히 주목할 만한 점은 "gotcha" 케이스에서의 성능입니다. 이는 대명사의 성별이 해당 직업의 다수 성별과 일치하지 않으면서 직업이 정답인 경우를 의미합니다. LLaMA-65B는 이러한 케이스에서 "her/her/she" 대명사의 경우 75.0%, "his/him/he" 대명사의 경우 63.3%의 정확도를 보여, 일반적인 경우보다 현저히 낮은 성능을 보였습니다. 이는 모델이 직업과 성별 사이의 사회적 고정관념을 학습했음을 명확하게 보여줍니다.

### 진실성 평가

TruthfulQA 벤치마크를 통해 LLaMA의 진실성, 즉 실제 세계에 대한 사실적 주장을 식별하는 능력을 평가했습니다. 이 벤치마크는 신념 체계나 전통의 맥락에서만 참인 주장이 아닌, 실제 세계에 대한 문자 그대로의 진실을 다룹니다. 문제들은 다양한 스타일로 작성되었으며, 38개 카테고리를 포함하고 있습니다.

평가 결과, LLaMA는 GPT-3보다 높은 점수를 기록했습니다. 구체적으로 LLaMA-65B는 진실성 점수에서 0.57, 진실성과 정보성을 동시에 고려한 점수에서 0.53을 기록했습니다. 그러나 이는 여전히 낮은 수준의 정답률로, 모델이 잘못된 답변을 생성할 가능성이 상당히 높다는 것을 보여줍니다. 이러한 결과는 대규모 언어 모델이 허위 정보를 생성할 수 있는 위험성을 명확하게 보여주며, 이는 실제 응용에서 중요하게 고려해야 할 사항입니다.

### 탄소 발자국 분석

LLaMA 모델의 학습 과정에서 발생한 에너지 소비와 탄소 배출량을 체계적으로 분석했습니다. Wu와 연구진이 제안한 방법론을 따라 전력 소비량과 탄소 배출량을 계산했습니다. 전력 소비량은 다음과 같은 수식으로 산출됩니다.

\\[ \textrm{Wh} = \textrm{GPU-h} \times (\textrm{GPU power consumption}) \times \textrm{PUE} \\]

여기서 PUE(Power Usage Effectiveness)는 데이터 센터의 전력 사용 효율성을 나타내는 지표로, 1.1로 설정되었습니다.

탄소 배출량의 공정한 비교를 위해 데이터 센터의 위치에 따른 차이를 배제하고, 미국 전력망의 평균 탄소 집약도인 0.385 kg CO₂eq/KWh를 기준으로 다음 수식을 적용했습니다.

\\[ \textrm{tCO}_2\textrm{eq} = \textrm{MWh} \times 0.385 \\]

실제 학습에 사용된 컴퓨팅 자원을 살펴보면, OPT의 경우 992대의 A100-80GB GPU를 34일 동안 사용했으며, LLaMA 모델 시리즈 개발에는 2048대의 A100-80GB GPU를 약 5개월 동안 활용했습니다. 이를 기준으로 계산했을 때, LLaMA 모델 시리즈의 개발 과정에서 약 2,638 MWh의 전력이 소비되었으며, 이는 총 1,015 톤의 이산화탄소 배출량에 해당합니다.

연구진은 이러한 환경적 영향을 인정하면서도, 개발된 모델을 공개함으로써 향후 유사한 모델의 중복 학습을 방지하고, 특히 작은 규모의 모델들이 단일 GPU에서도 구동 가능하다는 점을 강조했습니다. 이는 결과적으로 미래의 탄소 배출량 감소에 기여할 수 있음을 시사합니다.

이러한 분석은 대규모 AI 모델 개발이 환경에 미치는 영향을 투명하게 공개하고, 향후 더욱 지속 가능한 AI 연구 방향을 모색하는 데 중요한 기초 자료가 될 것입니다.

### 언어 모델의 역사적 발전

언어 모델은 단어, 토큰 또는 문자의 시퀀스에 대한 확률 분포를 모델링하는 것을 목표로 합니다. Shannon이 1948년과 1951년에 제시한 이 개념은 자연어 처리 분야의 핵심 과제로 자리잡았으며, 특히 다음 토큰을 예측하는 과제로 구체화되었습니다. Turing이 1950년에 제안한 "모방 게임"을 통한 기계 지능 측정 방법론은 언어 모델링을 인공지능 발전의 중요한 지표로 확립하는 계기가 되었습니다.

### 아키텍처의 진화

언어 모델의 아키텍처는 시간에 따라 크게 발전해왔습니다. 초기에는 n-gram 통계를 기반으로 하는 모델이 주를 이루었으며, 희소한 이벤트의 추정을 개선하기 위해 다양한 스무딩 기법이 제안되었습니다. 지난 20년 동안 신경망이 언어 모델링 분야에 성공적으로 적용되기 시작했는데, 이는 피드포워드 모델에서 시작하여 순환 신경망과 LSTM으로 발전했습니다.

최근에는 셀프 어텐션을 기반으로 하는 트랜스포머 네트워크가 중요한 발전을 이루었습니다. 특히 트랜스포머는 장거리 의존성을 포착하는 데 탁월한 성능을 보여주었습니다.

### 스케일링의 역사

언어 모델의 스케일링은 모델 크기와 데이터셋 크기 모두에서 꾸준한 발전을 이루어왔습니다. 2007년 Brants와 연구진은 2조 토큰으로 학습된 언어 모델이 기계 번역의 품질을 향상시킬 수 있음을 보여주었습니다. 이 연구는 'Stupid Backoff'라는 간단한 스무딩 기법을 사용했지만, 이후 Heafield와 연구진은 Kneser-Ney 스무딩을 웹 규모의 데이터에 적용하는 방법을 개발했습니다.

신경망 기반 언어 모델의 맥락에서는, Jozefowicz와 연구진이 10억 개의 파라미터를 가진 LSTM을 스케일링하여 One Billion Word 벤치마크에서 최고 성능을 달성했습니다. 이후 트랜스포머의 스케일링은 BERT, GPT-2, Megatron-LM, T5와 같은 주목할 만한 모델들을 통해 많은 자연어 처리 과제에서 성능 향상을 이끌어냈습니다.

GPT-3의 등장은 1,750억 개의 파라미터를 가진 모델로 중요한 돌파구를 마련했으며, 이는 Jurassic-1, Megatron-Turing NLG, Gopher, Chinchilla, PaLM, OPT, GLM과 같은 대규모 언어 모델의 시대를 열었습니다.

### 스케일링 법칙 연구

스케일링이 딥러닝 모델의 성능에 미치는 영향에 대한 체계적인 연구도 진행되었습니다. Hestness와 연구진, Rosenfeld와 연구진은 모델 크기, 데이터셋 크기, 시스템 성능 사이에 멱법칙 관계가 존재함을 보여주었습니다. Kaplan과 연구진은 트랜스포머 기반 언어 모델에 특화된 멱법칙을 도출했으며, 이는 나중에 Hoffmann과 연구진에 의해 데이터셋 스케일링에 따른 학습률 스케줄 조정을 통해 더욱 정교화되었습니다. Wei와 연구진은 스케일링이 대규모 언어 모델의 능력에 미치는 영향을 연구했습니다.

### 결론과 연구의 의의

본 연구는 공개적으로 접근 가능한 데이터만을 활용하여 최첨단 성능을 달성한 언어 모델 시리즈를 소개했습니다. 특히 주목할 만한 성과는 LLaMA-13B가 GPT-3보다 10배 이상 작은 규모임에도 더 우수한 성능을 보여주었다는 점이며, LLaMA-65B는 Chinchilla-70B와 PaLM-540B와 같은 최신 대규모 모델들과 견줄 만한 성능을 달성했다는 것입니다.

이전 연구들과 달리, 본 연구는 비공개 데이터셋에 의존하지 않고도 최고 수준의 성능을 달성할 수 있다는 것을 입증했습니다. 이는 언어 모델 연구의 투명성과 재현성 향상에 중요한 이정표가 될 것입니다. 연구진은 이러한 모델들을 연구 커뮤니티에 공개함으로써 대규모 언어 모델의 발전을 가속화하고, 유해성과 편향성과 같은 알려진 문제점들을 개선하기 위한 노력을 지원하고자 합니다.

Chung과 연구진의 연구에서 관찰된 것처럼, 이러한 모델들을 지시어로 파인튜닝했을 때 매우 유망한 결과를 얻을 수 있었습니다. 연구진은 향후 연구에서 이 부분을 더욱 깊이 있게 탐구할 계획입니다. 또한 모델의 규모를 키우면서 지속적인 성능 향상이 관찰되었기 때문에, 더 큰 사전 학습 데이터셋으로 학습된 더 큰 규모의 모델을 향후 공개할 계획입니다.

이러한 연구 성과는 대규모 언어 모델 분야에서 중요한 전환점이 될 것으로 기대됩니다. 특히 공개 데이터만으로도 최고 수준의 성능을 달성할 수 있다는 점은, 향후 언어 모델 연구의 민주화와 발전 방향에 큰 영향을 미칠 것입니다.

- - -
### References
* [LLaMA: Open and Efficient Foundation Language Models](http://arxiv.org/pdf/2302.13971v1)